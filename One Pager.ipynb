{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python files that contains 'sklearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigquery_python_framework.GithubPython import GithubPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43629 files containing 'sklearn'\n"
     ]
    }
   ],
   "source": [
    "sklearnFile = GithubPython().uniqueFiles().contains('sklearn').excludeByRepoName('sklearn').getCount().run()[0][0]\n",
    "print(\"There are {} files containing 'sklearn'\".format(sklearnFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalFile = GithubPython().uniqueFiles().getCount().run()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5995653 files in total\n"
     ]
    }
   ],
   "source": [
    "print('There are {} files in total'.format(totalFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 0.727677%\n"
     ]
    }
   ],
   "source": [
    "print('Percentage: %f%%'%(sklearnFile/totalFile*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules that import most 'sklearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moduleImportMostSklearn = GithubPython().module_with_most_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seckcoder/lang-learn', 121),\n",
       " ('magic2du/contact_matrix', 96),\n",
       " ('jpzk/evopy', 87),\n",
       " ('GbalsaC/bitnamiP', 71),\n",
       " ('loli/sklearn-ensembletrees', 61),\n",
       " ('chaluemwut/fbserver', 57),\n",
       " ('zooniverse/aggregation', 51),\n",
       " ('B3AU/waveTree', 49),\n",
       " ('valexandersaulys/airbnb_kaggle_contest', 47),\n",
       " ('kedz/cuttsum', 47),\n",
       " ('Tjorriemorrie/trading', 47),\n",
       " ('southpaw94/MachineLearning', 47),\n",
       " ('akhilpm/Masters-Project', 46),\n",
       " ('salma1601/nilearn', 45),\n",
       " ('diogo149/CauseEffectPairsPaper', 42),\n",
       " ('NicovincX2/Python-3.5', 38),\n",
       " ('chemelnucfin/tensorflow', 37),\n",
       " ('abenicho/isvr', 37),\n",
       " ('ainafp/nilearn', 37),\n",
       " ('weissercn/MLTools', 37)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moduleImportMostSklearn[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printContext(fileName = 'RFC.csv', num = 10):\n",
    "    import csv\n",
    "    with open('context/{}'.format(fileName),'r') as f:\n",
    "        spamreader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        i = 0\n",
    "        for row in spamreader:\n",
    "            if i < 2:\n",
    "                i += 1\n",
    "                continue\n",
    "            if i >= num + 2:\n",
    "                break\n",
    "            print(row[0])\n",
    "            print('\\n------------------------------Separator------------------------------\\n')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "\n",
      "# classification models\n",
      "classifiers = {'K-Nearest Neighbors (Braycurtis norm)':\n",
      "               KNeighborsClassifier(n_neighbors=3, algorithm='auto',\n",
      "                                    metric='braycurtis'),\n",
      "               'Random Forest':\n",
      "               RandomForestClassifier(n_estimators=80, n_jobs=1),\n",
      "               'SVM': SVC(gamma=2, C=1),\n",
      "               'Linear Support Vector Machine': SVC(kernel=\"linear\", C=0.025),\n",
      "               'Decision Tree': DecisionTreeClassifier(max_depth=5),\n",
      "               'Ada Boost': AdaBoostClassifier(n_estimators=80,\n",
      "                                               learning_rate=0.4),\n",
      "               'Naive Bayes': GaussianNB(),\n",
      "               }\n",
      "vc = VotingClassifier(estimators=list(classifiers.items()), voting='hard')\n",
      "\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "classifers_dict = {'AdaBoost': AdaBoostClassifier(n_estimators=50,\n",
      "                                                  learning_rate=1),\n",
      "                    'Random Forest': RandomForestClassifier(n_estimators=10,\n",
      "                                                            # max_depth=None,\n",
      "                                                            min_samples_split=50,\n",
      "                                                            random_state=0),\n",
      "                    'Naive Bayes': GaussianNB(),\n",
      "                    'SVM': SVC(C=1.5, kernel='poly', degree=2, gamma=0.),\n",
      "                    'Tree': DecisionTreeClassifier(),\n",
      "                    'KNN': KNeighborsClassifier(n_neighbors=22)\n",
      "                    }\n",
      "                    \n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "    # train_data = train_data[train_data['Credit_History'] == 1.0]\n",
      "    # print train_data['Month_Pay'].value_counts()\n",
      "\n",
      "    baseline_prediction = train_data.apply(classify, axis=1)\n",
      "    baseline_score = metrics.accuracy_score(baseline_prediction, train_data['Loan_Status'])\n",
      "    print \"Baseline: {:.3f}%\".format(baseline_score*100)\n",
      "    print confusion_matrix(train_data['Loan_Status'], baseline_prediction)\n",
      "\n",
      "    # model = LogisticRegression(C=1, class_weight='balanced')\n",
      "    # model = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5)\n",
      "    # model = RandomForestClassifier(n_estimators=100)\n",
      "    # model = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(10, 3), random_state=1)\n",
      "    # model = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)\n",
      "    # model = SVC(C= 1.0, kernel= 'rbf', class_weight={'Y': 4, 'N':1})\n",
      "    # model = SGDClassifier(class_weight='balanced')\n",
      "    # model = GaussianNB()\n",
      "\n",
      "    # params = {'n_estimators': 100, 'max_depth': 5, 'subsample': 0.5,\n",
      "    #       'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n",
      "    # model = ensemble.GradientBoostingClassifier(**params)\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import precision_score\n",
      "from sklearn.metrics import recall_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import numpy as np\n",
      "\n",
      "from skml.problem_transformation import LabelPowerset\n",
      "from skml.datasets import load_dataset\n",
      "\n",
      "X, y = load_dataset('yeast')\n",
      "clf = LabelPowerset(RandomForestClassifier())\n",
      "clf.fit(X, np.array(y))\n",
      "y_pred = clf.predict(X)\n",
      "\n",
      "print(\"real: \", y.shape)\n",
      "print(\"y_pred: \", y_pred.shape)\n",
      "\n",
      "print(\"hamming loss: \")\n",
      "print(hamming_loss(y, y_pred))\n",
      "\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "df = pd.read_csv(\"parsed.csv\")\n",
      "y1 = df[\"admission_type_id\"].values\n",
      "y2 = df[\"discharge_disposition_id\"].values\n",
      "columns = list(df)[1:4] + list(df)[8:49]\n",
      "print columns\n",
      "X = df[columns].values\n",
      "      \n",
      "X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size = 0.20)\n",
      "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y2, test_size = 0.20)\n",
      "\n",
      "clf1 = RandomForestClassifier()\n",
      "clf2 = RandomForestClassifier()\n",
      "\n",
      "clf1.fit(X1_train, y1_train)\n",
      "clf2.fit(X2_train, y2_train)\n",
      "y1_pred = clf1.predict(X1_test)\n",
      "y2_pred = clf2.predict(X2_test)\n",
      "\n",
      "acc1 = accuracy_score(y1_test, y1_pred)\n",
      "acc2 = accuracy_score(y2_test, y2_pred)\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "    count_enrollment = df_sub['3COURSEID'].value_counts()\n",
      "    #print \"Number of %s enrollment: %s\"%(subject,count_enrollment)\n",
      "\n",
      "    A = df_sub.as_matrix()\n",
      "    X = A[:,4:]\n",
      "    X = X.astype(np.int64, copy=False)\n",
      "    y = A[:,2]\n",
      "    y = y.astype(np.int64, copy=False)\n",
      "\n",
      "    #Training data\n",
      "    forest = RandomForestClassifier(n_estimators=10, max_depth=None, \n",
      "            min_samples_split=1, random_state=None, max_features=None)\n",
      "    clf = forest.fit(X, y)\n",
      "    scores = cross_val_score(clf, X, y, cv=5)\n",
      "    print scores\n",
      "    print \"Random Forest Cross Validation of %s: %s\"%(subject,scores.mean())\n",
      "    precision_rf[subject] = scores.mean()\n",
      "    df_precision.loc[subject]=precision_rf[subject]\n",
      "    print \"-----------------------------------\"\n",
      "    \n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "\n",
      "    plt.xlabel('Relative Importance')\n",
      "    plt.title('%s: Top Features' %(dataName))\n",
      "    plt.grid('off')\n",
      "    plt.ion()\n",
      "    plt.show()\n",
      "    plt.savefig(str(dataName)+'TopFeatures.png',dpi=200)\n",
      "\n",
      "def altPlotFeaturesImportance(X,y,featureNames,dataName):\n",
      "    \"http://nbviewer.ipython.org/github/cs109/2014/blob/master/homework-solutions/HW5-solutions.ipynb\"\n",
      "    clf = RandomForestClassifier(n_estimators=50)\n",
      "\n",
      "    clf.fit(X,y)\n",
      "    importance_list = clf.feature_importances_\n",
      "    # name_list = df.columns #ORIG\n",
      "    name_list=featureNames\n",
      "\n",
      "    importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
      "    plt.barh(range(len(name_list)),importance_list,align='center')\n",
      "    plt.yticks(range(len(name_list)),name_list)\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "# strings.\n",
      "train_data_features = vectorizer.fit_transform(train_data).toarray()\n",
      "\n",
      "print(train_data_features.shape)\n",
      "\n",
      "\n",
      "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
      "\n",
      "# Initialize a Random Forest classifier with trees\n",
      "# use as many CPU cores as possible (n_jobs=-1)\n",
      "forest = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, verbose=2) \n",
      "\n",
      "# Fit the forest to the training set, using the bag of words as \n",
      "# features and the sentiment labels as the response variable\n",
      "#\n",
      "# This may take a few minutes to run\n",
      "forest = forest.fit(train_data_features, train[\"sponsored\"])\n",
      "\n",
      "\n",
      "#Run the prediction...\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "    y2 += [dep(test)]\n",
      "  return x1,y1,x2,y2\n",
      "\n",
      "def learns(tests,trains,indep=lambda x: x[:-1],\n",
      "                    dep = lambda x: x[-1],\n",
      "                    rf  = Abcd(),\n",
      "                    lg  = Abcd(),\n",
      "                    dt  = Abcd(),\n",
      "                    nb  = Abcd()):\n",
      "  x1,y1,x2,y2= trainTest(tests,trains,indep,dep) \n",
      "  forest = RandomForestClassifier(n_estimators = 50)  \n",
      "  forest = forest.fit(x1,y1)\n",
      "  for n,got in enumerate(forest.predict(x2)):\n",
      "    rf(predicted = got, actual = y2[n])\n",
      "  logreg = linear_model.LogisticRegression(C=1e5)\n",
      "  logreg.fit(x1, y1)\n",
      "  for n,got in enumerate(logreg.predict(x2)):\n",
      "    lg(predicted = got, actual = y2[n])\n",
      "  bayes =  GaussianNB()\n",
      "  bayes.fit(x1,y1)\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n",
      "    :param selectKBest: The number of best features to select\n",
      "    :type selectKBest: int\n",
      "    :param kfold: The number of folds to use in K-fold CV\n",
      "    :type kfold: int\n",
      "    :return: A list of predicted labels across the k-folds\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Prepare data\n",
      "        X, y = numpy.array(X), numpy.array(y)\n",
      "        # Define classifier\n",
      "        clf = ensemble.RandomForestClassifier(n_estimators=estimators, criterion=criterion, max_depth=maxdepth)\n",
      "        if selectKBest > 0:\n",
      "            X_new = SelectKBest(chi2, k=selectKBest).fit_transform(X, y)\n",
      "            predicted = cross_val_predict(clf, X_new, y, cv=kfold).tolist()\n",
      "        else:\n",
      "            predicted = cross_val_predict(clf, X, y, cv=kfold).tolist()\n",
      "    except Exception as e:\n",
      "        prettyPrintError(e)\n",
      "        return []\n",
      "\n",
      "\n",
      "------------------------------Separator------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printContext()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
