match,path,repo_name,count
,,,5982056
"    plt.yticks(tick_marks, target_names, fontsize = 15,  rotation=45)
    plt.tight_layout()
    plt.ylabel('True label', fontsize = 20)
    plt.xlabel('Predicted label', fontsize = 20)
    plt.savefig('figures//' + clfname + '_' + title)

""""""
Train all kinds of classifiers
""""""
def AllClassifer(Xtrain, ytrain):
    clf = svm.SVC(kernel = 'linear', C=1e6)
    clf.fit(Xtrain, ytrain)
    print ""linear SVM score: "", clf.score(Xtrain, ytrain)

    clf = svm.SVC(kernel = 'rbf', C=1e6)
    clf.fit(Xtrain, ytrain)
    print ""rbf SVM score: "", clf.score(Xtrain, ytrain)

    clf = svm.SVC(kernel = 'sigmoid', C=1e6)
    clf.fit(Xtrain, ytrain)",code only split/InstrumentRecognition/InstrumentRecognition.py,tian-zhou/Surgical-Instrument-Dataset,1
"    #For now we are just looking that their mean cross-validation accuracy. We can easily look into more
    #informative metrics like F1-score, confusing metrics etc. But, cv-accuracy should be enough, I think!
    #Here, I have commented some to save some time. You can try adding more and un-commmenting these codes.
    #NOTE: You might need to import the models.
    classifiers = [(""Multinomial Naive Bayes Classifier"", MultinomialNB(alpha=.01)),\
                   #(""Bagging Classifier (Decision Tree)"", BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=seed)),\
                   (""SVM"", SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\
                   (""Random Forest Classifier"", RandomForestClassifier(n_estimators=100, max_features=10)),\
                   (""Ridge Classifier"", RidgeClassifier(tol=1e-2, solver=""sag"")),\
                   (""Logistic Regression"", LogisticRegression(solver=""sag"", multi_class=""multinomial"")),\
                   (""Linear SVC"", LinearSVC(random_state=seed)),\
                   (""Perceptron"", Perceptron(random_state=seed))]


    #MAJORITY VOTING CLASSIFIER
    #This model is our ""ultimate"" model ;) This looks into predictions from all the classifiers
    #and outputs the majority class.
    #NOTICE ""estimators=classifiers[:]"", we should be careful here. ""classifiers[:]"" is used to aviod
    #the infinite recursion. This does the hard copy of lists. In python, copy of list is ""reference copy""
    #by default.",analysis/ghost_model_picking.py,mab1290/GHOST,1
"from sklearn.cross_validation import cross_val_score , KFold
from sklearn.feature_extraction.text import TfidfVectorizer

def get_stop_words():
    result = set()
    for line in open('stopwords_id.txt','rU').readline():
        result.add(line.strip())
    return result
    
stopwords = get_stop_words()  
clf_svc = SVC()  
features_1 = []
features_2 = []
features_3 = []
features_4 = []
features_5 = []
features_6 = []
features_7 = []
all_features = []
label = []",processing_builder_svm_classifier.py,satyanugraha/classifying-twitter-user-as-resident-or-tourist,1
"    :return: highest score, a list of highest paras set
    """"""
    param_distribs = {
        'estimator__kernel':[ 'linear', 'poly', 'rbf', 'sigmoid'],
        'estimator__C' : list(np.linspace(0.01,1,20)),
        ""estimator__degree"": [1, 2, 3, 4],
        ""estimator__class_weight"":['balanced'],
        ""estimator__probability"":[True]
    }

    ovsr = OneVsRestClassifier(SVC(),n_jobs=-1)

    def score_func(y, y_pred, **kwargs):
        return recall_score(y,y_pred,average='macro')

    rnd_search = RandomizedSearchCV(ovsr, param_distribs, n_iter=num_iter, cv=5,scoring=make_scorer(score_func))
    rnd_search.fit(x_train, y_train)

    cvres = rnd_search.cv_results_
    scores_list = []",model_ppl.py,zacwentzell/Group2,1
"

#########################################################
### your code goes here ###
from sklearn import svm
from sklearn.metrics import accuracy_score

#features_train = features_train[:len(features_train)/100] 
#labels_train = labels_train[:len(labels_train)/100] 

clf=svm.SVC(C=10000.,kernel='linear')

'''
for C in [10.,100.,1000.,10000.]:
	clf=svm.SVC(C=C,kernel='rbf')
	clf.fit(features_train,labels_train)
	pred=clf.predict(features_test)
	accuracy=accuracy_score(pred,labels_test)
	print accuracy,C
'''",Intro to Machine Learning/svm/svm_author_id.py,myselfHimanshu/Udacity-DataML,1
"
# if you're running this in a jupyter notebook, print out the graphs
NOTEBOOK = 1

def define_clfs_params(grid_size):

    clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) 
            }

    large_grid = { 
    'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},",Pipeline/magicloops.py,anisfeld/MachineLearning,1
"clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.2303

clf = neighbors.KNeighborsClassifier(15, weights='distance')
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.13869999

from sklearn import svm
clf = svm.SVC()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.362300000000000001

clf = svm.SVC(kernel='linear', C=1)
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.3110999
",experiment_logs/first_submission_logs.py,deepanjanroy/aml3,1
"        pass

    def dump(self, f):
        pickle.dump(self.clf, f)

    def load(self, f):
        self.clf = pickle.load(f)

    def learn(self, X, Y):
        if not hasattr(self, 'clf'):
            self.clf = svm.SVC()
        self.clf.fit(X, Y)

    def predict(self, X):
        return self.clf.predict(X)


class BoardParser:
    def __init__(self):
        pass",learn_board.py,shurain/bbb,1
"    return clas.ada(args)

  elif(args[0] == ""GaussianNB""):
    return clas.gaussianNB(args)
  elif(args[0] == ""BernoulliNB""):
    return clas.bernoulliNB(args)
  elif(args[0] == ""MultinomialNB""):
    return clas.multinomialNB(args)

  elif(args[0] == ""NuSVC""):
    return clas.SVC(args)
  elif(args[0] == ""SVC""):
    return clas.SVC(args)

  elif(args[0] == ""KNeighborsClassifier""):
    return clas.kNeighboors(args)
  elif(args[0] == ""RadiusNeighborsClassifier""):
    return clas.radiusNeighboors(args)
  elif(args[0] == ""CentroidClassifier""):
    return clas.ncentroid(args)",recipe/load_method.py,RecipeML/Recipe,1
"    '''
    Prints the number list
    '''
    print(""The numbers are:%s"" % numbers2str(values))

def predictViaSVM(X, y, X_test):
    '''
    Uses the SVM in sklearn package to predict the numbers. Like logistic regression, it works
    well.
    '''
    model = LinearSVC()
    model.fit(X, y)
    values = model.predict(X_test)
    return values

def predictViaKNN(X, y, X_test):
    '''
    kNN is a bad choice to predict the numbers, it can not distingusih some number well. For example,
    8 and 0
    '''",tvish.py,wul/tvish,1
"        return TrivialClassifier(Y[0])

    # Search space
    C_range     = 10.0 ** np.arange( -5, 9 )
    gamma_range = 10.0 ** np.arange( -5, 9 )

    # Grid search?
    if do_grid:
        print '\t\tperforming grid search'

        estimates = LinearSVC()
        parameters = [ {'C':C_range } ]

        # Find best classifier
        clf = GridSearchCV(estimates, parameters, score_func = f1_score)
        clf.fit(X, Y)

    else:
        clf = LinearSVC()
        clf.fit(X, Y)",cliner/machine_learning/sci.py,text-machine-lab/CliNER,1
"

class ErrorGenBadHost(ErrorGenBase):
    CLASS = SCMPClass.ROUTING
    TYPE = SCMPRoutingClass.BAD_HOST
    DESC = ""bad host""

    def _build_pkt(self):
        pkt = super()._build_pkt()
        pkt.set_payload(IFIDPayload.from_values(77))
        pkt.addrs.dst.host = HostAddrSVC(99, raw=False)
        return pkt


class ErrorGenBadPktLenShort(ErrorGenBase):
    CLASS = SCMPClass.CMNHDR
    TYPE = SCMPCmnHdrClass.BAD_PKT_LEN
    DESC = ""bad pkt length (data missing)""

    def _send_pkt(self, spkt):",test/integration/scmp_error_test.py,FR4NK-W/osourced-scion,1
"# target is just last column
target = data_train[:,-1]

# data is all but last column
train = data_train[:,:-1]


print('Train Data : ' , train.shape)
print('Target Data : ' , target.shape)

clf = SVC(gamma=0.001, C=100)
clf.fit(train, target)
#print('clf = ', clf)

test = data_test[:,:-1]
print('Test Data : ' , test.shape)

output = clf.predict(test)
#print('output = ', output)
",Python/Basic/test_svm.py,wahibhaq/android-speaker-audioanalysis,1
"        else:
            model.fit_transform(X, y)

        if store:
            joblib.dump(model, '{}/{}.pkl'.format(MODEL_FOLDER, name))
        return (model, model.get_support(True))


    def linear_svc(self, X, y, C=0.02, penalty='l1', dual=False, store=True):
        if store:
            lsvc = LinearSVC(C=C, penalty=penalty, dual=dual).fit(X, y)
            joblib.dump(lsvc, '{}/{}.pkl'.format(MODEL_FOLDER, 'linearSVC_1'))
        else:
            lsvc = joblib.load('{}/{}.pkl'.format(MODEL_FOLDER, 'linearSVC_1'))
        model = SelectFromModel(lsvc, prefit=True)
        return model.get_support(True)



def main():",immo/scikit/feature_analysis.py,bhzunami/Immo,1
"trainX = mnist.data[trindex]
testX = mnist.data[tsindex]
trainY = mnist.target[trindex]
testY = mnist.target[tsindex]
#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
#for train_index, test_index in sss:
#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


clf = svm.SVC(kernel=arc_cosine)
#clf = svm.SVC(kernel = 'rbf', C=2.8, gamma=.0073) #gaussian kernel is used
clf.fit(trainX, trainY)

pred = clf.predict(testX)
print accuracy_score(testY, pred)
print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",arc_cosine/arc_cosine.py,akhilpm/Masters-Project,1
"    X_train, X_test, y_train, y_test = train_test_split(
        scaled_X, y, test_size=0.2, random_state=rand_state)

    print('X_train',X_train.shape)
    print('y_train',y_train.shape)
    print('X_test',X_test.shape)
    print('y_test',y_test.shape)
    print('Feature vector length:', len(X_train[0]))

    # Use a linear SVC (support vector classifier)
    svc = LinearSVC()
    # Train the SVC
    print('Training...')
    svc.fit(X_train, y_train)

    print('Test Accuracy of SVC = ', svc.score(X_test, y_test))
    print('My SVC predicts: ', svc.predict(X_test))
    print('For labels: ', y_test)

# image=cv2.imread('datasets/object-dataset/1478019952686311006.jpg')",train-hog.py,karolmajek/VehicleTracking,1
"
    print len(raw_train_data), len(raw_train_data[""CCA_f1""])
    print len(raw_test_data), len(raw_test_data[""CCA_f1""])
    print len(train_labels)
    print len(test_labels)

    if multiclass:
        # model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=1000, max_samples=0.2, max_features=1.0)
        # model = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1, base_estimator=DecisionTreeClassifier(max_depth=1))
        model = RandomForestClassifier(n_estimators=1000, max_depth=10)
        # model = LinearSVC()
        svm = LinearSVM(C=1)
        # svm = SGDClassifier(shuffle=True, loss=""hinge"")
        # svm = LogisticRegression()

        svm.classes_ = [1, 2, 3]

        pre_model = BernoulliRBM(learning_rate=0.1, n_components=5, n_iter=20)
        # pipeline = MyPipeline(steps=[(""rbm"", pre_model), (""svm"", svm)])
        # model = pipeline",src/main.py,kahvel/MAProject,1
"# Requires sci-kit learn and matplotlib

import matplotlib.pyplot as pyplot

from sklearn import datasets
from sklearn import svm

digits = datasets.load_digits()

#Gamma breaks when greater than 0.01. Maintains high accuracy at 0.001
clf = svm.SVC(gamma=0.001, C=100)

x,y = digits.data[:-1], digits.target[:-1]
clf.fit(x,y)

#Will return a prediction and display the last digit in dataset
print('Prediction:',clf.predict(digits.data[-1]))

pyplot.imshow(digits.images[-1], cmap=pyplot.cm.gray_r, interpolation=""nearest"")
pyplot.show()",num-prediction.py,FrizzBolt/machine-learning-sandbox,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def processFrame(self, dataURL, identity):
        head = ""data:image/jpeg;base64,""
        assert(dataURL.startswith(head))
        imgdata = base64.b64decode(dataURL[len(head):])
        imgF = StringIO.StringIO()
        imgF.write(imgdata)
        imgF.seek(0)
        img = Image.open(imgF)",websocket-server.py,guitarmind/face-learner,1
"        
        (X,y) = joblib.load('trainTest.bin')
        sum_accuracy,sum2_accuracy = 0.,0.
        for train_idx,test_idx in KFold(len(y), n_folds=n_folds, shuffle=True):
            pca = PCA()
            X_train,y_train = X[train_idx],y[train_idx]
            X_test,y_test = X[test_idx],y[test_idx]
            X_train = pca.fit_transform(X_train)
            X_test = pca.transform(X_test)
            clf = RandomForestClassifier(n_estimators=500)
            #clf = LinearSVC(C=0.05)
            clf = clf.fit(X_train, y_train)
            
            y_pred = clf.predict(X_test)    
            accuracy = np.mean(y_pred==y_test)
            sum_accuracy += accuracy
            sum2_accuracy += accuracy**2
        accuracy = sum_accuracy / float(n_folds)
        sum2_accuracy /= float(n_folds)
        std_dev = np.sqrt(sum2_accuracy-accuracy**2)",train.py,gabrielhuang/OhSnap,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix(y)
    n_samples, n_features = X.shape

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight = RFS.rfs(X[train, :], Y[train, :], gamma=0.1)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeature/example/test_RFS.py,jundongl/scikit-feature,1
"for i in range(y_base.shape[1]) :
    print i
    
    y = y_base[:, i]
    if len(np.unique(y)) == 2 : 
        rf = RandomForestClassifier(n_estimators = 10, n_jobs = 16)
        rf.fit(X_numerical_base, y)
        X_meta.append(rf.predict_proba(X_numerical_meta))
        X_test_meta.append(rf.predict_proba(X_test_numerical))

        svm = LinearSVC()
        svm.fit(X_sparse_base, y)
        X_meta.append(svm.decision_function(X_sparse_meta))
        X_test_meta.append(svm.decision_function(X_test_sparse))
        
X_meta = np.column_stack(X_meta)
X_test_meta = np.column_stack(X_test_meta)

# <codecell>
",others/morph.py,timpalpant/KaggleTSTextClassification,1
"gain, j = mutual_info_classif(data[:, 8:-1], data[:, -1], discrete_features='auto', n_neighbors=3, copy=True, random_state=None), 0
for i in np.arange(len(gain)):
	if gain[i] <= 0.001:
		data = np.delete(data, 8+i-j, 1)
		j += 1

X_train, X_test, y_train, y_test = train_test_split(data[:, 0:-1], data[:, -1], test_size = 0.4, random_state = 0)

start = timer()

clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
y_prob1 = clf1.predict_proba(X_test)[:,1]
y_pred1 = clf1.predict(X_test)
y_acc1 = accuracy_score(y_test, y_pred1)

clf2 = MLPClassifier(solver='lbfgs', activation = 'logistic', learning_rate = 'adaptive', hidden_layer_sizes = (5), random_state = 1)
clf2 = clf2.fit(X_train, y_train)
y_prob2 = clf2.predict_proba(X_test)[:,1]
y_pred2 = clf2.predict(X_test)
y_acc2 = accuracy_score(y_test, y_pred2)",Holdout/WeightedAveraging.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"
        if n_classes > 2:
             raise NotImplementedError(""multiclass is not implemented yet."")

    idx_min_1 = (y == info['classes'][0])
    y = np.ones(y.shape)
    y[idx_min_1] = -1
    return X, y, info

def linear_init(X, y, fit_intercept=True):
    svm = LinearSVC(fit_intercept=fit_intercept)
    svm.fit(X, y)
    if fit_intercept:
        intercept = svm.intercept_[0]
    else:
        intercept = svm.intercept_
    return svm.coef_[0, :], intercept",src/objectives/hinge.py,Bihaqo/exp-machines,1
"from sklearn import svm
from sklearn import metrics
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier



def apply_algorithm(paras, X, y):

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    else:
        print str(""unknown classifier"") ",python/backup/methods.py,Healthcast/RSV,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(1,101):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        args=[str(dim)+ ""Dgauss_bdt_AD"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),1]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/bdt_gauss/bdt_Gauss_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"
# Not sure whats going on in here
# written by feesta


class L1LinearSVC(LinearSVC):

    def fit(self, X, y):
        # The smaller C, the stronger the regularization.
        # The more regularization, the more sparsity.
        self.transformer_ = LinearSVC(penalty=""l1"",
                                      dual=False, tol=1e-3)
        X = self.transformer_.fit_transform(X, y)
        return LinearSVC.fit(self, X, y)

    def predict(self, X):
        X = self.transformer_.transform(X)
        return LinearSVC.predict(self, X)

",consumer/test_readble.py,konfabproject/konfab-consumer,1
"            ('flux-diff-scaler', ReactionDiffScaler()),
            ('vect1', vect1),
            ('vt', vt),
            ('inv_vec1', InverseDictVectorizer(vect1, vt)),
            ('vect2', vect2),
            ('skb', skb),
            ('inv_vec2', InverseDictVectorizer(vect2, skb)),
            ('pathway_scoring', PathwayFvaScaler()),
            ('vect3', DictVectorizer(sparse=False)),
            ('pca', PCA()),
            # ('clf', SVC(C=1e-6, kernel='rbf', random_state=0))
            # ('clf', KNeighborsClassifier(n_neighbors=31))
            # ('clf', DecisionTreeClassifier())
            # ('clf', RandomForestClassifier(n_estimators=10000, n_jobs=-1))
            # ('clf', LinearSVC(C=0.1e-5, random_state=43))
            ('clf', LogisticRegression(C=0.3e-6, random_state=43))
            # ('clf', MLPClassifier(activation=""logistic"",
            #                       random_state=43,
            #                       hidden_layer_sizes=(300, 100),
            #                       verbose=True,",src/classifiers/fva_disease_classifier.py,MuhammedHasan/metabolitics,1
"

class FromSolutionSolutionLevelDiseaseClassifier(BaseDiseaseClassifier):

    def __init__(self):
        super().__init__()
        self._pipe = Pipeline([
            ('scaler_most_active', MostActivePathwayScaler()),
            ('vect', DictVectorizer(sparse=False)),
            ('pca', PCA()),
            ('clf', SVC(C=0.1, kernel='rbf', random_state=0))
        ])",src/classifiers/from_solution_solution_level_disease_classifier.py,MuhammedHasan/disease-diagnosis,1
"import sklearn.ensemble
import sklearn.svm


print ""Loading data and transforming to toxin features""
X,Y = data.load_toxin_features(substring_length=3, positional=True)
print X[0]
def run_classifiers(X,Y):
  print ""Data shape"", X.shape
  for c in [0.0001, 0.001, 0.01, 0.1, 1]:
    svm = sklearn.svm.LinearSVC(C=c)
    print ""SVM C ="", c
    print np.mean(sklearn.cross_validation.cross_val_score(svm, X, Y, cv = 10))
 
  n_classifiers = 1000
  rf = sklearn.ensemble.RandomForestClassifier(n_classifiers)
  print ""Random Forest""
  print np.mean(sklearn.cross_validation.cross_val_score(rf, X, Y, cv = 10))

""""""",Jan21_toxin_positional.py,iskandr/immuno,1
"    ----------
    .. [1] `Wikipedia entry on the Hinge loss
            <http://en.wikipedia.org/wiki/Hinge_loss>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
         random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",sklearn/metrics/metrics.py,B3AU/waveTree,1
"		C_values = [1, 2, 8, 32, 128, 512, 2048, 8192]
		gamma_values = [0.001,0.01,0.05,0.1,0.3,0.5,0.7]

		#C_values = [2]
		#gamma_values = [0.5]

		best_C = 0
		best_G = 0
		best_percentage = 0

		#clf = svm.SVC(C=2.0,gamma=0.5)

		C_range = np.logspace(-2, 10, num=13, base=2)
		gamma_range = np.logspace(-5, 1, num=7, base=10)
		param_grid = dict(gamma=gamma_range, C=C_range)
		cv = StratifiedShuffleSplit(trainingLabels, n_iter=3, test_size=0.31, random_state=42)
		grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
		grid.fit(trainingSet, trainingLabels)
		#C_range = np.logspace(-1, 1, num=2, base=2)
		#gamma_range = np.logspace(-1, 1, num=2, base=10)",CrossValidateSVM.py,JessMcintosh/SonoGestures,1
"	y_2d = y[y < 2]
	# Test part
	T_2d = np.delete(T,range(25,36)+range(50,64),axis=0)
	yy_2d = yy[yy < 2]
	#------------------------------ Standardize data ------------
	scaler = StandardScaler()
	X_2d = scaler.fit_transform(X_2d)
	T_2d_scaled = scaler.transform(T_2d)
	#------------------------------ Create Classifier ------------------
	manual_param = {'C':Csvm_al,'gamma':Gsvm_al}
	clf = SVC(gamma=manual_param['gamma'], C=manual_param['C'])
	clf.fit(X_2d, y_2d)
	#------------------------------ Create Classifier ------------------
	C_range = np.logspace(1, 3, 3)
	gamma_range = np.logspace(-3, -1, 3)
	param_grid = dict(gamma=gamma_range, C=C_range)
	cv = StratifiedShuffleSplit(y_2d, n_iter=100, test_size=0.2, random_state=42)
	grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
	#------------------------------ Best parameter Aprox.---------------
	grid.fit(X_2d, y_2d)",EpocArmData/Features/feat_extraction.py,maberyick/RPi-EPOC,1
"
    def model_postfix(self):  # Must be unique for each model.
        return ""svmc""

    def model_description(self):
        return (""Implements the Support Vector Machine (SVM) Classifier defined in the SKLearn modules.\n""
                "" This SVM (postfix svmc) is configured as a label classifier.\n""
                "" For more information, Google SKLearn and read the documentation.\n"")

    def model_define(self):
        return OneVsRestClassifier(svm.SVC(kernel=str(""rbf""), probability=True, C=1e3, gamma=0.00001))


######################################################################################################
#
# Random Forest Implemented as a classifier.
#
######################################################################################################

",OSMSKLearnClassify.py,kellerberrin/OSM-QSAR,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ll_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",PyFeaST/example/test_ll_l21.py,jundongl/PyFeaST,1
"    X =  tfv.transform(traindata) 
    X_test = tfv.transform(testdata)
    
    # Initialize SVD
    svd = TruncatedSVD()
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    svm_model = SVC()
    
    # Create the pipeline 
    clf = pipeline.Pipeline([('svd', svd),
    						 ('scl', scl),
                    	     ('svm', svm_model)])
    
    # Create a parameter grid to search for best parameters for everything in the pipeline
    param_grid = {'svd__n_components' : [300],
                  'svm__C': [9]}",Search_result/srr1.py,tanayz/Kaggle,1
"    plt.barh(pos, feature_importance[sorted_idx], align='center', color='#7A68A6')

    #plt.yticks(pos, np.asanyarray(df.columns.tolist())[sorted_idx]) #ORIG
    plt.yticks(pos, np.asanyarray(featureNames)[sorted_idx])

    plt.xlabel('Relative Importance')
    plt.title('Features Importance')
    plt.show()

'Works!'
def PlotPerfPercentFeatures(X,y,est=LinearSVC()):
    '''
    Performance of a classifier (default: SVM-Anova)
    varying the percentile of features selected (F-test) .

    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py

    See Also: (Similar but with model seelction from among classifiers):
    http://nbviewer.ipython.org/github/bugra/pydata-nyc-2014/blob/master/6.%20Scikit%20Learn%20-%20Model%20Selection.ipynb
",ProFET/feat_extract/PipeTasks.py,ddofer/ProFET,1
"    data = X[:len(data)]
    test = X[len(data):]
    print data.shape, test.shape

    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.2, random_state=42)

    #print X_train.shape, y_train.shape, y_train
    #print X_test.shape, y_test.shape, y_test
    #print ""data set balance: "", sum(target), float(len(target)-sum(target)) / len(target)

    #clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    #print clf.score(X_test, y_test)  
    
    """"""
    clf = svm.SVC(kernel='linear', C=1)
    scores = cross_validation.cross_val_score(clf, data, target, cv=10)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    """"""
    '''
    myRF_AUC(data, target)",sklearnClassifiers/simpleClassifier3.py,rampasek/seizure-prediction,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,
                       scoring='%s_weighted' % score)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_params_)
    print()
    print(""Grid scores on development set:"")
    print()",projects/scikit-learn-master/examples/model_selection/grid_search_digits.py,DailyActie/Surrogate-Model,1
"
from extract_labels import get_metadata, create_dataframe
from CrossValidate import split_training_data

def linear_classifier(X_train, y_train, X_test, y_test):
    # X_train = np.load('data/training_data.npy')
    # y_train = np.load('data/training_labels.npy')
    # X_test = np.load('data/testing_data.npy')
    # y_test = np.load('data/testing_data.npy')

    clf = LinearSVC(verbose=2)
    clf.fit(X_train, y_train)
    print('Linear SVC Training: ', clf.score(X_train, y_train))
    print('Linear SVC Testing: ', clf.score(X_test, y_test))

def build_classifier(X_train, y_train):
    clf = RandomForestClassifier()
    clf.fit(X_train, y_train)
    print('Random Forest Training ', clf.score(X_train, y_train))
    return clf",random_forest.py,SSYoung/Gender-Census-ANN,1
"from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE

# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

# Create the RFE object and rank each pixel
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_.reshape(digits.images[0].shape)

# Plot pixel ranking
import matplotlib.pyplot as plt
plt.matshow(ranking)
plt.colorbar()
plt.title(""Ranking of pixels with RFE"")",examples/plot_rfe_digits.py,loli/sklearn-ensembletrees,1
"from Config import *

featuresDict = {}
trainedModel = {}

print ""Loading the features...""
pickledFeaturesFileHandle = open(pickledFeaturesFileName, 'rb')
featuresDict = pickle.load(pickledFeaturesFileHandle)

print ""Training using SVC""
svcModel = LinearSVC(loss='l2', dual=False, tol=1e-3)
svcModel.fit(featuresDict[""trainingFeatures""], featuresDict[""trainingClasses""])

trainedModel[""model""] = svcModel

pickledModelFileNameHandle = open(pickledModelFileName,'wb')
pickle.dump(trainedModel, pickledModelFileNameHandle)
pickledModelFileNameHandle.close()

",Train.py,vikram-gupta/QuestionClassifier,1
"

def plot_cross_val_selection():
    iris = load_iris()
    X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data,
                                                              iris.target,
                                                              random_state=0)

    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
                  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
    grid_search = GridSearchCV(SVC(), param_grid, cv=5)
    grid_search.fit(X_trainval, y_trainval)
    results = pd.DataFrame(grid_search.results_)[15:]

    best = np.argmax(results.test_mean_score.values)
    plt.figure(figsize=(10, 3))
    plt.xlim(-1, len(results))
    plt.ylim(0, 1.1)
    for i, (_, row) in enumerate(results.iterrows()):
        scores = row[['test_split%d_score' % i for i in range(5)]]",mglearn/plot_grid_search.py,JoostVisser/ml-assignment2,1
"
label = feature.getlabels()


def poly(X,Y):
    """"""
    Returns a polynomial kernal svm
    Args:
    """"""
    #Polynomial kernel=======================================================
    clf = svm.SVC(kernel='poly',C=1,probability=True)
    clf.fit(X,Y)

    return clf


def fit(train_percentage,fold=5):
    """""" Radomly choose songs from the dataset, and train the classfier 
        Accepts parameter: train_percentage, fold;
        Returns clf    ",mysvm/svm.py,indrajithi/mgc-django,1
"
    print('Decision Tree score:\n')

    #print ""%.3f\n"" %(a_tree)

    gs_rbf.fit(X_complete,y_complete)

    print(""%.3f\n"" %(gs_rbf.best_score_))


    clf_4 = SVC(kernel = 'linear')

    parameters_4 = {'C' : [10**-3 , 10**-2 , 0.1, 1, 10, 10**2 , 10**3 ]}

    gs_rbf = model_selection.GridSearchCV(clf_4,param_grid=parameters_4,cv = 10, n_jobs = -1) #grid search hyper parameter optimization

    print('SVM lin score:\n')

    #print ""%.3f\n"" %(a_svm_lin)
",ml/rdf_predict.py,MultimediaSemantics/entity2vec,1
"    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_1(self):
        x = array([[1, 1], [2, 2]])
        y = array([0, 1])
        measure = SVCDistanceNCMeasure()
        clf = SVC(decision_function_shape='ovr')
        clf.fit(x, y)
        measures = measure.evaluate(clf, x)
        self.assertAlmostEqual(measures[0, 0], -.63212056)
        self.assertAlmostEqual(measures[0, 1], .63212056)
        self.assertAlmostEqual(measures[1, 0], .63212056)
        self.assertAlmostEqual(measures[1, 1], -.63212056)

    def tests_2(self):
        x = array([[1, 1], [2, 2], [3, 3]])",tests/nc_measures/SVMTest.py,SergioGonzalezSanz/conformal_predictors,1
"    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg10_ficuslike.py,lioritan/Thesis,1
"
# Training and validation data (k-fold = 30%)
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3, random_state=1)


# With a RBF kernel, gamma=0.01, corpus_number_perdomain = 5000 and
# vectors_size = 20 to reach the maximum score, however we
# use a linear kernel because we have a 0.1 less precison but we less
# support vectors. This is important in classification time

clf = svm.SVC(kernel='linear', probability=False)
clf.fit(X_train, y_train)  ## classifier generated

# Save model to disk and also a vectorizer index
joblib.dump(clf, 'models/%s/svm_model.pkl'%lang)
with open('models/%s/vectorizer.pkl'%lang, 'wb') as o_file:
    pickle.dump(vectorizer, o_file)

# Dump info about the model
print(""\nSupported vectors length: %s"" % str(clf.support_vectors_.shape))",svm-training.py,rmaestre/SVM-for-domains-detection,1
"

def select_models(X_train, type_pred, is_labeled_data, is_text_data, is_number_categories_known,
                  is_few_important_features, is_just_looking):
    names = []
    models = []
    if type_pred == 'category':
        if is_labeled_data:
            # classification
            names.append(""SVM"")
            models.append(svm.SVC())
            if is_text_data:
                names += [""GaussianNB"",
                          ""MultinomialNB"",
                          ""BernoulliNB""]
                models += [naive_bayes.GaussianNB(),
                           naive_bayes.MultinomialNB(),
                           naive_bayes.BernoulliNB()]
            else:
                names += [""KNeighborsClassifier"",",models.py,llautert/psychoPYTHON,1
"        
        data, targets, le = self.prepare(table_name, feature_names, target_name)
        
        #create train and test data
        n_samples = len(data)
        X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, targets, test_size=0.4, random_state=250)
        
        #pipeline for classification
        pipeline = Pipeline([
            ('vect', DictVectorizer()),
            ('clf', svm.SVC()),
        ])

        #fit model, test model
        pipeline.fit(X_train, y_train)
        predicted = pipeline.predict(X_test)
        precision = metrics.precision_score(y_test, predicted, average=""weighted"")
        recall = metrics.recall_score(y_test, predicted, average='weighted')

        self.persist(le, pipeline)",ml_demo/train.py,mmqm4544/rta,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)


def test_ajd():
    """"""Test if Approximate joint diagonalization implementation obtains same
    results as the Matlab implementation by Pham Dinh-Tuan.
    """"""",mne/decoding/tests/test_csp.py,alexandrebarachant/mne-python,1
"mlp = MLPClassifier()
mlp.fit(X_train, y_train)
y_pred1 = mlp.predict(X_test)
print(""MLP Accuracy: {}"".format(accuracy_score(y_test, y_pred1)))

knc = KNeighborsClassifier()
knc.fit(X_train, y_train)
y_pred2 = knc.predict(X_test)
print(""KNC Accuracy: {}"".format(accuracy_score(y_test, y_pred2)))

svc = SVC()
svc.fit(X_train, y_train)
y_pred3 = svc.predict(X_test)
print(""SVC Accuracy: {}"".format(accuracy_score(y_test, y_pred3)))

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)
y_pred4 = dtc.predict(X_test)
print(""DTC Accuracy: {}"".format(accuracy_score(y_test, y_pred4)))
",data-scripts/mongo-class.py,JohnStarich/github-code-recommendations,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",sklearn/metrics/tests/test_metrics.py,loli/sklearn-ensembletrees,1
"
    d = {}
    d['piece'] = 'mid/moonlight_sonata.mid'
    d['chords'] = [
        'C#m', 'C#m', 'Am', 'C#m', 'C#m', 'G#', 'C#m', 'Em', 'Em', 'Em', 'G', 'Cm', 'Bm', 'Bm', 'Bm', 'Em', 'B', 'Em', 'Bm', 'C#', 'Gm', 'F#m', 'F#m', 'C#m', 'F#m', 'G#', 'C#m', 'G#', 'G#', 'C#m', 'C#m', 'B#m', 'C#m', 'C#m', 'B#m', 'B#m', 'B#m', 'G#', 'G#', 'G#', 'G#', 'C#m', 'G#', 'C#m', 'Em', 'Em', 'B', 'Em', 'G#', 'Dm', 'C#m', 'F#m', 'F#m', 'C#m', 'F#m', 'C#m', 'Am', 'F#m', 'C#m', 'C#m', 'G#', 'C#m', 'B#m', 'C#m', 'B#m', 'C#m', 'C#m', 'C#m', 'C#m']
    l.append(d)

    return l

if __name__ == '__main__':
    svc = svm.SVC(kernel='rbf', C=10000)
    rforest = RandomForestClassifier(n_estimators=100)
    lr = linear_model.LogisticRegression(C=1)

    if len(sys.argv) == 1:
        max_ = 0
        count, scores = 0, []
        truth = chord_truths()[0]
        musicpiece = data.piece(truth['piece'])
        from sklearn.externals import joblib",chords.py,fabeschan/midigeneration,1
"        #test_set = Sel.transform(feature[test])
        clf.fit(train_set, labels[train])
        pred = clf.predict(test_set)
        score.append(accuracy_score(labels[test], pred))
        score.append(precision_score(labels[test], pred))
        score.append(recall_score(labels[test], pred))
        score.append(f1_score(labels[test], pred))
        scores.append(score)
    avg = np.average(scores, axis=0)
    return avg
print validate(SVC(C=10000, gamma=0.75), feature, 500)
print validate(LinearSVC(C=100), feature, 500)
print validate(LogisticRegression(C=100), feature, 500)

#
# Grid search for parameters
#sfk = cv.StratifiedShuffleSplit(labels, 40)
#params = {'C':[1, 100, 10000, 100000]}
#gs = GridSearchCV(
#        SVC(), params, scoring='f1',",learning/final2.py,fcchou/CS229-project,1
"    
    #Prep target
    labelBase = [sample[target] for sample in data]

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(featData_MinMax, labelBase, test_size=0.1, random_state=0)
    
    classifiers = [
#        ('ada-10', AdaBoostClassifier(n_estimators=10)),
#        ('ada-50', AdaBoostClassifier(n_estimators=50)),
 #       ('ada-100', AdaBoostClassifier(n_estimators=100)),
#        ('svm-1', svm.SVC(cache_size=1000,C=1.0)),
#        ('svm-05', svm.SVC(cache_size=1000,C=0.5)),
 #       ('forest-10', RandomForestClassifier(n_estimators=10)),
 #       ('forest-20', RandomForestClassifier(n_estimators=20)),
        ('forest-50', RandomForestClassifier(n_estimators=50)),
        ('forest-50-min5', RandomForestClassifier(n_estimators=50,min_samples_leaf=5)),
        ('forest-50-min10', RandomForestClassifier(n_estimators=50,min_samples_leaf=10)),
 #       ('forest-5010', RandomForestClassifier(n_estimators=50,max_features=10)),
 #       ('forest-5020', RandomForestClassifier(n_estimators=50,max_features=20)),
    ]    ",laurent/coFeeClassif.py,sankar-mukherjee/CoFee,1
"            instance.append(float(j))
        X.append(np.array(instance[1:]))
        y.append(instance[0])
X = np.array(X)
y = np.array(y)


base_pipe = Pipeline([('saxizer', SAXTransformer(points_per_symbol=1)),
                     ('features', FeatureUnion([('countvect', CountVectorizer(min_df=1, analyzer='char', ngram_range=(1, 10))),
                                                ('tfidfvect', TfidfVectorizer(min_df=1, analyzer='char', ngram_range=(1, 2)))])),
                     ('svc', svm.LinearSVC())])

bop_pipe = Pipeline([('saxizer', SAXTransformer(points_per_symbol=1)),
                     ('features', FeatureUnion([('countvect', CountVectorizer(min_df=1, analyzer='char', ngram_range=(1, 10))),
                                                ('tfidfvect', TfidfVectorizer(min_df=1, analyzer='char', ngram_range=(1, 2)))])),
                     ('svc', svm.LinearSVC())])


for i in [bop_pipe, base_pipe]:
    score, permutation_scores, pvalue = permutation_test_score(",sandbox.py,tarmstro/mvbop,1
"
class TestClassificationMetrics(tm.TestCase):

    def setUp(self):
        import sklearn.svm as svm
        digits = datasets.load_digits()
        self.data = digits.data
        self.target = digits.target
        self.df = expd.ModelFrame(digits)

        estimator1 = self.df.svm.LinearSVC(C=1.0, random_state=self.random_state)
        self.df.fit(estimator1)

        estimator2 = svm.LinearSVC(C=1.0, random_state=self.random_state)
        estimator2.fit(self.data, self.target)
        self.pred = estimator2.predict(self.data)
        self.decision = estimator2.decision_function(self.data)

        # argument for classification reports
        self.labels = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])",expandas/skaccessors/test/test_metrics.py,sinhrks/expandas,1
"

def select_models(X_train, type_pred, is_labeled_data, is_text_data, is_number_categories_known,
                  is_few_important_features, is_just_looking):
    names = []
    models = []
    if type_pred == 'category':
        if is_labeled_data:
            # classification
            names.append(""SVM"")
            models.append(svm.SVC())
            if is_text_data:
                names += [""GaussianNB"",
                          ""MultinomialNB"",
                          ""BernoulliNB""]
                models += [naive_bayes.GaussianNB(),
                           naive_bayes.MultinomialNB(),
                           naive_bayes.BernoulliNB()]
            else:
                names += [""KNeighborsClassifier"",",models.py,Rafaelahelbing/psychoPYTHON,1
"import os
import classifier_eval_simplified
from sklearn.svm import SVC

# Write a function like this called 'main'
def main(job_id, params):
    print 'Anything printed here will end up in the output directory for job #%d' % job_id
    print params
    #comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data_optimisation.0.0.txt"",os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data_optimisation.200.1.txt"")]
    comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high4Dgauss_optimisation_10000_0.5_0.1_0.0_1.txt"",os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high4Dgauss_optimisation_10000_0.5_0.1_0.01_1.txt"")]    
    clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)

    args=[""dalitz"",""particle"",""antiparticle"",100,comp_file_list,2,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]

    result= classifier_eval_simplified.classifier_eval(2,0,args)

    with open(""optimisation_values.txt"", ""a"") as myfile:
        myfile.write(str(params['aC'][0])+""\t""+ str(params['agamma'][0])+""\t""+str(result)+""\n"")
    return result",Dalitz_simplified/optimisation/svm/classifier_eval_wrapper.py,weissercn/MLTools,1
"            operator_text += ('''{OUTPUT_DF}['lrc{OPERATOR_NUM}-classification'] = lrc{OPERATOR_NUM}.predict('''
                              '''{OUTPUT_DF}.drop('class', axis=1).values)\n''').format(OUTPUT_DF=result_name,
                                                                                        OPERATOR_NUM=operator_num)

        elif operator_name == '_svc':
            C = float(operator[3])
            if C <= 0.:
                C = 0.0001

            operator_text += '\n# Perform classification with a C-support vector classifier'
            operator_text += '\nsvc{OPERATOR_NUM} = SVC(C={C})\n'.format(OPERATOR_NUM=operator_num, C=C)
            operator_text += ('''svc{OPERATOR_NUM}.fit({INPUT_DF}.loc[training_indices].drop('class', axis=1).values, '''
                              '''{INPUT_DF}.loc[training_indices, 'class'].values)\n''').format(OPERATOR_NUM=operator_num,
                                                                                                INPUT_DF=operator[2])
            if result_name != operator[2]:
                operator_text += '{OUTPUT_DF} = {INPUT_DF}.copy()\n'.format(OUTPUT_DF=result_name, INPUT_DF=operator[2])
            operator_text += ('''{OUTPUT_DF}['svc{OPERATOR_NUM}-classification'] = svc{OPERATOR_NUM}.predict('''
                              '''{OUTPUT_DF}.drop('class', axis=1).values)\n''').format(OUTPUT_DF=result_name,
                                                                                        OPERATOR_NUM=operator_num)
",tpot/export_utils.py,pronojitsaha/tpot,1
"from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC

iris = load_iris()


def test_transform_linear_model():
    for clf in (LogisticRegression(C=0.1),
                LinearSVC(C=0.01, dual=False),
                SGDClassifier(alpha=0.1, n_iter=10, shuffle=True, seed=0)):
        for thresh in (None, "".09*mean"", ""1e-5 * median""):
            for func in (np.array, sp.csr_matrix):
                X = func(iris.data)
                clf.set_params(penalty=""l1"")
                clf.fit(X, iris.target)
                X_new = clf.transform(X, thresh)
                if isinstance(clf, SGDClassifier):
                    assert_true(X_new.shape[1] <= X.shape[1])",venv/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_selector_mixin.py,GbalsaC/bitnamiP,1
"from sklearn.cross_validation import train_test_split



def svc_training(target, control):
    np_fps = []
    for fp in target + control:
        arr = numpy.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp, arr)
        np_fps.append(arr)
    lin_svc = svm.LinearSVC(C=1.0)
    ys_fit = [1] * len(target) + [0] * len(control)
    lin_svc.fit(np_fps, ys_fit)
    return lin_svc


def svc_test(target, control, lin_svc):
    ys_fit = [1] * len(target) + [0] * len(control)
    print lin_svc.score(target+control, ys_fit)
",training_methods/classifier/linear_svc_analysis.py,dkdeconti/PAINS-train,1
"            logging.warn(""Can not train an intent classifier. Need at least 2 different classes. "" +
                         ""Skipping training of intent classifier."")
        else:
            y = self.transform_labels_str2num(labels)
            X = intent_features

            # dirty str fix because sklearn is expecting str not instance of basestr...
            tuned_parameters = [{'C': [1, 2, 5, 10, 20, 100], 'kernel': [str('linear')]}]
            cv_splits = max(2, min(MAX_CV_FOLDS, np.min(np.bincount(y)) // 5))  # aim for 5 examples in each fold

            self.clf = GridSearchCV(SVC(C=1, probability=True),
                                    param_grid=tuned_parameters, n_jobs=num_threads,
                                    cv=cv_splits, scoring='f1_weighted', verbose=1)

            self.clf.fit(X, y)

    def process(self, intent_features):
        # type: (np.ndarray) -> Dict[Text, Any]
        """"""Returns the most likely intent and its probability for the input text.""""""
",rasa_nlu/classifiers/sklearn_intent_classifier.py,verloop/rasa_nlu,1
"    # Transform data into a vector of TF-IDF values
    count_vect = CountVectorizer(ngram_range=(1, 2))
    X_train_counts = count_vect.fit_transform(X_train)
    tfidf_transformer = TfidfTransformer(use_idf=True)
    X_train_dtm = tfidf_transformer.fit_transform(X_train_counts)
    # Transform test data
    X_test_counts = count_vect.transform(X_test)
    X_test_dtm = tfidf_transformer.fit_transform(X_test_counts)

    # Not optimized
    clf = svm.SVC(kernel='linear', C=1.0)
    clf.fit(X_train_dtm, y_train)
    y_pred_class = clf.predict(X_test_dtm)

    # utilities.print_misclassified_samples(X_test, y_pred_class, y_test)
    # utilities.print_stats(y_pred_class, y_test)


if __name__ == '__main__':
    # probably not the best way to measure time, but, we only want a ballpark figure",scikit_stress_test.py,anmolshkl/oppia-ml,1
"    #https://github.com/danielfrg/tsne

    ###########################################################
    #class prior: [ 0.49482973  0.50517027] log_loss = 0.693
    ###########################################################

    #cv = StratifiedKFold(ytrain,8,shuffle=True)
    cv = StratifiedShuffleSplit(ytrain,n_iter=20,test_size=0.2)

    #model = RandomForestClassifier(n_estimators=100,max_depth=None,min_samples_leaf=5,n_jobs=2, max_features=Xtrain.shape[1]/3,oob_score=False)
    #model = SVC(C=1,kernel='rbf',probability=True) #cv 8fold ~ 80 min on 4 procs!!! 20000 samples ~ 2min. per fold 40000 12 min.
    #model = LogisticRegression(C=1.0,penalty='l2')
    #model = LogisticRegression(C=100,penalty='l1')
    #model = KernelRidge()
    #model = XgboostClassifier(n_estimators=200,learning_rate=0.01,max_depth=2, NA=0,subsample=.5,colsample_bytree=1.0,min_child_weight=5,n_jobs=4,objective='binary:logistic',eval_metric='logloss',booster='gbtree',silent=1,eval_size=0.0)
    model = KerasNN(dims=Xtrain.shape[1],nb_classes=2,nb_epoch=40,learning_rate=0.1,validation_split=0.0,batch_size=1024,verbose=1,activation='relu', layers=[20,20], dropout=[0.2,0.2],loss='categorical_crossentropy')
    #model = VotingClassifier(estimators=[('lr', model1),('xgb', model2) ,('nn', model3)], voting='soft',weights=[1,1,1])
    #model = KerasNN(dims=Xtrain.shape[1],nb_classes=2,nb_epoch=40,learning_rate=0.05,validation_split=0.0,batch_size=64,verbose=1,activation='sigmoid', layers=[20,20], dropout=[0.0,0.1],loss='categorical_crossentropy')
    #model = CalibratedClassifierCV(model,cv=8,method='sigmoid')
    model = Pipeline([('scaler', StandardScaler()), ('m',model)])",numerai/numerai.py,chrissly31415/amimanera,1
"		y = int(line.split(""\n"")[0])
		Y.append(y)

	split = len(X) / 2
	Xtrain = X[:split]
	Ytrain = Y[:split]
	Xtest = X[split:]
	Ytest = Y[split:]

	# Create the cosine similarity SVM
	cos_pipe = svm.SVC(C=0.5, kernel=cosine_similarity, decision_function_shape='ovr')

	# Initialize the counter vectorizer and tfidf transformer with the correct params
	count_vect = CountVectorizer(lowercase=True, stop_words='english')
	tfidf_transformer = TfidfTransformer(sublinear_tf=True, norm='l2')
	tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=True, stop_words='english')

	# Create a pipeline for the cosine similarity SVM
	#cos_pipe = Pipeline([('vect', count_vect), ('tfidf', tfidf_transformer), ('clf', cos_clf), ])
	Xtrain = tfidf_vectorizer.fit_transform(Xtrain)",stance/stance.py,project-em/ns3-sentiment,1
"tst_data1 = tst_data[N/2:,:]

norm_tst_data = (tst_data - tst_data.mean(axis=0)) / np.sqrt(tst_data.var(axis=0,ddof=1))
norm_tst_data0 = norm_tst_data[:N/2,:]
norm_tst_data1 = norm_tst_data[N/2:,:]

tst_labels = np.hstack(( np.zeros(N/2), np.ones(N/2) ))

sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)
errors['lda'] = (1-sklda.score(norm_tst_data, tst_labels))
errors['knn'] = (1-skknn.score(norm_tst_data, tst_labels))
errors['svm'] = (1-sksvm.score(norm_tst_data, tst_labels))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",tests/mpm_yousef.py,binarybana/samcnet,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 5, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = [100.0, 100.0],
        sigma = 1.0,
        beta = 0.9,
        meta_model = meta_model) 

    return method
",evopy/examples/problems/SchwefelsProblem21/CMAESSVC.py,jpzk/evopy,1
"# Import Support Vector Machine classifier
from sklearn.svm import SVC

### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

#########################################################
### your code goes here ###
clf = SVC(kernel=""linear"")


def lessCode():
    ## Les code version
    clf.fit(features_train, labels_train)
    print clf.score(features_test, labels_test)


def timingVersion():",learning/ud120-projects/svm/svm_author_id.py,dmytroKarataiev/MachineLearning,1
"    dataset=tweet.objects.all()
    for d in dataset:
        kalimat.w.append(d.preproc)
        kalimat.p.append(d.polarity)
    X_train, X_test, y_train, y_test = train_test_split(kalimat.w, kalimat.p)
    vect = CountVectorizer()
    vect.fit(X_train)
    X_train_dtm = vect.transform(X_train)
    X_train_dtm = vect.fit_transform(X_train)
    X_test_dtm = vect.transform(X_test)
    svm=LinearSVC()
    svm.fit(X_train_dtm, y_train)
    y_pred_class = svm.predict(X_test_dtm)
    acc=metrics.accuracy_score(y_test, y_pred_class)
    return 'svm='+str(acc)



def nbimp():
",MLProgramming/MLProgramming/MLProgramming/MLP/mylib.py,DhifliMed/DhifliHammami,1
"    print(target.sum())
    target.shape


    cols.remove('LABELS')
    cols.remove('LABELS_ROCK_TYPE')

    imputer = pre.Imputer()
    scalar = pre.StandardScaler()
    n_components=20
    svc = svm.SVC()

    pca = decomp.PCA(n_components=n_components, whiten=True)

    tx = pipe.make_pipeline(imputer, pca)

    x_train, x_test, y_train, y_test = crossval.train_test_split(cleaned[cols], target, test_size=0.4)

    print x_train
    print y_train",ML_stuff.py,johnny555/2d3g,1
"from sklearn.decomposition import PCA, RandomizedPCA
from sklearn.pipeline import make_pipeline

f = ""../cyvcf2/cyvcf2/1kg.sites.bin.gz""

tmp = np.fromstring(gzip.open(f).read(), dtype=np.uint8).astype(np.int32)

genos = tmp.reshape((23556, len(tmp) / 23556)).T

clf = make_pipeline(RandomizedPCA(n_components=4, whiten=True, copy=False),
                    svm.SVC(C=2, probability=True))


ipops = ""AFR AMR EAS EUR SAS"".split()

# https://hangouts.google.com/webchat/u/0/frame?v=1463175081&hl=en-US&pvt=AMP3uWbeJzucAfSaxyKZreeU3oew-CyaV-gwRMAvHpeHN9VKU_EhC1Fj75C7UKU-cpIk6HvlXK6K&prop=aChromeExtension#zSoyz
pops = np.array([x['super_pop'] for x in ts.reader('integrated_call_samples_v3.20130502.ALL.panel')])
target = np.array([ipops.index(x) for x in pops])

#print ""|"".join(map(str, target))",scripts/pca.py,brentp/peddy,1
"Y = dataset[:, -1]

split = 0.75

spltv = int(split * len(Y))
X_train = X[:spltv,:]
Y_train = Y[:spltv]
X_test  = X[spltv:,:]
Y_test  = Y[spltv:]

svc = svm.SVC(kernel='poly', degree=2)
svc.fit(X_train, Y_train)

print(""testing data..."")
predicted = svc.predict(X_test)
print(metrics.classification_report(Y_test, predicted))
print(metrics.confusion_matrix(Y_test, predicted))",aula8/svm.py,elmadjian/pcs5735,1
"        '''
        Defines all relevant parameters and classes for classfier objects.
        Edit these if you wish to change parameters.
        '''
        # These are the classifiers
        self.clfs = {
            'RF': RandomForestClassifier(n_estimators = 50, n_jobs = -1),
            'ET': ExtraTreesClassifier(n_estimators = 10, n_jobs = -1, criterion = 'entropy'),
            'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth = [1, 5, 10, 15]), algorithm = ""SAMME"", n_estimators = 200),
            'LR': LogisticRegression(penalty = 'l1', C = 1e5),
            'SVM': svm.SVC(kernel = 'linear', probability = True, random_state = 0),
            'GB': GradientBoostingClassifier(learning_rate = 0.05, subsample = 0.5, max_depth = 6, n_estimators = 10),
            'NB': GaussianNB(),
            'DT': DecisionTreeClassifier(),
            'SGD': SGDClassifier(loss = 'log', penalty = 'l2'),
            'KNN': KNeighborsClassifier(n_neighbors = 3)
            }
        # These are the parameters which will be run through
        self.params = {
             'RF':{'n_estimators': [1,10,100,1000], 'max_depth': [10, 15,20,30,40,50,60,70,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10], 'random_state': [1]},",pipeline/model_loop.py,aldengolab/fake-news-detection,1
"from sklearn import svm
from sklearn.externals import joblib

FILENAME = 'clf.pkl'

try:
	clf = joblib.load(FILENAME)
except:
	import asl
	clf = svm.SVC(gamma = 0.0001, C = 50, probability = True)
	clf.fit(asl.data, asl.target)",classifier.py,paolo-torres/Sign-Language-Translator,1
"        {
            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
            'kernel': ['rbf'],
            'gamma': ['auto', 0.001, 0.01, 0.1, 1]
            }
        ]

    best_params = {}
    best_accuracy = 0

    clf = SVC(verbose=False)
    for d in param_grid:
        keys = d.keys()
        for v1 in d[keys[0]]:
            for v2 in d[keys[1]]:
                for v3 in d[keys[2]]:
                    params = {keys[0]: v1, keys[1]: v2, keys[2]: v3}
                    print 'Params:', params
                    clf.set_params(**params)
                    clf.fit(train_X, train_y)",code/svm_train.py,Shuailong/StockPrediction,1
"y = iris.target
n_classes = np.unique(y).size

# Some noisy data not correlated
random = np.random.RandomState(seed=0)
E = random.normal(size=(len(X), 2200))

# Add noisy data to the informative features for make the task harder
X = np.c_[X, E]

svm = SVC(kernel='linear')
cv = StratifiedKFold(y, 2)

score, permutation_scores, pvalue = permutation_test_score(
    svm, X, y, scoring=""accuracy"", cv=cv, n_permutations=100, n_jobs=1)

print(""Classification score %s (pvalue : %s)"" % (score, pvalue))

###############################################################################
# View histogram of permutation scores",examples/plot_permutation_test_for_classification.py,B3AU/waveTree,1
"    ## ================
    X, y = datasets.make_classification(n_samples=options.n_samples,
                                        n_features=options.n_features,
                                        n_informative=options.n_informative)

    ## 2) Build Workflow
    ## =================
    time_start = time.time()
    ## CV + Grid search of a pipeline with a nested grid search
    cls = Methods(*[Pipe(SelectKBest(k=k),
                         SVC(kernel=""linear"", C=C))
                    for C in C_values
                    for k in k_values])
    pipeline = CVBestSearchRefit(cls,
                                 n_folds=options.n_folds_nested,
                                 random_state=random_state)
    wf = Perms(CV(pipeline, n_folds=options.n_folds),
               n_perms=options.n_perms,
               permute=""y"",
               random_state=random_state)",examples/run_single_process.py,neurospin/pylearn-epac,1
"plt.show()


#############################################################################
print(50 * '=')
print('Section: Tuning hyperparameters via grid search')
print(50 * '-')


pipe_svc = Pipeline([('scl', StandardScaler()),
                     ('clf', SVC(random_state=1))])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]

param_grid = [{'clf__C': param_range,
               'clf__kernel': ['linear']},
              {'clf__C': param_range,
               'clf__gamma': param_range,
               'clf__kernel': ['rbf']}]
",code/optional-py-scripts/ch06.py,wei-Z/Python-Machine-Learning,1
"    test = pca.transform(test)
    return train, test

def normalize(train, test):
    norm = preprocessing.Normalizer()
    train = norm.fit_transform(train)
    test = norm.transform(test)
    return train, test

def createSVM():
    clf = SVC()
    return clf

def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf",kaggle-data-science-london-scikitlearn/src/classify.py,KellyChan/Kaggle,1
"def objective(args):
    print args
    global best
    score = []
    for train, test in StratifiedShuffleSplit(_Y):
        trainX = _X[train]
        trainY = _Y[train]
        testX = _X[test]
        testY = _Y[test]

        m = SVC(
            kernel=args[0],
            C=(2 ** -5) * (2 ** args[1]),
            gamma=(2 ** -15) * (2 ** args[2]),
        )
        try:
            m = m.fit(trainX, trainY)
            score.append(m.score(testX, testY))
        except Exception, e:
            print e",kagura/find_best_SVC_param.py,nishio/kagura,1
"    from sklearn import datasets
    from sklearn import model_selection
    from sklearn import svm
    import xam


    X, y = datasets.load_digits(n_class=2, return_X_y=True)

    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=42)

    clf = svm.SVC(probability=True)

    clf.fit(X_train, y_train)

    analyser = xam.error_analysis.BinaryClassificationErrorAnalyser(clf)

    observations, probabilites = analyser.get_false_positives(X_test, y_test)

    print(observations.shape)
",xam/error_analysis/binary_classification.py,MaxHalford/xam,1
"    
####################################################################
# Dalitz operaton
####################################################################

for i in range(100):
	comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.{0}.0.txt"".format(i), os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.2{0}.1.txt"".format(str(i).zfill(2))))
    
#clf = tree.DecisionTreeClassifier('gini','best',46, 100, 1, 0.0, None)
#clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.95,n_estimators=440)
#clf = SVC(C=1.0,gamma=0.0955,probability=True, cache_size=7000)
#args=[""dalitz_dt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
#For nn:
clf=""This shouldn't be used. Keras mode""
args=[""dalitz_nn_4layers_100neurons"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0,100,4]

classifier_eval_simplified.classifier_eval(0,1,args)

",Dalitz_simplified/evaluation_of_optimised_classifiers/nn_Dalitz/nn_Dalitz_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"    def __init__(self):
        self.name = ""Transfer Learning Base Line""

    @memory.cache
    def runBaseLine(self):

        X, Y = MultiDomainSentimentData().getDataArray(""books"")
        X, Y = MultiDomainSentimentData().getXY(X, Y)
        X_train, Y_train, X_test, Y_test = MultiDomainSentimentData().splitData(X, Y, 0.6)

        svm = SVC(C=1.0,kernel=""linear"")
        svm.fit(X_train,Y_train)
        YT_pred = svm.predict(X_test)
        print(metrics.accuracy_score(YT_pred,Y_test))

    @memory.cache
    def sqr(x):
        print('Running...')
        return x * x;
",textanalysis/tl/baseline.py,arunreddy/text-analysis,1
"    def get_vectorizer(self):
        return TfidfVectorizer(
            ngram_range = (1, 2),
            min_df = 2,
            strip_accents = None,
            charset_error = 'ignore',
            stop_words = None
        )
    
    def get_classifier(self, kernel = 'linear'):
        return SVC(
            C = 1,
            kernel = kernel,
            class_weight = 'auto'
        )
    
    def fit(self, stories, labels):
        pass
    
    def predict(self, stories):",Python/Code/web/monitor/classifiers/static/base.py,siddhantgoel/thesis,1
"csvFile = pandas.read_csv('Datasets/test_metadata_clean_category_balanced.csv')
data = numpy.array(csvFile)
X_test = data[:,2:-1]
y_test = data[:,-1]

X_train_scaled = preprocessing.scale(X_train)
X_test_scaled = preprocessing.scale(X_test)


print '\nValidation of the rbf kernel'
rbftest = svm.SVC(C=100, kernel='rbf', gamma=0.001)
rbftest.fit(X_train_scaled, y_train)
pred = rbftest.predict(X_test_scaled)
print rbftest.support_vectors_
print rbftest.support_vectors_.shape
print sklearn.metrics.confusion_matrix(y_test, pred)
print sklearn.metrics.accuracy_score(y_test, pred)

print '\nValidation of the polynomial kernel'
polytest = svm.SVC(C=10, kernel='poly', degree=3)",SVM_test.py,murq/movie-datamining,1
"                ""RBF SVM"",
                ""Gaussian Process"",
                ""Decision Tree"",
                ""Random Forest"",
                ""Neural Net"",
                ""AdaBoost"",
                ""Naive Bayes"",
                ""QDA""]
            classifiers = [
                KNeighborsClassifier(3),
                SVC(kernel=""linear"", C=0.025),
                SVC(gamma=2, C=1),
                GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
                DecisionTreeClassifier(max_depth=5),
                RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                MLPClassifier(alpha=1),
                AdaBoostClassifier(),
                GaussianNB(),
                QuadraticDiscriminantAnalysis()]
            for name, clf in zip(names, classifiers):",rf.py,schollz/find,1
"    test_set = H5PYDataset(
        dataset_path, which_sets=('test',), sources=('features', 'targets'),
        load_in_memory=True)
    test_features, test_targets = test_set.data_sources

    if use_c is None:
        best_error_rate = 1.0
        best_C = None
        for log_C in numpy.linspace(log_min, log_max, num_steps):
            C = numpy.exp(log_C)
            svm = LinearSVC(C=C)
            svm.fit(train_features, train_targets.ravel())
            error_rate = 1 - numpy.mean(
                [svm.score(valid_features[1000 * i: 1000 * (i + 1)],
                           valid_targets[1000 * i: 1000 * (i + 1)].ravel())
                 for i in range(10)])
            if error_rate < best_error_rate:
                best_error_rate = error_rate
                best_C = C
            print('C = {}, validation error rate = {} '.format(C, error_rate) +",experiments/semi_supervised_svhn.py,IshmaelBelghazi/ALI,1
"
        # --- SVM smote
        # Unlike the borderline variations, the SVM variation uses the support
        # vectors to decide which samples are in danger (near the boundary).
        # Additionally it also introduces extrapolation for samples that are
        # considered safe (far from boundary) and interpolation for samples
        # in danger (near the boundary). The level of extrapolation is
        # controled by the out_step.
        if kind == 'svm':
            # Store SVM object with any parameters
            self.svm = SVC(random_state=self.random_state, **self.kwargs)

    def fit(self, X, y):
        """"""Find the classes statistics before to perform sampling.

        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
            Matrix containing the data which have to be sampled.
",imblearn/over_sampling/smote.py,dvro/UnbalancedDataset,1
"	print testX.shape

	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)

	#fit the svm model and compute accuaracy measure
	#clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	#clf = svm.SVC(kernel='linear')
	clf = svm.SVC(kernel=kernel.arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",kpcaWithUVFS/mnistBackImage/mnistIMAGE.py,akhilpm/Masters-Project,1
"grid = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001, 0.0001], 'kernel': ['rbf']},
 ]

grid = [
  {'C': [100], 'gamma': [0.01], 'kernel': ['rbf']}, 
  {'C': [1000], 'gamma': [0.001], 'kernel': ['rbf']}
 ]

model = svm.SVC()

#SVC needs feature scaling
scaler = preprocessing.StandardScaler().fit(train_x)
train_x = scaler.transform(train_x)
test_x  = scaler.transform(test_x)


#GradientBoostingClassifier
grid = [{'loss': ['deviance', 'exponential'],",pipeline/training.py,edublancas/titanic,1
"	iris = load_iris()
	X = iris.data
	y = iris.target
	#To make sure there are only two classes and then shuffling
	data=np.c_[X[:100,:],y[:100]]
	np.random.shuffle(data)
	X = data[:,:-1]
	y = data[:,-1]	
	#print(X.shape)
	#print(y)
	clf= SVC(probability=True)
	clf.fit(X[:50,:],y[:50])
	
	print(""KS p value : "",p_value_scoring_object(clf,X[50:,:],y[50:]))
	print(""AD p value : "",p_value_scoring_object_AD(clf,X[50:,:],y[50:]))

",Dalitz_simplified/p_value_scoring_object.py,weissercn/MLTools,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in getargspec(estimator.fit)[0]


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",net-p3/lib/python3.5/site-packages/sklearn/utils/validation.py,uglyboxer/linear_neuron,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",projects/scikit-learn-master/examples/applications/svm_gui.py,DailyActie/Surrogate-Model,1
"from sklearn.svm.classes import LinearSVC

from ..Classifier import Classifier
from ...language.Java import Java


class LinearSVCJavaTest(Java, Classifier, TestCase):

    def setUp(self):
        super(LinearSVCJavaTest, self).setUp()
        self.mdl = LinearSVC(C=1., random_state=0)

    def tearDown(self):
        super(LinearSVCJavaTest, self).tearDown()",tests/classifier/LinearSVC/LinearSVCJavaTest.py,nok/sklearn-porter,1
"            self.assertEqual(2, len(lines))

        os.remove(feedback_file)

    def test_predict(self):
        digits = datasets.load_digits()

        from sklearn import svm
        from sklearn import cross_validation

        clf = svm.SVC(gamma=0.001, C=100)
        clf = clf.fit(digits.data, digits.target)
        cross_validation.cross_val_score(clf, digits.data[:-1], digits.target[:-1], cv=5)
        predicted = clf.predict(digits.data[-1])",tests/test_machine_loader.py,k-nish/number_recognizer,1
"np.random.shuffle(X)

np.random.seed(42)
np.random.shuffle(y)





# creating the classifier
clf = svm.SVC()

# trainning the classifier with 9000 examples
clf.fit(X[:9000], y[:9000])


# verifying the accuracy for the model
predicted = clf.predict(X[9000:])
print accuracy_score(predicted, y[9000:])",src/examples/3-linear-svm.py,daniellima/cifar-10-image-recognition,1
"            identify_value.append(0)
            identity_len += 1
        while identity_len > 11:
            identify_value.pop()
            identity_len -= 1
        trainset.append(identify_value)

    X = np.array(trainset)
    y = np.array(label)

    clf = SVC()
    clf.fit(X, y)
    joblib.dump(clf, DATA_FILE_NAME)

    print(""Finish!"")
    ",src/Model/generate.py,zeqing-guo/CaptchaReader,1
"



def fit_model_linear(data, truth, Chere=10e5):  
    ''' Linear Support Vector Classification.
    http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html
    Dual select the algorithm to either solve the dual or primal optimization problem. 
    Prefer dual=False when n_samples > n_features. '''

    model = sklsvm.LinearSVC(dual=False, C=Chere) 

    model = model.fit(data, truth) 

    return model




def fit_model_SVC(data, truth):",MLNet-2.0/classifiers/svm/src/main.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"
    @staticmethod
    def SklearnLogisticRegression():
        return SklearnClassifierWrapper(LogisticRegression)

    @staticmethod
    def SklearnSGDClassifier():
        return SklearnClassifierWrapper(lambda: SGDClassifier(loss='log'))

    @staticmethod
    def SklearnSVC():
        return SklearnClassifierWrapper(lambda : SVC(probability=True))

    @staticmethod
    def SklearnLinearSVC():
        return SklearnClassifierWrapper(LinearSVC)

    @staticmethod
    def SklearnNuSVC():
        return SklearnClassifierWrapper(lambda : NuSVC(probability=True))",analysis/textclassification/SklearnClassifierFactory.py,dhermyt/WONS,1
"
    @abstractmethod
    def get_workflow(self):
        """"""Retrieve workflow""""""
        return None


class WFExample1(WorkflowExample):

    def get_workflow(self):
        wf = Methods(*[SVC(kernel=""linear"", C=C) for C in [1, 3]])
        return wf


class WFExample2(WorkflowExample):

    def get_workflow(self):
        ####################################################################
        ## EPAC WORKFLOW
        # -------------------------------------",epac/tests/wfexamples2test.py,neurospin/pylearn-epac,1
"#%%
train_data = df.values

#%%
Xtrain = train_data[:,2:]
ytrain = train_data[:,1]

#%%
from sklearn import svm
from sklearn.metrics import accuracy_score
clf = svm.SVC()
clf.fit(Xtrain, ytrain)
ypred = clf.predict(Xtrain)

#%%
print accuracy_score(ytrain, ypred)
",Titanic/Titanic.py,shengshuyang/WindowsProjects,1
"    return X, y


def plot_rbf_svm_parameters():
    X, y = make_handcrafted_dataset()

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, C in zip(axes, [1e0, 5, 10, 100]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='rbf', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)

    fig, axes = plt.subplots(1, 4, figsize=(15, 3))
    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])
        svm = SVC(gamma=gamma, kernel='rbf', C=1).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""gamma = %f"" % gamma)",day3-machine-learning/plots/plot_rbf_svm_parameters.py,bsipocz/AstroHackWeek2015,1
"    
    #############
    # Train SVM #
    #############
	if (not exists(conf.modelPath)) | OVERWRITE:
		if VERBOSE: print str(datetime.now()) + ' training liblinear svm'
		if VERBOSE == 'SVM':
			verbose = True
		else:
			verbose = False
		clf = svm.LinearSVC(C=conf.svm.C)
		if VERBOSE: print clf
		clf.fit(train_data, all_images_class_labels[selTrain])
		with open(conf.modelPath, 'wb') as fp:
			dump(clf, fp)
	else:
		if VERBOSE: print 'loading old SVM model'
		with open(conf.modelPath, 'rb') as fp:
			clf = load(fp)
",phow_train.py,md100play/BirdID,1
"'''

if __name__=='__main__':
    #input Feature & output label
    X,Y = excel_data(Train_File_name)
    X_norm=X_normalized = normalize(X, norm='l2')

    #Draw(Y,X)
    
    #Now Create & Train Our Classifier
    clsf=SVC(kernel='rbf',gamma=1,C=1) #SVM Classier
    print 'Training Started ..'
    a = datetime.datetime.now()
    clsf.fit(X_norm, Y)
    b=datetime.datetime.now()
    print 'Training Is completed, Time taken for training :',b-a
    
    #Now Load Testing dataset
    P,Q=excel_data(Test_file_name)
    ",SVM Classifier/Image Source Classification/classifier.py,sudhanshuptl/Machine-Learning,1
"				feature_vectors = np.array(feature_vectors)
				print(""*** FITTING SVR MODEL ***"")

				# This classifier supports nonleniar models, applies a kernel, uses libsvm implementation,
				# clf = SKSVR(verbose=True)

				# print(""Vectors size are {}"".format(feature_vectors.shape))

				# Using grid search for fitting hyper barameters (Penalty, C)
				tuned_parameters = [{'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 1, 10, 100, 1000]}]
				#grid_clf = GridSearchCV(LinearSVC(dual=False, tol=0.0001, random_state=41), tuned_parameters, cv=3, scoring='recall', n_jobs=-1)
				grid_clf = SKSVR(verbose=True)
				grid_clf.fit(feature_vectors, labels)
				# print(""Best parameters set found on development set: {}"").format(grid_clf.best_estimator_)

				# Using LinnearSVC instead of SVC, it uses the implementation of liblinear, should be faster for linear models.
				# Setting the classifier with the best parameters found in gridsearch
				# clf = LinearSVC(penalty=grid_clf.best_params_['penalty'], loss='squared_hinge', dual=False,C=grid_clf.best_params_['C'], random_state=41)
				# Learning the model
",lib/svr.py,anasalzogbi/SciPRec,1
"    n_samples = 100
    X, y = make_classification(n_samples=2 * n_samples, n_features=6,
                               random_state=42)

    sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))
    X_train, y_train, sw_train = \
        X[:n_samples], y[:n_samples], sample_weight[:n_samples]
    X_test = X[n_samples:]

    for method in ['sigmoid', 'isotonic']:
        base_estimator = LinearSVC(random_state=42)
        calibrated_clf = CalibratedClassifierCV(base_estimator, method=method)
        # LinearSVC does not currently support sample weights but they
        # can still be used for the calibration step (with a warning)
        msg = ""LinearSVC does not support sample_weight.""
        assert_warns_message(
            UserWarning, msg,
            calibrated_clf.fit, X_train, y_train, sample_weight=sw_train)
        probs_with_sw = calibrated_clf.predict_proba(X_test)
",projects/scikit-learn-master/sklearn/tests/test_calibration.py,DailyActie/Surrogate-Model,1
"useAditional = True
dataAugmentation = True

clasif = ""SVM""  # RF/SVM
SEPARATOR = ""============================================================="" + \
            ""===================""


def svc(traindata, trainlabel, validData, validLabel, testData):
    print(""Start training SVM...\n"" + SEPARATOR)
    svcClf = SVC(kernel=""rbf"", verbose=True, decision_function_shape='ovo', probability=True, cache_size=1500)
    svcClf.fit(traindata, trainlabel)

    score = svcClf.score(validData, validLabel)
    sleep(1)
    print(""Mean validation accuracy: "" + str(score))
    pred_test = svcClf.predict_proba(testData)

    return pred_test
",Python-scripts/CNN-to-Machine-Learning.py,mmaguero/Intel-mobileodt-cervical-cancer-screening,1
"from sklearn.tree import DecisionTreeClassifier

# Loading some example data
iris = datasets.load_iris()
X = iris.data[:, [0, 2]]
y = iris.target

# Training classifiers
clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = KNeighborsClassifier(n_neighbors=7)
clf3 = SVC(kernel='rbf', probability=True)
eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),
                                    ('svc', clf3)],
                        voting='soft', weights=[2, 1, 2])

clf1.fit(X, y)
clf2.fit(X, y)
clf3.fit(X, y)
eclf.fit(X, y)
",projects/scikit-learn-master/examples/ensemble/plot_voting_decision_regions.py,DailyActie/Surrogate-Model,1
"    # data_X_test = X_test[:int(DATA_SIZE*0.8)]
    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]

    print ""Train data size: "" + str(int(DATA_SIZE*0.8))
    print ""Test  data size: "" + str(len(X_target)-int(DATA_SIZE*0.8))


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))
    regr = OneVsRestClassifier(linear_model.LinearRegression())

    # train the model using the training data
    # regr.fit(X_train, X_target)
    regr.fit(data_X_train, data_Y_train)

    # predict to test
    print ""Predicting...""
    test_result = regr.predict(data_X_test)",DST_v1.01.py,totuta/deep-supertagging,1
"        if isinstance(X, spmatrix):
            indices = np.arange(len(r))
            r_sparse = coo_matrix(
                (r, (indices, indices)),
                shape=(len(r), len(r))
            )
            X_scaled = X * r_sparse
        else:
            X_scaled = X * r

        lsvc = LinearSVC(
            C=self.C,
            fit_intercept=self.fit_intercept,
            max_iter=10000
        ).fit(X_scaled, y)

        mean_mag =  np.abs(lsvc.coef_).mean()
        coef_ = (1 - self.beta) * mean_mag * r + self.beta * (r * lsvc.coef_)
        intercept_ = (1 - self.beta) * mean_mag * b + self.beta * lsvc.intercept_
",fsa/model/nbsvm.py,bobflagg/sentiment-analysis,1
"        else:
            self.logger = logger

    def transform(self, X):
        return Misc.exclude_cols(X, self.excluded_features)

    def get_min_C(self, X, y):
        high = 1
        low = 0
        while (True):
            learner = sklearn.svm.LinearSVC(penalty = 'l1',
                dual = False, C = high, verbose=False,  class_weight='auto')
            learner.fit(X, y)
            t_fc = np.sum(learner.coef_ != 0)
            if (t_fc > 1):
                high = (low + high) / 2
            elif (t_fc < 1):
                low = high
                high = high * 2
            else:",RatBoost/learner.py,adrinjalali/Network-Classifier,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",ext_lib/email/test/test_email.py,sugarguo/Flask_Blog,1
"print(len(X_train), 'train sequences')
print(len(X_test), 'test sequences')
print(""Pad sequences (samples x time)"")
X_train = sequence.pad_sequences(X_train, maxlen=maxlen)
X_test = sequence.pad_sequences(X_test, maxlen=maxlen)
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)
x = X_train
y = y_train
print('Build model...')
modle = svm.SVC()
modle.fit(x, y)
predict_result = modle.predict(X_test)
accurate = 0.
num = 0.
total = len(y_test)
for i in range(len(y_test)):
    if predict_result[i] == y_test[i]:
        num += 1
    else:",my_svm_sid.py,daleloogn/mython,1
"    ""Sepal_Length"",
    ""Sepal_Width"",
    ""Petal_Length"",
    ""Petal_Width""
]

# import the iris training set
irisDF = pd.read_csv(os.path.join(args.indir, ""iris.csv""), names=cols)

# fit the model
svc = svm.SVC(kernel='linear', C=1.0).fit(irisDF[features], irisDF[""Species""])

# output a text description of the model
f = open(os.path.join(args.outdir, 'model.txt'), 'w')
f.write(str(svc))
f.close()

# persist the model
joblib.dump(svc, os.path.join(args.outdir, 'model.pkl'))",doc/examples/ml/iris/python/iris-train-python-svm/pytrain.py,pachyderm/pachyderm,1
"    X = preprocessing.scale(X)

    return X,y


def Analysis():
    test_size = 1000
    X, y = Build_Data_Set()
    print(len(X))

    clf = svm.SVC(kernel=""linear"", C=1.0)
    clf.fit(X[:-test_size], y[:-test_size])

    correct_count = 0

    for x in range(1, test_size + 1):
        if clf.predict(X[-x])[0] == y[-x]:
            correct_count += 1

    print(""Accuracy:"", (correct_count / test_size) * 100.00)",Regression/linear-svc-machine-learning.py,Vaibhav/Stock-Analysis,1
"                         ""my_data_processed/acceleration_train_y.csv"",
                         ""my_data_processed/acceleration_train_z.csv"",
                         ""my_data_processed/gyroscope_train_x.csv"",
                         ""my_data_processed/gyroscope_train_y.csv"",
                         ""my_data_processed/gyroscope_train_z.csv"")

data = ndarray(shape=(len(data), 21), dtype=float, buffer=np.asanyarray(data))
data = data[:, 0:12]
target = ndarray(shape=(len(target),), dtype=int, buffer=np.asanyarray(target))

clf = svm.SVC()
clf.fit(data, target)

target, data = load_data(""my_data_processed/activity_test.csv"",
                         ""my_data_processed/acceleration_test_x.csv"",
                         ""my_data_processed/acceleration_test_y.csv"",
                         ""my_data_processed/acceleration_test_z.csv"",
                         ""my_data_processed/gyroscope_test_x.csv"",
                         ""my_data_processed/gyroscope_test_y.csv"",
                         ""my_data_processed/gyroscope_test_z.csv"")",Activity_and_Context_Recognition/Classifier/test_dataset_acceleration.py,VizLoreLabs/LCI-FIC2-SE,1
"            self._crossvalidation.crossvalidate(\
                scaled_cv_feasibles, scaled_cv_infeasibles)

        # @todo WARNING maybe rescale training feasibles/infeasibles (!) 
        fvalues = [f.value for f in self._selected_feasibles]
        ivalues = [i.value for i in self._selected_infeasibles]

        points = ivalues + fvalues
        labels = [-1] * len(ivalues) + [1] * len(fvalues) 

        self._clf = svm.SVC(kernel = 'linear', C = self._best_parameter_C, tol = 1.0)
        self._clf.fit(points, labels)  
        self.logger.log()

        return True

    def get_normal(self):
        # VERY IMPORTANT
        w = self._clf.coef_[0]
        nw = w / sqrt(sum(w ** 2))",evopy/metamodel/svc_linear_meta_model.py,jpzk/evopy,1
"
def main():
    kernel = sys.argv[1]
    interp = sys.argv[2]

    print('Loading data')
    X_train, y_train, X_test = mnist.load_kaggle()
    X_train = mnist.shrink_data_dims(X_train, 14, interp=interp)

    gamma = 1E-6 if X_train.shape[1] < 200 else 1E-7
    model = SVC(kernel=kernel) if kernel == 'poly' else SVC(kernel=kernel, gamma=gamma)

    print('Training')
    model.fit(X_train, y_train)

    print('Saving')
    mnist.save_model('svn_%s_%s_%i' % (kernel, interp, int(time.time())), model)
        
    return 0
",train_svm.py,notkarol/kaggle_mnist,1
"
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('false positive rate')
plt.ylabel('true positive rate')
plt.title('Receiver Operator Characteristic')
plt.legend(loc=""lower right"")
plt.show()

pipe_svc = Pipeline([('scl', StandardScaler()), \
        ('clf', SVC(random_state=1))])

pipe_svc = pipe_svc.fit(X_train2, y_train)
y_pred2 = pipe_svc.predict(X_test[:, [4, 14]])

print('ROC AUC: %.3f' % roc_auc_score( \
        y_true=y_test, y_score=y_pred2))
print('Accuracy: %.3f' % accuracy_score( \
        y_true = y_test, y_pred = y_pred2))",HPTuning/roc_curve.py,southpaw94/MachineLearning,1
"

def test_data_size(training_features, training_classes, test_features, test_classes):
    index = np.arange(0, len(training_classes))
    np.random.shuffle(index)
    test_size = np.linspace(0.1, 1, 50) * len(index)
    test_size = [int(i) for i in test_size]
    f_train = []
    f_cv = []

    clf = svm.SVC(C=1.833, gamma=0.1366, cache_size=1000)

    for iii in test_size:
        clf.fit(training_features[index[0:iii]], training_classes[index[0:iii]])

        f_train = np.append(f_train, np.mean(F1Score(training_features[index[0:iii]],
                                                     training_classes[index[0:iii]], clf).values()))
        f_cv = np.append(f_cv, np.mean(F1Score(test_features, test_classes, clf).values()))

    return f_train, f_cv, test_size",Evaluation/testDataSize.py,andimarafioti/AIAMI,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0520_2015.py,magic2du/contact_matrix,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Lib/test/test_email/test_email.py,ArcherSys/ArcherSys,1
"    test = pd.read_csv(""parsed_test.csv"")

    y_train = train[""fault_severity""]
    X_train = train.ix[:, 1:-1]
    X_test = test.ix[:, 1:]

    X_train = np.array(X_train)
    X_test = np.array(X_test)
    y_train = np.array(y_train)

    clf = SVC(kernel='poly', degree=9, probability=True)
    logging.info(""Beginning training"")
    clf.fit(X_train, y_train)
    logging.info(""Done training"")

    print ()

    logging.info(""Beginning prediction"")
    predictions = pd.DataFrame(clf.predict_proba(X_test),
                               columns=[""predict_0"", ""predict_1"", ""predict_2""])",Telstra Network Disruptions/SVM.py,nickmarton/kaggle,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",venv/lib/python2.7/site-packages/nltk/parse/transitionparser.py,MyRookie/SentimentAnalyse,1
"df.drop(['open'], 1, inplace=True)

addDailyReturn(df)

X = np.array(df.drop(['UpDown'],1))
y = np.array(df['UpDown'])
print y

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)

clf = svm.SVC() 
clf.fit(X_train,y_train)

accuracy = clf.score(X_test,y_test)

print accuracy

test_set = np.array([[104,106]])

prediction = clf.predict(test_set)",ifc/ml/sklearn_svm.py,Darthone/bug-free-octo-parakeet,1
"        self.assertIs(df.pipeline.Pipeline, pipeline.Pipeline)
        self.assertIs(df.pipeline.FeatureUnion, pipeline.FeatureUnion)
        self.assertIs(df.pipeline.make_pipeline, pipeline.make_pipeline)
        self.assertIs(df.pipeline.make_union, pipeline.make_union)

    def test_Pipeline(self):

        iris = datasets.load_iris()
        df = pdml.ModelFrame(iris)

        estimators1 = [('reduce_dim', df.decomposition.PCA()), ('svm', df.svm.SVC())]
        pipe1 = df.pipeline.Pipeline(estimators1)

        estimators2 = [('reduce_dim', decomposition.PCA()), ('svm', df.svm.SVC())]
        pipe2 = pipeline.Pipeline(estimators2)

        df.fit(pipe1)
        pipe2.fit(iris.data, iris.target)

        result = df.predict(pipe1)",pandas_ml/skaccessors/test/test_pipeline.py,pandas-ml/pandas-ml,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_backward.svm_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeature/example/test_svm_backward.py,jundongl/scikit-feature,1
"    print(label_)

    X = sklearn.preprocessing.scale([d['feature'] for d in train_['data']])
    Y = [d['class'] for d in train_['data']]
    # X = X[500:1000]
    # Y = Y[500:1000]
    print(len(X))
    print(X.shape)
    # print(train_['data'][:50])

    lin_clf = svm.LinearSVC(max_iter=1000, verbose=1, dual=False)
    lin_clf.fit(X, Y)

    X = sklearn.preprocessing.scale([d['feature'] for d in test_['data']])
    Y = [d['class'] for d in test_['data']]
    # X = X[500:1000]
    # Y = Y[500:1000]
    accuracy = lin_clf.score(X, Y)

    print('Accuracy(N=%d) = %f' % (len(X), accuracy))",source/main/svm_classify.py,memray/cs2770,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",bp-revok/python/lib/python2.7/email/test/test_email.py,fkolacek/FIT-VUT,1
"    
    #X_val = np.vstack(X_val)
    #y_val = np.concatenate(y_val)

    n_features = X_train.shape[1]   
    print ""XX:"", n_features

    g =   1.0/float((3*n_features))
    print g
 
    clf_lsvm = svm.SVC(kernel='linear', C=10)
    clf_ksvm = svm.SVC(kernel='rbf', C=10, gamma=g )
    clf_lasso = LogisticRegression(C=1000,penalty='l1',random_state=0)
    clf_ridge = LogisticRegression(C=0.01,penalty='l2',random_state=0) 
    clf_rf = RandomForestClassifier(n_estimators=850, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf_etree = ExtraTreesClassifier(n_estimators=1000, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"", n_estimators=500, random_state=74494, learning_rate=0.8) 
    clf_gboost = GradientBoostingClassifier(n_estimators=200, random_state=74494, learning_rate=0.2) 

    print ""Training.""",python_scripts_from_net/ensemble.py,sankar-mukherjee/DecMeg2014,1
"        y_train_bin = dm.data_bin_organize(trainset_l, y)
        y_devel_bin = dm.data_bin_organize(develset_l, yd)

        curSupervecPath_bin = os.path.join(supervecPath, ""trainset_"" + str(fold), str(nMixtures_bin));
        #TODO LOAD FEATURES
        trainFeatures = utl.readfeatures(curSupervecPath_bin, y)
        testFeatures = utl.readfeatures(curSupervecPath_bin, yd)
        trainClassLabels = y_train_lab
        scaler = preprocessing.MinMaxScaler(feature_range=(-1,1));
        scaler.fit(trainFeatures);
        svm = SVC(C=C_bin, kernel='rbf', gamma=gamma_bin);
        svm.fit(scaler.transform(trainFeatures), trainClassLabels);
        predLabels = svm.predict(scaler.transform(testFeatures));

        print ""Multiclass Predictions""
        C_class = Cs_class[fold - 1];
        gamma_class = gammas_class[fold - 1];

        curSupervecPath_class = os.path.join(supervecPath, ""trainset_"" + str(fold), str(nMixtures_class));
        # TODO LOAD FEATURES",Supervectors/hier_class_test.py,vespero89/Snoring_Challenge,1
"    neigh = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)

    neigh.fit(X, y) 
    pred_labels = neigh.predict(X_test)
    return pred_labels


def eval_svc(X, y, X_test, conf, whitening = False, normalize = True, svm_opts_dict = svm_opts):
    num_classes = len(np.unique(y))

    svc = sklearn.svm.SVC(**svc_opts)

    if normalize:
        min_max_scaler = sklearn.preprocessing.MinMaxScaler()
        X = min_max_scaler.fit_transform(X)
        X_test = min_max_scaler.fit_transform(X_test)

    svc.fit(X, y)

    pred_labels = svc.predict(X_test)",src/HONHelpers.py,voanna/Deep-Features-or-Not,1
"    :return: results: a list of tuples (instance_id, label) where labels are predicted by the best classifier
    '''

    results = []


    # implement your code here

    # SVM
    # for English, set C=.10,others 1.0
    svm_clf = svm.LinearSVC(C=.10, verbose=0, random_state=0)
    # knn_clf = neighbors.KNeighborsClassifier(14)
    # the label encoder seems not necessary
    label_encoder = preprocessing.LabelEncoder()
    label_encoder.fit(y_train.values())
    # FATAL & VITAL!!!
    # DictVectorizer `REORDERS` key values in the X_train `DICTIONARY`
    _y_train = map(lambda key: y_train[key], X_train.keys())

    X_train_arr = sparse.csr_matrix(X_train.values())",coursera/nlpintro-001/Assignment3/B.py,Alexoner/mooc,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_local_test.py,magic2du/contact_matrix,1
"                    assert(name in colnames)
                except AssertionError:
                    raise AssertionError('ERROR: \'%s\' not found in %s' % (name, ', '.join(colnames)))

            assert(np.all(TEST_X == x))

            assert(np.all(TEST_Y == y))

            # generate and test the mRMR portion
            mrmr = MRMR(
                estimator=SVC(kernel='linear'),
                n_features_to_select=ARGS.NUM_FEATURES,
                method=ARGS.MRMR_METHOD,
                normalize=ARGS.MRMR_NORMALIZE,
                similar=ARGS.SIMILAR
                )

            mrmr.fit(x, y)

    finally:",idepi/test/_discrete.py,nlhepler/idepi,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in signature(estimator.fit).parameters


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",phy/lib/python2.7/site-packages/sklearn/utils/validation.py,marcsans/cnn-physics-perception,1
"    has_all = 'all' in cmd_class
    if has_all or 'svm' in cmd_class:
        # see http://scikit-learn.org/stable/modules/svm.html#classification
        kernels = args.svm_kernels
        if 'all' in kernels:
            kernels = SVM_KERNELS
        for k in kernels:
            if k == 'linear-ovr':
                classifiers[
                    'SVC kernel=linear OvR'] = multiclass.OneVsRestClassifier(
                    svm.SVC(kernel='linear'))
            elif k == 'linear-svc':
                classifiers['Linear SVC'] = svm.LinearSVC()
            else:
                if args.svm_gamma is None:
                    g = 0.0
                    if k == 'sigmoid':
                        # TODO: Document this magic number
                        # Maximum dot product of the vectors in our data set
                        g = 1.0 / 962.0",lird.py,vsaw/LIRD,1
"        # No need to reverse, we have done that in feaure level
        #x, y = y, x
        y_type = type_map.loc[name][""A type""]
    else:
        # normal direction
        y_type = type_map.loc[name][""B type""]


   
    if(y_type == ""Binary"" or y_type == ""Categorical""):
        clf = SVC()
    else:
        clf = SVR()

    x_arr = np.array(x, ndmin=2)
    X = x_arr.transpose()


    scaler = preprocessing.StandardScaler().fit(X)      
    X  = scaler.transform(X)",features/feature_functions_spearman.py,ssamot/causality,1
"		labels.append(0)

	if hard_neg:
		for feat_path in glob.glob(os.path.join(""features/hardneg"",""*.feat"")):
			print feat_path
			fd = joblib.load(feat_path)
			fds.append(fd)
			labels.append(0)


	clf = LinearSVC()
	clf.fit(fds, labels)

	if not os.path.isdir(os.path.split(model_path)[0]):
		os.makedirs(os.path.split(model_path)[0])
	joblib.dump(clf, model_path)
	print ""Classifier saved""

def hard_negative_mining():
	neg_im_path = ""data/negative_hard""",HOG/Trainer.py,nikv96/Feature-Detection,1
"    print ""Split Num = %d"" % i
    d2v_model.train_test_split(i)  #  len(train) = 9587 len(test) = 1084
    d2v_model.count_data()

    d2v_model.create_bag_of_centroids(w2v_model) # 75.3 c=0.1 word vec bag of centroids
    # d2v_model.create_bag_of_centroids(w2v_model, cre_adjust=True) # 76.2 c=0.1 word vec cre tfidf bag of centroids

    # d2v_model.cre_sim_doc_vecs(w2v_model)
    # d2v_model.get_new_bag_of_words(w2v_model)

    text_clf = LinearSVC(C=1)
    _ = text_clf.fit(d2v_model.train_doc_vecs, d2v_model.train_labels)
    perf = text_clf.score(d2v_model.test_doc_vecs, d2v_model.test_labels) 
    perf2 = text_clf.score(d2v_model.train_doc_vecs, d2v_model.train_labels)
    print "" Train accuracy:"" + str(perf2)
    print "" Test accuracy:"" + str(perf)

    # clf = SVC(kernel='rbf',C=c, gamma=g).fit(train, train_Label)
    # perf2 = clf.score(train, train_Label)
    # perf = clf.score(test, test_Label)",sentiment-analysis-w2v-kmean.py,linbojin/dv4sa,1
"                self.features.append(lambda x,c=const,r=relation: 1 if is_in_relation(x, self.relations[r],r,c) else 0)
        
        if len(self.features)==0:
            self.query_tree.justify='leafed(thresh/constistant)'
            self.query_tree.chosen_query=None
            return
        
        self.table= zeros((len(self.objects), len(self.features)))
        for j,new_feature in enumerate(self.features):
            self.table[:, j]= array([new_feature(ent) for ent in self.objects])
        clf = SVC(kernel='linear', C=10)
        clf.fit(self.table, self.tagging)
        
        self.query_tree=clf #my SVM classifier #root
        self.is_svm=True
        
        #self.logfile.write(' '*len(self.transforms)+'training done. num_nodes: '+str(num_nodes)+'. depth: '+str(depth)+'\n')
    
    def predict(self, entities, flag=False):  
        #if depth > 0, object is an entity list. otherwise, problem!!!!",problems/alg10_ohsumed_nontree.py,lioritan/Thesis,1
"            is maximized. """"""

        tuned_parameters = [{
            'kernel': ['rbf'], 
            'gamma': self._gamma_range,
            'C': self._C_range}]

        X = array([f.value for f in feasible] + [i.value for i in infeasible])
        y = array([1] * len(feasible) + [-1] * len(infeasible))

        clf = GridSearchCV(SVC(), tuned_parameters, cv=self._cv_method)

        clf.fit(X, y)
        best_accuracy = clf.best_score_

        return (feasible,
            infeasible,
            clf.best_estimator_.C,
            clf.best_estimator_.gamma,
            best_accuracy)",evopy/metamodel/cv/svc_cv_sklearn_grid_rbf.py,jpzk/evopy,1
"
alphabet = string.ascii_letters + string.digits

_atoi = dict((x, i) for (i, x) in enumerate(alphabet))
_itoa = dict((i, x) for (i, x) in enumerate(alphabet))



class Classifier(object):
    def __init__(self):
        self._svc = svm.SVC(scale_C=True)


    def train(self, pairs):
        """"""
        Trains the classifier with a number of (image, solution) pairs.
        """"""
        images, solutions = zip(*pairs)
        flattened = _flatten(images)
        indices = [_atoi[solution] for solution in solutions]",craptcha/classify.py,lvh/craptcha,1
"
print train_df.columns
print 'Training...'
#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(n_estimators=100)#0.79

#from sklearn.neighbors import KNeighborsClassifier  
#clf=KNeighborsClassifier(n_neighbors=7)#0.7037

#from sklearn import svm   
#clf=svm.SVC(C=10,gamma=0.0029)#0.7878
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))

#from sklearn.naive_bayes import GaussianNB      #nb for 高斯分布的数据
#clf=GaussianNB() #0.7867       

#from sklearn.linear_model import LogisticRegression
#clf = LogisticRegression(C=1,penalty='l2',tol=0.0001)#0.7946",myfirstforest.py,zlykan/my_test,1
"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=random_state)

print('Data shape:')
print('X_train: %s, X_valid: %s, X_test: %s \n' % (X_train.shape, X_valid.shape, X_test.shape))

# First layer (individual classifiers)

# defining the classifiers
clfs = {
    'LR': LogisticRegression(random_state=random_state),
    'SVM': SVC(probability=True, random_state=random_state),
    'RF': RandomForestClassifier(n_estimators=100, n_jobs=3, random_state=random_state),
    'GBM': GradientBoostingClassifier(n_estimators=50, random_state=random_state),
    'ETC': ExtraTreesClassifier(n_estimators=100, n_jobs=3, random_state=random_state),
    'KNN': KNeighborsClassifier(n_neighbors=30),
    'XGB': XGBClassifier(n_estimators=100, seed=random_state)
}

# predictions on the validation and test sets
p_valid = []",python/model_architecture.py,suresh/notes,1
"X = nifti_masker.fit_transform(func_filename)
# Apply our condition_mask
X = X[condition_mask]
session = session[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",examples/decoding/plot_haxby_anova_svm.py,salma1601/nilearn,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5, 4.5, 4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0, 10.0, 10.0, 10.0]]),",evopy/examples/problems/SchwefelsProblem240/ORIDSESAlignedSVC.py,jpzk/evopy,1
"Y_train = training_data['Survived']
X_test = test_data.copy()

# Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, Y_train)
Y_pred = logreg.predict(X_test)
acc_log = round(logreg.score(X_train, Y_train) * 100, 2)

# Support Vector Machines
svc = SVC()
svc.fit(X_train, Y_train)
Y_pred = svc.predict(X_test)
acc_svc = round(svc.score(X_train, Y_train) * 100, 2)

# k-Nearest Neighbors
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, Y_train)
Y_pred = knn.predict(X_test)
acc_knn = round(knn.score(X_train, Y_train) * 100, 2)",titanic_twomodels_S.py,michael-hoffman/titanic,1
"
        # --- SVM smote
        # Unlike the borderline variations, the SVM variation uses the support
        # vectors to decide which samples are in danger (near the boundary).
        # Additionally it also introduces extrapolation for samples that are
        # considered safe (far from boundary) and interpolation for samples
        # in danger (near the boundary). The level of extrapolation is
        # controled by the out_step.
        if self.kind == 'svm':
            # Store SVM object with any parameters
            self.svm = SVC(random_state=self.random_state, **self.kwargs)",imblearn/over_sampling/smote.py,dvro/imbalanced-learn,1
"        data[i,j] = re.sub('[^AZERTYUIOPQSDFGHJKLMWXCVBNazertyuiopqsdfghjklmwxcvbn]', '', data[i,j]).lower()
        
# Delcaration des pre-processing / classifier
ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=0)
transformer = TfidfTransformer()

# Chaque paramètres représente la meilleur performance de l'ensemble
classifiers = [
    SGDClassifier(alpha = 1e-4),
    DecisionTreeClassifier(max_depth=None),
    SVC(gamma=2, C=1),
    RandomForestClassifier(n_estimators=60),
    AdaBoostClassifier()]
    

Result = numpy.empty((0,3), float)

# Boucle sur les classifiers
for clf in classifiers:
",BIM_Classifier.py,HugoMartin78/bim-classifier,1
"############################
# Setup
############################
print ""Setup""
# Multi-output Regressior
from sklearn.linear_model import SGDRegressor, SGDClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import tree
h = SVC()
h = KNeighborsClassifier(n_neighbors=10)
h = SGDClassifier()
from sklearn.ensemble import RandomForestClassifier
#h = tree.DecisionTreeClassifier()
h = RandomForestClassifier(n_estimators=100)
#h = RandomForestClassifier(n_estimators=100)

############################
# Initial window training",pyfiles/prediction/run_demo.py,aalto-trafficsense/regular-routes-server,1
"        #                  ('logistic', LogisticRegression(C=1))])),
        # ('NN 20:5', skflow.TensorFlowDNNClassifier(hidden_units=[20, 5],
        #                                            n_classes=data['n_classes'],
        #                                            steps=500)),
        # ('NN 500:200 dropout',
        # ('CNN', skflow.TensorFlowEstimator(model_fn=conv_model,
        #                                    n_classes=10,
        #                                    batch_size=100,
        #                                    steps=20000,
        #                                    learning_rate=0.001)),
        # ('SVM, adj.', SVC(probability=False,
        #                   kernel=""rbf"",
        #                   C=2.8,
        #                   gamma=.0073,
        #                   cache_size=200)),
        # ('SVM, linear', SVC(kernel=""linear"", C=0.025, cache_size=200)),
        # ('k nn', KNeighborsClassifier(3)),
        # ('Gradient Boosting', GradientBoostingClassifier())
    ]
",ML/hasy/classifier_comp.py,MartinThoma/algorithms,1
"    n = input()
    testData = []
    for _ in xrange(n):
        data = json.loads(raw_input())
        testData.append(data['question'] + '\r\n' + data['excerpt'])
    testData = transformer.fit_transform(testData)
    return testData
    
def main():    
    trainData, trainLabel = read_train_data()
    svm = LinearSVC()
    svm.fit(trainData, trainLabel)
    testData = read_test_data()
    testLabel = svm.predict(testData)
    for e in testLabel: 
        print e
    
if __name__ == ""__main__"":",practice/ai/machine-learning/stack-exchange-question-classifier/stack-exchange-question-classifier.py,EdisonCodeKeeper/hacker-rank,1
"
if __name__ == ""__main__"":
    from sklearn.svm import LinearSVC, SVR
    x = np.random.randn(1000, 1000)
    z = np.random.randn(1000)
    z = x.sum(axis=1) ** 2
    z -= z.mean()
    z /= z.std()
    y = (z > 0) + 0
    raise Exception
    clf = LinearSVC()
    sm = classification_sampling_machine(clf)
    sm.fit(x, y)  # 0.40s
    print (sm.predict(x) == y).sum()  # 689
    clf.fit(x, y)  # 0.94
    print (clf.predict(x) == y).sum()  # 1000

    clf2 = SVR(kernel='linear')
    sm2 = regression_sampling_machine(clf)
    sm2.fit(x, z)  # 54.53s",helper/sampling_machine.py,diogo149/dooML,1
"    ys.append(y)
    xs.append(x)

  # convert to numpy array
  ys = np.asarray(ys)
  xs = np.asarray(xs)

  # dir(clf) # check all methods for clf

  # train model
  clf = svm.SVC(kernel='linear', C=1)
  run_svm(clf, xs, ys)

  # clfw = svm.SVC(kernel='linear', C=1, probability=True, class_weight={1: 10})
  # run_svm(clfw, xs, ys)
",fertility/svm_predict_engine.py,Josephu/svm,1
"        print()
        clf_descr = str(clf).split('(')[0]
        return clf_descr, score, train_time, test_time


    results = []
    for penalty in [""l2"", ""l1""]:
        print('=' * 80)
        print(""%s penalty"" % penalty.upper())
        # Train Liblinear model
        results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                                dual=False, tol=1e-3)))



    # make some plots

    indices = np.arange(len(results))

    results = [[x[i] for x in results] for i in range(4)]",svm.py,yuhui-lin/web_page_classification,1
"					% (grid.best_params_, grid.best_score_))

			# Now we need to fit a classifier for all parameters in the 2d version
			# (we use a smaller set of parameters here because it takes a while to train)

			C_2d_range = [1e-2, 1, 1e2]
			gamma_2d_range = [1e-1, 1, 1e1]
			classifiers = []
			for C in C_2d_range:
				for gamma in gamma_2d_range:
					clf = SVC(C=C, gamma=gamma)
					clf.fit(X_2d, y_2d)
					classifiers.append((C, gamma, clf))

			##############################################################################
			# visualization
			#
			# draw visualization of parameter effects

			plt.figure(figsize=(8, 6))",Dalitz_simplified/classifier_eval_simplified_backup.py,weissercn/MLTools,1
"
print(""read training data"")
path = '../Data/'
train = pd.read_csv(path+'train.csv')
label = train['target']
trainID = train['id']
del train['id'] 
del train['target']

np.random.seed(131)
svc = svm.SVC(kernel='rbf',C=10,probability=True,verbose=True) 
svc.fit(train.values, label)
#calibrated_svc = CalibratedClassifierCV(OneVsRestClassifier(svc,n_jobs=-1), method='isotonic', cv=5)
#calibrated_svc.fit(train.values, label)

print(""read test data"")
test  = pd.read_csv(path+'test.csv')
ID = test['id']
del test['id']
",new/src/1st_level/svm.py,puyokw/kaggle_Otto,1
"# for machine learning with scikit-learn
fmri_masked = nifti_masker.fit_transform(data.func[0])

# Restrict the classification to the face vs house discrimination
fmri_masked = fmri_masked[condition_mask]

### Prediction ################################################################

# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

# And we run it
svc.fit(fmri_masked, target)
prediction = svc.predict(fmri_masked)

### Cross-validation ##########################################################

from sklearn.cross_validation import KFold
",plot_haxby_simple.py,ainafp/nilearn,1
"		print('支持向量机linear')
		print('开始训练模型')		
		scores = ['precision', 'recall']
		vect = TfidfVectorizer(ngram_range=(1,1), min_df=2, max_features=1000)
		xvec = vect.fit_transform(train_seg)
		X_train, X_test, y_train, y_test = train_test_split(xvec, train_label, train_size=0.7, random_state=0)
		for score in scores:
                    print(""# Tuning hyper-parameters for %s"" % score)
                    print()
                
                    clf = GridSearchCV(SVC(C=1), self.tuned_para, cv=5,
                                       scoring='%s_weighted' % score)
                    clf.fit(X_train, y_train)
                
                    print(""Best parameters set found on development set:"")
                    print()
                    print(clf.best_params_)
                    print()
                    print(""Grid scores on development set:"")
                    print()",classify_mode.py,huaijun-lee/classify,1
"import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cross_validation import train_test_split
from sklearn import metrics
from sklearn.linear_model import SGDClassifier
from sklearn import linear_model
from sklearn import svm

linear = svm.SVC()

df = pd.read_csv('FINALOBAMA.csv', header=0)

xtrain, xtest, ytrain, ytest = train_test_split(df['Close'], df['Approval'], train_size = 0.8)

linear.fit(pd.DataFrame(xtrain), pd.DataFrame(ytrain))

output = linear.predict(pd.DataFrame(xtest))
",app/machine-learning/Python/prediction-data.py,zebogen/pollarity,1
"    cl = [""ben""]
    C = 2.0  # SVM regularization parameter

    for classifier in cl:
        args.classifier = classifier
        print(""RUNNING FOR "", classifier)

        if args.classifier == 'poly':
            classifier = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                   (""extra trees"",
                                    svm.SVC(kernel=""poly"", degree=degree, C=C, gamma=gamma, probability=True))])
        elif args.classifier == 'linear':
            classifier = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                   (""extra trees"", svm.LinearSVC(C=C))])
        elif args.classifier == 'rbf':
            classifier = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                   (""extra trees"", svm.SVC(kernel='rbf', gamma=gamma, C=C, probability=True))])
        elif args.classifier == 'ben':
            classifier = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                   (""extra trees"", BernoulliNB())])",train_model_pipeline.py,eamosse/word_embedding,1
"    # test with X as list
    clf = MockListClassifier()
    scores = cval.cross_val_score(clf, X.tolist(), y)

    assert_raises(ValueError, cval.cross_val_score, clf, X, y,
                  scoring=""sklearn"")


def test_cross_val_score_precomputed():
    # test for svm with precomputed kernel
    svm = SVC(kernel=""precomputed"")
    iris = load_iris()
    X, y = iris.data, iris.target
    linear_kernel = np.dot(X, X.T)
    score_precomputed = cval.cross_val_score(svm, linear_kernel, y)
    svm = SVC(kernel=""linear"")
    score_linear = cval.cross_val_score(svm, X, y)
    assert_array_equal(score_precomputed, score_linear)

    # Error raised for non-square X",sklearn/tests/test_cross_validation.py,B3AU/waveTree,1
"            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier
            estimator = GradientBoostingClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'linear-svm':
            from sklearn.svm import SVC
            estimator = SVC(probability=True,
                            random_state=self.random_state, **self.kwargs)
        else:
            raise NotImplementedError

        # Create the different folds
        skf = StratifiedKFold(y, n_folds=self.cv, shuffle=False,
                              random_state=self.random_state)

        probabilities = np.zeros(y.shape[0], dtype=float)",imblearn/under_sampling/instance_hardness_threshold.py,glemaitre/UnbalancedDataset,1
"
    Returns:
        best_params -- values for C and gamma that gave the highest accuracy with
                       cross-validation
        best_score -- highest accuracy with cross-validation
    """"""
    C_range = np.logspace(-2, 10, 13)
    gamma_range = np.logspace(-9, 3, 13)
    param_grid = dict(gamma=gamma_range, C=C_range)
    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
    grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
    grid.fit(X, y)
    print(""The best parameters are %s with a score of %0.2f""
        % (grid.best_params_, grid.best_score_))
    return grid.best_params_, grid.best_score_

def uniqify_filename_list(filename_list, idfun=None):
   # based on code by Peter Bengtsson
   # https://www.peterbe.com/plog/uniqifiers-benchmark
   if idfun is None:",hvc/utils.py,NickleDave/hybrid-vocal-classifier,1
"# ****** Create average vectors for the training and test sets
print ""Creating average feature vecs for training reviews""

trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

print ""Creating average feature vecs for test reviews""

testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


svc = svm.LinearSVC()
param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}
print ""Training SVM""
svc = GridSearchCV(svc, param, cv=10)
svc = svc.fit(trainDataVecs, train[""sentiment""])
pred = svc.predict(testDataVecs)

print ""Optimized parameters:"", svc.best_estimator_
print ""Best CV score:"", svc.best_score_
",MovieReviewSentimentAnalysis/MovieReveiw/SVM_Word2vec.py,anilcs13m/Projects,1
"                    #{'kernel': ['linear'], 'C': [10, 50, 100, 500, 1000]}]

# Ignore linear for now, too slow

scores = [('f1',make_scorer(f1_score))]

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score[0])
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=k,
                       scoring=score[1])
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_params_)
    print()
    print(""Grid scores on development set:"")
    print()",grid.py,Tweety-FER/tar-polarity,1
"    
    ###########
    # Train SVM
    ###########
    if (not exists(conf.modelPath)) | OVERWRITE:
        if VERBOSE: print str(datetime.now()) + ' training liblinear svm'
        if VERBOSE == 'SVM':
            verbose = True
        else:
            verbose = False
        clf = svm.LinearSVC(C=conf.svm.C)
        if VERBOSE: print clf
        clf.fit(train_data, all_images_class_labels[selTrain])
        with open(conf.modelPath, 'wb') as fp:
            dump(clf, fp)
    else:
        if VERBOSE: print 'loading old SVM model'
        with open(conf.modelPath, 'rb') as fp:
            clf = load(fp)
",phow_birdid.py,lbarnett/BirdID,1
"import logging


class SklearnIntentClassifier(object):
    def __init__(self):
        self.le = LabelEncoder()
        self.tuned_parameters = [
            {'C': [1, 2, 5, 10, 20, 100], 'kernel': ['linear']}
        ]
        self.score = 'f1'  # 'precision'
        self.clf = GridSearchCV(SVC(C=1), self.tuned_parameters, cv=2, scoring='%s_weighted' % self.score)

    def transform_labels(self, labels):
        y = self.le.fit_transform(labels)
        return y

    def train(self, X, y):
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.1, random_state=0)
        self.clf.fit(X_train, y_train)",nlp/classifiers/sklearn_intent_classifier.py,kreuks/liven,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

C_range = np.logspace(-4, 3, 50)
gamma_range = np.logspace(-4, 3, 50)
print C_range,gamma_range
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=42)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, y)

print(""The best parameters are %s with a score of %0.2f""
      % (grid.best_params_, grid.best_score_))

# plot the scores of the grid
# grid_scores_ contains parameter settings and scores
# We extract just the scores
scores = [100*(1 - x[1]) for x in grid.grid_scores_]",best_hyper_parameter_estimation.py,haramoz/RND-ss14,1
"        self.assertIs(df.multiclass.OneVsRestClassifier, multiclass.OneVsRestClassifier)
        self.assertIs(df.multiclass.OneVsOneClassifier, multiclass.OneVsOneClassifier)
        self.assertIs(df.multiclass.OutputCodeClassifier, multiclass.OutputCodeClassifier)

    def test_Classifications(self):
        iris = datasets.load_iris()
        df = expd.ModelFrame(iris)

        models = ['OneVsOneClassifier', 'OneVsOneClassifier']
        for model in models:
            svm1 = df.svm.LinearSVC(random_state=self.random_state)
            svm2 = svm.LinearSVC(random_state=self.random_state)
            mod1 = getattr(df.multiclass, model)(svm1)
            mod2 = getattr(multiclass, model)(svm2)

            df.fit(mod1)
            mod2.fit(iris.data, iris.target)

            result = df.predict(mod1)
            expected = mod2.predict(iris.data)",expandas/skaccessors/test/test_multiclass.py,sinhrks/expandas,1
"
# setup
seed = 123
folds = 5

# hyperparameters -> usually more tunings, just for illustration
parameters = {'clf__kernel': ('rbf', 'poly', 'linear', 'sigmoid'),
              'clf__gamma': ('auto', 1),
              'clf__C': (10, 1.0, 0.1)}

piper = Pipeline([('clf', SVC(random_state=seed))])

grid_search = GridSearchCV(piper, parameters, n_jobs=3, verbose=1,
                           refit=True, cv=folds)

grid_search.fit(X_train, y_train)

print('Best score: %0.3f' % grid_search.best_score_)
print(grid_search.best_estimator_)
y_pred = grid_search.predict(X_test)",examples/text_classification.py,ebommes/breweries,1
"
@click.group(context_settings=CONTEXT_SETTINGS)
def cli():
    pass


def create_classifier(svm_type, c, **kwargs):
    assert svm_type == 'linear' or svm_type == 'rbf'

    if svm_type == 'linear':
        return LinearSVC(C=c,
                         penalty=""l2"",
                         loss=""squared_hinge"",
                         dual=False, # better False if n_samples > n_features
                         multi_class='ovr',
                         verbose=0)
    elif svm_type == 'rbf':
        return SVC(C=c,
                   kernel='rbf',
                   gamma=kwargs['gamma'],",classify_svm.py,ivanyu/kaggle-digit-recognizer,1
"# Maybe some original features where good, too?
selection = SelectKBest(k=1)
# Build estimator from PCA and Univariate selection:
combined_features = FeatureUnion([(""pca"", pca), (""univ_select"", selection)])

# Use combined features to transform dataset:
XtrainAll = combined_features.fit(XtrainAll, label).transform(XtrainAll)


# Classify:
clf = svm.SVC()
clf.fit(XtrainAll, label)

# Do grid search over k, n_components and C:

pipeline = Pipeline([(""features"", combined_features), (""svm"", clf)])

param_grid = dict(features__pca__n_components=[1, 2, 3],
                  features__univ_select__k=[1, 2],
                  svm__C=[0.1, 1, 10])",scikit_algo/fetureUnionGridsearch.py,sankar-mukherjee/CoFee,1
"
        if PrintParams.printReport:
            Report.editReadme('a', report)

        return [clf, vec]

    @staticmethod
    def evaluateOneVsRest(X, Y, printReport=False):
        time = datetime.datetime.now()
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
        clf = OneVsRestClassifier(LinearSVC(random_state=0))
        clf.fit(X_train, Y_train)
        if printReport:
            print 'Training time:' + str(datetime.datetime.now() - time)
            print 'Evaluation result: OneVsRest: ' + str(clf.score(X_test, Y_test))
        Y_test = clf.predict(X_test)
        if printReport:
            print '0: ' + str((Y_test == 0).sum())
            print '1: ' + str((Y_test == 1).sum())
            print '2: ' + str((Y_test == 2).sum())",Src/classification.py,hazemalsaied/IdenSys,1
"    preds = get_preds(X_train, y_train, X_test, C, sample_weight=None, penalty='l2', loss='squared_hinge', dual=True)
    r = recall_score(y_test, preds, pos_label=score_label)
    p = precision_score(y_test, preds, pos_label=score_label)
    f1 = f1_score(y_test, preds, pos_label=score_label)
    acc = accuracy_score(y_test, preds)
    print(""Gold has %d instances of target class"" % (len(np.where(y_test == score_label)[0])))
    print(""System predicted %d instances of target class"" % (len(np.where(preds == score_label)[0])))
    print(""Accuracy is %f, p/r/f1 score is %f %f %f\n"" % (acc, p, r, f1))

def get_preds(X_train, y_train, X_test, C=1.0, sample_weight=None, penalty='l2', loss='squared_hinge', dual=True):
    svc = svm.LinearSVC(C=C, penalty=penalty, loss=loss, dual=dual)
    svc.fit(X_train, y_train, sample_weight=sample_weight)
    preds = svc.predict(X_test)
    return preds

def get_f1(X_train, y_train, X_test, y_test, score_label, C=1.0, sample_weight=None, penalty='l2', loss='squared_hinge', dual=True):
    preds = get_preds(X_train, y_train, X_test, C=C, sample_weight=None, penalty='l2', loss='squared_hinge', dual=True)
    f1 = f1_score(y_test, preds, pos_label=score_label)
    return f1
",scripts/uda_common.py,tmills/uda,1
"    corpus_idf = tfidf_model[corpus]
    #corpus_lsi = lsi_model[corpus_idf]
    num_terms = len(corpus.dictionary)
    #num_terms = 400
    corpus_sparse = matutils.corpus2csc(corpus_idf, num_terms).transpose(copy=False)
    #print corpus_sparse.shape
    #corpus_dense = matutils.corpus2dense(corpus_idf, len(corpus.dictionary))
    #print corpus_dense.shape
    penalty = 'l2'
    clf = SGDClassifier(loss='hinge', penalty=penalty, alpha=0.0001, n_iter=50, fit_intercept=True)
    #clf = LinearSVC(loss='l2', penalty=penalty, dual=False, tol=1e-3)
    y = np.array(corpus.cls_y)
    #print y.shape
    clf.fit(corpus_sparse, y)
    filename = os.path.join(HERE, 'sgdc_clf.pkl')
    _ = joblib.dump(clf, filename, compress=9)
    print 'train completely'

    X_test = []
    X_label = []",pull/summ.py,jannson/Similar,1
"                                       test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    with warnings.catch_warnings(record=True):  # dep
        gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    with use_log_level('error'):",mne/decoding/tests/test_time_gen.py,olafhauk/mne-python,1
"             feat=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,2))),
            dict(name=""tfidf_ng3"",
             feat=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,3))),
           ]

# classifiers
classifiers = [
               dict(name=""nb"", clf=MultinomialNB()),
               dict(name=""bnb"", clf=BernoulliNB(binarize=0.5)),
               dict(name=""svm"",
                    clf=LinearSVC(loss='l2', penalty=""l2"",
                                  dual=False, tol=1e-3)),
              ]

# Load the data
print(60*""-"")
data = datas[0]
print(""Loading data:"", data['name'])
(d_train, y_train, d_test, y_test) = labr.get_train_test(**data['params'])
",python/demo.py,mohamedadaly/labr,1
"    ax2.set_ylabel(""Count"")
    ax2.legend(loc=""upper center"", ncol=2)

    plt.tight_layout()


# Plot calibration curve for Gaussian Naive Bayes
plot_calibration_curve(GaussianNB(), ""Naive Bayes"", 1)

# Plot calibration curve for Linear SVC
plot_calibration_curve(LinearSVC(), ""SVC"", 2)

plt.show()",projects/scikit-learn-master/examples/calibration/plot_calibration_curve.py,DailyActie/Surrogate-Model,1
"                   'Blue_merged', 'all']:
        print
        print monkey
        print
        X, y, labels = _load_files_labels(monkey, include_noise=False)

        X_train, X_test, y_train, y_test = train_test_split(X, y)
        pipeline = Pipeline([('data', AudioLoader(highpass=None,
                                                  normalize=True)),
                             ('selector', SelectPercentile(f_classif)),
                             ('svm', SVC(kernel='rbf', gamma=1e-5))])

        paramdist = {'data__nfilt': scipy.stats.randint(10, 50),
                     'data__stacksize': scipy.stats.randint(11, 51),
                     'selector__percentile': scipy.stats.randint(10, 100),
                     'svm__C': np.logspace(0, 2, 50)}

        clf = RandomizedSearchCV(pipeline, paramdist, verbose=3, n_iter=1000,
                                 cv=StratifiedKFold(y_train, n_folds=2),
                                 n_jobs=35)",sup_clf_gs.py,bootphon/monkey_business,1
"    #SVM
    #ExtraTrees
    #Adaboost
    #Random
    #DecisionTree
    #GradientBoosting

    rng = np.random.RandomState(1)

    classifiers = [
    LinearSVC(C=0.01, penalty=""l1"", dual=False),
    ensemble.ExtraTreesClassifier(n_estimators = 100, random_state = 0),
    DecisionTreeClassifier(max_depth=10),
    ensemble.RandomForestClassifier(max_depth=10, n_estimators=100),
    ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=300),
    ensemble.GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.1, max_depth = 10 , random_state = 0)]

    scores = np.zeros((6, 6))

    i = 0 ",classifyAkash_2.py,nmetts/sp2016-csci7000-bda-project,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_backward.decision_tree_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeature/example/test_decision_tree_backward.py,jundongl/scikit-feature,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_20_2015.py,magic2du/contact_matrix,1
"from sklearn.neighbors import KNeighborsClassifier

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)


result1 = tpot_data.copy()

# Perform classification with a C-support vector classifier
svc1 = SVC(C=0.1)
svc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['svc1-classification'] = svc1.predict(result1.drop('class', axis=1).values)

# Subset the data columns
subset_df1 = result1[sorted(result1.columns.values)[4042:5640]]
subset_df2 = result1[[column for column in ['class'] if column not in subset_df1.columns.values]]
result2 = subset_df1.join(subset_df2)

# Perform classification with a k-nearest neighbor classifier",tutorials/tpot_iris_pipeline.py,bartleyn/tpot,1
"    X , y = make_blobs(n_samples = 1000, centers = 10, random_state=123)
    y = np.take([True, False], (y < 5))
    
    C_2d_range = [3.0517578125e-05, 0.0312, 1, 32768.0]
    gamma_2d_range = [32768.0, 0.0312, 1, 3.0517578125e-05]
    
#     lin_svc = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
#                   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
#                   max_iter=-1, probability=False, random_state=None, shrinking=True,
#                   tol=0.001,verbose=False)
    rbf_svc = svm.SVC(C=1.0, kernel='rbf', gamma=0.7)
#     poly_svc = svm.SVC(C=1.0, kernel='poly', degree=3)
    
    #palette = itertools.cycle(seaborn.color_palette(n_colors = 10))
    scores_lin = []
    scores_rbf = []
    scores_poly = []
    lin_roc_auc_scorer = []
    rbf_roc_auc_scorer = []
    poly_roc_auc_scorer = []",kernel_selection3.py,lidalei/DataMining,1
"    print('Performing ' + method + ' Classification...')
    print('Size of train set: ', X_train.shape)
    print('Size of test set: ', X_test.shape)
    print('Size of train set: ', y_train.shape)
    print('Size of test set: ', y_test.shape)
    

    classifiers = [
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        neighbors.KNeighborsClassifier(),
        SVC(degree=100, C=10000, epsilon=.01),
        AdaBoostRegressor(),
        AdaBoostClassifier(**parameters)(),
        GradientBoostingClassifier(n_estimators=100),
        QDA(),
    ]

    scores = []

    for classifier in classifiers:",scripts/Algorithms/regression_helpers.py,scorpionhiccup/StockPricePrediction,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/constraints_dsessvc/setup.py,jpzk/evopy,1
"

class MetaboliteLevelDiseaseClassifier(BaseDiseaseClassifier):

    def __init__(self):
        super().__init__()
        self._pipe = Pipeline([
            ('vect', DictVectorizer(sparse=False)),
            ('scaler', MetabolicStandardScaler()),
            ('pca', PCA()),
            ('clf', SVC(C=0.01, kernel='linear', random_state=0))
        ])",src/classifiers/metabolite_level_disease_classifier.py,MuhammedHasan/disease-diagnosis,1
"            data['meta']['features'].append({'index': feature.index,
                                          'text': text.replace('_', ' '),
                                          'count': feature.document_frequency})

        return data

    def do_training(self, source=None):
        data = self.load_to_scikit_learn_format(training_portion=0.50, use_tfidf=False, source=source)

        from sklearn import svm
        lin_clf = svm.LinearSVC()
        trainingInput = data['training']['X']
        trainingOutput = data['training']['y']

        lin_clf.fit(trainingInput, trainingOutput)

        # get predictions
        prediction = lin_clf.predict(trainingInput)
        distances = lin_clf.decision_function(trainingInput)
",msgvis/apps/enhance/models.py,hds-lab/coding-ml,1
"import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel=""linear"")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data
clf.fit(features_train, labels_train)

#### store your predictions in a list named pred
pred = clf.predict(features_test)",intro_to_machine_learning/lesson/lesson_2_svm/SVM.py,tuanvu216/udacity-course,1
"    for formula_name, formula in formula_map.iteritems():

        print ""name=%s formula=%s"" % (formula_name,formula)

        y_train,X_train = dmatrices('Survived ~ ' + formula, 
                                    train_df_filled,return_type='dataframe')
        print ""Running SVM with formula : %s"" % formula
        print ""X_train cols=%s "" % X_train.columns
        y_train = np.ravel(y_train)
        for kernel in kernel_types:
            #model = svm.SVC(kernel=kernel,gamma=3)
            model = svm.SVC(kernel=kernel)
            print ""About to fit...""
            svm_model = model.fit(X_train, y_train)
            print ""Kernel: %s"" % kernel
            print ""Training score:%s"" % svm_model.score(X_train,y_train)
            X_test=dmatrix(formula,test_df_filled)
            predicted=svm_model.predict(X_test)
            print ""predicted:%s\n"" % predicted[:5]
            assert len(predicted)==len(test_df)",MasteringPandas/2060_11_Code/run_svm_titanic.py,moonbury/pythonanywhere,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,TomDLT/mne-python,1
"	    train_data = rawData_s_r[range(seperate)]
	    train_class = cID_s_r[range(seperate)]

	    test_data = rawData_s_r[range(seperate, sel_t_num)]
	    test_class = cID_s_r[range(seperate, sel_t_num)]
	    
	    # SVM fit classifier
	    X =  train_data
	    Y =  np.ravel(train_class)
	    
	    clf = svm.SVC()
	    clf.decision_function_shape='ovo'
	    clf.kernel='linear'
	    clf.fit(X, Y) 
	    
	    # make prediction
	    predict_class = clf.predict(test_data)
	    #pdb.set_trace()
	    for i in range(0, predict_class.shape[0]):
	        row = predict_class[i]-1",Compiled/testMultiClassSVM.py,jacobbaron/Neuron_Segmentation,1
"tfidfts = FeatureUnion(estimators).transform(corpusts)

targets_tr = traindf['cuisine']

predictors_ts = tfidfts


classifier = LinearSVC(C=1, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
# classifier = RandomForestClassifier(n_estimators=200)

classifier=classifier.fit(predictors_tr,targets_tr)
",deep_learn/whatiscooking/ReadCookingPlusPlus.py,zhDai/CToFun,1
"

df_train = pd.read_csv(""train.csv"")
feats = df_train.drop(""revenue"", axis=1) 

X = feats.values #features
y = df_train[""revenue""].values #target


# Create the RFE object and compute a cross-validated score.
svc = SVC(kernel=""linear"")
# The ""accuracy"" scoring is proportional to the number of correct
# classifications
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2),
              scoring='accuracy')

t = StandardScaler()
X = t.fit_transform(X)
y = t.fit_transform(y)
",Ari/preprocessing/RecursiveFeatureElimination.py,WesleyyC/Restaurant-Revenue-Prediction,1
"sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

g = float(input(""Input gamma: ""))

svm = SVC(C=1.0, kernel='rbf',gamma=g, random_state = 0, probability=True)
svm.fit(X_train_std, y_train)

PlotFigures.plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))
plt.show()

for X_test_val in X_test_std:
    print(svm.predict_proba(X_test_val.reshape(1, -1)))",Chapter3/NLLog.py,southpaw94/MachineLearning,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",pylibs/email/test/test_email.py,j5shi/Thruster,1
"fmri_masked = nifti_masker.fit_transform(func_filename)

# Restrict the classification to the face vs cat discrimination
fmri_masked = fmri_masked[condition_mask]

###########################################################################
# The decoding

# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

# And we run it
svc.fit(fmri_masked, target)
prediction = svc.predict(fmri_masked)

###########################################################################
# Compute prediction scores using cross-validation

from sklearn.cross_validation import KFold",plot_haxby_simple.py,NeuroStat/Python-scripts,1
"    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for logistic regression: ', auc)
    
    plot_curve(fpr, tpr, 'Logistic regression ' + str(auc))
    return clf


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_3.py,remigius42/code_camp_2017_machine_learning,1
"    },
    'random_forest': {
        'clf': RandomForestClassifier(),
        'param_grid': {
            'max_depth': np.linspace(4, 10, num=4, dtype=int),
            'max_features': np.linspace(4, 8, num=3, dtype=int),
            'n_estimators': np.linspace(50, 250, num=5, dtype=int),
        },
    },
    'svm_linear': {
        'clf': LinearSVC(),
        'param_grid': {'C': np.logspace(-2, 4, num=7)},
    },
    'svm_rbf': {
        'clf': SVC(),
        'param_grid': {
            'C': np.logspace(-2, 4, num=7),
            'gamma': np.logspace(-2, 4, num=7),
        },
    },",main.py,jeremyn/kaggle-titanic,1
"##########################################################################
##  Tests
##########################################################################

class ClassBalanceTests(VisualTestCase):

    def test_class_report(self):
        """"""
        Assert no errors occur during classification report integration
        """"""
        model = LinearSVC()
        model.fit(X,y)
        visualizer = ClassBalance(model, classes=[""A"", ""B""])
        visualizer.score(X,y)
        self.assert_images_similar(visualizer)",tests/test_classifier/test_class_balance.py,pdamodaran/yellowbrick,1
"rf=RandomForestClassifier(random_state=1)

classifiers = [
    KNeighborsClassifier(3),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    RandomForestClassifier(random_state=1),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()
	]

svcl=LinearSVC(random_state=1)
svcg=SVC(random_state=1)
gnb=GaussianNB(2)
qda=QuadraticDiscriminantAnalysis(2)

names=['svcl','lr']
classifiers=[svcl,lr]
params = [
         {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]},
	 {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}",scripts/05-histogram-random-forest.py,jmrozanec/white-bkg-classification,1
"from sklearn.preprocessing import *
import matplotlib.pyplot as plt

# Data Processing :

data_file = shuffle(np.loadtxt('spambase.data.csv', delimiter=','))
X_train, X_test, y_train, y_test = train_test_split(data_file[:, np.arange(57)], data_file[:, 57], test_size=0.50)
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
model = SVC(kernel='linear').fit(X_train, y_train)

###########################################################################

# Experiment 1 :

y_pred = model.predict(X_test)
print('\nExperiment 1 :\n\tAccuracy\t= '+str(accuracy_score(y_test, y_pred))+'\n\tPrecision\t= '+str(precision_score(y_test, y_pred)) +'\n\tRecall\t\t= '+str(recall_score(y_test, y_pred)))
y_score = model.decision_function(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_score)",SVM_Feature_Selection_SkLearn.py,Nandini-K/Artificial_Intelligence_and_Machine_Learning,1
"    X, y = make_blobs(centers=2, random_state=4, n_samples=30)
    # a carefully hand-designed dataset lol
    y[7] = 0
    y[27] = 0

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))

    for ax, C in zip(axes, [1e-2, 1, 1e2]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='linear', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)",notebooks/figures/plot_linear_svc_regularization.py,amueller/scipy_2015_sklearn_tutorial,1
"#
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)


#
# TODO: Create an SVC classifier named svc
# Use a linear kernel, and set the C value to C
#
from sklearn.svm import SVC
svc = SVC(C=C, kernel=kernel)

#
# TODO: Create an KNeighbors classifier named knn
# Set the neighbor count to 5
#
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)

",Module6/assignment1.py,szigyi/DAT210x,1
"                      'clf__alpha': (1e-4, ),
                     }
                     
    train('SGDClassifier', SGD_pipeline, SGD_parameters, train_reviews, train_targets)
    """"""
    
    """"""
    # Linear Support Vector Classification
    LSVC_pipeline = Pipeline([('vect', CountVectorizer(stop_words = None)), 
                             ('tfidf', TfidfTransformer(norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = True)), 
                             ('clf', LinearSVC(C = 1.0, penalty = 'l2', loss = 'squared_hinge', dual = False)),
                            ])
    
    LSVC_parameters = {'vect__ngram_range': [(1, 3)],
                       'clf__tol': (1e-4, ),
                      }
                     
    train('LinearSVC', LSVC_pipeline, LSVC_parameters, train_reviews, train_targets)
    """"""
    ",bia-660/final/experiment_reviews.py,hulingfei/Shaka,1
"
sl_ccrf = SKSupervisedLearning(CalibratedClassifierCV, X, Y_train, Xt, Y_test)
sl_ccrf.train_params = \
    {'base_estimator': RandomForestClassifier(**{'n_estimators' : 7500, 'max_depth' : 200}), 'cv': 10}
sl_ccrf.fit_standard_scaler()
ll_ccrf_trn, ll_ccrf_tst = sl_ccrf.fit_and_validate()

print ""Calibrated log loss: "", ll_ccrf_tst
conf_ccrf = plot_confusion(sl_ccrf)

#predicted = cross_val_predict(SVC(**sl.train_params), sl.X_train_scaled, n_jobs = -1, y = Y_train, cv=10)

#fig,ax = plt.subplots()
#ax.scatter(Y_train, predicted)
#ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
#ax.set_xlabel('Measured')
#ax.set_ylabel('Predicted')
#ax.xticks(np.unique(Y_test))
#ax.yticks(np.unique(Y_test))
",Learning/svm_experiments.py,fierval/KaggleMalware,1
"    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
#    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg10_superflatten.py,lioritan/Thesis,1
"from sklearn.svm.classes import NuSVC

from ..Classifier import Classifier
from ...language.Java import Java


class NuSVCJavaTest(Java, Classifier, unittest.TestCase):

    def setUp(self):
        super(NuSVCJavaTest, self).setUp()
        self.mdl = NuSVC(kernel='rbf', gamma=0.001, random_state=0)

    def tearDown(self):
        super(NuSVCJavaTest, self).tearDown()

    @unittest.skip('The generated code would be too large.')
    def test_existing_features_w_binary_data(self):
        pass

    @unittest.skip('The generated code would be too large.')",tests/classifier/NuSVC/NuSVCJavaTest.py,nok/sklearn-porter,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_11_24_2014.py,magic2du/contact_matrix,1
"from __future__ import division
import random
import numpy as np
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn import tree
from sklearn.naive_bayes import GaussianNB


def train_support_vector_machine(features):
	rate = train(SVC(), features)
	print 'support vector machine:', rate


def train_stochastic_gradient_descent(features):
	rate = train(SGDClassifier(), features)
	print 'stochastic gradient descent:', rate


def train_decision_tree(features):",src/learn.py,nh0815/py-congress,1
"X = nifti_masker.fit_transform(dataset_files.func)
# Apply our condition_mask
X = X[condition_mask]
session = session[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",plot_haxby_anova_svm.py,abenicho/isvr,1
"    def __init__(self, feature_selector, kernel=GaussianKernel(), C=1,
                 cache_size=200):
        super(SVMModel, self).__init__(feature_selector)
        self.C = C
        self.kernel = kernel
        self.cache_size = cache_size

    def train(self, documents):
        X, Y = self.feature_selector.build(documents)
        if hasattr(self.kernel, 'sklearn_name'):
            self.svm = svm.SVC(C=self.C, kernel=self.kernel.sklearn_name,
                               probability=True, cache_size=self.cache_size,
                               **self.kernel.sklearn_params)
        else:
            self.svm = svm.SVC(C=self.C, kernel=self.kernel.compute)
        self.svm.fit(X, np.concatenate(Y))

    def predict(self, document, n=1):
        x = self.feature_selector.get_x(document)
        probs = self.svm.predict_proba([x])[0]",classifier/models.py,VaclavDedik/classifier,1
"def make_classifiers(method, balanced, labels, selectors=None, columns=None, random_state=None):
    estimators = {}
    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression(n_jobs=1)
    elif 'svm' in method:
        estimator = SVC(probability=False)
    elif 'rf' in method:
        estimator = RandomForestClassifier(n_jobs=1)
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})
",clf_br.py,daniaki/ppi_wrangler,1
"    [LogisticRegression(random_state=42)],
    [LogisticRegression(random_state=42, multi_class='multinomial', solver='lbfgs')],
    [LogisticRegression(random_state=42, fit_intercept=False)],
    [LogisticRegressionCV(random_state=42)],
    [SGDClassifier(random_state=42)],
    [SGDClassifier(loss='log', random_state=42)],
    [PassiveAggressiveClassifier(random_state=42)],
    [Perceptron(random_state=42)],
    [RidgeClassifier(random_state=42)],
    [RidgeClassifierCV()],
    [LinearSVC(random_state=42)],
    [OneVsRestClassifier(LogisticRegression(random_state=42))],
])
def test_explain_linear(newsgroups_train, clf):
    assert_multiclass_linear_classifier_explained(newsgroups_train, clf,
                                                  explain_prediction)
    if isinstance(clf, OneVsRestClassifier):
        assert_multiclass_linear_classifier_explained(
            newsgroups_train, clf, explain_prediction_sklearn)
",tests/test_sklearn_explain_prediction.py,TeamHG-Memex/eli5,1
"
    width = xmax - xmin
    height = ymax - ymin

    xmin -= width*.1
    xmax += width*.1
    ymin -= height*.1
    ymax += height*.1

    # fit support vector machine on output of PCA
    clf = svm.SVC(C=100, gamma=.01, probability=True, scale_C=True)
    clf.fit(X_train_pca, y_train, sample_weight=w_train)

    # plot the decision function
    xx, yy = np.meshgrid(np.linspace(xmin, xmax, 500), np.linspace(ymin, ymax, 500))

    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    channel_name = samples.CHANNEL_NAMES[channel]",etc/pca.py,S-Bahrasemani/hhana,1
"    return [pca.fit_transform(X) for X in Xs]


def find_best_svm(X, y):
    print(""Searching SVMs"")
    scores = []
    for kernel in ['linear', 'rbf', 'sigmoid']:
        print(kernel)
        # For some reason, poly needed really long or
        # did even get stuck, so removed it
        clf = svm.SVC(kernel=kernel, C=1.0)
        score = cross_val_score(clf, X, y, cv=5)
        print(""%s: %0.2f"" % (kernel, score.mean()))

        scores.append((score.mean(), kernel))

    return max(scores)


def semi_supervised_set(clf, X, y, X_test, thresh=0.9):",scikit-learn/predictor.py,aufziehvogel/kaggle,1
"def loadLabel(fileName_1, delim='\t'):
  
    label = np.genfromtxt(fileName_1, dtype=int, delimiter = ',')
    return mat(label)

def label_classification(data_0, data_1):
    # build classifier
    data_train = np.array(data_0)
    label_train = np.array(data_1)
     
    clf = OneVsRestClassifier(LinearSVC()).fit(data_train, label_train) 
    score = cross_validation.cross_val_score(clf, data_train, label_train, scoring='f1', cv=10)
    return score.mean(), score.mean()+score.std()*2, score.mean()-score.std()*2
    
if __name__=='__main__':
     mata_0=loadData(data)  
     mata_1=loadLabel(label) 
     score_mean, mean_upper, mean_lower = label_classification(mata_0, mata_1)
     print score_mean, mean_upper, mean_lower",Scikit/Long/CV.py,AthenaSTM/ExperimentalMLScripts,1
"        in both sets, so we take the odd and even elements for the sets.
        """"""
        self._train_set = self._text_analysis.text_features_library[0:][::2]
        self._test_set = self._text_analysis.text_features_library[1:][::2]
        self.train_models()

    def initialize_models(self):
        self._models[""Multinomial Nayve Bayes Classifier""] = SklearnClassifier(MultinomialNB())
        self._models[""Logistic Regression Classifier""] = SklearnClassifier(LogisticRegression())
        self._models[""K-Nearest-Neighbors Classifier""] = SklearnClassifier(KNeighborsClassifier())
        self._models[""Linear SVC Classifier""] = SklearnClassifier(LinearSVC())
        
    def train_models(self):
        for classifier in self._models:
            self._models[classifier].train(self._train_set)
    
    def test_accuracy(self):
        for classifier_name in self._models:
            classifier = self._models[classifier_name]
            print(classifier_name + "" Algorithm accuracy percent:"", ",accuracy.py,dgovedarska/who-wrote-me,1
"                ['hinge', 'squared_hinge'],
                ['l1', 'l2'],
                [True, False],
                [True, False]):
        features = input_data.drop('class', axis=1).values.astype(float)
        labels = input_data['class'].values

        try:
            # Create the pipeline for the model
            clf = make_pipeline(preprocessor,
                                LinearSVC(C=C,
                                          loss=loss,
                                          penalty=penalty,
                                          dual=dual,
                                          fit_intercept=fit_intercept,
                                          random_state=324089))
            # 10-fold CV score for the pipeline
            cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
            accuracy = accuracy_score(labels, cv_predictions)
            macro_f1 = f1_score(labels, cv_predictions, average='macro')",preprocessor_model_code/LinearSVC.py,rhiever/sklearn-benchmarks,1
"label = np.array([1,0,1,0])

print(data)
print(label)

print(data.shape)
print(label.shape)

print(round(31.123))

clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(data,label)

#f = open('5a.txt', 'r')
#R = np.loadtxt(f, unpack =True, usecols=range(1,2))
#R = R.reshape(1,-1)
#f.close()
",Other/obsolete/SVM_learning_multidim.py,feranick/SpectralMachine,1
"def make_classifiers(method, balanced, labels, selectors=None, columns=None, random_state=None):
    estimators = {}
    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression()
    elif 'svm' in method:
        estimator = SVC(probability=False)
    elif 'rf' in method:
        estimator = RandomForestClassifier()
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})
",recall_precision.py,daniaki/ppi_wrangler,1
"
        target_col: array
            The name of the target column in the DataFrame.
    """"""

    # define the range of the sample sizes
    sample_sizes = np.concatenate((np.arange(100, 1000, 100), np.arange(1000, 10000, 1000),
                                             np.arange(10000, 100001, 10000), [200000, 300000]))

    # initialise the classifiers
    svm_rbf = SVC(kernel='rbf', gamma=0.01, C=100, cache_size=2000)
    svm_sigmoid = SVC(kernel='sigmoid', gamma=0.001, C=1000, cache_size=2000)
    svm_poly = LinearSVC(C=0.1, loss='squared_hinge', penalty='l1', dual=False, multi_class='ovr',
                         fit_intercept=True, random_state=21)
    logistic = LogisticRegression(penalty='l1', dual=False, C=1, multi_class='ovr', solver='liblinear', random_state=21)
    forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight='auto', random_state=21)

    # train SVM with RBF kernel (this will take a few hours)
    lc_svm_rbf = learning_curve(data, feature_cols, target_col, svm_rbf, sample_sizes, random_state=2,
        normalise=True, pickle_path='pickle/04_learning_curves/lc_svm_rbf.pickle')",mclearn/classifier.py,chengsoonong/mclass-sky,1
"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

x = np.random.rand(100, 2)
y = [ 1 if i + j > 1 else 0 for [i, j] in x]
s = svm.SVC()
s.fit(x, y)

""""""
以0.1作间隔，得到平面上的网格，计算每个网格上每个点的预测值
""""""
x_grid, y_grid = np.meshgrid(np.arange(0, 1.1, 0.1), np.arange(0, 1.1, 0.1))
grid_points = np.c_[x_grid.ravel(), y_grid.ravel()]

""""""",python/sklearn/svm.py,yuncliu/Learn,1
"    #
    # print ohlc.columns
    # print ohlc.data_frame.jump_empty_down
    # x = ohlc.norm_data
    # y = ohlc.feature_return(5)
    # x = x[:y.shape[0], :]
    # print 'this is x data: ', x
    # y = (y>0.10).astype(np.float64)
    # from sklearn.svm import SVC
    # print x.shape
    # svc = SVC()
    # print svc
    # train_x = x[:-100, :]
    # train_y = y[:-100]
    # test_x = x[-100:, :]
    # test_y = y[-100:]
    # svc.fit(x, y)
    # result = svc.predict(test_x)
    # print np.sum(result == test_y)*1.0/y.shape[0]
    # vol_values = ohlc.data_frame.volume.values",test/test_mysql.py,squall1988/lquant,1
"from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=10, max_depth=15, min_samples_split=2, n_jobs=-1, random_state=42)
rfc.fit(XStrain, ytrain)

# Determine RF classifications for the test set
ypredRFC = rfc.predict(XStest)


# Instantiate the SVM classifier
from sklearn.svm import SVC
svm = SVC(random_state=42)
svm.fit(XStrain,ytrain)

# Use dask to determing the SVM classifications for the test set

from dask import compute, delayed
import dask.threaded

def processSVM(Xin):
    return svm.predict(Xin)",SpIESHighzQuasarsS82all.py,gtrichards/QuasarSelection,1
"    Tests for the linear SVM implementation provided
    by the sklearn library.
    """"""

    def test_simple(self):
        """"""
        Run the simple SVM test listed on their web page.
        """"""
        X = np.asarray([[0,0],[1,1]])
        y=[0,1]
        clf = svm.SVC()
        clf.fit(X,y)

        # predict point on side of 1 class
        self.assertEqual(clf.predict([2.,2.])[0],1)

        # predict point on side of 0 class
        self.assertEqual(clf.predict([-2.,-2.])[0],0)

    #def test_multi_class(self):",src/py/cssigps/test_offlineclassifier.py,j-rock/cs598ps,1
"    vote1 = learnSVM(trainFeat, trainClass, testFeat)
    vote2 = learnKNN(trainFeat, trainClass, testFeat)
    vote3 = learnMaxEnt(trainFeat, trainClass, testFeat)
    res = [0] * len(testFeat)
    for i in range(0, len(testFeat)):
        if vote1[i] + vote2[i] + vote3[i] > 1:
            res[i] = 1
    return res

def learnSVM(trainFeat, trainClass, testFeat):
    model = svm.SVC()
    model.fit(trainFeat, trainClass)
    return model.predict(testFeat)

def learnKNN(trainFeat, trainClass, testFeat):
    model = neighbors.KNeighborsClassifier(15, weights='distance')
    model.fit(trainFeat, trainClass)
    return model.predict(testFeat)

def learnMaxEnt(trainFeat, trainClass, testFeat):",classical_nlp.py,metzzo/Paraphrase_Identification,1
"print len(trainArticles)
print len(testArticles)
listOfYears = []


testArticleLookupDict = {}
for title in range(0,len(testArticles)):
    #print (eval(testArticles[title])['title'])
    testArticleLookupDict[eval(testArticles[title])['title']] = title

clf = neural_network.MLPClassifier()#svm.SVC(probability=True)#tree.DecisionTreeClassifier()
titles = []
weights = []

G = {}#nx.DiGraph()#G is an empty graph


#A
def getArticle(article):
    singleSets = []",classifier-tripple-title.py,JFriel/honours_project,1
"        iris_data = Iris.objects.exclude(species=None)
        X_fields = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
        X_values = iris_data.values_list(*X_fields)
        y_values = iris_data.values_list('species', flat=True)
        test_size = 0.4
        random_state_int = np.random.randint(1000)
        X_train, X_test, y_train, y_test = train_test_split(X_values,
                                                            y_values,
                                                            test_size=test_size,
                                                            random_state=random_state_int)
        clf = svm.SVC()
        model = clf.fit(X_values, y_values)
        run_date = datetime.datetime.now(tz=timezone(settings.TIME_ZONE))
        score = clf.score(X_test, y_test)

        # save model and metadata
        svm_model_object = SVMModels()
        svm_model_object.test_size = test_size
        svm_model_object.random_state_int = random_state_int
        svm_model_object.model_pickle = cPickle.dumps(model)",irisfinder/management/commands/regenerate_SVM.py,DistrictDataLabs/django-data-product,1
"    ...                                     n_features=10,
    ...                                     n_informative=7,
    ...                                     random_state=5)
    >>> Xy = dict(X=X, y=y)
    >>> lda_estimator = Estimator(LDA())
    >>> lda_estimator.transform(**Xy)
    {'y/true': array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1]), 'y/pred': array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1])}
    >>> pipe = Pipe(SelectKBest(k=7), lda_estimator)
    >>> pipe.run(**Xy)
    {'y/true': array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1]), 'y/pred': array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1])}
    >>> pipe2 = Pipe(lda_estimator, SVC())
    >>> pipe2.run(**Xy)
    {'y/true': array([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1]), 'y/pred': array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1])}
    >>> cv = CV(Methods(pipe, SVC()), n_folds=3)
    >>> cv.run(**Xy)
    [[{'y/test/pred': array([0, 0, 0, 0, 0, 0]), 'y/train/pred': array([0, 0, 0, 0, 1, 0, 1, 1, 1]), 'y/test/true': array([1, 0, 1, 1, 0, 0])}, {'y/test/pred': array([0, 0, 0, 1, 0, 0]), 'y/train/pred': array([0, 0, 0, 0, 1, 0, 1, 1, 1]), 'y/test/true': array([1, 0, 1, 1, 0, 0])}], [{'y/test/pred': array([0, 0, 0, 0, 1]), 'y/train/pred': array([1, 0, 1, 1, 0, 0, 0, 0, 1, 1]), 'y/test/true': array([0, 0, 0, 1, 1])}, {'y/test/pred': array([1, 0, 0, 1, 1]), 'y/train/pred': array([1, 0, 1, 1, 0, 0, 0, 0, 1, 1]), 'y/test/true': array([0, 0, 0, 1, 1])}], [{'y/test/pred': array([1, 1, 0, 0]), 'y/train/pred': array([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1]), 'y/test/true': array([0, 0, 1, 1])}, {'y/test/pred': array([0, 0, 1, 0]), 'y/train/pred': array([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1]), 'y/test/true': array([0, 0, 1, 1])}]]
    >>> cv.reduce()
    ResultSet(
    [{'key': SelectKBest/LDA/SVC, 'y/test/score_precision': [ 0.5         0.33333333], 'y/test/score_recall': [ 0.75        0.14285714], 'y/test/score_accuracy': 0.466666666667, 'y/test/score_f1': [ 0.6  0.2], 'y/test/score_recall_mean': 0.446428571429},
     {'key': SVC, 'y/test/score_precision': [ 0.7  0.8], 'y/test/score_recall': [ 0.875       0.57142857], 'y/test/score_accuracy': 0.733333333333, 'y/test/score_f1': [ 0.77777778  0.66666667], 'y/test/score_recall_mean': 0.723214285714}])",epac/sklearn_plugins/estimators.py,neurospin/pylearn-epac,1
"        nDocs = len(vbTrain)
        mCorpusTrain = matutils.corpus2dense(oCorpTrain, nTerms, nDocs).T

        """"""This value of C is just high enough to properly predict 30% of 
        the cross-validation set properly and not so low that it doesn't
        predict the training set properly. 30% is apparently the best it can
        do with the cross-validation set. More training data would be helpful.

        I.e. it is general enough to fit the cross-validation set as best
        as it will be fit by change C.""""""
        oSVCModel = svm.SVC(C=35)
        oSVCModel.fit(mCorpusTrain, vbTrain) 
        vbPredTrain = oSVCModel.predict(mCorpusTrain)

        """"""Tess classifier on cross-validation set""""""
        oCorpCV = [oDict.doc2bow(text) for text in msCross]
        nCrossDocs = len(vbCross);
        mCorpCV = matutils.corpus2dense(oCorpCV, nTerms, nCrossDocs).T
        vbPredCross = oSVCModel.predict(mCorpCV)
                    ",mywork/task3.py,DoctorKhan/deals,1
"score = np.zeros((nb_iter, len(pairwise_groups)))
crossval = OrderedDict() 
pg_counter = 0
for pg in pairwise_groups:
    gr1_idx = data[data.DX_Group == pg[0]].index.values
    gr2_idx = data[data.DX_Group == pg[1]].index.values
    x = X[np.concatenate((gr1_idx, gr2_idx))]
    y = np.ones(len(x))
    y[len(y) - len(gr2_idx):] = 0

    estim = svm.SVC(kernel='linear')
    sss = cross_validation.StratifiedShuffleSplit(y, n_iter=nb_iter, test_size=0.2)
    # 1000 runs with randoms 80% / 20% : StratifiedShuffleSplit
    counter = 0
    for train, test in sss:
        Xtrain, Xtest = x[train], x[test]
        Ytrain, Ytest = y[train], y[test]
        Yscore = estim.fit(Xtrain,Ytrain)
        print pg_counter, counter
        score[counter, pg_counter] = estim.score(Xtest, Ytest)",learn_voxels_norm_baseline_fdg_pet_adni.py,mrahim/adni_fdg_pet_analysis,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't instantiate with non-transformers on the way
    # Note that NoTrans implements fit, but not transform
    assert_raises_regex(TypeError,
                        'All intermediate steps should be transformers'
                        '.*\\bNoTrans\\b.*',
                        Pipeline, [('t', NoTrans()), ('svc', clf)])",sklearn/tests/test_pipeline.py,kevin-coder/scikit-learn-fork,1
"                                 stop_words='english')



x_train = featureMatrix(reviews)

spam_test = featureMatrix(useless_exp);
ham_test = featureMatrix(useful_exp);


clf = LinearSVC(loss='l2', penalty=""l2"", dual=False, tol=1e-5)
#clf = SVC(kernel='poly',tol=1e-2, verbose=False)



clf.fit(x_train, labels)

predict_spam = clf.predict(spam_test)
predict_ham = clf.predict(ham_test)
",src/old_pipeline/fsvm.py,cudbg/Dialectic,1
"            else:  # elif model_type == SVMNB:
                self.params['kernel'] = 'linear'
                if self.params.get('alpha', None) is None:
                    self.params['alpha'] = 1
                if self.params.get('beta', None) is None:
                    self.params['beta'] = 0.25

            if self.params['kernel'] == 'linear':
                # override regularization parameter to avoid a conflict
                self.params['regularization'] = 'l2'
                self.model = svm.LinearSVC(C=self.params['alpha'])
            else:  # elif self.params['kernel'] != 'linear':
                if self.params.get('degree', None) is None:
                    self.params['degree'] = 3
                if self.params.get('gamma', None) is None:
                    self.params['gamma'] = 0.0
                if self.params.get('coef0', None) is None:
                    self.params['coef0'] = 0.0
                self.model = svm.SVC(C=self.params['alpha'], kernel=self.params['kernel'], degree=self.params['degree'],
                                     gamma=self.params['gamma'], coef0=self.params['coef0'])",core/models/sparse_model.py,dallascard/guac,1
"    tempX = np.column_stack((opens[1:], highs[1:], lows[1:], volumes[1:], changes[1:], changePcts[1:], averages[1:],
                         turns[1:], rs[1:], lastRs[1:], weekAgoRs[1:], amts[1:], lastAmts[1:]))
    X = np.hstack((tempX, techs))
    y = upOrDowns[2:]  # 涨跌数组向后移一位,表当前周数据预测下一周涨跌
    y.append(upOrDowns[-1])  # 涨跌数组最后一位按前一位数据补上
    return X, y, actionDates[1:]


def optimizeSVM(X_norm, y, kFolds=10):
    clf = pipeline.Pipeline([
        ('svc', svm.SVC(kernel='rbf')),
    ])
    # grid search 多参数优化
    parameters = {
        'svc__gamma': np.logspace(0, 3, 20),
        'svc__C': np.logspace(0, 3, 10),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['svc__gamma'], gs.best_params_['svc__C'], gs.best_score_",finance/WeekTest/WeekDataPrepare.py,Ernestyj/PyStudy,1
"    for house in [ 'A', 'B' , 'C']:
        for f in ['lastchange']:#,'last', 'change', 'data']:
            loc = data_loc_str.format(house=house,feature=f)
            data = load.data(loc)
            classify(data, house, f)



# Using pystruct
def train_SVM(X_train, y_train):
    svm = LinearSVC(dual=False, C=.1)
    svm.fit(np.vstack(X_train), np.hstack(y_train))

    #print(""Test score with linear SVM: %f"" % svm.score(np.vstack(X_test),
    #                                               np.hstack(y_test)))
    return svm


def test_SVM(svm, X_test, y_test):
    y_pred = svm.predict(np.vstack(X_test))",src/svm.py,ppegusii/cs689-final,1
"                                                user_label_matrix[test, :]

            contingency_matrix = chi2_contingency_matrix(X_train, y_train)
            community_weights = peak_snr_weight_aggregation(contingency_matrix)
            X_train, X_test = community_weighting(X_train, X_test, community_weights)

            ####################################################################################################
            # Train model
            ####################################################################################################
            # Train classifier
            model = OneVsRestClassifier(svm.LinearSVC(C=1,
                                                      random_state=None,
                                                      dual=False,
                                                      fit_intercept=True),
                                        n_jobs=number_of_threads)

            model.fit(X_train, y_train)
            ####################################################################################################
            # Make predictions
            ####################################################################################################",reveal_user_classification/entry_points/prototype_user_network_profile_classifier.py,MKLab-ITI/reveal-user-classification,1
"    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline

    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)

    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])

    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svm
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
",sklearn/pipeline.py,johnowhitaker/bobibabber,1
"    plt.show()
    
def w2vecSVMFit():
    # load the trained model which is provided by Google
    model = Word2Vec.load_word2vec_format('/users/raemoen/SemanticProj/opMining/GoogleNews-vectors-negative300.bin', binary=True)
    # prepare the data for SVM
    df=pd.DataFrame.from_csv(""data/training_vec.csv"", encoding=""utf-8"",header=None)
    xtrain=np.array(df.iloc[:,1:])
    ytrain=np.array(df.iloc[:,0])
    # provide the data to SVM
    svm = SVC(kernel='linear', C=1.0, random_state=0)
    svm.fit(xtrain, ytrain)
    return model,svm

def emotDetermine(wCountFixed,model,svm):
    emotSum=0.
    if(wCountFixed=={}):
        emot=0
    else:
        for key in wCountFixed:",amzAnalysis.py,chi-hung/notebooks,1
"        week_test_df = week_test_df[available_test]

        print(""Shape after filter: "")
        print(week_train_df.shape)

        #standlize the data
        week_train_df, scaler = preprocess_data(week_train_df)
        week_test_df, _ = preprocess_data(week_test_df, scaler)
        
        #train model
        clf = svm.SVC(probability=True)
        clf.fit(week_train_df, week_train_category)
        print(""Predicting on {0} rows"".format(len(week_test_df)))
        temp_predictions = clf.predict_proba(week_test_df)
        
        #concatenates results
        temp_predictions = pd.DataFrame(temp_predictions)
        temp_predictions.columns = clf.classes_
        #print temp_predictions
        predictions = predictions.append(temp_predictions, ignore_index=True)",src/week_windows_all_years.py,gnu-user/sf-crime-classification,1
"            + [(LT_RD, 'LT_RD')] \
            + [(RD_LT, 'RD_LT')] \
            + [(RT_LD, 'RT_LD')] \
            + [(LD_RT, 'LD_RT')] \
            + [(CIRCLE, 'CIRCLE')] \
            + [(NULL, 'NULL')]
    print XY
    X, Y = zip(*XY)
    X = map(lambda x: tuple(sum(map(list, x), [])), X)

    # learner = svm.LinearSVC()
    learner = neighbors.KNeighborsClassifier(weights='distance')
    predictor = learner.fit(X, Y)

    mouse = pymouse.PyMouse()
    hist_origin = []
    try:
        while True:
            pos = mouse.position()
            hist_origin.append(pos)",predict_mouse.py,TurpIF/gestures-learning,1
"#Merge the data and the Genre and Year labels
df_merged = pd.merge(df_clean, df_meta, how='left', on=['Track ID', 'Song ID'])

#Getting rid of UNCAT ones
df_merged = df_merged[df_merged[""Genre""] != 'UNCAT']

#Start SVM for Genre Classification
print """"
print ""*Start SVM Classification""
from sklearn.svm import SVC
svm_model = SVC(kernel = 'rbf', gamma = 1000)


y = df_merged[""Genre""]
X = df_merged.drop([""Genre"",""Song ID"",""Track ID""], axis = 1)

from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y)
svm_model.fit(X_train, y_train)",Code/Machine_Learning_Algos/10k_Tests/ml_classification_svm2.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"		best_C = SVM_find_best_C()

	#2c
	if sys.argv[1] == ""2c"":
		best_C  = 1.00521990733
		SVM_weights(best_C)
	
	#2d
	if sys.argv[1] == ""2d"":
		best_C  = 1.00521990733
		clf = svm.LinearSVC(loss=""hinge"", fit_intercept=False, C=best_C)
		clf.fit(train_data_norm, train_labels)
		print ""The accuracy of the linear SVM with C ="", best_C, ""is ="", (clf.score(test_data_norm,test_labels)*100), ""%""
	
	#3a
	if sys.argv[1] == ""3a"":
		best_eta = SGD()

	#3b
	if sys.argv[1] == ""3b"":",HW2/hw2.py,oferorgal/ml-intro,1
"        y = np.array(_ys[i])
        meta = metas[i]
        data.append(Data(X, y, meta))

    return data

def makeMinamiClassifier(path):
    dataset = import_training_set(path, selected=SELECTED_MINAMI, derived=DERIVED_MINAMI)
    data = dataset[0]
    dp.message('shape=%s |y|=%d' % (data.X.shape, len(data.y)))
    clf = SVC(C=32., kernel='rbf', gamma=8.)
    X = pp.scale(data.X)
    clf.fit(X, data.y)
    return clf

def makeTeraiClassifier(path):
    dataset = import_training_set(path, selected=SELECTED_TERAI, derived=DERIVED_TERAI)
    data = dataset[1]
    dp.message('shape=%s |y|=%d' % (data.X.shape, len(data.y)))
    clf = KNeighborsClassifier(6, weights='distance', metric='hamming')",scripts/make_loop_classifier.py,ebt-hpc/cca,1
"from sklearn.svm.classes import SVC

from ..Classifier import Classifier
from ...language.PHP import PHP


class SVCPHPTest(PHP, Classifier, TestCase):

    def setUp(self):
        super(SVCPHPTest, self).setUp()
        self.mdl = SVC(C=1., kernel='rbf',
                       gamma=0.001,
                       random_state=0)

    def tearDown(self):
        super(SVCPHPTest, self).tearDown()

    def test_linear_kernel(self):
        self.mdl = SVC(C=1., kernel='linear',
                       gamma=0.001,",tests/classifier/SVC/SVCPHPTest.py,nok/sklearn-porter,1
"RANDOM_STATE = 42

# Generate a dataset
X, y = datasets.make_classification(n_classes=3, class_sep=2,
                                    weights=[0.1, 0.9], n_informative=10,
                                    n_redundant=1, flip_y=0, n_features=20,
                                    n_clusters_per_class=4, n_samples=5000,
                                    random_state=RANDOM_STATE)

pipeline = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE),
                            LinearSVC(random_state=RANDOM_STATE))

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    random_state=RANDOM_STATE)

# Train the classifier with balancing
pipeline.fit(X_train, y_train)

# Test the classifier and get the prediction",examples/evaluation/plot_metrics.py,glemaitre/imbalanced-learn,1
"sample_annotation = data_file['sample_annotation']
feature_annotation = data_file['feature_annotation']
g = gt.load_graph(working_dir + '/graph.xml.gz')
cvs = pickle.load(open(working_dir + '/cvs.dmp', 'rb'))


def get_l1_svm(X, y, fc):
    high = 1/1000
    low = 0
    while (True):
        learner = sklearn.svm.LinearSVC(penalty = 'l1',
            dual = False, C = high, verbose=False)
        learner.fit(X, y)
        t_fc = np.sum(learner.coef_ != 0)
        if (t_fc > fc):
            high = (low + high) / 2
        elif (t_fc < fc):
            low = high
            high = high * 2
        else:",check_svm_selected_features.py,adrinjalali/Network-Classifier,1
"    model: Class 
        sklearn-type model
    
    
    """"""
    
    if kernel:
        if modelType == ""ridge"":
            model = KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel, coef0=coef0)
        if modelType == ""SVC"":
            model = SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0, degree=poly)
        if modelType == ""SVR"":
            model = SVR(C=C, kernel=kernel, gamma=gamma, coef0=coef0, epsilon=epsilon, degree=poly)
                    
    elif poly:
        if modelType == ""OLS"":
            model = make_pipeline(PolynomialFeatures(poly), LinearRegression(fit_intercept=fit_intercept, normalize=normalize))
        if modelType == ""ridge"":
            model = make_pipeline(PolynomialFeatures(poly), Ridge(alpha= alpha, normalize=normalize))
        if modelType == ""lasso"":",src/models/model_utils.py,Kleurenprinter/prompred,1
"		fname = ""{}/reps.csv"".format(self.embeddingsDir)
		embeddings = pd.read_csv(fname, header=None).as_matrix()

		# Clean the data by encoding 
		le = LabelEncoder().fit(labels)
		labelsNum = le.transform(labels)
		nClasses = len(le.classes_)
		print(""Training for {} classes."").format(nClasses)
		
		# initialize our LinearSVM classifier
		clf = SVC(C=1, kernel='linear',probability=True)
			# use LDA pipeline?
		# fit the classifier with embeddings and labels
		clf.fit(embeddings,labelsNum)	
		# Save classifier to disk
		fname = ""{}/classifier.pkl"".format(self.embeddingsDir)
		print(""Saving classifier to '{}'"").format(fname)
		with open(fname, 'w') as f:
			pickle.dump((le,clf), f)
",faceAPI/Classifier.py,xamgeis/project_vulcan,1
"    normalised_data = (data - data.min()) / (data.max() - data.min())

    return normalised_data

def fill_nan(input_fields, output_field, to_predict, method=""svr""):
    svm = None

    if method == ""svr"":
        svm = SVR()
    else:
        svm = SVC()

    train_df = get_all_training_data()

    expected_output = train_df.get([output_field]).values
    train_inputs = train_df.drop([""PassengerId"",
                                  ""Survived"", ""Name"",
                                  ""Ticket"", ""Cabin""],
                                 axis=1)
",src/utils.py,JakeCowton/titanic,1
"
    in_fnames = args.input
    data, target, test, test_names = readDataSet(in_fnames)
    
    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.2, random_state=42)

    #print X_train.shape, y_train.shape, y_train
    #print X_test.shape, y_test.shape, y_test
    #print ""data set balance: "", sum(target), float(len(target)-sum(target)) / len(target)

    #clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    #print clf.score(X_test, y_test)  
    
    '''PCA'''
    #RunLDA(data, target.astype(int), test, np.array([2]*len(test)))
    #RunPCA(data, labels=None, classVector=target.astype(int))
    #RunPCA(np.append(data, test, 0), labels=None, classVector=np.append(target, np.array([2]*len(test))).astype(int), trainSize=len(data))
    #Run3dPCA(np.append(test, data, 0), np.append(np.array([2]*len(test)), target).astype(int))
    #return
    ",sklearnClassifiers/simpleClassifier4.py,rampasek/seizure-prediction,1
"    logger.info('X.shape = %s, y.shape = %s', corpus.X.shape, corpus.y.shape)

    models = [
        ('Logistic Regression (L1)', linear_model.LogisticRegression(penalty='l1')),
        ('Logistic Regression (L2)', linear_model.LogisticRegression(penalty='l2')),
        # ('logistic_regression-L2-C100', linear_model.LogisticRegression(penalty='l2', C=100.0)),
        # ('randomized_logistic_regression', linear_model.RandomizedLogisticRegression()),
        # ('SGD', linear_model.SGDClassifier()),
        ('Perceptron', linear_model.Perceptron(penalty='l1')),
        # ('perceptron-L2', linear_model.Perceptron(penalty='l2')),
        # ('linear-svc-L2', svm.LinearSVC(penalty='l2')),
        # ('linear-svc-L1', svm.LinearSVC(penalty='l1', dual=False)),
        # ('random-forest', ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',
        #     max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='auto',
        #     bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0,
        #     min_density=None, compute_importances=None)),
        ('Naive Bayes', naive_bayes.MultinomialNB()),
        # ('Naive Bayes (Bernoulli)', naive_bayes.BernoulliNB()),
        # ('knn-10', neighbors.KNeighborsClassifier(10)),
        # ('neuralnet', neural_network.BernoulliRBM()),",tsa/analyses/grid.py,chbrown/tsa,1
"            cm = confusion_matrix(y_test, y_pred)

            # accuracy
            results[data][model_class_name_map[model]][""acc""].append(np.sum(cm.diagonal()) * 100.0 / np.sum(cm))

        pass
    pass

    # Linear SVC
    print ""model is LinearSVC.""
    model_ = LinearSVC()
    st = time.time()
    model_.fit(X_train, y_train)
    et = time.time()
    y_pred = model_.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    acc = np.sum(cm.diagonal()) * 100.0 / np.sum(cm)
    elapsed_time = et - st
    for epoch in epochs:  # add the same results to all epochs
        results[data][""LinearSVC""][""acc""].append(acc)",cw/evaluate_small_data.py,kzky/python-online-machine-learning-library,1
"    pointList = np.c_[np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)]
    return pointList

def applyFunction(points):
    return np.sign(points[:,1]-points[:,0]+0.25*np.sin(np.pi * points[:,0]))

def doAssignment13():
    experiments = 1000
    gama = 1.5
    numPoints = 100
    clf = svm.SVC(C= np.inf , kernel=""rbf"", coef0=1, gamma=gama)
    Ein0 = 0
    for i in range(experiments):    
        X = getPoints(numPoints)
        y = applyFunction(X)
        clf.fit(X,y)
        #print(clf.score(X,y))
        if(1-clf.score(X,y)==0):
            #print(""here"")
            Ein0 += 1",Final/t13.py,pramodh-bn/learn-data-edx,1
"    classifiers = {
        ""RandomForest"": RandomForestClassifier(n_jobs=-1)
        # ""RandomForest"": RandomForestClassifier(n_estimators=5),
        # ""KNeighbors"":KNeighborsClassifier(3),
        # ""GaussianProcess"":GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
        # ""DecisionTree"":DecisionTreeClassifier(max_depth=5),
        # ""MLP"":MLPClassifier(),
        # ""AdaBoost"":AdaBoostClassifier(),
        # ""GaussianNB"":GaussianNB(),
        # ""QDA"":QuadraticDiscriminantAnalysis(),
        # ""SVM"":SVC(kernel=""linear"", C=0.025),
        # ""GradientBoosting"":GradientBoostingClassifier(),
        # ""ExtraTrees"":ExtraTreesClassifier(),
        # ""LogisticRegression"":LogisticRegression(),
        # ""LinearDiscriminantAnalysis"":LinearDiscriminantAnalysis()
    }
    for key in classifiers:
        utils.print_success(key)
        clf = classifiers[key]
        utils.print_info(""\tFit"")",src/identify_singing_voice_gender.py,ybayle/ISMIR2017,1
"class NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,jart/tensorflow,1
"

    # In[ ]:

    scores = cross_val_score(rf,X,y,n_jobs=-1,cv=StratifiedShuffleSplit(y,n_iter=7,test_size=0.3))
    print(""X RF Accuracy: %0.3f (+- %0.2f)"" % (scores.mean(), scores.std() * 2))
#    scores_f1 = cross_val_score(rf,X,y,n_jobs=-1,cv=StratifiedShuffleSplit(y,n_iter=10,test_size=0.22),scoring='f1')
#    print(""X RF f1: %0.3f (+- %0.2f)"" % (scores_f1.mean(), scores_f1.std() * 2))

     # In[ ]:
    svc = LinearSVC(C=20, penalty='l1', dual=False)
    svc.fit(X, y)
    selected_feature_names = feature_cols[[list(set(np.where(svc.coef_ != 0)[-1]))]]
    X_svm = svc.transform(X)
    print(""X_svm L1 transformed:"", X_svm.shape)
    X=X_svm


     # In[ ]:
",ProFET/Output/VisualizebestFeatHist.py,ddofer/ProFET,1
"        # Here we specify the number of features that we will use for the analysis
        nFeatures = np.array([N_FEATURES, 200, 100, 50, 20, 10])  
        
        # In this section we specify the classification models.
        
        clfs = [
         BernoulliNB(),
         GaussianNB(),
         DecisionTreeClassifier(),
         RandomForestClassifier(n_estimators=10),
         OneVsRestClassifier(LinearSVC(random_state=0)),
         OneVsRestClassifier(LogisticRegression()),
         OneVsRestClassifier(SGDClassifier()),
         OneVsRestClassifier(RidgeClassifier()),
        ]
        
        # We get the names of each model.
        clfnames = map(lambda x: type(x).__name__
          if type(x).__name__ != 'OneVsRestClassifier'
          else type(x.estimator).__name__, clfs)",core/__init__.py,Rickyfox/MLMA2,1
"  xs = dataset[:][:,1:]

  logging.basicConfig(filename='predict.log',level=logging.DEBUG, format='')

  plt.clf()

  # train model
  plt.subplot(2,2,1)
  plt.title('C=1, gamma=0.0')

  clf = svm.SVC(kernel='rbf', C=1, gamma=0, probability=True)
  predict(clf, xs, ys)

  plt.subplot(2,2,2)
  plt.title('C=1, gamma=0.01')

  clf = svm.SVC(kernel='rbf', C=1, gamma=0.01, probability=True)
  predict(clf, xs, ys)

  plt.subplot(2,2,3)",wholesale_customers/predict_rbf.py,Josephu/svm,1
"        super().__init__()
        vectorizer = DictVectorizer(sparse=False)
        self._pipe = Pipeline([
            ('vect', vectorizer),
            ('scaler', MetabolicStandardScaler()),
            ('scaler_change', MetabolicChangeScaler()),
            ('scaler_solution', MetabolicSolutionScaler(vectorizer)),
            ('scaler_most_active', MostActivePathwayScaler())
            ('vect2', DictVectorizer(sparse=False)),
            ('pca', PCA()),
            ('clf', SVC(C=0.01, kernel='linear', random_state=0))
        ])",src/classifiers/solution_level_disease_classifier.py,MuhammedHasan/disease-diagnosis,1
"    >>> from sklearn import svm
    >>> from sklearn.datasets import samples_generator
    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline
    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)
    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svm
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
    >>> prediction = anova_svm.predict(X)
    >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS",phy/lib/python2.7/site-packages/sklearn/pipeline.py,marcsans/cnn-physics-perception,1
"
    # a carefully hand-designed dataset lol
    y[7] = 0
    y[27] = 0
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

    for ax, C in zip(axes, [1e-2, 1, 1e2]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='linear', C=C, tol=0.00001).fit(X, y)
        w = svm.coef_[0]
        a = -w[0] / w[1]
        xx = np.linspace(6, 13)
        yy = a * xx - (svm.intercept_[0]) / w[1]
        ax.plot(xx, yy, label=""C = %.e"" % C, c='k')
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())",scikit/Chapter 2/figures/plot_linear_svc_regularization.py,obulpathi/datascience,1
"	transformed = matrix.dot(v.T)
	print transformed.shape
	#Compute cov matrix
	# if os.path.isfile('svm.model'):
	#     print 'Loading Model file...'
	#     #Load models from file
	#     # with open('svm.model', 'rb') as file:
	#     #     Z = pickle.load(file)
	# else:
		#Start to train SVM
	Z = OneVsRestClassifier(SVC(kernel=""rbf"")).fit(transformed, labels)
	    # with open('svm.model', 'wb') as file:
	    #     pickle.dump(Z, file)

	Z = Z.predict(transformed)

	print Z[0]
	correct = 0.0
	for x in range(len(Z)):
		if labels[x] == Z[x]:",Src/pca.py,motian12ps/Bigdata_proj_yanif,1
"    mtest,ctest = make_matrix(test, train)

    #matrix = preprocessing.normalize(matrix).toarray()
    #mtest = preprocessing.normalize(mtest).toarray()
    model = MultinomialNB(alpha=1e-8)
    #model = GaussianNB()
    #model = ExtraTreesClassifier()
    #model = LogisticRegression()
    #model = KNeighborsClassifier() # Very bad
    #model = DecisionTreeClassifier()
    #model = SVC()
    
    model.fit(matrix, cats)
    #print(model.feature_importances_)
    
    ans = model.predict(mtest)
    
    print(metrics.classification_report(ctest, ans))
    print(metrics.confusion_matrix(ctest, ans))
    ",dmitrieva.py,agmt/sent_an,1
"
import sys

sys.path.append('src')

from data import get_featues, get_label


class SVMModel(object):
    def __init__(self):
        self.clf = OneVsOneClassifier(SVC())
        self.name = 'SVM'

    def get_params(self):
        return self.clf.get_params()

    def train(self, dframe):
        X = get_featues(dframe)
        y = get_label(dframe)
        self.clf.fit(X, y)",src/models/svm.py,artofai/overcome-the-chaos,1
"        print ""Review data not found.""
        return -1

    # Extract features and labels and create train/test sets
    features, labels = data
    X_train, X_test, y_train, y_test = \
        train_test_split(features, labels, test_size=.2, random_state=42)

    # Create pipeline, to tokenize, vectorize, transform, and classify data
    textClf = Pipeline([(""Vectorizer"", TfidfVectorizer(stop_words=""english"")),
                        (""Classifier"", LinearSVC(C=1.25))])

    # Train the classifier
    if verbose: print ""Training classifier...""
    textClf.fit(X_train, y_train)

    # Print accuracy score if verbose flag is no
    if verbose:
        y_pred = textClf.predict(X_test)
        print ""Training finished. Classifier accuracy:"", accuracy_score(y_test, y_pred)",sentiment_analysis.py,will-cromar/needy,1
"v = VAE.VAE(ARCHITECTURE, HYPERPARAMS, log_dir=settings.LOG_DIR, meta_graph=path_save)


#Extracting main charactheristics -> 2 variables per sample
print(""Coding training data"")
code_train = v.encode(X_train)  # [mu, sigma]
print(""Coding test data"")
code_test = v.encode(X_test)  # [mu, sigma]

#code_sample = sample_gaussian(code[0], code[1])
#clf = svm.SVC('ovr')
#clf.fit(code[0], mnist_test_labels)
#X = [[0], [1], [2], [3]]
#Y = [0, 1, 2, 3]

# Fitting SVM
print(""Training SVM"")
clf = svm.SVC(decision_function_shape='ovr')
clf.fit(code_train[0], y_train)    # Solo usando las media pq el codificador devuelve media y desviacino
",scripts/vae_scripts/testing_svm_one_region.py,juanka1331/VAN-applied-to-Nifti-images,1
"        print(""n_samples: %d"" % len(dataset.data))
        
        # split the dataset in training and test set:
        docs_train, docs_test, y_train, y_test = train_test_split(
            dataset.data, dataset.target, test_size=0.25, random_state=None)
    
        # TASK: Build a vectorizer / classifier pipeline that filters out tokens
        # that are too rare or too frequent
        vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,2))
        self.clf = Pipeline([('vect', vectorizer),
                        ('clf', LinearSVC(C=50)),
                        ])
        
        self.clf = self.clf.fit(docs_train,y_train)
        
        joblib.dump(self.clf,""sentiment_analysis.data"")
        # TASK: Predict the outcome on the testing set and store it in a variable
        # named y_predicted
        y_predicted = self.clf.predict(docs_test)
        print(""Complete"")",Scikit/Scikit/sentiment_analysis.py,TechnicHail/COMP188,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/parallel_benchmark/setup.py,jpzk/evopy,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED",shinken/external_command.py,wbsavage/shinken,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",sklearnLearning/statisticalAndSupervisedLearning/svm_gui.py,zhuango/python,1
"            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'linear-svm':
                from sklearn.svm import SVC
                self.estimator_ = SVC(probability=True,
                                      random_state=self.random_state,
                                      kernel='linear',
                                      **self.kwargs)
            else:
                raise NotImplementedError
        else:
            raise ValueError('Invalid parameter `estimator`. Got {}.'.format(
                type(self.estimator)))
",imblearn/under_sampling/instance_hardness_threshold.py,chkoar/imbalanced-learn,1
"from sklearn.learning_curve import  learning_curve
from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

digits = load_digits()
X = digits.data
y = digits.target
train_sizes, train_loss, test_loss= learning_curve(
        SVC(gamma=0.01), X, y, cv=10, scoring='mean_squared_error',
        train_sizes=[0.1, 0.25, 0.5, 0.75, 1])
train_loss_mean = -np.mean(train_loss, axis=1)
test_loss_mean = -np.mean(test_loss, axis=1)

plt.plot(train_sizes, train_loss_mean, 'o-', color=""r"",
             label=""Training"")
plt.plot(train_sizes, test_loss_mean, 'o-', color=""g"",
             label=""Cross-validation"")
",DeepLearnMaterials/tutorials/sklearnTUT/sk9_cross_validation2.py,MediffRobotics/DeepRobotics,1
"                # (lambda p: VarianceThreshold(threshold=(p * (1 - p))))(0.01),

                # See http://stackoverflow.com/a/41601532/341320
                # logtran = FunctionTransformer(snp.log1p, accept_sparse=True, validate=True)
                # X = logtran.transform(X)
                MaxAbsScaler(copy=False),
            )
        else:
            self.preprocess = make_pipeline(None)

        self.model = svm.SVC(**svc_parameters)


    def train(self, training_corpus):
        X, y = self.__convert_edges_features_to_vector_instances(training_corpus)
        X = self.preprocess.fit_transform(X)
        print_debug(""SVC after preprocessing, #features: {} && max value: {}"".format(X.shape[1], max(sklearn.utils.sparsefuncs.min_max_axis(X, axis=0)[1])))

        print_debug(""Train SVC with #samples {} - #features {} - params: {}"".format(X.shape[0], X.shape[1], str(self.model.get_params())))
        start = time.time()",nalaf/learning/lib/sklsvm.py,Rostlab/nalaf,1
"from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC

rng = np.random.RandomState(42)
iris = load_iris()
X = np.hstack([iris.data, rng.uniform(size=(len(iris.data), 5))])
X_train, X_test, y_train, y_test = train_test_split(X, iris.target, random_state=2)

selection_pipe = make_pipeline(SelectKBest(), LinearSVC())
param_grid = {'linearsvc__C': 10. ** np.arange(-3, 3),
              'selectkbest__k': [1, 2, 3, 4, 5, 7]}
grid = GridSearchCV(selection_pipe, param_grid, cv=5)
grid.fit(X_train, y_train)
print(""Best parameters: %s"" % grid.best_params_)
print(""Test set performance: %s"" % grid.score(X_test, y_test))",day3-machine-learning/solutions/pipeline_iris.py,dbouquin/AstroHackWeek2015,1
"        filename, {""testing_result"": testing_result, ""true_result"": y_te, ""location"": te_location})

    return [OA * 100, AA * 100, kappa * 100, label_accuracy, y_te_location, t_te_location, uniq_ele, filename]


def calculate(filename,selected_feature, method=""SVM"", kernel=""poly"", C=1.0, gamma=0.0):
    if method == ""Logistic"":
        clf = LogisticRegression()
    elif method == ""SVM"":
        if kernel == ""RBF"":
            clf = svm.SVC(kernel=""rbf"", C=C, gamma=gamma)
        elif kernel == ""Linear"":
            clf = svm.SVC(kernel=""linear"")
        else:
            clf = svm.SVC(kernel=""poly"")
    return predict(filename, clf, selected_feature)",multi_method.py,soyking/MewCoo,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5, 4.5, 4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0, 10.0, 10.0, 10.0]]),",evopy/examples/problems/SchwefelsProblem240/ORIDSESSVC.py,jpzk/evopy,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/beta_dsessvc/setup.py,jpzk/evopy,1
"##   SVM Training   ##
######################

'''
#1st svm fit
print(""Svm Classification with first 500 features of layer fc7"")
C=1.0

print(""\tfitting svc..."")
sttime= time.clock()
svc = svm.SVC(kernel='linear', C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting rbf_svc..."")
sttime= time.clock()
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting poly_svc..."")
sttime= time.clock()",Region-to-Image_Matching/example/extract_train_test_DBH.py,SelinaChe/Complex-Object-Detection-StackGAN,1
"# The accuracy of the model on the test data
# (this will be introduced in more details in chapter 4)
model.score(prepare_data(data_test), data_test[""Survived""])


# ### Non-linear model with Support Vector Machines

# In[10]:

from sklearn.svm import SVC
model = SVC()
model.fit(features, data_train[""Survived""])


# In[63]:

model.score(prepare_data(data_test), data_test[""Survived""])


# ### Classification with multiple classes: hand-written digits",examples/realWorldMachineLearning/Chapter+3+-+Modeling+and+prediction.py,remigius42/code_camp_2017_machine_learning,1
"# train the data on the new subset


        # iterate through parameters

        C_values = [0, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
        gamma_values = [0.001,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7]

        for params in itertools.product(C_values, gamma_values)

        clf = svm.SVC(C=2.0,gamma=0.5)
        clfoutput = clf.fit(trainingSet, trainingLabels)
# classify
        result = clf.predict(testingSet)
        predictions.append(result.tolist())
        
        num_correct = 0
        for j,k in zip(result,testingLabels):
            if j == k:
                num_correct += 1",Study/Test_speed/CrossValidate.py,JessMcintosh/EMG-classifier,1
"# Train classifiers
#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

C_range = np.logspace(-2, 10, 13)
gamma_range = np.logspace(-9, 3, 13)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, y)

print(""The best parameters are %s with a score of %0.2f""
      % (grid.best_params_, grid.best_score_))

# Now we need to fit a classifier for all parameters in the 2d version


C_2d_range = [1e-2, 1, 1e2]",SVM/rbf_parameters.py,imyeego/MLinPy,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/problems/SchwefelsProblem26/ORIDSESAlignedSVC.py,jpzk/evopy,1
"test_dat = []
for line in test_data:
    test_dat.append((line))

# end

##names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
##         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""]
##classifiers = [
##    KNeighborsClassifier(3),
##    SVC(kernel=""linear"", C=0.025),
##    SVC(gamma=2, C=1),
##    DecisionTreeClassifier(max_depth = 5),
##    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
##    AdaBoostClassifier(),
##    NaiveBayesClassifier()]


def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)",ml/mytask3b.py,john136/exercises,1
"    # split data into train (60%), test(40%)
    # TODO: add validation in the future? train (60%), validation (20%), test(20%)?
    trainingData, testData, trainingLabel, testLabel = train_test_split(data, labels, test_size=0.4)

    print ""numTrainingData  = "", len(trainingData)
    print ""numTestData = "", len(testData)
    print ""numTrainingLabel = "", len(trainingLabel)
    print ""numTestLabel == "", len(testLabel)

    # train the model
    model = svm.SVC(gamma=0.001, C=100)
    model.fit(trainingData, trainingLabel)

    # make a prediction
    testPredLabel = model.predict(testData)

    # calculate PRFS
    print ""testLabel""
    print testLabel
    print ""testPredictedLabel""",src/examples/cf_example.py,tiffanyj41/hermes,1
"from sklearn.svm import LinearSVC
from sklearn.datasets import load_svmlight_file

#data
data = load_svmlight_file(""leu"")

X_1 = data[0].todense().tolist()  # samples 72 features above 7192

y_1 = map(int,data[1])   # classes 2

lr = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validated:
predicted = cross_val_predict(lr, X_1, y_1, cv=10)

fig, ax = plt.subplots()
ax.scatter(y_1, predicted)
ax.plot([min(y_1), max(y_1)], [min(y_1), max(y_1)], 'k--', lw=4)
ax.set_xlabel('Measured')",l1SVM2.py,narendrameena/featuerSelectionAssignment,1
"                    latent = unsup.transform(d[x:x+W, y:y+W, :, :].reshape(W * W * 3, samples))
                    conv[x // 16, y // 16, :, :] += latent
                    #conv[x // 16, y // 16, :, :] = np.maximum(conv[x // 16, y // 16, :, :], latent)
            return conv.reshape(4 * K, samples)

        # Fitting SVN with input data
        print 'Extracting features for training data'
        d = data.get('trn')
        trn_feats = convolve(d.samples, stride)
        print 'Training SVM'
        clf = LinearSVC()
        clf.fit(trn_feats.T, d.labels)

        # Fitting SVN with input data
        print 'Extracting features for validation data'
        d = data.get('val')
        val_feats = convolve(d.samples, stride)
        print 'Predicting results'
        pred = clf.predict(val_feats.T)
        print 100.0 * d.accuracy(pred), '% for',",experiments/coates_unsupervised.py,arasmus/eca,1
"        # todo: parameterise hyperparams
        hparams = {
            'C': 0.0316228,
            'penalty': 'l2',
            'loss': 'l1',
            'class_weight': 'auto',
            'fit_intercept': True
        }

        hparams['dual'] = hparams['penalty'] == 'l2' and hparams['loss'] == 'l1'
        return LinearSVC(**hparams)

    @staticmethod
    def iter_pairwise_instances_with_sampling(docs, sampler, limit):
        toggle = True

        for doc in docs:
            for mention in doc.chains:
                if mention.resolution == None:
                    # ranking model can't learn from NILs",nel/learn/ranking.py,henningpeters/nel,1
"        from :class:`sklearn.neighbors.base.KNeighborsMixin` that will be used
        to find the k_neighbors.

    out_step : float, optional (default=0.5)
        Step size when extrapolating. Used with ``kind='svm'``.

    kind : str, optional (default='regular')
        The type of SMOTE algorithm to use one of the following options:
        ``'regular'``, ``'borderline1'``, ``'borderline2'``, ``'svm'``.

    svm_estimator : object, optional (default=SVC())
        If ``kind='svm'``, a parametrized :class:`sklearn.svm.SVC`
        classifier can be passed.

    n_jobs : int, optional (default=1)
        The number of threads to open if possible.

    Notes
    -----
    See the original papers: [1]_, [2]_, [3]_ for more details.",imblearn/over_sampling/smote.py,glemaitre/imbalanced-learn,1
"from sklearn.pipeline import Pipeline  # noqa
from sklearn.cross_validation import cross_val_score, ShuffleSplit  # noqa
from mne.decoding import ConcatenateChannels, FilterEstimator  # noqa


scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
concatenator = ConcatenateChannels()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                              ('scaler', scaler), ('svm', clf)])

data_picks = mne.pick_types(rt_epochs.info, meg='grad', eeg=False, eog=True,
                            stim=False, exclude=raw.info['bads'])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):
",examples/realtime/plot_compute_rt_decoder.py,Odingod/mne-python,1
"# from sklearn improt *
# iris = datasets.load_iris()
# digits =datasets.load_digits()

# print digits.data
from sklearn import svm
from sklearn import datasets
clf = svm.SVC()
iris = datasets.load_iris()
X, y = iris.data, iris.target
clf.fit(X, y)  




import pickle
s = pickle.dumps(clf)",first.py,DESHRAJ/Machine-Learning,1
"    :year: whether to include ""first attested century"" feature
    :verbose: whether to print updates on progress or not 
        (recommended if making new design matrices for the whole dataset)
    '''
    X, t = get_matrices(test_percent, iofilename, bow, letters, year,
                        syllables, verbose, new_design_matrix)

    if verbose:
        print(""Performing 5-fold CV with a linear SVM"")

    clf = svm.SVC(kernel='linear')

    scores = fit_and_score_CV(clf, X, np.asarray(t), n_folds=5, stratify=False)

    print(""Accuracy: {:.02f}"".format(scores[""CV accuracy""]))
    print(""Precision: {:.02f}"".format(scores[""CV precision_weighted""]))
    print(""Recall: {:.02f}"".format(scores[""CV recall_weighted""]))
    print(""F-score: {:.02f}"".format(scores[""CV F1_weighted""]))

",statistical_categorizer.py,Trevortds/Etymachine,1
"    s_labels.append(str(train[""median_relevance""][i]))
for i in range(len(test.id)):
    s=("" "").join([""q""+ z for z in BeautifulSoup(test[""query""][i]).get_text().split("" "")]) + "" "" + ("" "").join([""z""+ z for z in BeautifulSoup(test.product_title[i]).get_text().split("" "")]) + "" "" + BeautifulSoup(test.product_description[i]).get_text()
    s=re.sub(""[^a-zA-Z0-9]"","" "", s)
    s= ("" "").join([stemmer.stem(z) for z in s.split("" "")])
    t_data.append(s)

clf = pipeline.Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), 
('svd', TruncatedSVD(n_components=300, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)), 
('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), 
('svc', SVC(C=10))])


param_grid = {'svc__C': [10],'svc__gamma': [0,0.001,0.0001,0.00001],'svd__n_components': [300]}

model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, 
                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=5)
                                 
# Fit Grid Search Model
model.fit(s_data, s_labels)",Search_result/srr5.py,tanayz/Kaggle,1
"                sentiment = row[0]
                sentence  = row[2]
                self.train_set.append((sentiment, sentence))

    def train(self):
        sentence_set = [sentence for label, sentence in self.train_set]
        label_set    = [int(label) for label, sentence in self.train_set]

        model = Pipeline([('vect', CountVectorizer()),
                          ('tfidf', TfidfTransformer()),
                          ('clf', OneVsOneClassifier(LinearSVC()))
        ])

        model.fit(np.asarray(sentence_set), np.asarray(label_set))

        self.model = model

    def classify(self, sentence):
        prediction = self.model.predict(np.asarray([sentence]))
        return prediction[0]",ai/tfidf_models.py,beyeran/trier-sentiments,1
"    if output:
        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',
            Instances=n_instances, Results=metrics_results)

if __name__ == '__main__':
    from nltk.classify import NaiveBayesClassifier, MaxentClassifier
    from nltk.classify.scikitlearn import SklearnClassifier
    from sklearn.svm import LinearSVC

    naive_bayes = NaiveBayesClassifier.train
    svm = SklearnClassifier(LinearSVC()).train
    maxent = MaxentClassifier.train

    demo_tweets(naive_bayes)
    # demo_movie_reviews(svm)
    # demo_subjectivity(svm)
    # demo_sent_subjectivity(""she's an artist , but hasn't picked up a brush in a year . "")
    # demo_liu_hu_lexicon(""This movie was actually neither that funny, nor super witty."", plot=True)
    # demo_vader_instance(""This movie was actually neither that funny, nor super witty."")
    # demo_vader_tweets()",libs/nltk/sentiment/util.py,adazey/Muzez,1
"print fpr
print tpr
print thresholds
print(metrics.classification_report(y_test, yscore))
print(metrics.confusion_matrix(y_test, yscore))
print(pd.crosstab(y_test, yscore, rownames=['True'], colnames=['Predicted'], margins=True))

print """"
print ""SVC""
from sklearn.svm import SVC
clf = SVC()
clf.fit(X_train, y_train)
yscore=clf.predict(X_test)
fpr, tpr, thresholds = metrics.roc_curve(y_test, yscore, pos_label=2)
print fpr
print tpr
print thresholds
print(metrics.classification_report(y_test, yscore))
print(metrics.confusion_matrix(y_test, yscore))
print(pd.crosstab(y_test, yscore, rownames=['True'], colnames=['Predicted'], margins=True))",model.py,sebadiaz/ModelDesign,1
"        trainData.append(trainFeatureVectors[ID])
        stars.append(trainingSet.reviews[ID].stars)
    #for i, vec in enumerate(data):
    #    print(targets[i], "" | "", vec)

    featureCount = sum([sum(v) for v in trainData])
    # print(""Feature found: "", featureCount, ""times."")


    classifiers = [DecisionTreeClassifier(),
                    SVC(kernel=""linear""), 
                    SVC(), 
                    LinearSVC(), 
                    MultinomialNB(),
                    GaussianNB(),
                    RandomForestClassifier(),
                    LogisticRegression(),]

    # Cross validation
    if testSetFilename == None:",src/machineLearning.py,kbuschme/irony-detection,1
"    return K
    
  
def dataset3Params(X,y,X_val,y_val):
    C_vals = np.array([0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30])
    sigma_vals = np.array([0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30])
    scores = np.zeros((len(C_vals),len(sigma_vals)))
    for i in range(0,len(C_vals)):
        for j in range(0,len(sigma_vals)):
            gamma = 1.0/(2.0*sigma_vals[j]**2)
            clf = svm.SVC(kernel='rbf',gamma=gamma,C=C_vals[i])
            clf.fit(X,y.ravel())  # train svm on training set
            scores[i,j] = clf.score(X_val,y_val.ravel())  # mean prediction accuracy on validation set
    i,j = np.unravel_index(scores.argmax(), scores.shape)  # find indices for which accuracy is max

    return C_vals[i], sigma_vals[j]
    

if __name__ == '__main__':
 ",Ex6_SVM/Ex6.py,gpiatkovska/Machine-Learning-in-Python,1
"    Train and evaluate a Decision Tree classifier over the speller 
    features in 'speller_features_file', which was previously created by the
    script 'save_features_for_training.py'.
    '''

    # Divide the data into 5 stratified samples
    skf = StratifiedKFold(classLabels, n_folds=5)

    # Make the Decision TRee classifier
    clf = tree.DecisionTreeClassifier()
    #clf = svm.LinearSVC()
    
    # Loop through each fold, training the classifier and printing the results
    for train_index, test_index in skf:
    	
        data_train, data_test = trainingSamples[train_index], trainingSamples[test_index]
        classes_train, classes_test = classLabels[train_index], classLabels[test_index]
        
        clf = clf.fit(data_train, classes_train)
        pred = clf.predict(data_test)",src/train_and_save_g2p.py,gustavoauma/aeiouado_g2p,1
"Xtrain, ytrain, Xtest, ytest = cub_full.get_train_test(feature_extractor_full.extract_one, feature_extractor.extract_one)
Xtrain_f, ytrain_f, Xtest_f, ytest_f = cub_full.get_train_test(feature_extractor_full_f.extract_one, feature_extractor_f.extract_one)

print Xtrain.shape, ytrain.shape
print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(numpy.concatenate((Xtrain, Xtrain_f)), numpy.concatenate((ytrain, ytrain_f)))
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(Xtest)
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_augment_jitter.py,yassersouri/omgh,1
"        result = svm_model(_train_set, _test_set)
        time_cost.append(result[0])
        correct_cost.append(result[1])
    print(""svm time avg :""+ str(sum(time_cost) / len(time_cost)))
    print(""svm correct avg :""+ str(sum(correct_cost) / len(correct_cost)))
    pass


def svm_model(_train_set, _test_set):
    _start_time = datetime.now().timestamp()
    clf = svm.SVC()
    clf = clf.fit(_train_set[0], _train_set[1])
    num = 0
    correct = 0
    for i in clf.predict(_test_set[0]):
        if i == _test_set[1][num]:
            correct += 1
        num += 1
    _end_time = datetime.now().timestamp()
    return (_end_time - _start_time, correct / num)",src/hw1_Classification/PerformanceReoprt.py,MyXOF/Data-Mining-Algorithm,1
"        scores = ['accuracy']
        #scores = ['average_precision']

        for score in scores:
            print()
            print(""PLEASE WAIT"")
            print()
            print(""# Tuning parameters for %s"" % score)
            print()

            clf = GridSearchCV(SVC(), tuned_parameters, cv=10, scoring='%s' % score)
            clf.fit(X_train, y_train)

            print(""--------------------------------------------------------"")
            print(""FOR SUPPORT VECTOR MACHINE"")
            print(""Best parameters set found on development set:"")
            print()
            print(clf.best_params_)
            print()
            print(""--------------------------------------------------------"")",scikit-learn/parameter estimation/parameterEstimation.py,alcmrt/Machine-Learning,1
"    Args:
        params (dict): Dictionary of hyperparameters.

    Returns:
        model (sklearn.svm.SVC): Selected model.

    Raises:
        None
    """"""

    model = svm.SVC(
        decision_function_shape='ovo',
        C=params['C'],
        degree=params['degree'],
        kernel=params['kernel']
    )

    return model

",models/sklearn_launcher.py,wwunlp/sner,1
"    clf = MockClassifier(allow_nd=False)
    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)


def test_cross_val_score_predict_groups():
    # Check if ValueError (when groups is None) propagates to cross_val_score
    # and cross_val_predict
    # And also check if groups is correctly passed to the cv object
    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)

    clf = SVC(kernel=""linear"")

    group_cvs = [LeaveOneGroupOut(), LeavePGroupsOut(2), GroupKFold(),
                 GroupShuffleSplit()]
    for cv in group_cvs:
        assert_raise_message(ValueError,
                             ""The groups parameter should not be None"",
                             cross_val_score, estimator=clf, X=X, y=y, cv=cv)
        assert_raise_message(ValueError,
                             ""The groups parameter should not be None"",",site/lib/python2.7/site-packages/sklearn/model_selection/tests/test_validation.py,asnorkin/sentiment_analysis,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",BitmessageKit/Vendor/static-python/Lib/email/test/test_email_renamed.py,Voluntarynet/BitmessageKit,1
"    (""fu"", FeatureUnion([
        (""feat1"", Pipeline([
            (""kbest"", SelectKBest()),
            ])),
        (""feat2"", Pipeline([
            (""pca"", PCA()),
            (""norm"", Normalizer())
            ])),
        ])),
    (""kbest"", SelectKBest()),
    (""svm"", SVC()),
    ])

cv_params = dict([
    ('fu__feat1__kbest__k', [1,2,3,4,5]),
    ('fu__feat2__pca__n_components', [100,200,300]),
    ('fu__feat2__norm__norm', ['l1', 'l2']),
    ('kbest__k', [1,2,3,4,5]),
    ('svm__C', [1,10,100,1000]),
])",examples/example_file.py,tkerola/pipeline_grid_search,1
"    # by default the score used is the one returned by score method of the estimator (accuracy)
    scores = cross_val_score(clf, X, y, cv=cv)
    print (scores)
    print (""Mean score: {0:.3f} (+/-{1:.3f})"".format(numpy.mean(scores), sem(scores)))



def training(data):

    
    svc_1 = SVC(kernel='linear')
    
    
    #we create the target vector of -1 for sad images, 0 for normal, 
    #and 1 for happy images, the data  is composed by 15 sad image after 15 happy image and after 15 normal image
    zero=[int(i) for i in numpy.zeros(15)]
    one=[int(i) for i in numpy.ones(15)]
    minus1=[int(i) for i in numpy.repeat(-1,15)]
    target=numpy.concatenate((minus1,one,zero,),axis=0)
   ",training.py,arnomoonens/Mussy-Robot,1
"#XtrainAll = scaleAll.transform(XtrainAll)

#--------------------------------------------classification-------------------------------------------
##GradientBoost
#from sklearn.ensemble import GradientBoostingClassifier
#clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, 
#                                     max_depth=1, random_state=0)

## SVM                                     
#from sklearn import svm
#clf = svm.SVC()

#from sklearn.multiclass import OneVsOneClassifier
#from sklearn.multiclass import OutputCodeClassifier
#clf = OutputCodeClassifier(svm.SVC())

## RandomForest
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(min_samples_leaf=10)
",scikit_algo/All.py,sankar-mukherjee/CoFee,1
"    t_tc = time.clock()

    print ""Training...""

    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.3.py,totuta/deep-supertagging,1
"from sklearn import svm
from sklearn.metrics import confusion_matrix
from numpy import array
from math import sin, cos
from decimal import Decimal
from random import random, randint

ac, af, av = processWav('../pyaudio/aaa.wav', 1)
ec, ef, ev = processWav('../pyaudio/eee.wav', 1)

clf = svm.SVC()
clf.fit(af[:100] + ef[:100], [0 for i in range(100)] + [1 for i in range(100)])
print clf.predict(af[120])",speech/main.py,darylsew/audiolearn,1
"        ""brknna"": BRKNeighborsClassifier(mode='a', n_neighbors=options.k, use_lsh_forest=options.lshf,
                                         algorithm='brute', metric='cosine', auto_optimize_k=options.grid_search),
        ""brknnb"": BRKNeighborsClassifier(mode='b', n_neighbors=options.k, use_lsh_forest=options.lshf,
                                         algorithm='brute', metric='cosine', auto_optimize_k=options.grid_search),
        ""listnet"": l2r_classifier,
        ""l2rdt"": ClassifierStack(base_classifier=l2r_classifier, n_jobs=options.jobs, n=options.k, dependencies=options.label_dependencies),
        ""mcknn"": MeanCutKNeighborsClassifier(n_neighbors=options.k, algorithm='brute', metric='cosine', soft=False),
        # alpha 10e-5
        ""bbayes"": OneVsRestClassifier(BernoulliNB(alpha=options.alpha), n_jobs=options.jobs),
        ""mbayes"": OneVsRestClassifier(MultinomialNB(alpha=options.alpha), n_jobs=options.jobs),
        ""lsvc"": OneVsRestClassifier(LinearSVC(C=4, loss='squared_hinge', penalty='l2', dual=False, tol=1e-4),
                                    n_jobs=options.jobs),
        ""logregress"": logregress,
        ""sgd"": sgd,
        ""rocchio"": RocchioClassifier(metric = 'cosine', k = options.k),
        ""sgddt"": ClassifierStack(base_classifier=sgd, n_jobs=options.jobs, n=options.k),
        ""rocchiodt"": ClassifierStack(base_classifier=RocchioClassifier(metric = 'cosine'), n_jobs=options.jobs, n=options.k),
        ""logregressdt"": ClassifierStack(base_classifier=logregress, n_jobs=options.jobs, n=options.k),
        ""mlp"": mlp,
        ""nam"": ThresholdingPredictor(MLP(verbose=options.verbose, final_activation='sigmoid'), alpha=options.alpha, stepsize=0.01, verbose=options.verbose),",Code/lucid_ml/run.py,quadflor/Quadflor,1
"cv = ShuffleSplit(n_samples, n_iter=50, train_size=500, test_size=500,
    random_state=0)
%time scores = cross_val_score(SVC(C=10, gamma=0.005), X, y, cv=cv)
print(mean_score(scores))",unit_20/parallel_ml/notebooks/solutions/05A_large_cross_validation.py,janusnic/21v-python,1
"from sklearn.svm.classes import LinearSVC

from ..Classifier import Classifier
from ...language.Ruby import Ruby


class LinearSVCRubyTest(Ruby, Classifier, TestCase):

    def setUp(self):
        super(LinearSVCRubyTest, self).setUp()
        self.mdl = LinearSVC(C=1., random_state=0)

    def tearDown(self):
        super(LinearSVCRubyTest, self).tearDown()",tests/classifier/LinearSVC/LinearSVCRubyTest.py,nok/sklearn-porter,1
"        
        for vect in CaveIterator():
            if vect is not None:
                X.append(vect)
                y.append(label)

    print ""All CAVE collection complete.""

    print ""Now training SVM...""

    clf = svm.SVC(probability=True)
    clf.fit(X, y)
    output = open(FILENAME, 'wb')

    output_obj = (clf, (X,y))

    pickle.dump(output_obj, output)

    print ""SVM data written to \""%s\"""" % FILENAME
",object-recognition/trainer.py,cuauv/software,1
"y = array[:, 8]
seed = 21
kfold = model_selection.KFold(n_splits=10, random_state=seed)

#SUB-models
estimators = []
LR_model = LogisticRegression()
estimators.append(('logistic', LR_model))
DT_model = DecisionTreeClassifier()
estimators.append(('cart', DT_model))
SV_model = SVC()
estimators.append(('svm', SV_model))

#Ensemble creation
ensemble = VotingClassifier(estimators)
results = model_selection.cross_val_score(ensemble, X, y, cv=kfold)

print('results: ')
print(results)
print()",Python/Machine Learning/ScikitClassifiers/Classifiers/Voting_Ensemble.py,sindresf/The-Playground,1
"        
        #get test data
        #featurize test data

        #generate spatio temporal features

        #generate additional features

        #train classifier
        #l1
        #model = svm.LinearSVC(C=1.0, penalty='l1',dual=False,n_jobs=8)
        #l2
        #model = svm.LinearSVC(C=1.0, penalty='l2',n_jobs=8)

        #tune hyper parameter
        # run randomized search
        n_iter_search = 10
        param_grid = {'C':np.logspace(1.0, 500.0, num=n_iter_search)}
        grid_search = GridSearchCV(model, param_grid=param_grid)
        grid_search.fit(X_train, y_train)",run.py,benbo/Summer_Hack_CP1,1
"# 	predictions = lr.predict(x_test)
# 	print metrics.accuracy_score(predictions, y_test)



# # SVMs
# for kernel in ['linear', 'rbf']:
# 	for C in [1000, 100, 30, 10, 3, 1]:
# 		print C
# 		print kernel
# 		svm = SVC(kernel=kernel, C=C)
# 		svm.fit(x_train, y_train)
# 		predictions = svm.predict(x_test)
# 		print metrics.accuracy_score(predictions, y_test)

# # Linear regression
# lr = LinearRegression()
# lr.fit(x_train, y_train)
# predictions = lr.predict(x_test).round().clip(0,3)
# print metrics.accuracy_score(predictions, y_test)",BoW.py,HristoBuyukliev/fakenews,1
"    print ""OPF: time elapsed in predicting: %f secs"" % (time.time()-t)

    print ""Classification report for OPF:\n%s\n"" % (metrics.classification_report(label_test_32, predicted))
    print ""Confusion matrix:\n%s"" % metrics.confusion_matrix(label_test_32, predicted)

  opf()

  print ""-""*20, ""SVM"", ""-""*20
  def _svm():

    clf = svm.SVC()

    t = time.time()
    clf.fit(data_train, label_train)
    print ""SVM: time elapsed in fitting: %f secs"" % (time.time()-t)

    t = time.time()
    predicted = clf.predict(data_test)
    print ""SVM: time elapsed in predicting: %f secs"" % (time.time()-t)
",examples/handwritten.py,victormatheus/LibOPF,1
"from sklearn.svm.classes import NuSVC

from ..Classifier import Classifier
from ...language.PHP import PHP


class NuSVCPHPTest(PHP, Classifier, TestCase):

    def setUp(self):
        super(NuSVCPHPTest, self).setUp()
        self.mdl = NuSVC(kernel='rbf', gamma=0.001, random_state=0)

    def tearDown(self):
        super(NuSVCPHPTest, self).tearDown()",tests/classifier/NuSVC/NuSVCPHPTest.py,nok/sklearn-porter,1
"        
        clf=DecisionTreeClassifier()
        clf.fit(self.X_train,self.y_train)
        
        return clf


    def getSVM (self) :
        ""Get the model Fit to the training data""
        
        clf=SVC()
        clf.fit(self.X_train,self.y_train)
        
        return clf


    def getRandomForest (self) :
        ""Get the model Fit to the training data""
        
        clf=RandomForestClassifier()",TrainModels.py,SoimulPatriei/TMCleaner,1
"    print(""n_samples: %d"" % len(dataset.data))
    
    # split the dataset in training and test set:
    docs_train, docs_test, y_train, y_test = train_test_split(
        dataset.data, dataset.target, test_size=0.25, random_state=None)

    # TASK: Build a vectorizer / classifier pipeline that filters out tokens
    # that are too rare or too frequent
    vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,2))
    pipe = Pipeline([('vect', vectorizer),
                    ('clf', LinearSVC(C=50)),
                    ])
    # TASK: Build a grid search to find out whether unigrams or bigrams are
    # more useful.
    parameters = {
                'vect__ngram_range': [(1, 1), (1, 2)],
                #'vect__min_df': [2,3,4],
                #'clf__C':[50,500,1000]
                }
    # Fit the pipeline on the training set using grid search for the parameters",Scikit/Scikit/exercise_3_2.py,TechnicHail/COMP188,1
"    import acorn.numpy as np
    rng = np.random.RandomState(2)
    X += 2 * rng.uniform(size=X.shape)

    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import train_test_split
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
    
    from sklearn.svm import SVC
    svc = SVC()
    svc.fit(X_train, y_train)

    yL=svc.predict(X_test)

    from db import db_entries
    sentries, uuids = db_entries(""sklearn"")
    
    #Unfortunately, py2 and py3 are getting different stack lengths on the
    #calls, so we have to handle them (almost) separately. Since this is the",tests/test_0sklearn.py,rosenbrockc/acorn,1
"        [0., 0., 0.1, 0., 0.1, 0., 0., 0., 0., 0.1, 0.]])
    ok_(numpy.allclose(matrix.todense(), result))


def test_SklExtractorCanBeUsedInPipeline():
    tokenizer = bc.BasicTokenizer()
    extractor = bc.FrequenciesExtractor(tokenizer)
    predictor = pipeline.Pipeline([
        ('extractor', bc.SklExtractor(extractor)),
        ('svd', decomposition.TruncatedSVD(10, random_state=123)),
        ('svm', svm.SVC())])

    books, authors = trainingCollection.as_arrays()
    predictor.fit(books, authors)
    #books, authors = testingCollection.as_arrays()
    eq_(list(predictor.predict(books)), list(authors))


def test_SklExtractorCanBeUsedInCrossValidation():
    tokenizer = bc.BasicTokenizer()",book_classification/tests/sklearn_compat_test.py,alepulver/tp-final-incc,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CIFE.cife(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_CIFE.py,jundongl/scikit-feast,1
"    #                                 )
    # clf_stacking='nn'
    clf = Mayou(base_estimators={'xgb': None}, bagging_base=None, bagging_stack=8,
                stacking=clf_stacking, features_stack=branch_names,
                transform=False, transform_pred=False)
    # clf = SklearnClassifier(GaussianNB())
    # clf = SklearnClassifier(BaggingClassifier(n_jobs=1, max_features=1.,
    # bootstrap=False, base_estimator=clf, n_estimators=20, max_samples=0.1))
    # clf = XGBoostClassifier(n_estimators=400, eta=0.1, nthreads=6)
    # clf = SklearnClassifier(BaggingClassifier(clf, max_samples=0.8))
    # clf = SklearnClassifier(NuSVC(cache_size=1000000))
    # clf = SklearnClassifier(clf)
    if folding:
        X_train = X_test = X
        y_train = y_test = y
        w_train = w_test = w
        clf = FoldingClassifier(clf, n_folds=5)

    clf.fit(X_train, y_train, w_train)
",raredecay/tools/estimator.py,mayou36/raredecay,1
"#testacc=N.zeros(nruns)
#for run in range(nruns):
try:
    run=int(sys.argv[1])
except:
    run=0
if 1:
    N.random.shuffle(labels)
    pred=N.zeros(len(labels))
    for train,test in skf:
        clf=LinearSVC()
        clf.fit(data[:,train].T,labels[train])
        pred[test]=clf.predict(data[:,test].T)



    testacc=N.mean(pred==labels)
    #print 'test accuracy=%0.2f'%N.mean(pred==labels)

N.savetxt(os.path.join(outdir,'randlabel/testacc_randlabels_SVM_%d.txt'%run),[testacc])",openfmri_paper/4.1_randomize_wholebrain.py,poldrack/openfmri,1
"                            'type':'category',
                            'bound':['True', 'False']
                            }
                       }
    
    def obj(self, cfg):
        
        try:
            
            # create the regressor with given params
            clsf = LinearSVC(C = 10.0 ** cfg['C'], 
                       loss= cfg['loss'], 
                       penalty=cfg['penalty'], 
                       dual=cfg['dual'] == 'True')
            
            # fit the regressor
            clsf.fit(self.X, self.Y)
            
            # get the validation score
            score = clsf.score(self.Xv, self.Yv)",example_problems.py,iaroslav-ai/black-box-optimization,1
"from std_msgs.msg import String

# scikit libs
import numpy as np
from sklearn import svm
from sklearn import cross_validation
from sklearn.externals import joblib

rospack = rospkg.RosPack()
node_path = rospack.get_path('tug_test')
clf_svm = svm.SVC()


def learn():

    data_path = node_path + '/data/sit_vs_others.csv'
    f = open(data_path)
    f.readline()
    db_raw = np.genfromtxt(f, delimiter="","")
    # print ""raw_data_shape:"" , np.shape(db_raw)",human_motion_analysis/tug_test/scripts/learn.py,RIVeR-Lab/fall_risk_assessment,1
"        for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):
            print sentiment, text


#Parte i)
def do_SVM(x, y, xt, yt):
    Cs = [0.01, 0.1, 10, 100, 1000]
    model = []
    for i in range (0,len(Cs),1):
        print ""El valor de C que se esta probando: %f"" % Cs[i]
        model.append( LinearSVC(C=Cs[i]) )
        model[i] = model[i].fit(x, y)
        score_the_model(model[i], x, y, xt, yt, ""SVM"")
    return model

print ""\n i) Support Vector Machine\n""
for i in range(0,my_switch ,1):
    if (i==0):
        print ""STEMMING:\n""
    if (i==1):",Parte2/parte2.py,topotech/AID_tarea3,1
"y_true = [0, 1, 2, 3]
y_pred = [0, 2, 1, 3]

from sklearn.metrics import fbeta_score, make_scorer

ftwo_scorer = make_scorer(fbeta_score, beta=2)

from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC

grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)",examples/iris.py,DailyActie/Surrogate-Model,1
"
X_test = np.array(['nice day in nyc',
                   'welcome to london',
                   'hello welcome to new york. enjoy it here and london too'])   

target_names = ['New York', 'London']

classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(SVC(kernel='linear')))])

classifier.fit(X_train, y_train_binarized)

predicted = classifier.predict(X_test)

for item, preds in zip(X_test, predicted):
     norm_preds = [(0 if x < 0.5 else 1) for x in preds.tolist()]
     pred_targets = ["""" if x[1] == 0 else target_names[x[0]]
                     for x in enumerate(norm_preds)]",ScikitlearnPlayground/MultiLabelClassification.py,eboreapps/Scikit-Learn-Playground,1
"  l = len(data)
  X = data[:,1:]
  y = data[:,0]

  train_x = X[:-validation_size]
  train_y = y[:-validation_size]
  test_x  = X[-validation_size:]
  test_y  = y[-validation_size:]
  """"""
  # Create and fit the svm classifier
  svc = svm.SVC(kernel='linear')
  svc.fit(train_x, train_y)
  predict_labels = svc.predict(test_x)
  print(""predicted labels :"")
  print(predict_labels)
  print(""real labels :"")
  print(test_y)
  print(""mean deviation :"")
  predict_label_np = np.array(predict_labels)
  test_y_np = np.array(test_y)",cheminfo/prediction_molecules_properties_fingerprint/sk_svm.py,jajoe/machine_learning,1
"
from epitopes import features, imma2

print ""Loading data and transforming to toxin features""
imm, non = imma2.load_classes()
X, Y = features.toxin_features(imm, non, substring_length = 3, positional=True)

def run_classifiers(X,Y):
  print ""Data shape"", X.shape
  for c in [0.0001, 0.001, 0.01, 0.1, 1]:
    svm = sklearn.svm.LinearSVC(C=c)
    print ""SVM C ="", c
    print np.mean(sklearn.cross_validation.cross_val_score(svm, X, Y, cv = 10))

  n_classifiers = 1000
  rf = sklearn.ensemble.RandomForestClassifier(n_classifiers)
  print ""Random Forest""
  print np.mean(sklearn.cross_validation.cross_val_score(rf, X, Y, cv = 10))

""""""",Jan21_toxin_positional.py,hammerlab/immuno_research,1
"#     i += 1
#   X = np.array(X)
#   if L2norm:
#     norm = np.linalg.norm(X,ord=None,axis=1)
#     X /= norm.reshape((X.shape[0],1))
#   return X

def create_clfs(C, nb):
  clfs = []
  for i in range(nb):
    clf = LinearSVC(dual=True, C=C, loss='hinge', tol=1e-7, fit_intercept=False, max_iter=3000)
    #clf = SVC(kernel='linear',C=C)
    clfs.append(clf)
  return clfs

def train_multilabel(clfs, XList, yList):
  global log
  nbLabel = len(yList)
  for lbl in range(nbLabel):
    if log: print 'Fit...'",src/experiments/train_svm2.py,Cadene/Deep6Framework,1
"X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

confidences = []
predictions = []
start_time = time.time()
for i in range(10):
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)

    #clf = neighbors.KNeighborsClassifier()
    clf = svm.SVC()


    clf.fit(X_train, y_train)
    confidence = clf.score(X_test, y_test)
    print(confidence)
    confidences.append(confidence)

    example_measures = np.array([[4,2,1,1,1,2,3,2,1]])
    example_measures = example_measures.reshape(len(example_measures), -1)",PracMachLrng/sentex_ML_demo17_Support_Vector_Machine.py,aspiringguru/sentexTuts,1
"from sklearn.naive_bayes import GaussianNB
naivebayes_model = GaussianNB()
naivebayes_model.fit(X_cropped, y_cropped)
y_validation_predicted = naivebayes_model.predict(X_validation)
print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)


# Start SVM Classification
print ""Performing SVM Classification:""
from sklearn.svm import SVC
svm_model = SVC(kernel='rbf' ,probability=True, max_iter=100000)
svm_model.fit(X_cropped, y_cropped)
y_train_predicted = svm_model.predict(X_train)
print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# plt.show()

y_validation_predicted = svm_model.predict(X_validation)
print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
",Code/Machine_Learning_Algos/training_t2.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"    
## covert list into numpy array
X = np.array(X)
y = np.array(y)
print X.shape
print y.shape

from sklearn import svm
from sklearn.externals import joblib
print 'start learning SVM.'
lin_clf = svm.LinearSVC()
lin_clf.fit(X, y)  
#clf = svm.SVC()
#clf.fit(X, y)  
print 'finish learning SVM.'",01_pedestrian_detector/notebooks/train_before.py,payashim/python_visual_recognition_tutorials,1
"# global var which indicates the order and name of the surgical instruments
INSTRU_NAMES = ['scalpel','retractor','scissors','hemostat','forceps']


def GetClassifier(name):
    """"""
    return the clf based on the @arg name
    the parameters are chosen to be default
    """"""
    if name == 'linearSVM':
        clf = svm.SVC(kernel = 'linear', probability = True)
    elif name == 'rbfSVM':
        clf = svm.SVC(kernel = 'rbf', probability = True)
    elif name == 'sigmoidSVM':
        clf = svm.SVC(kernel = 'sigmoid')
    elif name == 'polySVM':
        clf = svm.SVC(kernel = 'poly')
    elif name == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif name == 'randomforest':",InstrumentRecognition/InstrumentRecognition - Copy.py,tian-zhou/Surgical-Instrument-Dataset,1
"from sklearn.decomposition import PCA




def apply_algorithm(paras, data):
    X = data[""X""]
    y = data[""y""]

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    elif paras['clf'] == 'lr':
        clf = linear_model.LogisticRegression(C=0.5)",python/per_year_predict/methods.py,Healthcast/RSV,1
"#print iris
#print type(iris)
sent = pickle.load(open('sent.pkl'))
print sent
#X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data,iris.target, test_size=0.4,random_state=0)
# make the labels into 0 and 1 to make it easier
target = sent['target'] ==3
X_train, X_test, y_train, y_test = cross_validation.train_test_split(sent['data'],target, test_size=0.2,random_state=0)
print (X_train.shape,y_train.shape)
print (X_test.shape,y_test.shape)
clf = svm.SVC(kernel='linear',C=1).fit(X_train,y_train)
score = clf.score(X_test,y_test)
print score
#print type(iris.target[0])
#print iris.target",breakdown_detector_svm.py,leahrnh/ticktock_text_api,1
"
    ###########
    # Train SVM
    ###########
    if (not exists(conf.modelPath)) | OVERWRITE:
        if VERBOSE: print str(datetime.now()) + ' training liblinear svm'
        if VERBOSE == 'SVM':
            verbose = True
        else:
            verbose = False
        clf = svm.LinearSVC(C=conf.svm.C)
        if VERBOSE: print clf
        clf.fit(train_data, all_images_class_labels[selTrain])
        with open(conf.modelPath, 'wb') as fp:
            dump(clf, fp)
    else:
        if VERBOSE: print 'loading old SVM model'
        with open(conf.modelPath, 'rb') as fp:
            clf = load(fp)
",phow_birdid_mser.py,lbarnett/BirdID,1
"#         self.__clfs = None
#         self.__params = {'C': C, 'kernel': kernel, 'degree': degree, 'gamma': gamma, 'coef0': coef0,
#                          'shrinking': shrinking, 'probability': probability, 'tol': tol,
#                          'cache_size': cache_size, 'class_weight': class_weight, 'verbose': verbose,
#                          'max_iter': max_iter, 'decision_function_shape': decision_function_shape,
#                          'random_state': random_state}
#
#     def fit(self, x, y, sample_weight=None):
#         self.classes_ = np.unique(y)
#         n_classes_ = len(self.classes_)
#         self.__clfs = [SVC(C=self.__params['C'], kernel=self.__params['kernel'],
#                            degree=self.__params['degree'], gamma=self.__params['gamma'],
#                            coef0=self.__params['coef0'], shrinking=self.__params['shrinking'],
#                            probability=self.__params['probability'], tol=self.__params['tol'],
#                            cache_size=self.__params['cache_size'],
#                            class_weight=self.__params['class_weight'],
#                            verbose=self.__params['verbose'], max_iter=self.__params['max_iter'],
#                            decision_function_shape=self.__params['decision_function_shape'],
#                            random_state=self.__params['random_state'])
#                        for i in range(0, n_classes_)]",conformal_predictors/predictors/SVC.py,SergioGonzalezSanz/conformal_predictors,1
"#    test_data=N.load('zstat_run2.npy')


skf=StratifiedKFold(labels,8)
#loo=LeaveOneOut(len(labels))


if trainsvm:
    pred=N.zeros(len(labels))
    for train,test in skf:
        clf=LinearSVC()
        data[train].shape
        data[test].shape
        clf.fit(data[:,train].T,labels[train])
        pred[test]=clf.predict(data[:,test].T)


testacc=N.mean(pred==labels)
print 'test accuracy = %f'%N.mean(pred==labels)
cm=confusion_matrix(labels,pred)",openfmri_paper/4_classify_task_wholebrain.py,poldrack/openfmri,1
"    train_features_sparse, true_train_classes = load_svmlight_file(train_file)
    test_features_sparse, true_test_classes = load_svmlight_file(test_file)

    # 缺省加载为稀疏矩阵。转化为普通numpy array
    train_features = train_features_sparse.toarray()
    test_features = test_features_sparse.toarray()

    print ""Train: %dx%d"" % (train_features.shape)

    # 线性SVM，L1正则
    model = svm.LinearSVC(penalty='l1', dual=False)

    # 在training文件上训练
    print ""Training... Predicting ..."",
    model.fit(train_features, true_train_classes)
    print ""Done.""

    # 在test文件上做预测
    pred_train_classes = model.predict(train_features)
    pred_test_classes = model.predict(test_features)",classEval.py,poodarchu/SogouPersona,1
"    def wrapped(**kwargs):
        return -fun(**kwargs)
    return wrapped

def make_svm_objfun(data, labels, num_folds):

    @optunity.cross_validated(x=data, y=labels, num_folds=5, regenerate_folds=True)
    def svm_rbf_tuned_auroc(x_train, y_train, x_test, y_test, logC, logGamma):
        if type(logC) == np.array: logC = logC[0]
        if type(logGamma) == np.array: logGamma = logGamma[0]
        model = sklearn.svm.SVC(C=10 ** logC, gamma=10 ** logGamma).fit(x_train, y_train)
        decision_values = model.decision_function(x_test)
        auc = optunity.metrics.roc_auc(y_test, decision_values)
        return auc

    return svm_rbf_tuned_auroc

def svm_search_space(logC, logGamma):
    return {'logC': hp.uniform('logC', logC[0], logC[1]),
            'logGamma': hp.uniform('logGamma', logGamma[0], logGamma[1])}",benchmarks/optunity/hpolib_generator.py,claesenm/optunity-benchmark,1
"    # print repr(max_shape)

    hogs = [feature_extractor(i) for i in imgs]
    X = hogs
    Y = [path.basename(f).split('_')[0].replace('-', '_').upper() for f in clean]

    sel = feature_selection.VarianceThreshold(threshold=(.8 * (1 - .8)))
    sel.fit(X, Y)
    X = [sel.transform(x).flatten() for x in X]

    clf = svm.LinearSVC()
    clf.fit(X, Y)

    # clf = neighbors.KNeighborsClassifier(n_neighbors=1, weights='uniform')
    # clf.fit(X, Y)

    if save_if_not_exists: joblib.dump(clf, fn)
    return (clf, sel)

",hearme.py,sskiswani/HearMe,1
"    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=20,
                                 random_state=generator, max_depth=2)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)

    # Check if the supports are equal
    assert_array_equal(rfe.get_support(), rfe_svc.get_support())


def test_rfe():
    generator = check_random_state(0)",sklearn/feature_selection/tests/test_rfe.py,CforED/Machine-Learning,1
"
    print ""Training...""

    # commented out to delete .toarray() option, because len(data_X[0]) is not defined
    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.4.py,totuta/deep-supertagging,1
"    # concatenate all features togethe
    X = sparse.csr_matrix(sparse.hstack(feature_matrices))
    column_names = np.concatenate(column_names)
    if verbose > 0:
        print ""Full feature martix size:"", X.shape

    #return items, column_names, X
    if model_type == 'LR':
        model = lr(penalty=regularizer, C=alpha)
    elif model_type == 'SVM':
        model = svm.LinearSVC(C=alpha, penalty=regularizer)
    else:
        sys.exit('Model type ' + model_type + ' not supported')

    y = labels.as_matrix().ravel()

    skf = StratifiedKFold(y, folds,random_state=17)
    f1 = cross_val_score(model, X, y, cv=skf,scoring=score_eval,n_jobs=n_jobs).mean()

    print f1",botc/classify_test.py,dallascard/botc,1
"X = X.reshape((n_samples, -1))
# add 200 non-informative features
X = np.hstack((X, 2 * np.random.random((n_samples, 200))))

###############################################################################
# Create a feature-selection transform and an instance of SVM that we
# combine together to have an full-blown estimator

transform = feature_selection.SelectPercentile(feature_selection.f_classif)

clf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0))])

###############################################################################
# Plot the cross-validation score as a function of percentile of features
score_means = list()
score_stds = list()
percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)

for percentile in percentiles:
    clf.set_params(anova__percentile=percentile)",projects/scikit-learn-master/examples/svm/plot_svm_anova.py,DailyActie/Surrogate-Model,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = RSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'project')

    method = CMAESRRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 5.0,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/sum_constraints_cmaes_cmaesrrsvc/setup.py,jpzk/evopy,1
"                            #regression
                            type(LassoLarsCV()): 'mse',
                            type(SVR()): 'mae',
                            type(LinearSVR()): 'mae',
                            type(KNeighborsRegressor()): 'mse',
                            type(DecisionTreeRegressor()): 'mse',
                            type(RandomForestRegressor()): 'mse',
                            #classification
                            type(SGDClassifier()): 'r2',
                            type(LogisticRegression()): 'r2',
                            type(SVC()): 'r2',
                            type(LinearSVC()): 'r2',
                            type(RandomForestClassifier()): 'r2',
                            type(DecisionTreeClassifier()): 'r2',
                            type(DistanceClassifier()): 'silhouette',
                            type(KNeighborsClassifier()): 'r2',
            }[type(self.ml)]


        # Columns to always ignore when in an operator",few/few.py,lacava/few,1
"
#gmm_model = mixture.GMM(n_components=GMM_N_COMPONETNS)

#-------------------
# train
#-------------------
def train(x_train,y_train):
    
    #pipeline = Pipeline([
    #    ('scl', StandardScaler()),
    #    ('clf', SVC(probability=True))
    #])
        
    #param_grid = [{'clf__kernel': ['linear'], 'clf__C': [1, 1.5, 2, 5]}]
    #estimator = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
    #print 'x_train len:',len(x_train)
    #print 'y_train len:',len(y_train)
    #print 'y_train:',y_train
    #estimator = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
    estimator = SVC()",main_predict.py,aixiwang/audio_feature_sender,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,cmoutard/mne-python,1
"
    # Train classifier
    #
    # For an initial search, a logarithmic grid with basis
    # 10 is often helpful. Using a basis of 2, a finer
    # tuning can be achieved but at a much higher cost.
    C_range = 10.0 ** np.arange(-2, 9)
    gamma_range = 10.0 ** np.arange(-5, 4)
    param_grid = dict(gamma=gamma_range, C=C_range)
    cv = StratifiedKFold(y=y_train, n_folds=3)
    grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
    grid.fit(X_train, y_train)
    
    # Get the C and gamma parameters
    C, gamma = grid.best_params_['C'], grid.best_params_['gamma']

    # Evaluate preformance on test data    
    frac_correct_test = (grid.predict(X_test)==y_test).sum()/len(y_test)
    
    # Estimate the error probability",classify.py,kalleknast/phee_classification,1
"  xs = dataset[:][:,1:]

  logging.basicConfig(filename='predict.log',level=logging.DEBUG, format='')

  plt.clf()

  # train model
  plt.subplot(2,2,1)
  plt.title('No weight, C=1')

  clf = svm.SVC(kernel='linear', C=1, probability=True)
  predict(clf, xs, ys)

  plt.subplot(2,2,2)
  plt.title('Weight 1:3, C=1')

  clf = svm.SVC(kernel='linear', C=1, probability=True, class_weight={1: 3})
  predict(clf, xs, ys)

  plt.subplot(2,2,3)",wholesale_customers/predict_linear.py,Josephu/svm,1
"from sklearn.naive_bayes import GaussianNB
naivebayes_model = GaussianNB()
naivebayes_model.fit(X_cropped, y_cropped)
y_validation_predicted = naivebayes_model.predict(X_validation)
print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)


# Start SVM Classification
print ""Performing SVM Classification:""
from sklearn.svm import SVC
svm_model = SVC(kernel='rbf' ,probability=True, max_iter=10000)
svm_model.fit(X_cropped, y_cropped)
y_train_predicted = svm_model.predict(X_train)
print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# plt.show()

y_validation_predicted = svm_model.predict(X_validation)
print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
",Code/Machine_Learning_Algos/training_t4.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"from sklearn import svm
#Values <=2 classified as 0. Values >2 classified as 1.
print(""The intent is for the machine to learn to classify coordinates <=[2,2] as 0, and coordinates >[2,2] as 1."")
X=[[0,0], [1,1], [2,2], [3,3], [4,4], [5,5]]
y=[0,0,0,1,1,1]
machine=svm.SVC()
machine.fit(X,y)

print(""[-1,-1] classified as: ""+str(machine.predict([[-1.,-1.]])))
print(""[1,2] classified as: ""+str(machine.predict([[1,2]])))
print(""[2.4,2.4] classified as ""+str(machine.predict([[2.4,2.4]])))
print(""[2.5,2.5] classified as ""+str(machine.predict([[2.5,2.5]])))
print(""[2.6,2.6] classified as ""+str(machine.predict([[2.6,2.6]])))
print(""[6,6] classified as: ""+str(machine.predict([[6,6]])))",Cloudera/Code/SVM/simple_test.py,cybercomgroup/Big_Data,1
"
        clf=None
        
        # convert into  a large array
        training_X = convert_image_list( training_images )  
        training_Y = np.ravel( np.concatenate( tuple(j for j in training_output ) ) )

        if options.debug: print(""Fitting..."")

        if options.method==""SVM"":
            clf = svm.SVC()
        elif options.method==""nuSVM"":
            clf = svm.NuSVC()
        elif options.method=='NN':
            clf = neighbors.KNeighborsClassifier(options.n)
        elif options.method=='RanForest':
            clf = ensemble.RandomForestClassifier(n_estimators=options.n,random_state=options.random)
        elif options.method=='AdaBoost':
            clf = ensemble.AdaBoostClassifier(n_estimators=options.n,random_state=options.random)
        elif options.method=='tree':",tools/error_correct.py,BIC-MNI/pyezminc,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also
    --------",summary/sumy/sklearn/svm/classes.py,WangWenjun559/Weiss,1
"                                                      'model__n_estimators':[10, 20, 50]
                                                      }),
          ('ExtraTrees', ExtraTreesClassifier(), {
                                                  'model__criterion':['gini', 'entropy'],
                                                  'model__n_estimators':[10, 20, 50]
                                                  }),
          ('KNN', KNeighborsClassifier(warn_on_equidistant=False,
                                       weights='distance'), {
                                                             'model__n_neighbors':[5,10,20]
                                                             }),
          ('SVR', SVC(), {'model__kernel':['linear', 'rbf']})]

num = 50
ncols = order[:num]
tmp = pred_data[['HIVD-m']+ncols].dropna()

X = tmp[ncols].values
y = tmp['HIVD-m'].values < 0
num_select = 20
default_params = {'select__k':range(5, 40, 10)}",NewCannabAnalysis.py,JudoWill/ResearchNotebooks,1
"    print(""n_samples: %d"" % len(dataset.data))

    # split the dataset in training and test set:
    docs_train, docs_test, y_train, y_test = train_test_split(
        dataset.data, dataset.target, test_size=0.25, random_state=None)

    # TASK: Build a vectorizer / classifier pipeline that filters out tokens
    # that are too rare or too frequent
    pipeline = Pipeline([
        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),
        ('clf', LinearSVC(C=1000)),
    ])

    # TASK: Build a grid search to find out whether unigrams or bigrams are
    # more useful.
    # Fit the pipeline on the training set using grid search for the parameters
    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
    }
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)",projects/scikit-learn-master/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py,DailyActie/Surrogate-Model,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_13_2014_server.py,magic2du/contact_matrix,1
"
def get_train_test_split(dataset, test_size=0.33):
    # convert data to csr for faster row slicing
    data = dataset[:, :-1].tocsr()
    target = dataset[:, -1].toarray().ravel()
    return cross_validation.train_test_split(data, target, test_size=test_size)


def train_model(train_data, train_target):
    # we use a linear SVC for the sake of this example
    model = svm.LinearSVC()
    print 'training the model ...'
    model.fit(train_data, train_target)
    return model


def evaluate(model, test_data):
    print 'evaluating the model ...'
    return model.predict(test_data)
",tools/tutorial.py,alexksikes/elasticsearch-vectorize,1
"					% (grid.best_params_, grid.best_score_))

			# Now we need to fit a classifier for all parameters in the 2d version
			# (we use a smaller set of parameters here because it takes a while to train)

			C_2d_range = [1e-2, 1, 1e2]
			gamma_2d_range = [1e-1, 1, 1e1]
			classifiers = []
			for C in C_2d_range:
				for gamma in gamma_2d_range:
					clf = SVC(C=C, gamma=gamma)
					clf.fit(X_2d, y_2d)
					classifiers.append((C, gamma, clf))

			##############################################################################
			# visualization
			#
			# draw visualization of parameter effects

			plt.figure(figsize=(8, 6))",Dalitz_simplified/classifier_eval_simplified.py,weissercn/MLTools,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx, feature_score, subset_score = trace_ratio.trace_ratio(X[train], y[train], num_fea, style='fisher')

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",skfeature/example/test_trace_ratio.py,jundongl/scikit-feature,1
"
    # Prescaling
    scaler = preprocessing.Scaler().fit(data.features)
    scaledFeatures = scaler.transform(data.features)

    # Feature selection
    selector = feature_selection.SelectKBest(feature_selection.f_regression).fit(scaledFeatures, data.labels)
    selectedFeatures = selector.transform(scaledFeatures)

    # Train a classifier
    clf = SVC(kernel='linear', C=1).fit(selectedFeatures, data.labels)

    # Save to files
    if not os.path.exists(modelDir(sensor)):
        os.makedirs(modelDir(sensor))
    joblib.dump(clf, modelDir(sensor) + 'model.pkl')
    joblib.dump(scaler, modelDir(sensor) + 'scaler.pkl')
    joblib.dump(selector, modelDir(sensor) + 'selector.pkl')
    saveObject(labels, modelDir(sensor)+'labels.pkl')
",virtual_sensor/train_test.py,IoT-Expedition/Edge-Analytics,1
"# Extract the hog features
list_hog_fd = []
for feature in features:
    fd = hog(feature, orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier",handwritingRecognition/generateClassifier_unreshaped.py,forrestgtran/TeamX,1
"                                        n_features=n_features, n_informative=5,
                                        random_state=1)
print(len(X_1))
print(X_1)
'''
# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features / 5)

clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
                       tol=1e-3),
             np.logspace(-2.3, -1.3, 10), X_1, y_1),
            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
                       tol=1e-4),
             np.logspace(-4.5, -2, 10), X_2, y_2)]

colors = ['b', 'g', 'r', 'c']

for fignum, (clf, cs, X, y) in enumerate(clf_sets):",example.py,narendrameena/featuerSelectionAssignment,1
"    print(X.columns.values)
    list_dicts = list()
    for train_index, test_index in skf:
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y[train_index], y[test_index]
        print(X_train.shape)
        if feature_selection == ""randomized_lasso"":
            feature_selector=RandomizedLasso(sample_fraction=0.5,n_resampling=50,verbose=False,n_jobs=-1)
        elif feature_selection == ""RFECV_linearSVM"":
#            print(feature_selection % ""selected"")
            feature_selector = RFECV(SVC(kernel=""linear""),step=1,cv=StratifiedKFold(y,5),scoring=""accuracy"")
        else:
            print(""Options are: randomized_lasso, RFECV_linearSVM"")
            
        feature_selector.fit(X_train,y_train)
        result = {'X_train':X_train,'y_train':y_train,'X_test':X_test,'y_test':y_test,'feature_selector':feature_selector}
        list_dicts.append(result)
        
        
    dict_for_attribute[attribute] = list_dicts",feature_selection_basic.py,himalayajung/neuropy,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,dimkal/mne-python,1
"        self.__selection = []
        self.__examples = []
        self.__clf = SGDClassifier(loss='log')

class SVMClassifier():

    def __init__(self, data):
        self.__data = data
        self.__examples = []
        self.__selection = []
        self.__clf = svm.SVC()

    def add_example(self, object_id, polarity):
        # add example
        self.__examples.append({
            'object': object_id,
            'polarity': polarity
        })

        self.__reselect()",scripts/backend/draw/Classification.py,TheSumitGogia/insight,1
"    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')
    X_train = vectorizer.fit_transform(data_train.data)
X_test = vectorizer.transform(data_test.data)

# slice the data
Y_train = Y_train[0:1000]
X_train = X_train[0:1000, 0:200]
X_test = X_test[:, 0:200]

ada = AdaBoostClassifier(n_estimators=5000, base_estimator=SVC(probability=True),
                         learning_rate=0.01)

# Begin Timing
start = time.clock()
# Train on sparse data
ada.fit(X_train, Y_train)
elapsed_sparse_train = time.clock() - start
print("" Sparse Training time: "", elapsed_sparse_train)
",Code/Miscellaneous/SparseAdaBoost.py,tchakravarty/PythonExamples,1
"from sklearn import svm


def myTradingSystem(DATE, OPEN, HIGH, LOW, CLOSE, VOL, OI, P, R, RINFO, exposure, equity, settings):

    def predict(momentum, CLOSE, lookback, gap, dimension):
        X = np.concatenate([momentum[i:i + dimension] for i in range(lookback - gap - dimension)], axis=1).T
        y = np.sign((CLOSE[dimension+gap:] - CLOSE[dimension+gap-1:-1]).T[0])
        y[y==0] = 1

        clf = svm.SVC()
        clf.fit(X, y)

        return clf.predict(momentum[-dimension:].T)

    nMarkets = len(settings['markets'])
    lookback = settings['lookback']
    dimension = settings['dimension']
    gap = settings['gap']
",sampleSystems/svmMomentum.py,Quantiacs/quantiacs-python,1
"    # containt 20% of the total
    x_train, x_test, y_train, y_test = cross_validation.train_test_split(data,
            target, test_size=0.20)
    # define the parameter search space
    parameters = {
        'kernel': ['linear', 'rbf'],
        'C': [1, 10, 100, 1000],
        'gamma': [0.01, 0.001, 0.0001]
    }
    # search for the best classifier within the search space and return it
    clf = grid_search.GridSearchCV(svm.SVC(), parameters, verbose=10).fit(x_train, y_train)
    classifier = clf.best_estimator_
    if print_metrics:
        print()
        print('Parameters:', clf.best_params_)
        print()
        print('Best classifier score')
        print(metrics.classification_report(y_test,
            classifier.predict(x_test)))
    return classifier",src/experiments/test_detecting_in_lol_or_not.py,ckcollab/twitch-experiments,1
"    stopWords = stopwords.words('english')
    X_train, y_train = format_data(emo, train_pos, train_neg)
    X_test, y_test = format_data(emo, test_pos, test_neg)
    lexicon_feat = LexiconVectorizer()
    embed_feat = EmbeddingVectorizer()
    ngram_feat = CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01)
#    ngram_feat.fit(X_train)
#    pickle.dump(ngram_feat.vocabulary_,open(""../vocab/vocab.""+emo+"".pkl"",""wb"")) # save vocabs
#    tfidf_feat = TfidfVectorizer(ngram_range=(1,3), analyzer='word', binary=False, stop_words=stopWords, min_df=0.01, use_idf=True)
    all_features = FeatureUnion([('lexicon_feature', lexicon_feat), ('embeddings', embed_feat), ('ngrams', ngram_feat)])
#    Select = SelectFromModel(svm.LinearSVC(C=10, penalty=""l1"", dual=False))
#    scaler = StandardScaler(with_mean=False)
    pipeline = Pipeline([('all_feature', all_features)])
    feat_train = pipeline.fit_transform(X_train, y_train)
    feat_test = pipeline.transform(X_test)
    feat_size = feat_train.shape[1]
    return feat_train, y_train, feat_test, y_test, feat_size


def feature_transformer2(data, emo, lexicon_feat, embed_feat):",src/FeatureTransformer.py,bluemonk482/emotionannotate,1
"
    `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_

    This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.

    >>> from pyspark.sql import Row
    >>> from pyspark.ml.linalg import Vectors
    >>> df = sc.parallelize([
    ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),
    ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()
    >>> svm = LinearSVC(maxIter=5, regParam=0.01)
    >>> model = svm.fit(df)
    >>> model.coefficients
    DenseVector([0.0, -0.2792, -0.1833])
    >>> model.intercept
    1.0206118982229047
    >>> model.numClasses
    2
    >>> model.numFeatures
    3",python/pyspark/ml/classification.py,jianran/spark,1
"from sklearn import svm, cross_validation
import utils
import numpy as np

def main():
    X, Y = utils.read_data(""../files/train_10.csv"")
    Y = map(int, Y)
    folds = 5
    stf = cross_validation.StratifiedKFold(Y, folds)
    loss = []
    svc = svm.SVC(probability=True)
    accs = []
    classMap = sorted(list(set(Y)))
    X, Y = np.array(X), np.array(Y)
    print ""Testing...""
    for i, (train, test) in enumerate(stf):
        X_train, X_test, y_train, y_test = X[train], X[test], Y[train], Y[test]
        svc.fit(X_train, y_train)
        predicted = svc.predict(X_test)
        probs = svc.predict_proba(X_test)",Eye Movement Tracking/src/svmBenchmark.py,shaileshahuja/MineRush,1
"    results.append(benchmark(clf))

# with open('knnclass.pkl', 'wb') as fid:
#     cPickle.dump(clf, fid)


# for penalty in [""l2"", ""l1""]:
#         print('=' * 80)
#         print(""%s penalty"" % penalty.upper())
#         # Train Liblinear model
#         results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
#                                                                                         dual=False, tol=1e-3)))

#         # Train SGD model
#         results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
#                                                                                      penalty=penalty)))

# # Train SGD with Elastic Net penalty
# print('=' * 80)
# print(""Elastic-Net penalty"")",data/test_algo_2_order.py,ajribeiro/rtapp,1
"        data = stripMagnets(data)
        return self.model.train(self.null_value, data, classes)

    def predict(self, vector):
        return self.model.classify(vector)



class wrapped_svm(predictor):
    def __init__(self, null_value):
        self.model = svm.LinearSVC()

    def fit(self, data, classes):
        self.classes_ = classes
        data = stripMagnets(data)
        return self.model.fit(data, classes)

    def predict(self, vector):
        classes = self.classes_
        vector = vector[:-3]",FingerprintsREST/views/MatchLocation/predictors.py,alexf101/indoor-tracking,1
"def get_accuracy(clf, train_features, train_labels):
    scores = cross_validation.cross_val_score(clf, train_features, train_labels, cv=10)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

def grid_search(train_features, train_labels):
    param_grid = [
        {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
        {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
    ]
    
    clf = GridSearchCV(svm.SVC(C=1), param_grid, n_jobs=-1)
    clf.fit(train_features, train_labels)
    print clf.best_estimator_
    

if __name__ == ""__main__"":
#    train_feature_file       = csv.reader(open(""train.csv"", ""rb""))
#    train_label_file = csv.reader(open(""trainLabels.csv"", ""rb""))
    train_feature_file = np.genfromtxt(open(""../data/train.csv"", ""rb""), delimiter="","", dtype=float)
    train_label_file = np.genfromtxt(open(""../data/trainLabels.csv"", ""rb""), delimiter="","", dtype=float)",data-science-london/src/grid_search.py,Lewuathe/kaggle-repo,1
"    if t>0:
        t=1
    else:
        t=-1
    minus.append(t)

xx, yy = np.meshgrid(np.linspace(0, 4, 500),
                     np.linspace(0, 4, 500))
np.random.seed(0)

clf=svm.SVC(kernel='linear',C=1)
clf=svm.SVC(kernel='poly',degree=7,C=5)
clf=svm.SVC(kernel='rbf',gamma=10,C=2)
clf=svm.LinearSVC()
sss = cross_validation.StratifiedShuffleSplit(label, n_iter=3, test_size=0.4, random_state=0)
for train_index, test_index in sss:
    X_train, X_test = x[train_index], x[test_index]
    y_train, y_test = label[train_index], label[test_index] 
print clf.fit(X_train,y_train)
scores = cross_validation.cross_val_score(clf, X_test, y_test, cv=5, score_func=None)",SVM.py,Sapphirine/Trading-Using-Nonparametric-Time-Series-Classification-Models,1
"             ""SVM gamma"",
             ""Decsion Tree"",
             ""Random Forest"",
             ""adaBoost""
            ]

    clfs = [
            MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=400),
            GaussianNB(),
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025,probability=True),
            SVC(gamma=2, C=1,probability=True),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier()
           ]

    if SUBMODELS and SUBSET_FEATURES:
        #feats = [0,2,3]
        feats = [0,2,3,4,5,6,7]",UFCML/ufcml.py,LittleLebowskiUrbanAchievers/octagon-api,1
"    data = cu.get_sample_data_frame(datasize)
    test = cu.get_test_data_frame(testsize)
    vocab = [w.strip() for w in file('vocab4.txt')][0:1000]
    vidx = get_vocab_index_lookup(vocab)
    print 'extracting data features'
    xdata = extract_svm_features(vidx, data)
    print 'extracting test features'
    xtest = extract_svm_features(vidx, test)
    labels = sorted(cu.labels)
    ydata = data.OpenStatus.apply(labels.index).tolist()
    model = svm.sparse.SVC(probability=True)
    print 'fitting model'
    model.fit(xdata, ydata)
    print 'rest'
    probs = model.predict_proba(xtest)
    new_priors = cu.load_priors('train.csv')
    old_priors = cu.compute_priors(data.OpenStatus)
    probs = cu.cap_and_update_priors(old_priors, probs, new_priors, 0.001)
    y_true = compute_y_true(test)
    score = multiclass_log_loss(y_true, probs)",src/lab.py,coreyabshire/stacko,1
"print __doc__


import numpy as np
from sklearn import cross_validation, datasets, svm

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)

scores = list()
scores_std = list()
for C in C_s:
    svc.C = C
    this_scores = cross_validation.cross_val_score(svc, X, y, n_jobs=1)
    scores.append(np.mean(this_scores))
    scores_std.append(np.std(this_scores))",python/sklearn/examples/exercises/plot_cv_digits.py,seckcoder/lang-learn,1
"        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",scikit-learn-0.17.1-1/sklearn/grid_search.py,RPGOne/Skynet,1
"    classifier = KNeighborsClassifier(weights='distance',n_neighbors=3)
    if cv:
        parameters = {  'n_neighbors'   : np.arange(3, 8),
                        'weights'       : ['uniform', 'distance'],
                        'algorithm'     : ['auto', 'ball_tree', 'kd_tree', 'brute'],
                        'leaf_size'     : np.arange(30,50)
                    }
elif algorithm == 'one-vs-one':
    classifier = OneVsOneClassifier(LinearSVC(random_state=0))
    if cv:
        parameters = {  'estimator'   : [LinearSVC(random_state=0), svm.LinearSVC()]}
elif algorithm == 'one-vs-rest':
    classifier = OneVsRestClassifier(LinearSVC(random_state=0)) 
    if cv:
        parameters = {  'estimator'   : [LinearSVC(random_state=0), svm.LinearSVC()]}
# gaussian naive bayes
elif algorithm == 'gaussian-nb':
    classifier = GaussianNB()  
    if cv:
        parameters = {}",knn/train.py,alod83/osiris,1
"from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model.logistic import LogisticRegression
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV

tokenizer = Tokenizer()
pipeline = Pipeline([
    ('vect', TfidfVectorizer(stop_words='english', max_features=None,
                             ngram_range=(1, 2), use_idf=False, norm='l2', tokenizer=tokenizer)),
    ('clf', LinearSVC())
])
parameters = {
    'vect__max_df': (0.1, 0.5, 0.75),
    # 'vect__max_features': (5000, 10000, None),
    # 'vect__ngram_range': ((1, 1), (1, 2)),
    # 'vect__use_idf': (True, False),
    # 'vect__norm': ('l1', 'l2'),
    # 'clf__loss': ('l1', 'l2'),
    'clf__C': (0.75, 1),",MasteringMLWithScikit-learn/8365OS_04_Codes/movies-gs.py,moonbury/pythonanywhere,1
"    """"""An introduction to machine learning with scikit-learn
    http://scikit-learn.org/stable/tutorial/basic/tutorial.html""""""

    def run(self, usedSavedData = False):
        
        
        digits = datasets.load_digits()
        trainCount = 1000
        predictCount = len(digits.data) - trainCount
    
        svc = svm.SVC(gamma = 0.001, C= 100.)
        
        
        if usedSavedData:   
            svc = joblib.load(""svc.data"")
        else:
            
            trainX = digits.data[:trainCount]
            trainY = digits.target[:trainCount]
            svc.fit(trainX, trainY)",Scikit/Scikit/exercise_1.py,TechnicHail/COMP188,1
"        
    return [numeric_features[i] for i in important_feature_list]   
    
def run_models(X_train, Y_train, scoring, seed, num_instances, num_folds, dependant):
 
    # Add each algorithm and its name to the model array
    models = []
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    #models.append(('SVM-sigmoid', SVC(kernel='sigmoid')))
    models.append(('SVM-rbf', SVC(kernel='rbf'))) #all kernels are providing results identical to rbf so keeping that only
    #models.append(('SVM-linear', SVC(kernel='linear')))
    #models.append(('SVM-poly', SVC(kernel='poly')))
    #models.append(('SVM-precomputed', SVC(kernel='precomputed')))
    models.append((RANDOM_FORESTS_CLASSIFIER, RandomForestClassifier(n_estimators=1000)))
    
    # Evaluate each model, add results to a results array,
    # Print the accuracy results (remember these are averages and std
    results = []",analyze/classification.py,aarora79/sb_study,1
"                (""shape"", Shaper(newshape=(-1,))),
                (""tfidf"", TfidfVectorizer(norm=""l2"", dtype=np.float32,
                                          decode_error=""replace""))])),
        ])),
        (""scaling"", Normalizer())
    ])

    X = transformer.fit_transform(records)
    y = np.array([r[0][""decision""] for r in records])

    grid = GridSearchCV(LinearSVC(),
                        param_grid={""C"": np.linspace(start=0.1, stop=1.0,
                                                     num=100)},
                        scoring=""accuracy"", cv=5, verbose=3)
    grid.fit(X, y)

    return Pipeline([(""transformer"", transformer),
                     (""classifier"", grid.best_estimator_)])

",inspire/modules/predicter/arxiv.py,Dziolas/inspire-next,1
"		self.getEstimatorList().append((name, preproc, xgb.XGBClassifier(**params)))

	def addKNC(self, preproc=None, params={}):
		name = 'KNC'
		self.getEstimatorList().append((name, preproc, KNeighborsClassifier(**params)))

	def addMLPC(self, preproc=None, params={}):
		name = 'MLPC'
		self.getEstimatorList().append((name, preproc, MLPClassifier(**params)))

	def addSVC(self, preproc=None, params={}):
		name = 'SVC'
		self.getEstimatorList().append((name, preproc, SVC(**params)))

	def addKNN(self, preproc=None, params={}):
		name = 'KNN'

		est = KerasClassifier(
			build_fn=params['build_fn'],
			nb_epoch=params['nb_epoch'],",MetaClassifier.py,uzbit/mlutils,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,pravsripad/mne-python,1
"    XList.append(XNew)
    yList.append(yOne)
  return XList, yList

# -------------------------------------------------------------------------------------------------
# Train and test classifier

def create_clfs(C, nb):
  clfs = []
  for i in range(nb):
    clf = LinearSVC(dual=True, C=C, loss='hinge', tol=1e-7, fit_intercept=False, max_iter=3000)
    #clf = SVC(kernel='linear',C=C)
    clfs.append(clf)
  return clfs

def train_multilabel(clfs, XList, yList):
  global log
  nbLabel = len(yList)
  for lbl in range(nbLabel):
    if log: print 'Fit...'",src/experiments/train_svm.py,Cadene/Deep6Framework,1
"        print 'TEST: Loaded ' + str(counter) + '/' + str(num_test_images)  + ' feature maps'
        sys.stdout.flush() # without flushing, no prints are shown until the end
print (""Loading test data: %.2f hours"" %((time.time()-t0)/3600))
sys.stdout.flush()

# Close files
file_train.close()
file_test.close()

# Train SVM using train data
clf = SVC(kernel='linear', C=1.0)
print 'Training SVM...'
sys.stdout.flush()
t0 = time.time()
clf.fit(data_train, labels_train)
print (""Training SVM: %.2f seconds"" %(time.time()-t0))
sys.stdout.flush()


# Test SVM",layer_analysis/fc_SVM.py,imatge-upc/affective,1
"- C              : numpy or list with the values of C to test.
- k_fold         : number of folds to be use in k-fold
'''  
def classify_SVM(XTrain, yTrain, XTest, yTest, 
    k_fold, out_prefix, verbose = False):
  print("":::Starting SVM Classifier:::"")
  # Dict witht he SVM parameters
  parameters = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1, 2, 5, 8, 10]}]

  # Run the hypertuning Parameter and k-fold for
  svmClf = svm.SVC()
  clf = GridSearchCV(svmClf, parameters, cv = k_fold, scoring = 'accuracy')
  clf.fit(XTrain, yTrain)

  # Evaluate the selected model on the validation dataset
  evaluate_gridsearch(clf, XTest, yTest, ""SVM"", out_prefix, verbose)

'''
@Description
  - Method to perform the audio-feature classification using the",musicgenre/audio_features/classifiers_comparison.py,Hguimaraes/torch-musicgenre,1
"rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    """"""
    Test parameters on classes that make use of libsvm.
    """"""
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[0.25, -.25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    """"""",python/sklearn/sklearn/svm/tests/test_svm.py,seckcoder/lang-learn,1
"iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))",test_sklearn.py,andrejsim/test,1
"#             obj = pickle.load(f)
#         
#         loaded = cls(**obj['params'])
#         loaded._model = obj['model']
#         return loaded

class LinearSVM(Classifier):
    
    def __init__(self, C):
        self._C = C
        self._model = SVC(kernel=""linear"", C=C, probability=True)
        
    def train(self, X, y):
        self._model.fit(X, y)
    
    def predict(self, X):
        return self._model.predict(X)
    
    def predict_proba(self, X):
        return self._model.predict_proba(X)",object_detector/classifier.py,penny4860/object-detector,1
"	y_casual_train = y_casual[:nTrain]
	y_regis_train = y_regis[:nTrain]
	y_total_train = y_total[:nTrain]
	Xtest = X[nTrain:,:]
	y_casual_test = y_casual[nTrain:]
	y_regis_test = y_regis[nTrain:]
	y_total_test = y_total[nTrain:]
	
	#linear
	#param_grid = {'C': [1, 5, 10, 100],}
	#clf = GridSearchCV(SVC(kernel='linear'), param_grid,n_jobs=-1)
	#clf = SVC(kernel='poly')
	#clf.fit(Xtrain,ytrain)
	#pred = clf.predict(Xtest)
	#print ""best estimator = "",clf.best_estimator_
	#print ""RMSE poly = "", rmsle(ytest, pred)

	#new stuff
	clf_regis = SVR(kernel='sigmoid')
	clf_regis.fit(Xtrain,y_regis_train)",svm/test_sigmoid_svm.py,agadiraju/519finalproject,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_server_test_0515.py,magic2du/contact_matrix,1
"    print ""Saving it""
    joblib.dump(cv, os.path.join(makepath('model'), DEFAULT_FILENAMES['CountVectorizer'] + '.pkl'))

    print ""TfIdf transforming""
    tfidf = TfidfTransformer()
    ti = tfidf.fit_transform(res)
    print ""Saving it""
    joblib.dump(tfidf, os.path.join(makepath('model'), DEFAULT_FILENAMES['TfIdf'] + '.pkl'))

    print ""Training classifier""
    cls = OneVsRestClassifier(LinearSVC(), -1)
    cls.fit(ti, train_label)
    print ""Saving it""
    joblib.dump(cls, os.path.join(makepath('model'), DEFAULT_FILENAMES['Classifier'] + '.pkl'))
    print ""Done""
    # test = cls.predict(ti)
    # print classification_report(train_label, test)


def get_edu_data():",skool/classify.py,daeatel/skool,1
"from sklearn import datasets
from sklearn import svm
import numpy as np
from sklearn import random_projection
import matplotlib.pyplot as plt


def ex1():  # loading an example dataset
    iris = datasets.load_iris()
    digits = datasets.load_digits()
    clf = svm.SVC(gamma=0.001, C=100.)
    clf.fit(digits.data[:-1], digits.target[:-1])


def ex2():  # Conventions: dtype
    rng = np.random.RandomState(0)
    X = rng.rand(10, 2000)
    X = np.array(X, dtype='float32')
    transformer = random_projection.GaussianRandomProjection()
    X_new = transformer.fit_transform(X)",practice_learn/1.py,Esmidth/DIP,1
"from sklearn.svm import SVC 
from sklearn.datasets import make_circles

# adapted from:
# http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html
# http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html


xx, yy = make_circles(n_samples=500, factor=0.1, noise=0.15)

clf = SVC(kernel='rbf')
clf.fit(xx, yy)

plt.figure(figsize=(8,6))

plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
            facecolors='none', zorder=10, s=300)
plt.scatter(xx[:, 0], xx[:, 1], c=yy, zorder=10, cmap=plt.cm.Paired, s=100)
#plt.scatter(xx[:, 0], xx[:, 1], c=yy, zorder=10, s=100)
",support-vector-machines-101/rbf-circles.py,sridhar-newsdistill/Data-Science-45min-Intros,1
"	    train_data = rawData_s_r[range(seperate)]
	    train_class = cID_s_r[range(seperate)]

	    test_data = rawData_s_r[range(seperate, sel_t_num)]
	    test_class = cID_s_r[range(seperate, sel_t_num)]
	    
	    # SVM fit classifier
	    X =  train_data
	    Y =  np.ravel(train_class)
	    #pdb.set_trace()
	    clf = svm.SVC()
	    clf.decision_function_shape='ovo'
	    clf.kernel='linear'
	    clf.fit(X, Y) 
	    
	    # make prediction
	    predict_class = clf.predict(test_data)
	    #pdb.set_trace()
	    for i in range(0, predict_class.shape[0]):
	        row = predict_class[i]-1",Compiled/multi_class_svm_test.py,jacobbaron/Neuron_Segmentation,1
"
                #(train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train[:1000],y_train[:1000]),(X_train[:1000],y_train[:1000]), (X_test[:1000],y_test[:1000])
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_0511.py,magic2du/contact_matrix,1
"		else:
			Vx = self.Vx
			Vy = self.Vy
		
		indexes = range(len(Vx))
		random.shuffle(indexes)
		X = [ Vx[i] for i in indexes ][:data_limit]
		Y = [ Vy[i] for i in indexes ][:data_limit]
		
		param_grid = [ {'C': [1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001, 0.0001]} ]
		clf = GridSearchCV(estimator=svm.SVC(), param_grid=param_grid)
		clf.fit( np.array( X ), np.array( Y ) )
		return clf.best_estimator_.gamma, clf.best_estimator_.C
	
	def knn_best_params(self, data_limit = 1500):
		if self.Vx == None:
			Vx = self.X
			Vy = self.Y
		else:
			Vx = self.Vx",Classification.py,HTCode/SimpleML,1
"    classifier = MultinomialNB().fit(data_train, target_train)
    predictedNB = classifier.predict(data_test)
    evaluate_model(target_test, predictedNB)
    # print mean_squared_error(target_test,predictedNB)


    data_train, data_test, target_train, target_test = cross_validation.train_test_split(dataSVM, target, test_size=0.2,
                                                                                         random_state=37)
    calculateMajorityClass(target_train)
    print ""Using SVM ""
    svm = SVC(probability=False,random_state=33,kernel='linear',shrinking=True)
    classifier = svm.fit(data_train, target_train)
    predicted = classifier.predict(data_test)
    evaluate_model(target_test, predicted)
    # print mean_squared_error(target_test,predicted)


def evaluate_model(target_true, target_predicted):
    print classification_report(target_true, target_predicted)
    print ""The accuracy score is {:.2%}"".format(accuracy_score(target_true, target_predicted))",Category_Classification.py,akshaykamath/ReviewPredictionYelp,1
"            self.pipe.svd = TruncatedSVD(n_components=n_comp)
            X_dev = self.pipe.svd.fit_transform(X_dev)
            print("" done!"")

        if 'grid' in conf.get('settings'):

            # will only run LinearSVC for now
            user_grid = conf.get('parameters')
            param_grid = {'linearsvc__C': np.logspace(-3, 2, 6)} if not \
                user_grid else user_grid
            steps = [('linearsvc', LinearSVC(random_state=seed,
                                             class_weight='balanced'))]

            # incorporate SVD into GridSearch
            if 'svd' in conf.get('settings') and not conf.get('components'):
                param_grid.update({'svd__n_components': [50, 100, 500, 1000]})
                steps = [('svd', TruncatedSVD())] + steps

            pipe = pipeline.Pipeline(steps)
            print(""grid: "", param_grid)",shed/pipes.py,cmry/shed,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_22_2015_02.py,magic2du/contact_matrix,1
"
    df = df[list(selected_features)]
    df.to_csv('clean_all.csv', header=True, encoding='utf-8')
    return
    X, y = sl.generate_matrix(df, 'price_brutto_m2')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

    models = {'random forest': RandomForestRegressor(),
              'ridge': Ridge(),
              'lasso': LassoLarsCV(),
              #'linearSVC_fit': LinearSVC(),
              #'svc': SVC(),
              'logistic': LogisticRegression(),
              'gauss': GaussianNB()}
    pdb.set_trace()
    sl.fit(X_train, y_train, X_test, y_test, models)

def ape(y_true, y_pred):
    return np.abs(y_true - y_pred) / y_true
",immo/scikit/jupyter.py,bhzunami/Immo,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'project')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 5.0,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/sum_constraints_cmaes_cmaessvc/setup.py,jpzk/evopy,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CMIM.cmim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_CMIM.py,jundongl/PyFeaST,1
"from sklearn import svm
from sklearn.externals import joblib


class SVM:
    """"""
    Initialization of the Support Vector Machine.
    """"""
    def __init__(self, username, input_vectors, output_labels):
        self.person_name = username
        self.machine = svm.SVC()
        self.machine.fit(input_vectors, output_labels)
        self.new_output_label = -1

    """"""
    Use of trained Support Vector Machine to classify new
    data and insert it to new_output_label
    """"""
    def classify(self, new_data):
        self.new_output_label = self.machine.predict(new_data)",server/webapp/svm.py,BrambleLLC/HackAZ-2016,1
"
        # Close files
        file_train.close()
        file_test.close()

        # Train an SVM for each value of N (C)
        i = 0
        for N in N_values:
            C = 2**N
            # Train SVM using train data
            clf = SVC(kernel='linear', C=C)
            print 'Training SVM with C='+str(C)+'...'
            sys.stdout.flush()
            t0 = time.time()
            clf.fit(data_train, labels_train)
            print (""Training SVM: %.2f s"" %(time.time()-t0))
            sys.stdout.flush()

            # Test SVM
            print 'Testing SVM with C='+str(C)+'...'",layer_analysis/twitter_SVM_linear.py,imatge-upc/affective,1
"    return features


# ### Cross-validation procedure

_PARAM_GRID = {
    'C': [0.01, 0.002, 0.005, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]
}

def train_cv_clf(topics_train, classes_train, features, n_folds=10, 
                 param_grid=_PARAM_GRID, tuned_clf=SVC(C=1, kernel='linear'),
                 scoring=util.weighted_f1, random_state=0):
    """"""Trains the topic type classifier, given the various parameters.
    
    """"""
    kf = cross_validation.KFold(len(topics_train), n_folds=n_folds, random_state=random_state)
    cv_clf = GridSearchCV(estimator=tuned_clf, param_grid=param_grid, cv=kf, scoring=scoring)
    topic_vectors_train = to_features(features, topics_train)
    cv_clf.fit(topic_vectors_train, classes_train)
    return cv_clf",dswont/topic_type.py,anonymous-ijcai/dsw-ont-ijcai,1
"    origin = inspect.stack() if peachpy.arm.function.active_function.collect_origin else None
    if source_y is None:
        (destination, source_x, source_y) = (destination, destination, source_x)
    instruction = ArithmeticInstruction('ADDSVS', Operand(destination), Operand(source_x), Operand(source_y),
                                        origin=origin)
    if peachpy.stream.active_stream is not None:
        peachpy.stream.active_stream.add_instruction(instruction)
    return instruction


def ADDSVC(destination, source_x, source_y=None):
    origin = inspect.stack() if peachpy.arm.function.active_function.collect_origin else None
    if source_y is None:
        (destination, source_x, source_y) = (destination, destination, source_x)
    instruction = ArithmeticInstruction('ADDSVC', Operand(destination), Operand(source_x), Operand(source_y),
                                        origin=origin)
    if peachpy.stream.active_stream is not None:
        peachpy.stream.active_stream.add_instruction(instruction)
    return instruction
",peachpy/arm/generic.py,pombredanne/PeachPy,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = ICAP.icap(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_ICAP.py,jundongl/PyFeaST,1
"class NaiveBayes(GIClassifier):
    def __init__(self, **args):
        self.clf = GaussianNB(**args)


class SVM(GIClassifier):
    def __init__(self, **args):
        self.param_dist_random = {'shrinking': [True, False],
                                      'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                                      'degree': sp_randint(2, 5)}
        self.clf = SVC(**args)


class LinearSVM(GIClassifier):
    def __init__(self, **args):
        self.param_dist_random = {'shrinking': [True, False],
                                      'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
                                      'degree': sp_randint(2, 5)}
        self.clf = LinearSVC(**args)
",app/classifier.py,WGierke/git_better,1
"class SVMClassifier(AbstractZnormClassifier):
    """"""Classifier which uses one-vs-one support vector machines""""""
    def __init__(self, C=1, **kwargs):
        # keyword arguments are passed on to scikit-learn's SVM implementation
        # see http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC
        # relevant kwargs (* indicates default):
        #     C (float): 1* (penalty term)
        #     kernel (string): ""linear"", ""poly"", ""rbf""*, ""sigmoid""
        #     degree (int): 1, 2, 3*, ... (degree for polynomial kernel)
        super(SVMClassifier, self).__init__(""SVM"", C=C, **kwargs)
        self._svm = SVC(C=C, **kwargs)

    # train a SVM classifier on a provided dataset
    def _train(self, X, Y):
        self._svm.fit(X, Y)

    # classify a set of test points
    def _classify(self, test_X):
        return self._svm.predict(test_X)
",classifier/SVMClassifier.py,kosigz/DiabetesPatientReadmissionClassifier,1
"        try:
            self.mongo_client = MongoClient(self.MONGO_ADDR, self.MONGO_PORT)
        except Exception as error:
            pretty_print('ERROR', str(error))
    
    ## Initialize SKlearn
    def init_sklearn(self):     
        pretty_print('SKLEARN', 'Initializing SKlearn')
        try:
            ## Learning Estimators
            self.svc_health = svm.SVC(kernel='rbf')
            self.svc_environment = svm.SVC(kernel='rbf')
            self.svc_activity = svm.SVC(kernel='rbf')
        except Exception as error:
            pretty_print('ERROR', str(error))
        
    ## Query Samples in Range to JSON-file
    def query_db(self, days, query_type):
        pretty_print('MONGO', 'Querying samples for last %s days' % str(days))
        result = []",hive-aggregator.py,nathanielrindlaub/hive-aggregator,1
"       
        for label in labels:
            index = target_names.index(label)            
            targets.append(index)
        
        y_train.append(targets)
        
    classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
    classifier.fit(X_train, y_train)
    
    
    docs_new = dict()
    site = stackexchange.Site(stackexchange.StackOverflow, 'CXRSlQ0gwe7r1lOU6taZ6A((')
    site.be_inclusive()
    all_questions=list()    
    while True:
        try:",stackoverflow/multilabelstack.py,littlecegian/GitOverflow,1
"

def compute_recommendations(profile, articles, data, show_training_data=True, max_suggestions=500):
    logger.info(""Loading training data for profile %s...""%profile)
    X_train, y_train = utils.get_training_set( profile )
    logger.debug(""%i samples in training set (%i positive)""%(len(y_train), (y_train>0).sum()))

    # See if conditions for fit are met
    if X_train.shape[0] > min_number_of_ham:
        logger.debug(""Fitting SVM..."")
        svm = LinearSVC()
        svm.fit(X_train, y_train)

        logger.debug(""Validating..."")
        predictions = svm.predict(X_train)
        logger.debug(""%f%% train accuracy""%(100*(predictions==y_train).mean()))

        logger.debug(""Predicting..."") 
        predictions = svm.predict(data)
        logger.debug(""%f%% relevant""%(100*(predictions==1).mean()))",scripts/compute_recommendations.py,fzenke/morla,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .05  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
'''
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",scikit_code/plot_classifier_comparison.py,chakpongchung/RIPS_2014_BGI_source_code,1
"        lcat[self.train_set['labels'] != cat] = -1
        lcat[self.train_set['labels'] == cat] = +1

        # -- build set the parameters for grid search
        log2c = np.logspace(-5,  20, 16, base=2).tolist()
        log2g = np.logspace(-15, 5, 16, base=2).tolist()

        search_space = [{'kernel':['rbf'], 'gamma':log2g, 'C':log2c, 'class_weight':['auto']}]
        search_space += [{'kernel':['linear'], 'C':log2c, 'class_weight':['auto']}]

        svm = GridSearchCV(SVC(random_state=self.__seed), search_space, cv=2, scoring='roc_auc', n_jobs=self.n_jobs)
        # svm = SVC(random_state=self.__seed)

        svm.fit(self.train_set['data'], lcat)

        return svm

    def __compute_stats(self):

        params = {}",modules/classification/svm.py,allansp84/license-plate,1
"        (""tfidf_vectorizer"", TfidfVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000, use_idf = True, smooth_idf = True,
                                             sublinear_tf = True)), 
        (""linear svc"", MultinomialNB())
    ])


    ct_svc = Pipeline([
        (""tfidf_vectorizer"", CountVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000)), 
        (""linear svc"", SVC(kernel=""linear"", probability=True))
    ])

    tfidf_svc = Pipeline([
        (""tfidf_vectorizer"", TfidfVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000, use_idf = True, smooth_idf = True,
                                             sublinear_tf = True)), 
        (""linear svc"", SVC(kernel=""linear"", probability=True))
    ])
    ",fyp/MLModelCreatorWord.py,Ninad998/deepstylometry-python,1
"        #print 'got it!! and shape is ',np.shape(xval)
        
    
        if featType == 'IMS':
            jobs = 16
            c = 0.01;
        else:
            jobs = 16
            c = 10;
    
        clf = LinearSVC(C = c)
        clf = clf.fit(xtrain, ytrain)
        
        saveName = '{}data/train-valSVM-{}.pkl'.format(baseDir,featType)
        with open(saveName,'w') as f:
            pickle.dump(clf,f)
            
if __name__==""__main__"":
    #processPredictions()
    processMBHval()",processing/saveCLFs.py,gurkirt/actNet-inAct,1
"# coding: utf8

import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn import svm

""""""
  SVC. Take into parameters the training dataset x_train and y_train, the test dataset x_test and y_test, the kernel (optional, ""linear"", ""rbf"", ""poly"", ""sigmoid""), the degree (only if kernel=""poly"") and gamma (optional, i.e kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’).
  Return [accuracy, y_predict], where y_predict are the predicted labels from the test dataset.
""""""
def do_SVC(x_train, y_train, x_test, y_test, kernel=""rbf"", degree=2, gamma=""auto""):
  if(kernel == ""poly""):
    svc = svm.SVC(kernel=kernel, gamma=gamma, degree=degree)
  else:
    svc = svm.SVC(kernel=kernel, gamma=gamma)
  svc.fit(x_train, y_train)
  score = svc.score(x_test, y_test)
  y_predict = svc.predict(x_test)
  return score, y_predict
",tools/python/svc.py,jajoe/machine_learning,1
"        self.prediction = self.outputN.fire()
        return self.prediction
    def learn(self, data, soln):
        self.outputN.learn(data, soln)

class svmLearner():
    def __init__(self, inputN):
        self.features = range(0, inputN)
        random.shuffle(self.features)
        self.features = self.features[:random.randint(1, inputN-1)/10]
        self.svm = svm.SVC()
    def predict(self, data):
        return self.svm.predict(data)
    def learn(self, data, soln):
        self.svm.fit(data, soln)

class Animal():
    def __init__(self):
        self.genome = None
        self.tag = random.random()",evolutionaryNN.py,divir94/News-Analytics,1
"import numpy as np


class LinearSVM(object):
    """"""
    Defines a Linear SVM model with helper functions to call the
    standard training and validation methods.
    :param **kwargs: See documentation from sklearn.svm.LinearSVC
    """"""
    def __init__(self, **kwargs):
        self.model = LinearSVC(**kwargs)

    def fit(self, X, y):
        """"""
        Fits the model
        :param X: Input features
        :param y: Input labels
        """"""
        self.model.fit(X, y)
",term1/P5_vehicle_detection_and_tracking/learning.py,ruleva1983/udacity-selfdrivingcar,1
"
#trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3)
#trainX, trainY = load_svmlight_file(""/home/akhi/Documents/miniproject/egMulticlass/data/train.dat"")
#testX, testY = load_svmlight_file(""/home/akhi/Documents/miniproject/egMulticlass/data/test.dat"")
trainX = np.loadtxt('/home/akhi/Documents/miniproject/egMulticlass/data/trainData.dat')
testX = np.loadtxt('/home/akhi/Documents/miniproject/egMulticlass/data/testData.dat')
trainY = np.loadtxt('/home/akhi/Documents/miniproject/egMulticlass/data/trainLabel.dat')
testY = np.loadtxt('/home/akhi/Documents/miniproject/egMulticlass/data/testLabel.dat')


#clf = svm.SVC(kernel=arc_cosine)
clf = svm.SVC(kernel=arc_cosine, probability=True)
y_score = clf.fit(trainX, trainY).decision_function(testX)
#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
#clf.fit(trainX, trainY)

pred = clf.predict(testX)
print accuracy_score(testY, pred)
print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))
",arc_cosine/test.py,akhilpm/Masters-Project,1
"# also load the iris dataset
iris = datasets.load_iris()
rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    # Test parameters on classes that make use of libsvm.
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    # Check consistency on dataset iris.",Sklearn_scipy_numpy/source/sklearn/svm/tests/test_svm.py,ryfeus/lambda-packs,1
"    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact be computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.96) than than the non
    # shuffling variant (around 0.86).

    digits = load_digits()
    X, y = digits.data[:800], digits.target[:800]
    model = SVC(C=10, gamma=0.005)
    n = len(y)

    cv = cval.KFold(n, 5, shuffle=False)
    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.88, mean_score)
    assert_greater(mean_score, 0.85)

    # Shuffling the data artificially breaks the dependency and hides the
    # overfitting of the model with regards to the writing style of the authors",net-p3/lib/python3.5/site-packages/sklearn/tests/test_cross_validation.py,uglyboxer/linear_neuron,1
"# -----------------------------------------------------------------------------
data = load_iris()

idx = np.random.permutation(data.target.size)
train = idx[:int(idx.size / 2)]
test = idx[int(idx.size / 2):]

# -----------------------------------------------------------------------------
# Train and calibrate
# -----------------------------------------------------------------------------
tcp = TcpClassifier(ClassifierNc(ClassifierAdapter(SVC(probability=True)),
                                 MarginErrFunc()))
tcp.fit(data.data[train, :], data.target[train])

# -----------------------------------------------------------------------------
# Predict
# -----------------------------------------------------------------------------
prediction = tcp.predict(data.data[test, :], significance=0.1)
header = np.array(['c0','c1','c2','Truth'])
table = np.vstack([prediction.T, data.target[test]]).T",examples/tcp_classification_svm.py,donlnz/nonconformist,1
"    vnum = G.vcount()
    if vnum < 3:
        raise ValueError(""graph must have at least three vertices"")
    denom = (vnum-1)*(vnum-2)
 
    temparr = [2*i/denom for i in G.betweenness()]
    max_temparr = max(temparr)
    return sum(max_temparr-i for i in temparr)/(vnum-1)

def create_svc_model(data, classification):
    clf = svm.SVC()
    clf.fit(data, classification) 
    return clf

def create_linear_svc_model(data, classification):
    clf = svm.LinearSVC()
    clf.fit(data, classification) 
    return clf

def classify_graph(clf, graph_instance):",trunk/prototypes/smartgraph/igraph-metrics.py,chemaar/smartgraph,1
"    devLog['nTesting(+)'] = len([i for i in yte if i==1])
    devLog['nTesting(-)'] = len([i for i in yte if i==-1])
    devLog['rTesting(+):(-)'] = devLog['nTesting(+)']/float(devLog['nTesting(-)'])
    devLog['rTesting:Devel'] = devLog['nTesting']/float(dataLog['nDevelResampled'])

    ## tuning
    clf = None
    if method=='esvm':
        clf  = eSVM(simMat=None)
    elif method=='psvm':
        clf = svm.SVC(kernel=clfParam['kernel'],probability=True)

    ## training
    print msg+': fitting nTr= '+str(len(ytr))
    trTic = time.time()

    if method=='esvm':
        clf.fit(xtr,ytr)
        devLog['labels'] = clf.labels()
        devLog['nSVM'] = clf.nSVM()",predictor/imbalance/devel.py,tttor/csipb-jamu-prj,1
"            ""Negative and positive histograms must have the same number of bins.""

        nBins = negative.shape[1]

        labels = [0]*len(negative) + [1]*len(positive)
        samples = np.vstack((negative, positive))

        if svcTakesScaleC:
            # old scikit-learn versions take scale_C as a parameter
            # new ones don't and default to True
            svm = SVC(C=1000, kernel=_histogramIntersectionKernel, scale_C=True)
        else:
            svm = SVC(C=1000, kernel=_histogramIntersectionKernel)

        svm.fit(samples, labels)
        cls._manager.add(svm, nBins, overwrite=True)

    @classmethod
    def predict(cls, X, method='classic'):
        '''",lazyflow/operators/opDetectMissingData.py,jakirkham/lazyflow,1
"            param_grid = [
              {'C': [0.1, 10, 100], 'kernel': ['linear']},
              {'C': [10], 'gamma': [1, 0.01], 'kernel': ['rbf']},
            ]
        else:
            param_grid = [
              {'C': [0.1, 1, 10, 50, 100, 1000, 10000], 'kernel': ['linear']},
              {'C': [0.1, 1, 10, 50, 100, 1000, 10000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']},
            ]
        
        e = SVC(C=1, kernel='linear', probability=False)
        clf = GridSearchCV(estimator=e, param_grid=param_grid, scoring=None,
                     fit_params=None, n_jobs=16, iid=True, refit=True, cv=CV_NUM,
                     verbose=0, pre_dispatch='2*n_jobs', error_score='raise')
        
        X, y, X_names, y_name = self._relations2features(relations)
        
        #if test:
        #    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
        #    X = X_test",jnt/isas/supervised.py,tudarmstadt-lt/taxi,1
"print ""- - - - -""

#########################################################
### your code goes here ###

print ""-----------------------------------------""
print ""- - - - - - - - RESULTS - - - - - - - - -""
print ""-----------------------------------------""

from sklearn.svm import SVC
# clf = SVC(kernel='linear') # ---> Accuracy: 88% (1% data training, no C value)
# clf = SVC(kernel='rbf') # ---> Accuracy 68% (1% data training, no C value)
clf = SVC(kernel='rbf', C=10000) # ---> Accuracy: 89% (1% data training) / 99% (full data training)

# One way to speed up an algorithm is to train it on a smaller training dataset.
# The tradeoff is that the accuracy almost always goes down when you do this.
# These lines effectively slice the training dataset down to 1% of its original size, tossing out 99% of the training data.
# You can leave all other code unchanged.
# features_train = features_train[:len(features_train)/100]
# labels_train = labels_train[:len(labels_train)/100]",Python/Machine-Learning/Introduction-Udacity/svm/svm_author_id.py,diegocavalca/Studies,1
"from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier
from sklearn.cross_validation import cross_val_score, StratifiedShuffleSplit

whole_bunch = fetch_abide_movements()

sites = np.unique(whole_bunch.pheno[""SITE_ID""])
dx_vals = np.unique(whole_bunch.pheno[""DX_GROUP""])

pipeline = Pipeline([(""scaler"", StandardScaler()),
                     (""clf"", SVC(C=10., kernel=""rbf""))])

all_scores = dict()

for site in sites:
    site_bunch = fetch_abide_movements(SITE_ID=site)
    print ""\nSite %s"" % site
    dx_group = site_bunch.pheno[""DX_GROUP""]
    repartition = (dx_group[:, np.newaxis] == dx_vals[np.newaxis, :]).sum(0)
    print ""Total subjects: %d"" % len(dx_group)",nilearn_private/baseline_intra_site.py,AlexandreAbraham/movements,1
"
  X = preprocessing.scale(X)

  return X,y

def Analysis():
  test_size = 1000
  X, y = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    if clf.predict(X[-x])[0] == y[-x]:
      correct_count += 1

  print(""correct_count=%s""%float(correct_count))
  print(""test_size=%s""%float(test_size))",p15.py,cleesmith/sentdex_scikit_machine_learning_tutorial_for_investing,1
"        meanNorm=[]
        stdNorm=[]
        for col in X.columns.values:
            meanNorm.append(np.mean(refX.loc[:,col],0))
            stdNorm.append(np.std(refX.loc[:,col],0))
            valX.loc[:,col]=(valX.loc[:,col]-np.mean(refX.loc[:,col],0))/np.std(refX.loc[:,col],0)  
            refX.loc[:,col]=(refX.loc[:,col]-np.mean(refX.loc[:,col],0))/np.std(refX.loc[:,col],0)
        resultados.loc[i,'meanNorm']=meanNorm
        resultados.loc[i,'stdNorm']=stdNorm
        
        _SVC=svm.SVC(par['_C'],par['_kernel'],par['_degree'],par['_gamma'],par['_coef0'],\
        par['_probability'],par['_shrinking'],random_state=par['_random_state'],class_weight='auto')
        #_SVC.predict()
        #Fazendo o fit e calculando a accuracy
        _SVC.fit(refX.drop('codeClass',1),refX.loc[:,'codeClass'])
        resultados.loc[i,'accRef']=_SVC.score(refX.drop('codeClass',1),refX.loc[:,'codeClass'])
        resultados.loc[i,'accVal']=_SVC.score(valX.drop('codeClass',1),valX.loc[:,'codeClass'])
        
        #Salvando modelo em resultsPath
        cPickle.dump(_SVC,open(resultsPath+'/_SVM_'+str(i)+'.p','wb'))",bin/helpers.py,gabrielusvicente/MC886,1
"    if not use_prediction:
        test_predictions.append(model_lda.decision_function([features])[0])  # score for classes_[1]
    else:
        test_predictions.append(model_lda.predict_proba([features])[0])

for i in range(target_count):
    print sum(test_labels_binary[i])

thresholds_for_bci = multiclassRoc(test_predictions, test_labels_binary)

# model = SVC(C=1000, kernel=""poly"", degree=2)
# print ""SVM""
# test_model(model)

# pickle.Pickler(file(""U:\\data\\test\\5_targets\\model0.pkl"", ""w"")).dump(model_lda)
# pickle.Pickler(file(""U:\\data\\test\\5_targets\\model0_mm.pkl"", ""w"")).dump(min_max)
# pickle.Pickler(file(""U:\\data\\test\\5_targets\\model0_thresh.pkl"", ""w"")).dump(thresholds_for_bci)

# print model_lda.coef_
# plt.figure()",src/test.py,kahvel/MAProject,1
"from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from time import time
t0 = time()
t1 = time()
print ""start""
print ""training""
clf_dt = DecisionTreeClassifier(min_samples_split = 40).fit(features_train,labels_train)
clf_rf = RandomForestClassifier(n_estimators=10).fit(features_train,labels_train)
clf_svc = SVC(kernel=""rbf"", C=80000).fit(features_train,labels_train)

print ""training time:"", round(time()-t0, 3), ""s""

print ""predicting""
pred_dt = clf_dt.predict(features_test)
pred_rf = clf_rf.predict(features_test)
pred_svc = clf_svc.predict(features_test)
print ""predicting time:"", round(time()-t1, 3), ""s""
",choose_your_own/your_algorithm.py,junhua/udacity_intro_to_ml,1
"C_parameters = list(np.arange(1, 1000, 100))
tuned_parameters_lin = [{'kernel': ['linear'], 'C': C_parameters}]
tuned_parameters_rbf = [{'kernel': ['rbf'], 'gamma': gamma_parameters, 'C': C_parameters}]
# tuned_parameters_sgm = [{'kernel': ['sigmoid'], 'gamma': gamma_parameters, 'C': C_parameters}]
score = 'precision_macro'
for i, tuned_parameters in enumerate([tuned_parameters_lin, tuned_parameters_rbf]):
	print(""# Tuning hyper-parameters for %s"" % tuned_parameters)
	print()

	cv = StratifiedKFold(n_splits=n_splitsForGridSearch, shuffle=True, random_state=0)
	clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=cv, scoring=score)
	clf.fit(X_train, y_train)

	print(""Grid scores on training set:"")
	means = clf.cv_results_['mean_test_score']
	stds = clf.cv_results_['std_test_score']
	for mean, std, params in zip(means, stds, clf.cv_results_['params']):
		print(""%0.3f (+/-%0.03f) for %r"" % (mean, std * 2, params))
	print()
	print(""Best parameters set found on training set:"")",svm/main_gs.py,aroonav/furry-strokes,1
"			clf=classifier.model.__class__(**classifier.params)
			res = learner.train_test(clf,test_size=1-args.trainsize, presample_class_size=None)
			results.append((clf, res))
		if args.modeloutputfile:
			learner.pickle_classifier(args.modeloutputfile)
	
		

#For 64x64 images
# Test accuracy: 0.912198824923
#y_predicttest=learner.train_test(LinearSVC(C = 0.001))
# LinearSVC(loss='l1', C = 0.000005)

# Test accuracy : 0.91957374675
# LogisticRegression('l2',C = 0.0001)
	",extraction/learner.py,mohamed-ezz/CargoClassifier,1
"            
        
        def fit_transform(self, X, y, **fit_params):
            self.fit(X,y)
            return self.transform(X)
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    svm_model = SVC(C=10.)
    
    # Create the pipeline 
    model = pipeline.Pipeline([('UnionInput', FeatureUnion([('svd', svd), ('dense_features', FeatureInserter())])),
    						 ('scl', scl),
                    	     ('svm', svm_model)])
    # Fit Model
    model.fit(X, y)

    preds = model.predict(X_test)",Search_result/srr8.py,tanayz/Kaggle,1
"        this GridSearchCV instance after fitting.

    verbose : integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., loss_func=..., n_jobs=1,",sklearn/grid_search.py,johnowhitaker/bobibabber,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = JMI.jmi(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_JMI.py,jundongl/scikit-feature,1
"def trim(s):
	""""""Trim string to fit on terminal (assuming 80-column display)""""""
	return s if len(s) <= 80 else s[:77] + ""...""


names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression""]


classifiers = [
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4)]

def main():

	#set the timer
	start = time()",UMKL/textData/20_dl_MKM.py,akhilpm/Masters-Project,1
"del train_set[""Cabin""]
del train_set[""Ticket""]
del train_set[""PassengerId""]

train_set.fillna(0, None, None, True)		
titanic_results = train_set[""Survived""]
del train_set[""Survived""]
###End data preparation

##Selftest
machine=svm.SVC()
machine.fit(train_set.values, titanic_results.values)
predicted_survival=machine.predict(train_set.values)

predictionSuccess=(1-np.mean(predicted_survival != titanic_results.values))*100
print(""Test against training set(self test): ""+str(predictionSuccess)+""% correctness"")
###End selftest


###Predict survival of test.csv",Cloudera/Code/SVM/titanic_data_prediction.py,cybercomgroup/Big_Data,1
"        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, datasets
    >>> from sklearn.model_selection import GridSearchCV
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",projects/scikit-learn-master/sklearn/model_selection/_search.py,DailyActie/Surrogate-Model,1
"        pfail(""iid_is parameters are not supported."")
    if param.size_is is not None:
        pfail(""size_is parameters are not supported."")
    if param.retval:
        pfail(""Unexpected retval parameter!"")
    if param.paramtype in ('out', 'inout'):
        pfail(""Out parameters are not supported."")
    if param.const or param.array or param.shared:
        pfail(""I am a simple caveman."")

def setOptimizationForMSVC(f, b):
    """""" Write a pragma that turns optimizations off (if b is False) or
    on (if b is True) for MSVC.
    """"""
    if b:
        pragmaParam = ""on""
    else:
        pragmaParam = ""off""
    f.write(""#ifdef _MSC_VER\n"")
    f.write('# pragma optimize("""", %s)\n'%pragmaParam)",B2G/gecko/js/xpconnect/src/qsgen.py,wilebeast/FireFox-OS,1
"y = df.loc[:, 1].values

# All malignant tumors will be represented as class 1, otherwise, class 0
le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, \
        test_size=0.20, random_state=1)

pipe_svc = Pipeline([('scl', StandardScaler()), \
        ('clf', SVC(random_state=1))])

pipe_svc.fit(X_train, y_train)
y_pred = pipe_svc.predict(X_test)
confmat = confusion_matrix(y_true = y_test, y_pred = y_pred)
print(confmat)

print('Precision: %.3f' % precision_score( \
        y_true=y_test, y_pred=y_pred))
print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))",HPTuning/ConfusionMatrix.py,southpaw94/MachineLearning,1
"    if probability is True:
        base_estimator = LinearSVR(**kwargs)
        return CalibratedClassifierCV(base_estimator=base_estimator,
                                      method=method, cv=cv)
    else:
        return LinearSVR(**kwargs)


def LinearSVC_Proba(probability=False, method='sigmoid', cv=5, **kwargs):
    if probability is True:
        base_estimator = LinearSVC(**kwargs)
        return CalibratedClassifierCV(base_estimator=base_estimator,
                                      method=method, cv=cv)
    else:
        return LinearSVC(**kwargs)


def SVC_Light(probability=False, method='sigmoid', cv=5, **kwargs):
    """"""
    Similar to SVC(kernel='linear') without having to store 'support_vectors_'",toolbox/clf.py,kingjr/meg_perceptual_decision_symbols,1
"        # self.create_pairs_for_validation_set()
        self.create_pairs_for_data_set()

        # create training array
        tr, inters = self.create_train_array(portion)
        self.tr = tr

        # train logistic regression model
        lr = LogisticRegression(solver='sag')
        lr.fit(tr, inters)
        # svm = LinearSVC()
        # print('start fitting')
        # svm.fit(tr, inters)
        # print('fit completed')

        # create validation array
        val, inters = self.create_val_array()
        self.val = val
        self.result = lr.predict(val)
        prob_re = lr.predict_proba(val)",Main.py,HaoboGu/Structure-Similarity,1
"		test_X,test_y = load_testingData(tempTrainingVectors, tempTestingVectors)

		# Neural network classifiers
		adam_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
		        activation = ""tanh"", solver = ""adam"", max_iter = 1000, random_state=0, alpha=0.001)
		sgd_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
		        activation = ""tanh"", solver = ""sgd"", max_iter = 1800, learning_rate = ""adaptive"", 
		        learning_rate_init=0.01, random_state=0, alpha=0.01)
		# SVM Classifiers
		# C is SVM regularization parameter
		rbf_svc_clf = svm.SVC(kernel='rbf', gamma=0.05, C=401)
		lin_svc_clf = svm.SVC(kernel='linear', C=801, gamma=0.01)
		# Training of neural networks classifiers
		sgd_clf.fit(X,y)
		adam_clf.fit(X,y)
		# Training of SVM classifiers
		lin_svc = lin_svc_clf.fit(X, y)
		rbf_svc = rbf_svc_clf.fit(X, y)

",svm/main_perf.py,aroonav/furry-strokes,1
"    clf = MockClassifier(allow_nd=False)
    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)


def test_cross_val_score_predict_labels():
    # Check if ValueError (when labels is None) propagates to cross_val_score
    # and cross_val_predict
    # And also check if labels is correctly passed to the cv object
    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)

    clf = SVC(kernel=""linear"")

    label_cvs = [LeaveOneLabelOut(), LeavePLabelOut(2), LabelKFold(),
                 LabelShuffleSplit()]
    for cv in label_cvs:
        assert_raise_message(ValueError,
                             ""The labels parameter should not be None"",
                             cross_val_score, estimator=clf, X=X, y=y, cv=cv)
        assert_raise_message(ValueError,
                             ""The labels parameter should not be None"",",projects/scikit-learn-master/sklearn/model_selection/tests/test_validation.py,DailyActie/Surrogate-Model,1
"def clasif_results():
	fileName  = 'Results/featList1.txt'
	data,target,featList=readData(fileName)
	clf = ExtraTreesClassifier(n_estimators=10,random_state=0)
	clf = clf.fit(data,target)
	model=SelectFromModel(clf,prefit=True)
	ex_data=model.transform(data)
	#get_importances(clf,data,featList)
	X_test,X_train,Y_test,Y_train= mySplit(ex_data,target)
	
	#svc = svm.SVC()
	#parameters={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],	'kernel': ['linear','poly','rbf'],'gamma' : 10.0**-np.arange(1,4),'random_state':[0]}
	#test_clasif('svm',svc,X_test,X_train,Y_test,Y_train,parameters,['objective','subjective'])
	
	#lr  = LogisticRegression()
	#parameters={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'random_state':[0]}
	#test_clasif('logistic',lr,X_test,X_train,Y_test,Y_train,parameters,['objective','subjective'])
	
	#lda = LinearDiscriminantAnalysis()
	#parameters={'solver':['svd','lsqr','eigen']}",main.py,The-Blitz/BSDT,1
"  #model = SGDClassifier(loss=""log"", eta0=1.0, learning_rate=""constant"",n_iter=5, n_jobs=4, penalty=None, shuffle=False)#~percetpron
  
  #model = DBN([Xtrain.shape[1], 500, -1],learn_rates = 0.3,learn_rate_decays = 0.9,epochs = 30,verbose = 1)#2.15
  #model = GradientBoostingClassifier(loss='deviance',n_estimators=100, learning_rate=0.1, max_depth=2,subsample=.5,verbose=False)
  #model = LogisticRegression(C=1.0)#3.02
  #print Xtrain.describe()
  #model = SGDClassifier(alpha=0.01,n_iter=250,shuffle=True,loss='log',penalty='l2',n_jobs=4,verbose=False)#mll=3.0
  #model = Pipeline(steps=[('rbm', BernoulliRBM(n_components =300,learning_rate = 0.1,n_iter=15, random_state=0, verose=True)), ('lr', LogisticRegression())])
  #model = LDA()#4.84
  #model = LinearSVC(penalty='l2', loss='l2', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None)
  #model = SVC(C=10, kernel='linear', shrinking=True, probability=True, tol=0.001, cache_size=200)#SLOW!15 min mll=2.52
  #model = SVC(C=100, kernel='rbf', shrinking=True, probability=True, tol=0.001, cache_size=200)
  #model = KNeighborsClassifier(n_neighbors=5)#13
  
  if isinstance(model,DBN) or isinstance(model,SGDClassifier) or isinstance(model,LogisticRegression) or isinstance(model,SVC):
    Xtrain = removeLowVariance(Xtrain)
    Xtrain = scaleData(Xtrain,normalize=True)
  
  model = buildModelMLL(model,Xtrain,ytrain,class_names,trainFull=False)
  #model = makeGridSearch(model,Xtrain,ytrain,n_jobs=4)",competition_scripts/plankton.py,chrissly31415/amimanera,1
"from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC

digits = load_digits()
X_digits, y_digits = digits.data, digits.target
X_digits_train, X_digits_test, y_digits_train, y_digits_test = train_test_split(X_digits, y_digits, random_state=1)

param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
              'gamma': [0.01, 0.1, 1, 10, 100]}

grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5, verbose=3)
grid.fit(X_digits_train, y_digits_train)
print('Best score for SVC: {}'.format(grid.score(X_digits_test, y_digits_test)))
print('Best parameters for SVC: {}'.format(grid.best_params_))",scipy-2017-sklearn-master/notebooks/solutions/18_svc_grid.py,RPGOne/Skynet,1
"


#from itertools import izip



X = np.array(term_doc_mat)
#y = np.array(train_labels)
y=train_labels
clf = svm.SVC(kernel='linear', C = 1.0)
clf.fit(X,y)

labels_obtained=list()

#test_X = np.array(test_term_doc_mat)
for ele in test_term_doc_mat:
	sol = (clf.predict(ele)).tolist()[0]
	labels_obtained.append(sol)
",Bag_of_Words_SVM/svm_term_doc_mat.py,shvaibhav/CNNDeepLearningHindiText,1
"import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# we create 40 separable points
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [-1] * 20 + [1] * 20

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",codes/Support Vector Machines/plot_separating_hyperplane.py,GaryLv/GaryLv.github.io,1
"        raise NotImplementedError(""Need to add multiclass data soon!"")

    @unittest.skip(""binary classifiers don't currently work as expected"")
    def test_binary_rocauc(self):
        """"""
        Test ROCAUC with a binary classifier
        """"""
        X_train, X_test, y_train, y_test = self.load_binary_data()

        # Create and fit the visualizer
        visualizer = ROCAUC(LinearSVC())
        visualizer.fit(X_train, y_train)

        # Score the visualizer
        s = visualizer.score(X_test, y_test)
        self.assertAlmostEqual(s, 0.93230159261495249)

        # Check the scores
        self.assertEqual(len(visualizer.fpr.keys()), 4)
        self.assertEqual(len(visualizer.tpr.keys()), 4)",tests/test_classifier/test_rocauc.py,DistrictDataLabs/yellowbrick,1
"
    from sklearn.metrics import f1_score, average_precision_score, accuracy_score

    if model == 'lr':
        from sklearn import linear_model

        model = linear_model.LogisticRegression(C=1)
    elif model == 'svm':
        from sklearn import svm

        model = svm.LinearSVC(C=1)

    clf = model.fit(X_train, y_train)
    y_predicted = clf.predict(X_test)
    y_score = clf.decision_function(X_test)

    f1 = f1_score(y_test, y_predicted)
    pr_auc = average_precision_score(y_test, y_score)
    acc = accuracy_score(y_test, y_predicted)
    return acc, f1, pr_auc",selection.py,michaelbrooks/feature-selection,1
"  xs = dataset[:][:,1:]

  logging.basicConfig(filename='predict.log',level=logging.DEBUG, format='')

  plt.clf()

  # train model
  #plt.subplot(2,2,1)
  plt.title('C=6, gamma=6.0')

  clf = svm.SVC(kernel='rbf', C=1, gamma=10)
  predict(clf, xs, ys)

  # plt.subplot(2,2,2)
  # plt.title('C=1, gamma=0.01')

  # clf = svm.SVC(kernel='rbf', C=1, gamma=0.01, probability=True)
  # predict(clf, xs, ys)

  # plt.subplot(2,2,3)",forestfires/predict_rbf.py,Josephu/svm,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Evaluating the trained classifier using the testing dataset, predicting the top-1 developer...""
predicted = clf.predict(X_test_vect_red)
import numpy as np
print ""The accuracy is: %s"" % np.mean(predicted == y_test)
from sklearn.metrics import classification_report
print ""Classification report: ""
print(classification_report(y_test, predicted))",Archive/RecoDev-Evaluator/recodev_eval1-1.py,amirhmoin/recodev,1
"def get_features(inputfile):
    """""" Returns the generated features from files. """"""
    df = pd.read_csv(inputfile)
    data = df.as_matrix()
    X = data[:, :-1]
    Y = np.asarray(data[:, -1], dtype=int)
    return X, Y

def classify(Xtrain, Ytrain, Xtest, Ytest):
    """""" Creates a classifier, tests it with test set and returns accuracy. """"""
    clf = LinearSVC()
    clf.fit(Xtrain,Ytrain)

    pred_test = [clf.predict([x]) for x in Xtest]
    acc_test = accuracy_score(Ytest, pred_test)
    return acc_test

if __name__ == '__main__':
    trainsizes = [1000, 2000, 4000, 8000, 16000, 32000]
    samplesize = 41999",test_feature_sets.py,tjkemp/image-tractor,1
"    train_input+=[indep(train)]
    train_output+=[dep(train)]
  return train_input, train_output

def launchSVM(model, settings=None, rows=None, verbose=False):
  if rows is None:
    rows = model._rows
  if settings is None:
    settings = svmSettings().defaults
  train_ip, train_op = formatForSVM(model, rows)
  svm = SVC(C=settings.C, kernel=settings.kernel, degree=settings.degree,
             gamma=settings.gamma, coef0=settings.coef0, probability=settings.probability,
             shrinking=settings.shrinking, tol=settings.tol, random_state=1)
  svm.fit(train_ip, train_op)
  return svm

def predictSVM(model, classifier, test):
  test_ip = [test.cells[:len(model.indep)]]
  return classifier.predict(test_ip)[0]
",Technix/SVM.py,ai-se/x-effort,1
"
	clf = svm.SVR()
	#parmas = {'alpha': np.logspace(1, -1, 9)}
	kf5 = cross_validation.KFold( xM.shape[0], n_folds=n_folds, shuffle=True)
	gs = grid_search.GridSearchCV( clf, svr_params, scoring = 'r2', cv = kf5, n_jobs = -1)

	gs.fit( xM, yV.A1)

	return gs

def gs_SVC( xM, yVc, params):
	""""""
	Since classification is considered, we use yVc which includes digital values 
	whereas yV can include float point values.
	""""""

	print(xM.shape, yVc.shape)

	clf = svm.SVC()
	#parmas = {'alpha': np.logspace(1, -1, 9)}",j3/jgrid.py,jskDr/jamespy,1
"
out_df = pd.DataFrame({'PassengerId' : test_data[0::, 0], 'Survived' : output})
out_df[""PassengerId""] = out_df[""PassengerId""].astype(""int"")
out_df[""Survived""] = out_df[""Survived""].astype(""int"")

out_df.to_csv('titanic-randomforest.csv', index=False)


from sklearn import svm

svc = svm.SVC(kernel='linear')

svc = svc.fit(train_data[0::, 2::], train_data[0::, 1])

# Take the same decision trees and run it on the test data
output = svc.predict(test_data[0::, 1::])

out_df = pd.DataFrame({'PassengerId' : test_data[0::, 0], 'Survived' : output})
out_df[""PassengerId""] = out_df[""PassengerId""].astype(""int"")
out_df[""Survived""] = out_df[""Survived""].astype(""int"")",src/main/python/titanic.py,Chaparqanatoos/kaggle-knowledge,1
"elif folding == 'kfolding':
    cv = KFold(n=y.shape[0], k=n_folds)
elif folding == 'leaveoneout':
    n_folds[0] = y.shape[0]
    cv = LeaveOneOut(n=y.shape[0])
else:
    print(""unknown crossvalidation method!"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
fs = SelectPercentile(f_classif, percentile=fs_n)
#-- grid search
#parameters = {'svm__C': (1e-6,1e-3, 1e-1, .4)}
#clf       = GridSearchCV(svm, parameters,n_jobs=1)",JR_toolbox/skl_svm.py,cjayb/kingjr_natmeg_arhus,1
"
    train,label = read_train(""train.tsv"")
    test,url_label = read_test(""test.tsv"")
            
    #下面的傻逼归一化程序是我自己写的，非常的难为情
    
    x = np.array(train)
    t = np.array(test)
    
    print ""开始训练""
    clf = svm.SVC()
    clf.fit(x,label)
    
    print ""开始预测""
    answer = clf.predict(t)

    f = open(""answer.csv"",""w"")
    f.write('urlid,label\n')

    for i in xrange(len(test)):",svm.py,ezhouyang/class,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,cmoutard/mne-python,1
"from sklearn.tree import DecisionTreeClassifier

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]
",projects/scikit-learn-master/examples/classification/plot_classifier_comparison.py,DailyActie/Surrogate-Model,1
"        # lr_adjustor.on_monitor(nn, valid_set, trainer)
        i += 1
        print ' '

if __name__ == '__main__':
    mnist = fetch_mldata('MNIST original')
    mnist.data = np.rint(mnist.data / 255)
    d = Data(dataset=mnist, train_perc=0.5, valid_perc=0.1, test_perc=0.1,
             shuffle=False)
    # from sklearn.svm import SVC
    # s = SVC()
    # s.fit(d.train_X, d.train_Y)
    # print s.score(d.test_X, d.test_Y)
    train(d=d)",max_mnist.py,ZebTech/dl_test,1
"    data = prng.rand(5, 5, 5, 8).astype(np.float32)
    # all MPI processes read the mask; the mask file is small
    mask = np.ones([5, 5, 5], dtype=np.bool)
    mask[0, 0, :] = False
    labels = [0, 1, 0, 1, 0, 1, 0, 1]
    # 2 subjects, 4 epochs per subject
    sl = Searchlight(sl_rad=1)
    mvs = MVPAVoxelSelector(data, mask, labels, 2, sl)
    # for cross validation, use SVM with precomputed kernel

    clf = svm.SVC(kernel='rbf', C=10)
    result_volume, results = mvs.run(clf)
    if MPI.COMM_WORLD.Get_rank() == 0:
        output = []
        for tuple in results:
            if tuple[1] > 0:
                output.append(int(8*tuple[1]))
        expected_output = [6, 6, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4,
                           4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 1]
        assert np.allclose(output, expected_output, atol=1), \",tests/fcma/test_mvpa_voxel_selection.py,IntelPNI/brainiak,1
"   X[i,]=dataset[(p[i]),]
   Y[i]=target_ascll[(p[i])]
   
#train and test
X=X.astype(np.float32)
Y=Y.astype(np.float32)
data_train, data_test, target_train, target_test = train_test_split(X, Y,test_size=0.1
)


clf = SVC()
clf.fit(data_train, target_train) 
print clf.score(data_test, target_test)",main.py,xinYyuan/-Recognition-of-handwritten-English-letters,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_server_test_0514.py,magic2du/contact_matrix,1
"    train_data = iter(data[1:-1])
    test_data = iter([data[0], data[-1]])

    # label junk food as -1, the others as +1
    y = np.ones(len(data))
    y[:6] = -1
    y_train = y[1:-1]
    y_test = np.array([y[0], y[-1]])

    pipeline = Pipeline([('vect', CountVectorizer(min_df=1)),
                         ('svc', LinearSVC())])

    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
        'svc__loss': ('l1', 'l2')
    }

    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)",venv/lib/python2.7/site-packages/sklearn/feature_extraction/tests/test_text.py,devs1991/test_edx_docmode,1
"y = data.iloc[:,4].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

# standard Scaling the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Implemeting Logistic Reg
clf = SVC(kernel='rbf', random_state=0)
clf.fit(X_train, y_train)

# predict
pred_y = clf.predict(X_test)

# confusion prediction
cm = confusion_matrix(y_test, pred_y)
print(""Confusion Matrix:\n"",cm)
",ML_A2Z/11_Kernel_SVM_Classifier.py,atulsingh0/MachineLearning,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_10_2014_server.py,magic2du/contact_matrix,1
"                               [-1.06594729, -1.08742261, -1.19447124,
                                1.14114654, -0.67860204],
                               [0.7839641, 1.53981364, 0.24948341,
                                0.82626557, 1.67902875],
                               [-0.91005552, -0.64101928, 1.48848987,
                                -0.78406328, -0.83182675]]]
        assert np.allclose(fake_corr, expected_fake_corr), \
            'within-subject normalization does not provide correct results'
    # for cross validation, use SVM with precomputed kernel
    # no shrinking, set C=1
    clf = svm.SVC(kernel='precomputed', shrinking=False, C=1)
    results = vs.run(clf)
    if MPI.COMM_WORLD.Get_rank() == 0:
        output = [None] * len(results)
        for tuple in results:
            output[tuple[0]] = int(8*tuple[1])
        expected_output = [7, 4, 6, 4, 4]
        assert np.allclose(output, expected_output, atol=1), \
            'voxel selection via SVM does not provide correct results'
    # for cross validation, use logistic regression",tests/fcma/test_voxel_selection.py,mihaic/brainiak,1
"    lowercase=True,
    stop_words=stopwords_list,
    tokenizer=tokenize
)

pipeline = Pipeline([
    ('union', FeatureUnion([
        ('vect', vectorizer),
        ('nums', NumericFieldsExtractor()),
    ])),
    ('cls', OneVsRestClassifier(LinearSVC(C=0.3, max_iter=300, loss='hinge'))),
])

def filter_tweets(data):
    return [ row['tweet'] for row in data ]

def filter_classes(data):
    y = []
    for row in data:
        # for now, we only use the weather type class",example7/example7.py,jramcast/ml_weather,1
"avg_score = sum(scores) / len(scores)
print('Scores = {}'.format(scores))
print('avg_score = {}'.format(avg_score))
print ""test_error of naive_bayes classifier:%.6f"" % test_error





# svm
clf_SVM = svm.SVC(kernel='rbf',C=1)
# cross validation
print ""Performance of SVM:""
K = 5

scores = cross_validation.cross_val_score(clf_SVM, X, Y, cv=K, scoring='accuracy')
avg_score = sum(scores) / len(scores)
print('Scores = {}'.format(scores))
print('avg_score = {}'.format(avg_score))",SVM_NB.py,jvanbrug/kaggle-crisis,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED",shinken/external_command.py,baloo/shinken,1
"		j += 1

precision, recall, f1, accuracy, support, fn = 0, 0, 0, 0, 0, 0

loo = LeaveOneOut()

start = timer()
for train, test in loo.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
	clf2 = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)
	clf3 = LogisticRegression().fit(X_train, y_train)
	clf4 = GaussianNB().fit(X_train, y_train)
	eclf = VotingClassifier(estimators=[('svm', clf1), ('knn', clf2), ('lr', clf3), ('gnb', clf4)], voting='soft')
	eclf = eclf.fit(X_train, y_train)
	y_pred = eclf.predict(X_test)
	precision += precision_score(y_test, y_pred, average = 'micro')
	recall += recall_score(y_test, y_pred, average = 'micro')
	f1 += f1_score(y_test, y_pred, average = 'micro')",LeaveOneOut/Voting.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"# The number of repetitions of 'regular' experiments
n_splits_regular = 50

# The number of repetitions of 'permutation' experiments
n_splits_permutation = 50

#######################
#  LEARNER OPTIONS  ###
#######################

model = RFE(LinearSVC(loss='hinge'), step=0.3)

# Set the estimator to be a GridSearchCV
param_grid = {
    'n_features_to_select': [10, 20, 50],
    'estimator__C': np.logspace(-4, 0, 5),
}

estimator = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=1)
",palladio/config_templates/default_config.py,slipguru/palladio,1
"    from atone.features import pool

    pipeline = Pipeline()
    pipeline.add(scale)
    pipeline.add(cut, [onset])
    pipeline.add(pool)
    return pipeline


pipe = pipeline()
model = SVC()
config = {""model"": model, ""pipeline"": pipe, ""subjects"": 1}
run_cv(config, **config)
",classifiers/pooling.py,wohlert/atone,1
"
print 'Scaling data...'
Xs, Ys, Ls = dataset.load_scaled_anklehip(d,m)
X = Xs[0]
Y = Ys[0]
L = Ls[0]

print 'Training classifiers...'
dt = tree.DecisionTreeClassifier()
dt.fit(X, Y.transpose())
sv1 = svm.SVC(kernel='linear', C=1)
sv2 = svm.SVC(kernel='linear', C=1)
sv1.fit(X, Y[0])
sv2.fit(X, Y[1])
nb1 = naive_bayes.GaussianNB()
nb2 = naive_bayes.GaussianNB()
nb1.fit(X, Y[0])
nb2.fit(X, Y[1])

print 'Loading test data...'",test_unseen.py,toonn/mlii,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",sklearn/metrics/tests/test_metrics.py,johnowhitaker/bobibabber,1
"        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                       dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",projects/scikit-learn-master/examples/text/document_classification_20newsgroups.py,DailyActie/Surrogate-Model,1
"from sklearn.linear_model import LogisticRegression
from settings import *


names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(25),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]


def main():
",kpcaWithTreeFS/mnistBackRandom/classifiers.py,akhilpm/Masters-Project,1
"                    lolo = LeaveOneLabelOut(labels)
                    dt = tree.DecisionTreeClassifier()
                    scores = cross_val_score(dt, Xcomb, Y[0], cv=lolo)
                    print >>f,('DT  Accuracy, trained: {mean}% (+/- {std}%)'
                            .format( mean=round(100*scores.mean(),1),
                                    std=round(100*scores.std(),1)))
                    scores = cross_val_score(dt, Xcomb, Y[1], cv=lolo)
                    print >>f,('DT  Accuracy, surface: {mean}% (+/- {std}%)'
                            .format( mean=round(100*scores.mean(),1),
                                    std=round(100*scores.std(),1)))
                    sv = svm.SVC(kernel='rbf', C=1)
                    scores = cross_val_score(sv, Xcomb, Y[0], cv=lolo)
                    print >>f,('SVM Accuracy, trained: {mean}% (+/- {std}%)'
                            .format( mean=round(100*scores.mean(),1),
                                    std=round(100*scores.std(),1)))
                    scores = cross_val_score(sv, Xcomb, Y[1], cv=lolo)
                    print >>f,('SVM Accuracy, surface: {mean}% (+/- {std}%)'
                            .format( mean=round(100*scores.mean(),1),
                                    std=round(100*scores.std(),1)))
                    nb = naive_bayes.GaussianNB()",classifyrunner/experiments/infeasible_features_combinations.py,toonn/mlii,1
"
    trainX = pickle.load(open(conv.redirect(trainmatfile)))
    trainy = [r[1] for r in tsv.reader(conv.redirect(""data|train.dat""))]

    # Optimation
    trainX = trainX.tocsr(False)
    
    _logger.info(""Training..."")
    if vs == '1vsR':
        if regularize == 'l1':
            clf = LinearSVC(loss='l2',penalty='l1',dual=False,C=C)
        else:
            clf = LinearSVC(loss='l1',penalty='l2',dual=True,C=C)
    elif vs == '1vs1':
        clf = SVC(kernel='linear')
    else:
        raise ""Not supported""
        
    clf.fit(trainX,trainy)
    ",model/svm/svm.py,luanjunyi/cortana,1
"		angles = list()
		labels = list()
		for c in db_content:
			_, *t = c
			labels.append(''.join([i for i in t[0] if not i.isdigit()]))
			angles.append(t[1:])
		
		np_angles = np.array(angles)
		np_labels = np.array(labels)

		self.clf = svm.SVC(decision_function_shape='ovr')
		self.clf.fit(angles, labels)

	# Train the classifier
	def guessValueSVM(self, angles):
		return self.clf.predict(angles)

	def getConfidence(self, angles):",taolu-enhancer/core/classifier.py,karolaya/taolu-enhancer,1
"from sklearn.svm.classes import SVC

from ..Classifier import Classifier
from ...language.JavaScript import JavaScript as JS


class SVCJSTest(JS, Classifier, TestCase):

    def setUp(self):
        super(SVCJSTest, self).setUp()
        self.mdl = SVC(C=1., kernel='rbf',
                       gamma=0.001,
                       random_state=0)

    def tearDown(self):
        super(SVCJSTest, self).tearDown()

    def test_linear_kernel(self):
        self.mdl = SVC(C=1., kernel='linear',
                       gamma=0.001,",tests/classifier/SVC/SVCJSTest.py,nok/sklearn-porter,1
"
#           *********** Load Testing Data **********

Test_number=int(sys.argv[1])
Pre=np.fromfile(cur_dir+""/vectors.txt"",float,2500*Test_number,""  "")
Pre=Pre.reshape(Test_number,dimension)
dimension=len(X[0])# get new dimension after applying KPCA
Pre=np.array(pca.transform(list(Pre)))

Y=[0]*300+[1]*300#set Labels for training vectors
lin_clf = svm.LinearSVC()
lin_clf.fit(X, Y) 
result=lin_clf.predict(Pre)
number_of_test_vectors=Test_number

a=lin_clf.decision_function(Pre)# get decision function values to select most confident 

two_D=[]
for i in range(number_of_test_vectors):
    two_D+=[[a[i],i]]",bin/MAD_classifier.py,vaibhav9518/FaceReck,1
"le = LabelEncoder()

y_str = labels
y = le.fit_transform(y_str)
print('Label encoding done')


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=13)

print('Initializing classifier')
#clf = SVC(C=1000.0) #88.3 0:00:03.069055
print('Classifier initialized')
#clf = RandomForestClassifier() #91.84 0:00:00.019845
#clf = GradientBoostingClassifier() #94.07 0:00:00.108704
#clf = GaussianNB() #69.78 0:00:00.106093
#clf = KNeighborsClassifier(3) #71.90 0:00:16.034729
#clf = AdaBoostClassifier() #85.55 0:00:00.142586
clf = LogisticRegression() #87.42 0:00:00.062891
print('Initializing learning')
clf.fit(X_train, y_train)",classification/classificationSVM.py,pcomputo/webpage-classifier,1
"
def fisher_features(folder, gmm):
	folders = glob.glob(folder + ""/*"")
	features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}
	return features

def train(gmm, features):
	X = np.concatenate(features.values())
	Y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])

	clf = svm.SVC()
	clf.fit(X, Y)
	return clf

def success_rate(classifier, features):
	print(""Applying the classifier..."")
	X = np.concatenate(np.array(features.values()))
	Y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])
	res = float(sum([a==b for a,b in zip(classifier.predict(X), Y)])) / len(Y)
	return res",fisher.py,drewlinsley/draw_classify,1
"        if self.with_costs:
            t = time.time() - start_time
            return err, np.array([[t]])
        else:
            return err

    def _train_and_validate(self, x, train, train_targets, valid, valid_targets):
        C = np.exp(float(x[0, 0]))
        gamma = np.exp(float(x[0, 1]))

        clf = svm.SVC(gamma=gamma, C=C)

        clf.fit(train, train_targets)
        y = 1 - clf.score(valid, valid_targets)
        y = np.log(y)
        return np.array([[y]])

    def objective_function_test(self, x):
        if self.multi_task:
            x_ = x[:, :2]",robo/task/ml/svm_task.py,aaronkl/RoBO,1
"    # time_lapse = stop_time - start_time
    # print(""time lapse on bovw for {} clusters:"".format(n_clusters), time_lapse.total_seconds())

    # -----------------
    # TRAIN CLASSIFIERS
    # -----------------

    # setup training data
    X_train, y_train = split_into_X_y(train_set)

    # svm = LinearSVC(C=10.0)
    # # svm = SVC(C=10.0, gamma=10)
    # svm.fit(X_train, y_train)

    # # setup testing data
    # X_test, y_test = split_into_X_y(test_set)

    # y_pred = svm.predict(X_test)

    # tp = np.sum(y_test == y_pred)",lab1/lab1.py,emilioramirez/aacv2016,1
"# 2. Grid Search Experiments
################################################################

# setting up the parameter search space

param_grid_linear_svc = {'C':[0.001, 0.01, 0.1, 1.0, 2.0]}
param_grid_log_reg = {'C': [0.001, 0.01, 0.1, 1, 1.0, 2.0]}
param_grid_rf = {'n_estimators': [50, 100, 200]}

# setting up the function search space
model_dict = {'linear_svm':{'clf':LinearSVC(dual = False, penalty = ""l1""), 'param_grid' : param_grid_linear_svc},
              ""logistic"" :{'clf': LogisticRegression(penalty = ""l1""), 'param_grid' : param_grid_log_reg},
                ""RF"": {'clf': RandomForestClassifier(), 'param_grid': param_grid_rf}
                
                }
     
# a function to run gridsearches with multiple base model           
def test_models(model_dict, nfolds, X, y, score):
    results = {}
    for model_key in model_dict.keys():",Code/deeplearning.py,TimKreienkamp/TextMiningProject,1
"# colored output data.

np.random.seed(0)
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)
g = float(input(""Input gamma: ""))
# The gamma value here is effectively a cutoff parameter for a normal distribution sphere
# which is used to vary the decision boundary between groups.
# See the help page for more details: help(sklearn.svm.SVC)
svm = SVC(kernel='rbf', C=10.0, gamma=g, random_state = 0)
svm.fit(X_xor, y_xor)

PlotFigures.plot_decision_regions(X_xor, y_xor, classifier=svm, test_idx=range(105, 150))
plt.show()

#for X_test_val in X_test_std:
#    print(lr.predict_proba(X_test_val.reshape(1, -1)))",Chapter3/RandXor.py,southpaw94/MachineLearning,1
"createScript(""Classification"", ""sklearn_c_template"", ""logistic_regression"", ""default"", None, {""penalty"": 'l2', ""dual"": False, ""tol"": 0.0001, ""C"": 1.0, ""class_weight"": None, ""solver"": 'liblinear', ""max_iter"": 100, ""multi_class"": 'ovr'})
createScript(""Classification"", ""sklearn_c_template"", ""multilayer_perceptron"", ""default"", None, {""hidden_layer_sizes"": (100, ), ""activation"": 'relu', ""solver"": 'adam', ""alpha"": 0.0001, ""learning_rate"": 'constant', ""max_iter"": 200, ""tol"": 0.0001, ""early_stopping"": False})
createScript(""Classification"", ""sklearn_c_template"", ""random_forest"", ""default"", None, {""n_estimators"": 50, ""criterion"": 'gini', ""max_depth"": None, ""min_samples_split"": 2, ""min_samples_leaf"": 1, ""min_weight_fraction_leaf"": 0.0, ""max_leaf_nodes"": None, ""min_impurity_split"": 1e-07, ""bootstrap"": True, ""oob_score"": False, ""class_weight"": None})
createScript(""Classification"", ""sklearn_c_template"", ""sgd"", ""default"", None, {""penalty"": 'l2', ""alpha"": 0.0001, ""n_iter"": 5, ""epsilon"": 0.1, ""learning_rate"": 'optimal', ""class_weight"": None})
createScript(""Classification"", ""sklearn_c_template"", ""svm"", ""default"", None, {""C"": 1.0, ""kernel"": 'rbf', ""shrinking"": True, ""tol"": 0.001, ""class_weight"": None})

anova = ""F, score = f_classif(train_X, train_y)""
mutual_info = ""score = 1 - mutual_info_classif(train_X, train_y, n_neighbors={n_neighbors})""
random_forest_rfe = ""selector = RFE(RandomForestClassifier(n_estimators=50, random_state=R_SEED), n_features_to_select=1, step={step})""
random_logistic_regression = ""scorer = RandomizedLogisticRegression(C={C}, scaling={scaling}, sample_fraction={sample_fraction}, n_resampling={n_resampling}, selection_threshold={selection_threshold}, tol={tol}, fit_intercept=True, verbose=False, normalize=True, random_state=R_SEED)""
svm_rfe = ""selector = RFE(SVC(random_state=R_SEED, kernel='linear'), n_features_to_select=1, step={step})""

createScript(""FeatureSelection"", ""sklearn_f_template"", ""anova"", ""default"", ""score"", {})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""mutual_info"", ""default"", ""score"", {""n_neighbors"": 3})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""random_forest_rfe"", ""default"", ""rfe"", {""step"": 0.1})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""random_logistic_regression"", ""default"", ""coef"", {""C"": 1, ""scaling"": 0.5, ""sample_fraction"": 0.75, ""n_resampling"": 200, ""selection_threshold"": 0.25, ""tol"": 0.001})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""svm_rfe"", ""default"", ""rfe"", {""step"": 0.1})

## Failed (classification): gaussian_naivebayes gaussian_process qda
## Failed (feature selection): random_lasso",AlgorithmScripts/Helper/build_sklearn_scripts.py,srp33/ShinyLearner,1
"sample_weight = np.ones(y.shape[0])
classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))

###############################################################################
print(""PREPARE CLASSIFICATION"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()


#-- feature selection
if fs_n > 1:
    fs = SelectKBest(f_classif, k=fs_n)
elif fs_n == -1:",JR_toolbox/skl_king_parallel2.py,cjayb/kingjr_natmeg_arhus,1
"seed = 7
scoring = 'accuracy'

# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = model_selection.KFold(n_splits=num_folds, random_state=seed)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())",classifier.py,lschlessinger1/IrisSpeciesClassification,1
"
class TSSVCRandomForestClassifier(TSClassifier):
    '''
    A RandomForest model with features first selected by LinearSVC
    ''' 
    criterion = 'entropy'
    n_estimators = 48
    
    def classifier(self):
        clf = Pipeline([
          ('feature_selection', LinearSVC(penalty=""l1"", dual=False)),
          ('classification', RandomForestClassifier(
                n_estimators=self.n_estimators, 
                min_samples_split=1, max_depth=None,
                criterion=self.criterion,
                n_jobs=-1, random_state=42, verbose=2))
        ])
        return clf
    
class TSEnsembleClassifier(TSClassifier):",scripts/predictors.py,timpalpant/KaggleTSTextClassification,1
"            except ValueError:
                pass
        return vector

    def make_classifier(self, training_samples):
        tweet_array = np.zeros((training_samples, len(self.vocab)))
        for index, tweet in enumerate(self.train_df['SentimentText'][0:training_samples]):
            tweet_array[index] = self.vectorize(tweet)
        print 'Vectorization Complete'
        output = self.train_df['Sentiment'][0:training_samples]
        clf = SVC(kernel='linear', probability=True)
        X_csr = sparse.csr_matrix(tweet_array)
        clf.fit(X_csr, np.asarray(output))
        self.classifier = clf
        print 'Classifier Created'

    def test_classifier(self, training_samples, test_samples):
        test_array = np.zeros((test_samples, len(self.vocab)))
        for index, tweet in enumerate(self.train_df['SentimentText'][training_samples:training_samples+test_samples]):
            test_array[index] = self.vectorize(tweet)",sentimentML/ML_builder.py,sazlin/reTOracle,1
"import cProfile
from sklearn.svm import LinearSVC
from sklearn.datasets import load_svmlight_file
from sklearn.metrics import accuracy_score


X, y = load_svmlight_file(""data.txt"")

svc = LinearSVC()

cProfile.runctx('svc.fit(X, y)', {'svc': svc, 'X': X, 'y': y}, {})

svc.fit(X, y)
results = svc.predict(X)
accuracy = accuracy_score(y, results)
print(""Accuracy: {}"".format(accuracy))",profile/linearsvc.py,Evizero/SoftConfidenceWeighted.jl,1
"print(list(map(lambda x: int(x), linear.predict(test_data))))
print(int(linear.score(train_data, train_target) * 100))

# logistic regression
logistic = linear_model.LogisticRegression()
logistic.fit(train_data, train_target)
print(logistic.predict(test_data))
print(int(logistic.score(train_data, train_target) * 100))

# support vector machine
vector = svm.SVC()
vector.fit(train_data, train_target)
print(vector.predict(test_data))
print(int(vector.score(train_data, train_target) * 100))

# naive bayes
nb = GaussianNB()
nb.fit(train_data, train_target)
print(nb.predict(test_data))
print(int(nb.score(train_data, train_target) * 100))",ml/ml.py,manojvignesh/Python,1
"        List/array of shape [n_rois] with a relative weighting factor to be
        used in the voting procedure.
    """"""

    def __init__(self, mvp, clf=None, voting='soft', weights=None):

        self.voting = voting
        self.mvp = mvp

        if clf is None:
            clf = SVC(C=1.0, kernel='linear', probability=True,
                      decision_function_shape='ovo')
        self.clf = clf

        # If no weights are specified, use equal weights
        if weights is None:
            weights = np.ones(len(mvp.data_name))

        self.weights = weights
",skbold/estimators/multimodal_voting_classifier.py,lukassnoek/skbold,1
"
    # Scale data.
    X_scaled = preprocessing.scale(X)

    # Choose learn and test data.
    X_learn, X_test, y_learn, y_test = \
        cross_validation.train_test_split(X_scaled, y, test_size=0.5,
                                          random_state=seed)

    # Create SVM.
    clf = svm.SVC()

    # Brute-force parameters
    C_range = np.logspace(-2, 10, 13)
    gamma_range = np.logspace(-9, 3, 13)
    params = {'C': C_range, 'gamma': gamma_range}
    clf_p = grid_search.GridSearchCV(clf, params, cv=3)

    # Fit data.
    clf.fit(X_learn, y_learn)",6/color_svm.py,JelteF/statistics,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,TomDLT/mne-python,1
"
    accu_p = np.zeros(shape=(2,))
    accu_r = np.zeros(shape=(2,))
    accu_f = np.zeros(shape=(2,))
    accu_a = 0.0
    folds = 10
    for train_idx, test_idx in StratifiedKFold(y=y, n_folds=folds, shuffle=True):
        train_x, train_y = x[train_idx], y[train_idx]
        test_x, test_y = x[test_idx], y[test_idx]

        cls = svm.SVC(kernel='linear', C=2.0)

        # train
        train_x = vectorizer.fit_transform(train_x).toarray()

        cls.fit(train_x, train_y)

        # test
        test_x = vectorizer.transform(test_x).toarray()
",yelp-sentiment/experiments/sentiment_svm.py,canast02/csci544_fall2016_project,1
"    """"""
    from sklearn.base import clone
    from sklearn.utils import check_random_state
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import check_cv

    if clf is None:
        scaler = StandardScaler()
        svc = SVC(C=1, kernel='linear')
        clf = Pipeline([('scaler', scaler), ('svc', svc)])

    info = epochs_list[0].info
    data_picks = pick_types(info, meg=True, eeg=True, exclude='bads')

    # Make arrays X and y such that :
    # X is 3d with X.shape[0] is the total number of epochs to classify
    # y is filled with integers coding for the class to predict
    # We must have X.shape[0] equal to y.shape[0]",mne/decoding/time_gen.py,kingjr/mne-python,1
"			_svm(v_data_train, v_data_test, v_target_train, v_target_test, kernel, gamma=GAMMA[g])
			g += 1
		else:
			_svm(v_data_train, v_data_test, v_target_train, v_target_test, kernel)




def _svm(data, data_test, target, target_test, kernel, degree=None, gamma=None):
	if degree and kernel == ""poly"":
		clf = SVC(kernel=kernel, degree=degree)
	elif gamma and kernel == ""rbf"":
		clf = SVC(kernel=kernel, gamma=gamma)
	else:
		clf = SVC(kernel=kernel)
	clf.fit(data, target)
	train_score = clf.score(data, target)
	test_score = clf.score(data_test, target_test)
	print kernel, degree, gamma, train_score, test_score
",svm.py,jessrosenfield/supervised_learning,1
"print '#'*40
print 'NO Twitter Features'
print 'SVM - Linear Kernel'
print '#'*40
print 'ACC\tPR\tRE\tF1'
print '#'*40
i=1
for tr, ts in KFold(n=len(normalized_corpus), n_folds=10):
    train = X[tr]
    test = X[ts]
    clf = LinearSVC()
    clf.fit(train, labels[tr])
    ytrue = labels[ts]
    ypred = clf.predict(test)
    acc = (ytrue == ypred).sum() / (len(ypred)+.0)
    p, r, f, s = prfs(ytrue, ypred, average='binary')
    accs.append(acc)
    ps.append(p)
    rs.append(r)
    fs.append(f)",script.py,gppeixoto/pln-finalproject,1
"    #p = pstats.Stats(filename)
    #p.print_stats()



training, test = datasets.load_mnist()
#X, y = datasets.make_classification()
#training, test = utils.train_test_split(X, y)
accuracy_and_time(SCW1(), ""SCW1"", training, test)
accuracy_and_time(SCW2(), ""SCW2"", training, test)
accuracy_and_time(LinearSVC(), ""LinearSVC"", training, test)

#training, test = datasets.load_mnist()
X, y = training
run_profile(SCW1(), ""SCW1"", X, y)
run_profile(SCW2(), ""SCW2"", X, y)",runprofile.py,IshitaTakeshi/SCW,1
"#print(len(X_test), len(y_test))

#tfidf transformation is automately employed in Pipeline
## tfidf = TfidfVectorizer()
## tfidf.fit_transform(X_train)

# build a pipeline - SVC
from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),
                    ('tfidf', TfidfTransformer()),
                    ('clf', OneVsRestClassifier(LinearSVC(random_state=0)))
                     ])

# fit using pipeline
clf = text_clf.fit(X_train, y_train)
# predict
#predicted = clf.predict(X_test)
clf.score(X_train, y_train) 
clf.score(X_test, y_test) 
# print metrics",notebook/read_claims.py,Capstone2017/Machine-Learning-NLP,1
"        n_clusters_per_class=1,
        random_state=0
    )

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        X, y, test_size=0.4, random_state=0)

    C = 10 ** C_exp
    gamma = 10 ** gamma_exp

    clf = svm.SVC(C=C, gamma=gamma)

    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)


def main():
    from metaopt.core.optimize.optimize import optimize
    from metaopt.optimizer.saes import SAESOptimizer",examples/showcase/n_class_saes_global_and_local_timeout.py,cigroup-ol/metaopt,1
"from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier

from db import *

class L1LinearSVC(LinearSVC):

    def fit(self, X, y):
        # The smaller C, the stronger the regularization.
        # The more regularization, the more sparsity.
        self.transformer_ = LinearSVC(penalty=""l1"",
                                      dual=False, tol=1e-3)
        X = self.transformer_.fit_transform(X, y)
        return LinearSVC.fit(self, X, y)

    def predict(self, X):
        X = self.transformer_.transform(X)",consumer/consumer.py,konfabproject/konfab-consumer,1
"

# vdb is the validation data base, the rest of the training data bases
# are used for training
def svmInfomapCluster(vdb,featureSubset=features,th=.34,C=.82,kernel='linear',gamma=1E-3):
    newWordList = pd.DataFrame()
    fitting = trainingVectors[trainingVectors.db!=vdb]
    validation = training[training.db==vdb].copy()
    X = fitting[featureSubset].values
    y = fitting.target.values
    svClf = svm.SVC(kernel=kernel,C=C,gamma=gamma,
                    probability=True)
    svClf.fit(X,y)
    nprandom.seed(1234)
    random.seed(1234)
    svScores = svClf.predict_proba(validation[featureSubset].values)[:,1]
    validation['svScores'] = svScores
    scores = pd.DataFrame()
    wordlist = pd.DataFrame()
    concepts = validation.gloss.unique()",code/python2/svmInference.py,evolaemp/svmcc,1
"@author: Codev
""""""

import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn import svm
from sklearn.metrics import accuracy_score
#digits=datasets.load_digits()

#df=svm.SVC(gamma=0.0001, C=100)",mllearningstart.py,CyrisCodev/Custom-3D-Gesture-Recognition-Python-,1
"import pandas as pd
from sklearn import svm, grid_search
from sklearn.neighbors import KNeighborsClassifier as knn


def SVM(trainXY, testXY):
    print('trainXY[0].shape = {:s}'.format(trainXY[0].shape))
    print('trainXY[1][:, 0].shape = {:s}'.format(trainXY[1][:, 0].shape))
    print('type(trainXY[0]) = {:s}'.format(type(trainXY[0])))
    print('type(trainXY[1])= {:s}'.format(type(trainXY[1])))
    # clf = svm.SVC(decision_function_shape='ovr')
    params = {
        'kernel': ('linear', 'rbf', 'sigmoid', 'poly'),
        'C': [0.25, 0.5, 1, 2, 3, 10, 11],
        'class_weight': ['balanced', None],
        'tol': [0.001, 0.0001, 0.00001],
    }
    svr = svm.SVC()
    clf = grid_search.GridSearchCV(svr, params)
    # clf = svm.SVC()",src/python/clf.py,ppegusii/cs689-mini2,1
"
from sklearn.svm import LinearSVC
from sklearn.datasets import load_svmlight_file
from sklearn.metrics import accuracy_score


setup = """"""
from sklearn.svm import LinearSVC
from sklearn.datasets import load_svmlight_file
X, y = load_svmlight_file(""data.txt"")
svc = LinearSVC()
""""""

time = timeit.timeit('svc.fit(X, y)', setup=setup, number=1)
print(""Time: {}"".format(time))

X, y = load_svmlight_file(""data.txt"")
svc = LinearSVC()
svc.fit(X, y)
results = svc.predict(X)",profile/linearsvc.py,JuliaPackageMirrors/SoftConfidenceWeighted.jl,1
"    clf = KNeighborsClassifier(weights='distance')

    print ""Training KNN...""
    _plot_learning_curve(clf, x_train, y_train, 'KNN-Learning-Curve', make_scorer(accuracy_function))
    clf.fit(x_train, y_train)
    print ""Predicting Test Set...""
    print ""Score for test set: {}"".format(accuracy_function(y_test, clf.predict(x_test)))


def _run_svm_detection(x_train, x_test, y_train, y_test, accuracy_function):
    clf = SVC(C=9)

    print ""Training SVM...""
    _plot_learning_curve(clf, x_train, y_train, 'SVM-Learning-Curve', make_scorer(accuracy_function))
    clf.fit(x_train, y_train)
    print ""Predicting Test Set...""
    print ""Score for test set: {}"".format(accuracy_function(y_test, clf.predict(x_test)))


def _run_dummy_detection(x_train, x_test, y_train, y_test):",prediction/run.py,Brok-Bucholtz/Ultrasound-Nerve-Segmentation,1
"
#T = 5

#repeat the split many times and average the results in order to cancel random fluctuations
#for i in range(T):
#stratified split in train and test set 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, stratify = y, random_state = 20)

    # fit an SVM with rbf kernel
clf = SVC( kernel = 'rbf',cache_size = 1000)

#hyper-parameter optimization through grid-search cross validation

parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
gs_rbf.fit(X_train,y_train)

#select the best hyper-parameters",validation/FEIII2016/precision_recall_threshold_curve/ensemble_duke_T2_stacking_prfoutput.py,enricopal/STEM,1
"
def qdump__std____1__complex(d, value):
    qdump__std__complex(d, value)


def qdump__std__deque(d, value):
    if d.isQnxTarget():
        qdump__std__deque__QNX(d, value)
        return
    if d.isMsvcTarget():
        qdump__std__deque__MSVC(d, value)
        return

    innerType = value.type[0]
    innerSize = innerType.size()
    bufsize = 1
    if innerSize < 512:
        bufsize = 512 // innerSize

    (mapptr, mapsize, startCur, startFirst, startLast, startNode,",share/qtcreator/debugger/stdtypes.py,Philips14171/qt-creator-opensource-src-4.2.1,1
"tmp = train[['X', 'Y', 'Category']]
## Yの情報が壊れているものがあるので、90をomit
## 消してるからダメこれはあとで考えよう
tmp = tmp[tmp['Y' == 90 ]]
tmp.plot(x='X', y='Y', kind='scatter', label=tmp['Category'], legend=False)



## SVMで試してみfrom sklearn import svm
from sklearn import svm
clf = svm.SVC()
y = train['Category']
x = train[['Hour', 'Month', 'PdDistrict']]
dists = list(set(train['PdDistrict']))

for d in dists:
    category = []
    train[d] = 0
    for cat in train['PdDistrict']:
        if d == cat:",crime_classification/crime.py,kousukekikuchi1984/kaggle,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.
    from sklearn.cross_validation import StratifiedShuffleSplit
    from sklearn.grid_search import GridSearchCV
    C_range = np.logspace(-2, 4, 7)
#    gamma_range = np.logspace(-9, 3, 13)
    param_grid = dict( C=C_range)
    cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.2, random_state=42)
    grid = GridSearchCV(LinearSVC(), param_grid=param_grid, cv=cv,verbose=10)
    print ""starting grid search""
    grid.fit(encoded_features, y_train)
#    
    print(""The best parameters are %s with a score of %0.4f""",scripts/Keras_deep_calculate_cv_allkernels.py,nickgentoo/scikit-learn-graph,1
"
iris = datasets.load_iris()
rng = np.random.RandomState(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
n_classes = 3


def test_ovr_exceptions():
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    assert_raises(ValueError, ovr.predict, [])

    # Fail on multioutput data
    assert_raises(ValueError, OneVsRestClassifier(MultinomialNB()).fit,
                  np.array([[1, 0], [0, 1]]),
                  np.array([[1, 2], [3, 1]]))
    assert_raises(ValueError, OneVsRestClassifier(MultinomialNB()).fit,
                  np.array([[1, 0], [0, 1]]),
                  np.array([[1.5, 2.4], [3.1, 0.8]]))",projects/scikit-learn-master/sklearn/tests/test_multiclass.py,DailyActie/Surrogate-Model,1
"from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

iris = datasets.load_iris()

#estimators = [('reduce_dim', PCA()), ('clf', SVC())]
#pipe = Pipeline(estimators)
pipe = make_pipeline(PCA(whiten=False), SVC())

## Parameters of the estimators in the pipeline can be accessed using the <estimator>__<parameter> syntax:
params = dict(pca__n_components=[2, 3], svc__C=[0.1, 10, 100])

clf = GridSearchCV(pipe, param_grid=params)
clf.fit(iris.data, iris.target)
",deepwalk/scoring/cv_test.py,jimgoo/deepwalk,1
"#        If ""False"", it is impossible to make predictions using
#        this SequencialSearchCV instance after fitting.
#    verbose : integer
#        Controls the verbosity: the higher, the more messages.
#
#    Examples
#    --------
#    >>> from sklearn import svm, datasets
#    >>> iris = datasets.load_iris()
#    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
#    >>> svr = svm.SVC()
#    >>> clf = grid_search.SequencialSearchCV(svr, parameters)
#    >>> clf.fit(iris.data, iris.target)
#    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
#    SequencialSearchCV(cv=None, estimator=SVC(C=1.0, cache_size=...,
#        class_weight=..., coef0=..., degree=..., gamma=...,
#        kernel='rbf', max_iter=-1, probability=False, random_state=None,
#        shrinking=True, tol=..., verbose=False),
#            fit_params={}, iid=..., loss_func=..., n_jobs=1,
#            param_grid=..., pre_dispatch=..., refit=..., score_func=...,",pythonUtils/sklearn_tools/cross_validation.py,tgquintela/pythonUtils,1
"y_ten2 = cat2lab(y_te3)
print('score for CNN is :')
print(model2.evaluate(x_te3,y_te3, batch_size=50, show_accuracy=True, verbose=1))
print(classification_report(y_ten2,y_pred2))


cv = StratifiedKFold(labels,n_folds=10,shuffle=True)
params = {'C' : [1e1, 1e2, 1e3,1e4,1e5],
           'gamma' : [0.0001,0.0005,0.001,0.005,0.01]}

clf_grid = GridSearchCV(SVC(kernel='rbf'),params,cv=cv)

model3 = clf_grid.fit(reg_imgs,labels)
print('score for grid-searched SVM is :')
print(model3.best_score_,model3.best_score_)

#demonstration of upper GridSearchCV method

svc_rslt = []
for x,y in cv: ",modeling.py,Jesse-Back/mach_image_proc,1
"def load_model_by_key(key, regressor=False):
    '''
    Load an sklearn model by key.
    :param key: Model key string
    :param regressor: Regression flag
    :return: sklearn model object
    '''
    if key == KNN_MODEL_KEY:
        return KNeighborsRegressor() if regressor else KNeighborsClassifier()
    elif key == SVM_MODEL_KEY:
        return SVR() if regressor else SVC()
    elif key == RF_MODEL_KEY:
        return RandomForestRegressor() if regressor else RandomForestClassifier()
    raise ConfigurationError('[!] Invalid model key specified [%s]' % key)

def load_default_grid_by_key(key):
    '''
    Load default map of grid parameters for hyperparameter tuning.
    :param key: Model key string
    :return: dict of parameters",toy_train/trainer_utils.py,stevedicristofaro/toy_trainer,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([100.0, 100.0]),
        sigma = 1.0,
        beta = 1.0,
        meta_model = meta_model) 

    return method
",evopy/examples/problems/SchwefelsProblem26/CMAESSVC.py,jpzk/evopy,1
"y = data.iloc[:,4].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

# standard Scaling the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Implemeting Logistic Reg
clf = SVC(kernel='poly', degree=3, random_state=0)
clf.fit(X_train, y_train)

# predict
pred_y = clf.predict(X_test)

# confusion prediction
cm = confusion_matrix(y_test, pred_y)
print(""Confusion Matrix:\n"",cm)
",ML_A2Z/12_Poly_Kernel_SVM_Classifier.py,atulsingh0/MachineLearning,1
"        print im_features.shape
        np.savetxt(""/home/iglu/Desktop/features.txt"", im_features)
        nbr_occurences = np.sum((im_features > 0) * 1, axis=0)
        idf = np.array(np.log((1.0 * n + 1) / (1.0 * nbr_occurences + 1)), 'float64')

        # Scaling the words
        self.stdSlr = StandardScaler().fit(im_features)
        im_features = self.stdSlr.transform(im_features)

        # Train the Linear SVM
        self.clf = SVC(probability=True)
        self.clf.fit(im_features, names)
        print self.clf.classes_
        joblib.dump((self.clf, self.clf.classes_, self.stdSlr, self.k, self.voc), self.path, compress=3)
        return self.clf.classes_

    def trainwoBoW(self, descriptors,des_list,names, n):
        self.voc, variance = kmeans(descriptors, self.k, 1)
        im_features = np.zeros((n, self.k), ""float64"")
        for i in xrange(n-1):",src/IL_Complete/src/Descriptors/BoW.py,pazagra/catkin_ws,1
"

class svm(ClassifierMixin):
    def __init__(self, *args, **kwargs):
        """""" Assemble a proper SVM classifier""""""
        # Cherrypick arguments for model. Exclude 'steps', which is pipeline argument
        local_kwargs = {key: kwargs.pop(key) for key in list(kwargs.keys())
                        if key != 'steps' and len(key.split('__', 1)) == 1}
        self.pipeline = Pipeline([('empty_dims_remover', VarianceThreshold()),
                                  ('scaler', StandardScaler()),
                                  ('svm', SVC(*args, **local_kwargs))
                                  ]).set_params(**kwargs)

    def get_params(self, deep=True):
        return self.pipeline.get_params(deep=deep)

    def set_params(self, **kwargs):
        return self.pipeline.set_params(**kwargs)

    def fit(self, descs, target_values, **kwargs):",oddt/scoring/models/classifiers.py,oddt/oddt,1
"    results.append(benchmark(clf))

# with open('knnclass.pkl', 'wb') as fid:
#     cPickle.dump(clf, fid)


# for penalty in [""l2"", ""l1""]:
#         print('=' * 80)
#         print(""%s penalty"" % penalty.upper())
#         # Train Liblinear model
#         results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
#                                                                                         dual=False, tol=1e-3)))

#         # Train SGD model
#         results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
#                                                                                      penalty=penalty)))

# # Train SGD with Elastic Net penalty
# print('=' * 80)
# print(""Elastic-Net penalty"")",data/test_algo_2.py,ajribeiro/rtapp,1
"    # paramgrid = {'n_estimators': [200],
    #              'max_features': ['auto'],
    #              'criterion': ['gini', 'entropy'],
    #              'min_samples_split': [15, 16, 17, 18, 19, 20, 21, 22, 23],
    #              'min_samples_leaf': [5, 6, 7, 8],
    #              'max_depth': [12, 13, 14, 15, 16, 17],
    #              'bootstrap': [True]}
    # paramgrid = {'kernel': ['rbf'],
    #              'gamma': [.01, 'auto', 1.0, 5.0, 10.0, 11, 12, 13],
    #              'C': [.001, .01, .1, 1, 5]}
    # model = SVC(probability=True)
    model = RandomForestClassifier(n_jobs=-1)
    # model = GradientBoostingClassifier()
    # model, gridsearch = gridsearch(paramgrid, model, X_train_bw, y_train_b)
    model = evaluate_model(model, X_train_bw, y_train_b)
    print(""\nthis is the model performance on the training data\n"")
    view_classification_report(model, X_train_b, y_train_b)
    confusion_matrix(y_train_b, model.predict(X_train_b))
    print(""this is the model performance on the test data\n"")
    view_classification_report(model, X_test_b, y_test_b)",src/classification_model.py,brityboy/BotBoosted,1
"    else:
        X_train, X_test, y_train, y_test = X, None, y, None

    # set the parameters by cross-validation
    # tuned_parameters = [ {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000] },
    #                      {'kernel': ['linear'],                           'C': [1, 10, 100, 1000] } ]

    tuned_parameters = [ {'kernel': ['linear'], 'C': [0.1,1,10,100,1000,10000] } ]

    # tune parameters
    clf = GridSearchCV( svm.SVC( probability=True ), tuned_parameters, cv=K, scoring=power_at_5pc_FPR, n_jobs=6 )
    clf.fit( X_train, y_train )

    if( verbose ):
        print ""\nBest parameters found on development set:""
        print clf.best_estimator_
        print
        print ""Grid scores on development set:\n""
        for params, mean_score, cv_scores in clf.grid_scores_:
            print ""%0.6f (+/-%0.03f) for %r"" % (mean_score, cv_scores.std() / 2, params)",CFPselect_train_grid.py,rronen/HAF-score,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in signature(estimator.fit).parameters


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",mlens/externals/sklearn/validation.py,flennerhag/mlens_dev,1
"                uTest = np.zeros((len(ulabIdx),len(labIdx)),dtype=float)

                for r,i in enumerate(labIdx):
                    for c,j in enumerate(labIdx):
                        uTrain[r][c] = gramTrain[i][j]

                for r,i in enumerate(ulabIdx):
                    for c,j in enumerate(labIdx):
                        uTest[r][c] = gramTrain[i][j]

                model = svm.SVC(kernel='precomputed')
                model.fit(uTrain, labInt)
                uPred = model.predict(uTest)
                uDist = model.decision_function(uTest)
                for r,i in enumerate(uPred):
                    # The original paper used threshold = 1 But somehow it resulted on infinite loop on this implementation
                    if abs(uDist[r])>0:
                        intVector[ulabIdx[r]] = i

        model = svm.SVC(kernel='precomputed')",predictor/selfblm/selfblm.py,tttor/csipb-jamu-prj,1
"    # perform a grid search on the 'C' and 'gamma' parameter
    # of SVM
    print ""SEARCHING SVM""
    
    C_range = 2. ** np.arange(-15, 15, step=1)
    gamma_range = 2. ** np.arange(-15, 15, step=1)

    param_grid = dict(gamma=gamma_range, C=C_range)
    
    start = time.time()
    gs = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv, n_jobs = -1, verbose = 2)
    gs.fit(trainX, trainY)
 
    # print diagnostic information to the user and grab the
    # best model
    print ""done in %0.3fs"" % (time.time() - start)
    print ""best score: %0.3f"" % (gs.best_score_)
    print ""SVM PARAMETERS""
    bestParams = gs.best_estimator_.get_params()
 ",python/svm.py,mwalton/artificial-olfaction,1
"iris = sns.load_dataset('iris')

# Split the data
from sklearn.model_selection import train_test_split
X = iris.drop('species', axis=1)
y = iris['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# import a SVC object, train/fit it and predict
from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
pred = model.predict(X_test)

# report
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, pred))
print(confusion_matrix(y_test, pred))

# For optimiziation run a gridsearchcv for the best parameters, the list's are tested for best possibal fit",SVC/exp_SVC.py,dreadjesus/MachineLearning,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    concatenator = ConcatenateChannels()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,dengemann/mne-python,1
"    targets.append(letters_sequence[letter_index])

# Transform to array
targets = np.array(targets)

# Now we need to classify
X  = code_vectors_softmax[:Ndata]
y = targets
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.10)

clf_linear = svm.SVC(C=1.0, kernel='linear')
clf_linear.fit(X_train, y_train)
score = clf_linear.score(X_test, y_test) * 100.0
print('score', score)
prediction = clf_linear.predict(X_train)

run_name = '/indep'
f = h5py.File(file_location, 'r')

",predicting_columns.py,h-mayorquin/time_series_basic,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",jyhton/lib-python/2.7/email/test/test_email.py,tunneln/CarnotKE,1
"    #trainingMatrixFScaled = preprocessing.scale(trainingMatrixF)

    #Scale features between [-1,1]
    max_abs_scaler = preprocessing.MaxAbsScaler()
    trainingMatrixFScaled = max_abs_scaler.fit_transform(trainingMatrixF)

    # Make grid search for best set of parameters
    #Cs = np.logspace(-6, -1, 10)

    #svc = svm.SVC(kernel='rbf',class_weight={'1':weightNormal,'-1':weightFake})
    svc = svm.SVC()

    #clf = GridSearchCV(svc,dict(C=Cs),n_jobs=-1,param_grid={'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']})
    clf = GridSearchCV(svc,n_jobs=-1,param_grid={'C': list(range(1,1000,10)), 'gamma': np.arange(0.0001, 0.001,0.001), 'kernel': ['rbf'], 'class_weight':[{'1':weightNormal,'-1':weightFake}]})

    clf.fit(trainingMatrixFScaled, trainingMatrixL)
    npath = ""../models/"" + tt + ""-"" + illuminant + ""-"" + nameSpace + ""-"" + nameChannel + ""/""
    if not os.path.exists(npath):
        os.makedirs(npath)
    modelName = npath + ""model-"" + dataset + ""-"" + tt + ""-"" + illuminant + ""-"" + nameSpace + ""-"" + nameChannel + "".pkl""",thirdparty/illuminants/sourcecode/trainingSVM.py,FBSLikan/Cetico-TCC,1
"print len(X_test)
print len(y_test)


scaler = MinMaxScaler(feature_range=(0, 1))
X_train = scaler.fit_transform(X_train)

X_test = scaler.fit_transform(X_test)


model = SVC().fit(X_train, y_train)
print 'X_test values:'
print X_test
y_pred = model.predict(X_test)
print 'ypred values:'
print y_pred

score = model.score(X_test, y_test)
print 'Score: ' + str(score)
",estimations/confusion_matrix/cm-svc.py,albonthenet/uDPI,1
"                                             config.MODEL_FILE_NAME)))
    else:
        return _get_new_trained_model()


def _get_new_trained_model():
    logger.info('Training new model')
    training_samples, training_labels = samples.get_samples(
        os.path.join(config.INSTALL_DIR, config.POSITIVE_SAMPLE_DIR),
        os.path.join(config.INSTALL_DIR, config.NEGATIVE_SAMPLE_DIR))
    model = svm.SVC(kernel='linear')
    logger.info('Fitting new model')
    model.fit(training_samples, training_labels)

    return model


if __name__ == '__main__':
    model = get_trained_model(False)
    with open(os.path.join(config.INSTALL_DIR, config.MODEL_FILE_NAME), 'w') as f:",models.py,mattskone/garage-alarm,1
"


# def experiments_100():
# 	names = [""3 Nearest Neighbors"",  ""Decision Tree"",
# 	         ""Random Forest"", ""AdaBoost"",
# 	         ""Naive Bayes"", ""Logistic Regression""
# 	        ]  ## ""Linear SVM"", ""RBF SVM"", ""Linear Discriminant Analysis"", ""Quadratic Discriminant Analysis""
# 	classifiers = [
# 	    KNeighborsClassifier(3),
# 	#     SVC(kernel=""linear"", C=0.025),  ## very slow (can't wait)  
# 	#     SVC(gamma=2, C=1), ## very slow (can't wait)
# 	    DecisionTreeClassifier(max_depth=5),
# 	    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
# 	    AdaBoostClassifier(),
# 	    GaussianNB(),
# 	#     LinearDiscriminantAnalysis(),
# 	    LogisticRegression(penalty='l2', class_weight='balanced', solver='liblinear')
#     	]
",code/code/predict.py,Seondong/MSRA_proposal_wifi_log,1
"
iris = datasets.load_iris()
rng = np.random.RandomState(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
n_classes = 3


def test_ovr_exceptions():
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    assert_raises(ValueError, ovr.predict, [])

    # Fail on multioutput data
    assert_raises(ValueError, OneVsRestClassifier(MultinomialNB()).fit,
                  np.array([[1, 0], [0, 1]]),
                  np.array([[1, 2], [3, 1]]))
    assert_raises(ValueError, OneVsRestClassifier(MultinomialNB()).fit,
                  np.array([[1, 0], [0, 1]]),
                  np.array([[1.5, 2.4], [3.1, 0.8]]))",site/lib/python2.7/site-packages/sklearn/tests/test_multiclass.py,asnorkin/sentiment_analysis,1
"# Not starting this on a separate line because that breaks the EML parser.
EML_DATA = """"""x-store-info:sbevkl2QZR7OXo7WID5ZcVBK1Phj2jX/
Authentication-Results: hotmail.com; sender-id=none (sender IP is 98.138.90.157) header.from=sanjeerly@yahoo.com; dkim=pass (testing mode) header.d=yahoo.com; x-hmca=pass
X-SID-PRA: sanjeerly@yahoo.com
X-SID-Result: None
X-DKIM-Result: Pass(t)
X-AUTH-Result: PASS
X-Message-Status: n:n
X-Message-Delivery: Vj0xLjE7dXM9MDtsPTA7YT0xO0Q9MTtHRD0xO1NDTD0w
X-Message-Info: gamVN+8Ez8V+RHg+F+brAWseB3gKupOiF1HhBKvBFwkh/MnMBSYr9tg0qsxeDfsJLtFcOu9pxCBOEw6pLeEQwUe09i47LD+O1NxlrU6W+IdHONEqL12870AgmD/1L7IzM4iscTQgjn8=
Received: from nm9-vm2.bullet.mail.ne1.yahoo.com ([98.138.90.157]) by BAY0-MC4-F10.Bay0.hotmail.com with Microsoft SMTPSVC(6.0.3790.4900);
	 Mon, 27 Aug 2012 19:22:17 -0700
Received: from [98.138.90.48] by nm9.bullet.mail.ne1.yahoo.com with NNFMP; 28 Aug 2012 02:22:17 -0000
Received: from [98.138.226.168] by tm1.bullet.mail.ne1.yahoo.com with NNFMP; 28 Aug 2012 02:22:17 -0000
Received: from [127.0.0.1] by omp1069.mail.ne1.yahoo.com with NNFMP; 28 Aug 2012 02:22:17 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 62808.64696.bm@omp1069.mail.ne1.yahoo.com
Received: (qmail 83522 invoked by uid 60001); 28 Aug 2012 02:22:17 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1346120536; bh=JisZZrEeT9oXnpemRReNB+AHA9KkSpl9mBrb153Y+3k=; h=X-YMail-OSG:Received:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=g3tOd1WmlC95e/lejBs+ZKH7vJNFx81jdC1VDSdwdwPD7EyrS2XeUVBBtOUHlrOFc661uL2bR6UhXoiGZoRloAG7TlO+ik3m/1dfngTxTflfMOUTRmnOjLmZrj7Cg5qjdRQSZHSdBOJ9BxmvfYgyGeG7COvC555PVZvCTsep8H0=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;",crits/emails/tests.py,cdorer/crits,1
"        train_lengths = [x.shape[0] for x in X_train]
        test_lengths = [x.shape[0] for x in X_test]
        n_train = len(X_train)
        n_test = len(X_test)

        n_feat = data.n_features
        print(""# Feat:"", n_feat)

        # ------------------ Models ----------------------------
        if model_type == ""SVM"":
            svm = LinearSVC()
            svm.fit(np.vstack(X_train), np.hstack(y_train))
            P_test = [svm.predict(x) for x in X_test]

            # AP_x contains the per-frame probabilities (or class) for each class
            AP_train = [svm.decision_function(x) for x in X_train]
            AP_test = [svm.decision_function(x) for x in X_test]
            param_str = ""SVM""

        # --------- CVPR model ----------",code/TCN_main.py,jinwchoi/AP_TCN,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)",mne/decoding/tests/test_csp.py,cmoutard/mne-python,1
"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

clf_names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA"", ""Logistic Regression"", ""Logistic Regression CV""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    LogisticRegression(),",motifwalk/classification/simple_clf.py,gear/motifwalk,1
"'''This way we decide for the best parameters (starting from a wider range and 
   approaching more (closely)-precisely the range looking for the optimal values
   Search for RBF kernel '''

print 'Performing (grid search) for the optimal parameters of the rbf kernel'
start = time.time()
C_range = 2.0 ** np.arange(-2, 2.5, 0.5)
gamma_range = 2.0 ** np.arange(-10, -2, 0.5)
param_rbf_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedKFold(y=traint, n_folds=3) #cross validation 
grid_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid=param_rbf_grid, cv=cv)
grid_rbf.fit(train, traint)
print (""The best classifier is: "" , grid_rbf.best_estimator_)
print ('score ', grid_rbf.score(test,testt))
print_time_elapsed(start)


'''Same thing for the polynomial kernel'''

print 'Performing (grid search) for the optimal parameters of the polynomial kernel'",run_svm.py,chefarov/ocr_mnist,1
"# features = normalizer.binarize_features(features)

training_count = 200
results = [int(i) for i in results]
training_features = features[0:training_count]
training_results = results[0:training_count]

testing_features = features[training_count:]
testing_results = results[training_count:]

clf = svm.SVC(kernel='linear', C = 1.0)
clf.fit(training_features, training_results)

errors = 0
for i in range(len(testing_features)):
    res = clf.predict(testing_features[i])
    if res != testing_results[i]:
        errors += 1

print errors",prediction_models/svm.py,animeshramesh/pyCardio,1
"            train_data.append(np.concatenate([self._raw_data[i][:cell_size * offset],
                                              self._raw_data[i][cell_size * (offset + 1):]]))
            test_data.append(self._raw_data[i][cell_size * offset: cell_size * (offset + 1)])

            train_data_list.append(Parser.sliding(train_data[i]))
            test_data_list.append(Parser.sliding(test_data[i]))

        xs, ys = self.train(train_data_list)

        # predict block
        clf = SVC(kernel='linear')
        clf.fit(xs, ys)
        score = []
        for i in range(self._mode):
            print('now at mode %d' % i)
            result = []
            res = 0
            for j in range(len(test_data_list[i])):
                gap = np.mean(Parser.find_gaps(test_data_list[i][j]))
                print(gap)",lib.py,howeverforever/SuperMotor,1
"                train_tuple = train_tuple + (Xtrain_heads[fold],)
            if 'body' in parts:
                train_tuple = train_tuple + (Xtrain_bodies[fold],)
            Xtrain = np.concatenate((Xtrain, np.concatenate(train_tuple, axis=1)), axis=0)
            ytrain = np.concatenate((ytrain, ytrain_r))

    print Xtrain.shape, Xtest.shape

    # do classification
    tic = time()
    model = sklearn.svm.LinearSVC(C=0.0001)
    model.fit(Xtrain, ytrain)
    predictions = model.predict(Xtest)
    toc = time() - tic

    print 'classified in', toc
    print '--------------------'
    print 'parts', parts
    print 'add_noise', add_noise, 'to_oracle', to_oracle
    print 'augment_training', augment_training, 'augmentation_noise', augmentation_noise",src/scripts/nnsearch.py,yassersouri/omgh,1
"            print ""val="", p
        score = 0.0
        for train, test in rs2:
            X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]
            X_train = np.array([item for sublist in X_train for item in sublist])
            y_train = np.array([item for sublist in y_train for item in sublist])
            X_test = np.array([item for sublist in X_test for item in sublist])
            y_test = np.array([item for sublist in y_test for item in sublist])
            #print X_train.shape, y_train.shape, X_test.shape, y_test.shape
            if(algo == 'svc'):
                clf = LinearSVC(C=p[params.keys().index('C')],
                    penalty=""l1"", dual=False)               ## Larger C increases model complexity
            if(algo=='kNN'):
                clf = KNeighborsClassifier(n_neighbors=p[params.keys().index('k')], 
                    warn_on_equidistant=False, p=p[params.keys().index('p')])
            if(algo=='linearSVM'):
                clf = svm.SVC(kernel='linear', C=p[params.keys().index('C')])
            if(algo=='polySVM'):
                clf = svm.SVC(kernel='poly', degree = p[params.keys().index('degree')], 
                    C=p[params.keys().index('C')])",AdFisher/core/analysis/ml.py,tadatitam/info-flow-experiments,1
"        self._ker = kernel
        self._params = params
        self._deg = deg
        self._gam = gam
        self._c0 = c0
    def SVC_params(self):
        return self._params
    def SVC_fit(self,iterNum):
        nEst =  self._params[iterNum]
        mKer = self._ker
        return SVC(kernel=mKer,C=nEst,degree=self._deg,gamma=self._gam,
                   coef0 = self._c0)
    def SVC_coeffs(self,fitter):
        mKer = self._ker
        if (mKer == linStr):
            return fitter.coef_[0].toarray()[0]
        else: 
            return fitter.dual_coef_.toarray()[0]
            
def getTrialStats(svmObjs,label,valid,nTrials,forceRun,forcePlot):",mainSVM.py,prheenan/csci5622_titantic_ml,1
"    data -= data.mean(axis=0)

    data_train = data[:n_samples/2] 
    targets_train = digits.target[:n_samples/2]

    return data_train, targets_train



def createSVM():
    kernel_svm = svm.SVC(gamma=.2)
    linear_svm = svm.LinearSVC()
    return kernel_svm, linear_svm

def createPipeline():
    feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
    feature_map_nystroem = Nystroem(gamma=.2, random_state=1)
    fourier_approx_svm = pipeline.Pipeline([('feature_map', feature_map_fourier), \
                                            ('svm', svm.LinarSVC())])
    nystroem_approx_svm = pipeline.Pipeline([('feature_map', feature_map_nystroem), \",examples/scikit-learn/examples/general/explicit_feature_map_approximation_for_RBF_kernels.py,KellyChan/python-examples,1
"X = X[order]
y = y[order].astype(np.float)

X_train = X[:.9 * n_sample]
y_train = y[:.9 * n_sample]
X_test = X[.9 * n_sample:]
y_test = y[.9 * n_sample:]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    pl.figure(fig_num)
    pl.clf()
    pl.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=pl.cm.Paired)

    # Circle out the test data
    pl.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
",SVM.py,shuaiharry/EmailPrioritizationUsingRecommenderSystem,1
"    X_test = ch2.transform(X_test)

    feature_names = [word[i] for i
                     in ch2.get_support(indices=True)]
    #

    # for i in feature_names:
    #     print i.encode('utf-8')
    # feature_names = np.asarray(feature_names)
    # print feature_names
    clf = LinearSVC(penalty=""l1"", dual=False, tol=1e-3)

    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    score = metrics.accuracy_score(y_test, pred)
    print(""accuracy:   %0.3f"" % score)


import lda
def test_lda(train_file, out_file):",TopicalCrawl/TopicalCrawl/TopicalCrawl/classifier/build_dict.py,actlea/TopicalCrawler,1
"
        if len(set(labels)) < 2:
            raise Exception(""Can not train an intent classifier. Need at least 2 different classes."")
        y = self.transform_labels_str2num(labels)
        X = intent_features

        # dirty str fix because sklearn is expecting str not instance of basestr...
        tuned_parameters = [{'C': [1, 2, 5, 10, 20, 100], 'kernel': [str('linear')]}]
        cv_splits = max(2, min(MAX_CV_FOLDS, np.min(np.bincount(y)) // 5))  # aim for at least 5 examples in each fold

        self.clf = GridSearchCV(SVC(C=1, probability=True),
                                param_grid=tuned_parameters, n_jobs=num_threads,
                                cv=cv_splits, scoring='f1_weighted')

        self.clf.fit(X, y)

    def process(self, intent_features):
        # type: (np.ndarray) -> Dict[Text, Any]
        """"""Returns the most likely intent and its probability for the input text.""""""
",rasa_nlu/classifiers/sklearn_intent_classifier.py,beeva-fernandocerezal/rasa_nlu,1
"    # clf.fit(gram)
    # print clf

    # # user = users[0]
    # # data_test = numpy.asarray([[0, 1]])
    # print clf.predict(gram)
    # # svm.libsvm.predict_proba
    
    # X = numpy.array([[0, 0], [1, 1]])
    # y = [0, 1]
    # clf = svm.SVC(kernel='precomputed')

    # gram = numpy.dot(X, X.T)
    # print clf.fit(gram, y) 

    # print clf.predict(gram)",solution/kernel/tmall.py,wait4pumpkin/tmall,1
"			self.weights = reg.coef_
			self.fit_obj = reg

		elif self.rule == 'perceptron':
			reg = lm.Perceptron(fit_intercept=False)
			reg.fit(state_train.T, np.argmax(np.array(target_train), 0))
			self.weights = reg.coef_
			self.fit_obj = reg

		elif self.rule == 'svm-linear':
			reg = svm.SVC(kernel='linear')
			reg.fit(state_train.T, np.argmax(np.array(target_train), 0))
			self.weights = reg.coef_
			self.fit_obj = reg

		elif self.rule == 'svm-rbf':
			reg = svm.SVC(kernel='rbf')
			print(""Performing 5-fold CV for svm-rbf hyperparameters..."")
			# use exponentially spaces C...
			C_range = 10.0 ** np.arange(-2, 9)",modules/analysis.py,rcfduarte/nmsat,1
"    
    def init_fit(self, X,Y):
        """"""Initialize partitions and leaf models to minimize training error""""""
        best_model = None
        best_error = np.inf  
        
        for c in self.gen_random_cs():
            if self.is_leaf: 
                model = self.leaf_model(C=c)
            else:
                model = LinearSVC(C=c)
                
            model.fit(X,Y)
            error = model.score(X,Y)
            if err < best_error:
                best_model = model
                best_error = error
        self.model = best_model 
        if not self.is_leaf:
            pred = model.predict(X)",treelearn/viterbi_tree.py,capitalk/treelearn,1
"# Only use Petal length and Petal width features
# These are the training features.
X = iris.data[:, :2]
# These are the labeled outcomes, the species of iris.
# Iris setosa, Iris versicolour, and Iris virginica
y = iris.target

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0 # SVM regularization parameter
lin_svc = LinearSVC(C=C).fit(X, y)

# Set up data to be amenable to plotting.
# Create a mesh to plot in
h = .02 # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
",tutorial_3/3_svm_graph.py,chene5/Big-Data-in-Psychology,1
"
# Constructing the k-fold cross validation iterator (k=5)
cv = KFold(n=features.shape[0],  # total number of samples
           n_folds=10,           # number of folds the dataset is divided into
           shuffle=True,
           random_state=123)

t0 = time()

parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100, 1000]}
clf = grid_search.GridSearchCV(SVC(), parameters)
clf.fit(features, labels)

# print cross_val_score(clf, features, labels, cv=cv, scoring='accuracy')
print ""escape time : "", round(time()-t0, 3), ""s""

print ""best score is %s"" % clf.best_score_
print ""best parameter is %s"" % clf.best_params_
print clf.grid_scores_
",nytimes/step4_svm.py,dikien/Machine-Learning-Newspaper,1
"digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

K = 10

# データをK分割する
X_folds = np.array_split(X_digits, K)
y_folds = np.array_split(y_digits, K)

svc = svm.SVC(C=1, kernel='linear')

scores = list()
for k in range(K):
    # k番目をテストセットとして残り2つで学習
    # X_foldsは元のまま残しておくのでコピーする
    X_train = list(X_folds)
    X_test = X_train.pop(k)
    X_train = np.concatenate(X_train)
",sklearn/cross_validation.py,sylvan5/PRML,1
"    t_tc = time.clock()

    print ""Training...""

    X_dim = len(data_X[0])

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.0.py,totuta/deep-supertagging,1
"    X = scale_data(X)
    print(""Features Data scaled"")

#    SGD = SGDClassifier(penalty='elasticnet',class_weight='auto',n_jobs=-1,n_iter=35,l1_ratio =0.2)
    svc = LinearSVC(class_weight='auto')
    model_rf = RandomForestClassifier(n_jobs=-1, bootstrap=True, n_estimators=180,
                                        min_samples_leaf=3, min_samples_split =3,
                                        criterion='gini',compute_importances=True, max_depth=6)

    SVC_RBF= SVC(kernel=""rbf"", class_weight=""auto"", cache_size=2600, shrinking=True)
    SVC_linear= SVC(kernel=""poly"", cache_size=2700, shrinking=True)


    # model_rf.fit(X,y)
    # X_SGD = model_rf.transform(X, threshold='1.5*mean') # forests!
    X_SGD = model_rf.fit_transform(X,y)
    print('X Reduced (by RF) features amount:')
    print(X_SGD.shape)

    def ReducedFeaturesDF(X,y):",ProFET/feat_extract/Model_Parameters_CV.py,ddofer/ProFET,1
"
tic = time.clock()

df = pd.read_csv('dataset/winequality-white.csv', header=0, sep=';')

X = df[list(df.columns)[:-1]]
y = df['quality']
print ""1""
X_train, X_test, y_train, y_test = train_test_split(X, y)
print ""splitted""
model_lin = svm.LinearSVC(gamma=0.0001)
model_rbf = svm.SVC(gamma=0.0001)
print ""made model""
model_lin.fit(X_train, y_train)
model_rbf.fit(X_train, y_train)
print ""fitting""
y_predict_lin = model_lin.predict(X_test)
y_predict_rbf = model_rbf.predict(X_test)
print ""predicted""
mse_lin = mean_squared_error(y_predict_lin, y_test)",svm.py,behrtam/wine-quality-prediction,1
"
df = pd.read_csv('breast-cancer-data.txt')
df.replace('?', -99999, inplace=True)
df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)

clf = svm.SVC()
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)
print(accuracy)

example_measures = np.array([[4, 2, 1, 1, 1, 2, 3, 2, 1], [4, 2, 2, 2, 1, 2, 3, 2, 1]])
example_measures = example_measures.reshape(len(example_measures), -1)

prediction = clf.predict(example_measures)",Python/Machine_Learning/SVM_Scikit-Learn.py,Toemazz/Tutorials,1
"def run_model(session, verbose=False, debug=False):
    # names = ['DecisionTree']
    # names = ['NearestNeighbors', 'DecisionTree', ""RandomForest"", ""AdaBoost""]
    hyper_parameter = PIPELINE_CONFIG.HYPER_PARAMETER
    if verbose:
        print(hyper_parameter, end=', ')
    names = [""NearestNeighbors"", ""RBF SVM"",
             ""DecisionTree"", ""RandomForest"", ""AdaBoost"", ""KNN"", ""RadiusNeighbors""]
    classifiers = [
        KNeighborsClassifier(hyper_parameter),
        SVC(gamma=hyper_parameter, C=1),
        DecisionTreeClassifier(max_depth=hyper_parameter),
        RandomForestClassifier(max_depth=hyper_parameter, n_estimators=10, max_features=1),
        AdaBoostClassifier(n_estimators=hyper_parameter),
        KNN(num_neighbors=hyper_parameter, weight='distance'),
        RadiusNeighborsClassifier(hyper_parameter)]
    classifier_pool = dict(zip(names, classifiers))

    selected = PIPELINE_CONFIG.MODEL_NAME
",decoding/pipeline.py,colpain/NeuralDecoding,1
"import numpy as np
import matplotlib.pylab as plt
from sklearn.svm import SVC

X = np.array([[3, 4], [2, 2], [4, 4], [1, 4], [2, 1], [4, 3], [4, 1]])
y = np.array(['Red', 'Red', 'Red', 'Red', 'Blue', 'Blue', 'Blue'])

linear_svm = SVC(kernel = 'linear', C = 2 ** 15)
linear_svm.fit(X, y)
## w0 * X_1 + w1 * X_2 + b = 0 <=> X_2 = -w0 / w1 * X_1 - b / w1
w = linear_svm.coef_[0]
print('w: {}'.format(w))
print('Margin: %s'%(1.0 / np.linalg.norm(w)))
b = linear_svm.intercept_
print('b: {}'.format(b))
slope = -w[0] / w[1]
## points in the separating line",max_margin_classifier.py,lidalei/DataMining,1
"        X_digits = digits.data
        y_digits = digits.target
        n_samples = len(X_digits)
        X_train = X_digits[:int(.9 * n_samples)]
        y_train = y_digits[:int(.9 * n_samples)]
        X_test = X_digits[int(.9 * n_samples):]
        y_test = y_digits[int(.9 * n_samples):]
        svm = SVM(sparkSession, is_multi_class=True)
        mllearn_predicted = svm.fit(X_train, y_train).predict(X_test)
        from sklearn import linear_model, svm
        clf = svm.LinearSVC()
        sklearn_predicted = clf.fit(X_train, y_train).predict(X_test)
        self.failUnless(accuracy_score(sklearn_predicted, mllearn_predicted) > 0.95 )

    def test_naive_bayes(self):
        digits = datasets.load_digits()
        X_digits = digits.data
        y_digits = digits.target
        n_samples = len(X_digits)
        X_train = X_digits[:int(.9 * n_samples)]",src/main/python/tests/test_mllearn_numpy.py,niketanpansare/incubator-systemml,1
"classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))

###############################################################################
print(""PREPARE CLASSIFICATION"")


#-- classifier

clf = GridSearchCV(svm.SVC(kernel='linear', probability=True),
    {'C': svm_C}, score_func=precision_score)

#-- normalizer
scaler = Scaler()

#-- feature selection
if fs_n > 1:
    fs = SelectKBest(f_classif, k=fs_n)
elif fs_n == -1:",JR_toolbox/skl_king_parallel_gs.py,kingjr/natmeg_arhus,1
"        mm=cv2.moments(cnt)
        M.append(np.atleast_2d(np.array(mm.values())))
     M1=np.concatenate(M,0)
     
#     for features in range(24):
#         pl.hist(M1[pos_examples,features],100,normed=True)   
#         pl.hist(M1[neg_examples,features],100,normed=True)  
#         pl.pause(1)
#         pl.cla()
    from sklearn import svm
    clf = svm.SVC()
    X=M1[np.hstack([pos_examples[:70],neg_examples[:70]])]
    
    y=np.hstack([np.zeros_like(pos_examples[:70]),np.ones_like(neg_examples[:70])])
    clf.fit(X, y)
    lbs=clf.predict(M1)
    pl.imshow(np.max(masks_ws[lbs==1],0))
    pl.imshow(np.max(masks_ws[pos_examples],0)*10,alpha=.5)

#    with np.load(os.path.join(folder_in_check,'results_analysis.npz'))  as ld:",sandbox/scripts_labeling/neurofinder_template.py,agiovann/Constrained_NMF,1
"            self.SVM(obs_x, label_y, sigma, C)

            for j in range(len(pred_x)):
                # caution!
                if label_pred_y[j] == self.estimator.predict(pred_x[j].reshape(1, -1)):
                    success += 1.0

        return success / (window_num * 10)

    def SVM_scikit(self, x, y, sigma, C):
        self.estimator = SVC(C=C, gamma=1 / (2 * sigma), kernel='rbf',)
        self.estimator.fit(x, y)

    def descript(self, test_x, test_y, grid=True):
        s = self.result[0]
        c = self.result[1]

        x = self.x
        y = self.y
",H20Nakata/H20_Nakata_model.py,NlGG/Finance,1
"
  #C=[0.001,0.01,0,1,1,10,100,1000,10000,100000]
  #gamma=[1.0e-7,1.0e-6,1.0e-5,1.0e-4,0.001,0.01,0.1,1,10,100]

  C=[100000,1000000,10000000]
  gamma=[1.0e-6,1e-7,1.0e-8]

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/t1.py,n7jti/machine_learning,1
"
import app.analytics.filterSentences as fl
import matplotlib.pyplot as plt
#import networkx as nx
G = {}
np.seterr(divide='ignore',invalid='ignore')

trainArticles= eval(open('bigSingleSets','r').readlines()[0])#=importArticles.getData('train')
testArticles= eval(open('bigSingleSets','r').readlines()[0])[2000:2050]#=importArticles.getData('train')
listOfYears = []
clf = svm.SVC(probability=True)
probs = []
titles = []
#A

#B
def generateTrainDataPoints(tpl):
    X =tpl[0]
    Y = tpl[1]
    doubleSets = []",getDoubleSets.py,JFriel/honours_project,1
"dataset=np.zeros((1,c))
labels=np.zeros(1)

for l,i in feature:
	dataset=np.vstack((dataset,i))
	labels=np.hstack((labels,l))
dataset=np.delete(dataset,0,0)
labels=np.delete(labels,0)


clf=svm.LinearSVC()



training=dataset
trainingLabel=labels
clf.fit(training,trainingLabel)
feature={}
featureFile=open(""./Test/histrogram.out"")
data=featureFile.read().split(""\n"")",say-trees/tree-finder/TagMe/classifyOnTest.py,ebayohblr2014/eBay-Opportunity-Hack-Blr-2014,1
"	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#'''
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


	#clf = svm.SVC(kernel=arc_cosine, cache_size=4096)
	clf = svm.SVC(C=2) #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/stl10/arc_cosine.py,akhilpm/Masters-Project,1
"    x_min, x_max = x_train[:, 0].min() - 0.1, x_train[:, 0].max() + 0.1
    y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    ''''' SVM '''
    # title for the plots  
    titles = [
        'LinearSVC (linear kernel)', 'SVC with polynomial (degree 3) kernel',
        'SVC with RBF kernel', 'SVC with Sigmoid kernel'
    ]
    clf_linear = svm.SVC(kernel='linear').fit(x, y)          # 线性函数
    clf_poly = svm.SVC(kernel='poly', degree=3).fit(x, y)    # 多项式函数
    clf_rbf = svm.SVC().fit(x, y)                            # 径向基函数 
    clf_sigmoid = svm.SVC(kernel='sigmoid').fit(x, y)        # Sigmoid函数

    for i, clf in enumerate((clf_linear, clf_poly, clf_rbf, clf_sigmoid)):
        answer = clf.predict(np.c_[xx.ravel(), yy.ravel()])
        print(clf)
        print(np.mean(answer == y_train))
        print(answer)",pyscript/ml/svm.py,jarvisqi/learn_python,1
"    x_test = transform_sex_column(x_test)
    x_test = transform_family_size_column(x_test)
    x_test = transform_fare_column(x_test)
    x_test = transform_age_column(x_test)
    x_test = transform_name_column(x_test)
    x_test = transform_embarked_column(x_test)

    return x_test

def run_svm(x_train, y_train, x_test):
    svc = SVC(kernel=""poly"", degree=3)
    svc.fit(x_train, y_train)
    y_test = svc.predict(x_test)
    train_score = svc.score(x_train, y_train)
    scores = cross_val_score(svc, x_train, y_train, cv=5)

    return (y_test, scores.mean(), scores.std(), train_score)

def run_random_forest(x_train, y_train, x_test):
    global max_features, n_estimators",titanic/titanic.py,shawpan/kaggle,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.SVC(C=1., gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='js').export()
print(output)

""""""
// Array.prototype.fill polyfill:
[].fill||(Array.prototype.fill=function(a){for(var b=Object(this),c=parseInt(b.length,10),d=arguments[1],e=parseInt(d,10)||0,f=0>e?Math.max(c+e,0):Math.min(e,c),g=arguments[2],h=void 0===g?c:parseInt(g)||0,i=0>h?Math.max(c+h,0):Math.min(h,c);i>f;f++)b[f]=a;return b});
",examples/classifier/SVC/js/basics.py,nok/sklearn-porter,1
"rw_labels = []
for x in lines:
    rw_labels.append(int((x.split('	')[1]).replace('\n', '')))
f.close()
# Reduce labels to only two categories
training_labels = np.array(training_labels)
training_labels[training_labels > 0] = 1
rw_labels = np.array(rw_labels)
rw_labels[rw_labels > 0] = 1
# Support vector machine
svm_model = svm.SVC(kernel='linear', C=10)
svm_model.fit(training_set, training_labels)

svm_test_predict = svm_model.predict(test_set)
svm_rw_predict = svm_model.predict(rw_set)

print(""\nSVM\n"")
print(""Test set confusion matrix:\n%s"" % metrics.confusion_matrix(test_labels, svm_test_predict))
print(""Real world confusion matrix:\n%s"" % metrics.confusion_matrix(rw_labels, svm_rw_predict))
",Exercises/06_wheres_wally_traditional.py,peterwittek/qml-rg,1
"
    training_X = nlp_training_X
    testing_X = nlp_testing_X

  ##Choose classifier
  #linear SVC
  if name == ""svm"" :
    scaler = preprocessing.StandardScaler()
    training_X = scaler.fit_transform(training_X)
    testing_X = scaler.transform(testing_X)
    clf = svm.SVC(kernel='linear', degree=3, cache_size=1000)
  #Naive Bayes
  if name == ""MultinomialNB"":
    clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
  #Ensemble Methods
  if name == ""adaboost"" :
    clf = AdaBoostClassifier(n_estimators=100)
  if name == ""random_forest"" :
    clf = RandomForestClassifier(n_estimators=100)
  if name == ""decision_tree"" :",classification.py,dnr2/fml-twitter,1
"
#print(sub_data.shape)
X_1 = sub_data.todense().tolist()

#print(sparse.csr_matrix(X_1).shape)
y_1 = map(int,sub_sample)
#print(len(y_1))


#L2 SVM trained on the features selected by the L1 SVM subSampling
clf = LinearSVC(penalty='l1', dual=False,C=c).fit(X_1, y_1)
model = SelectFromModel(clf, prefit=True)
X = model.transform(X_1)

print(""number of featuers selected %d"",X.shape[1])
clf = LinearSVC(penalty='l2',dual=False,C=c)
scores = cross_validation.cross_val_score(clf, X, y_1, cv=10)

print(scores)
print(""L2 SVM trained on the features selected by the L1 SVM subsampling. \n  Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))",comparisionL2Svm.py,narendrameena/featuerSelectionAssignment,1
"        reshape_image = image.reshape((1, -1))
        self.data.extend(reshape_image)

    def merge(self, Personal_letters):
        self.target.extend(Personal_letters.target)
        self.images.extend(Personal_letters.images)
        self.data.extend(Personal_letters.data)


def trainSVM(X, y, file_name='objs.pickle'):
    clf = svm.SVC()
    clf.fit(X, y)
    with open(file_name, 'w') as f:
        pickle.dump(clf, f)


def loadSVM(file_name='objs.pickle'):
    with open(file_name) as f:
        clf = pickle.load(f)
        return clf",localSVM.py,eranroz/heocr,1
"            l = f.readline()
            if not l :
                break
            l = l.strip().split()
            result = []
            for i in range(len(l)):
                result.append(float(l[i]))
            test_x.append(result)
    print ""FINSIH LOADING TEST""
    from sklearn.svm import SVC
    x = SVC()
    x.fit(train_x, train_y)
    print ""TRAINING...""
    result = x.predict(test_x)
    print ""PREDICTING...""
    num = 0
    for i in range(len(test_y)):
        if test_y[i]!=result[i]:
            num+=1
    print float(num)/float(len(test_y))",svm_test.py,largelymfs/paragraph2vec,1
"	variables as features.
	Prohibitive run time for N > 20,000. Cross-validation even worse.
	Consider anova_svm instead."""""" 



	start = time.time()

	X, y = Build_Data_Set(dataset, DV, lower_limit, upper_limit)

	clf = SVC(kernel=""linear"", C= 1.0)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Linear SVC""
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 


# Unhash to test:",pipeline/__5_MCPS_Classifier.py,BridgitD/school-dropout-predictions,1
"    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import GaussianNB
    from sklearn.svm import SVC
    

    modeler.models = {    ""KNeighbors_default"": sklearn.neighbors.KNeighborsClassifier()
                      , ""RandomForest"": sklearn.ensemble.RandomForestClassifier()
                      , ""LogisticRegression"": sklearn.linear_model.LogisticRegression(penalty='l1', C=0.1)
                      , ""GaussianNB"": GaussianNB()
                      , ""SVC_rbf"": SVC(kernel = 'rbf', probability = True, random_state = 0)
                      , ""SVC_linear"": SVC(kernel = 'linear', probability = True,  random_state = 0)
                      , ""SVC_poly"": SVC(kernel = 'poly', degree = 3, probability = True,  random_state = 0)
                      }

    #Different method for KNeighbors allows us to compare multiple k's
    for i in range(3,13):
        modeler.models[""KNeighbors_{}"".format(i)] = sklearn.neighbors.KNeighborsClassifier(n_neighbors=i)

    #Attach training data",code/prediction/run_models.py,georgetown-analytics/housing-risk,1
"
        self._labels = labels

        if solving_algorithm not in ['w2v', 'ms', 'rk']:
            raise ValueError(""Bad solving algorithm value"")

        self._solving_algorithm = solving_algorithm
        self._zeroes_for_unknown = zeroes_for_unknown

        self._classifier = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),
                                     ('svm', SVC(kernel='linear'))])

        train_set = self.read_questions_from_json(False)

        x_train = map(lambda x: x['question'], train_set)
        x_target = map(lambda x: x['type'], train_set)

        self._classifier.fit(x_train, x_target)

        try:",deprecated/PUTIq.01/PUTIq/putiq/putiq/question_processor_core.py,dudenzz/word_embedding,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0706_2015_2.py,magic2du/contact_matrix,1
"
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA


def create_image_classifier():
    clf_pipe = Pipeline([
        ('pca', PCA(svd_solver='randomized', whiten=True)),
        ('svc', SVC(probability=True))])
    return clf_pipe


def eigenfaces_from_classifier(clf, h, w):
    pca = clf.named_steps['pca']
    eigenfaces = pca.components_.reshape((pca.n_components, h, w))
    return eigenfaces",faces/construction.py,AlexPereverzyev/ml,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_test_0518.py,magic2du/contact_matrix,1
"        unigramOverlap += 1
    additionalFeatures = []
    additionalFeatures.append(cosine_similarity(vec1, vec2))
    additionalFeatures.append(len(tokens_sentenceOne))  # length of sentence one
    additionalFeatures.append(len(tokens_sentenceTwo))  # length of sentence two
    additionalFeatures.append(unigramOverlap)   
    
    samples_x.append(vec1 + vec2 + additionalFeatures)
    samples_y.append(isParaphrase)
      
  mySvm = svm.SVC()
  mySvm.fit(samples_x, samples_y)
    
  print '\nTRAINING FINISHED', datetime.datetime.now().time()
  return mySvm
  
  
if __name__ == '__main__':
  emb_dict = loadEmbeddingFile('/mounts/data/proj/kann/3_RESCAL_EMB_200/result/word2embedding_giga_wiki_20140711.txt')
  ",TrainClassifier.py,Kelina/SentenceEmbeddings,1
"            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier
            estimator = GradientBoostingClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'linear-svm':
            from sklearn.svm import SVC
            estimator = SVC(probability=True,
                            random_state=self.random_state, **self.kwargs)
        else:
            raise NotImplementedError

        # Create the different folds
        skf = StratifiedKFold(y, n_folds=self.cv, shuffle=False,
                              random_state=self.random_state)

        probabilities = np.zeros(y.shape[0], dtype=float)",imbalanced-learn-master/imblearn/under_sampling/instance_hardness_threshold.py,RPGOne/Skynet,1
"
#random generated data
positive = np.transpose(np.random.multivariate_normal(mean1,cov1,n_p).T)
negative = np.transpose(np.random.multivariate_normal(mean2,cov2,n_n).T)

data = np.vstack((positive,negative))
label = [1]*n_l + (n_p+n_n-n_l)*[0]


# run classification
clf = svm.SVC(kernel='poly')
clf.fit(data, label)

#xy grid of points to be evaluated with classifier
xx, yy = np.meshgrid(np.linspace(-10, 10, 200),  np.linspace(-10, 10, 200))

#calculate ""distances"" to hyperplane
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
",SVMClassification_unlabeled.py,lcreyes/PULearningSelfTutorial,1
"except:
    print(""d"")

try:
    a = NaiveBayesClassifier.train(featuresets[:300])
    print(classify.accuracy(a, featuresets[300:]))
except:
    print(""a"")

try:
    e = SklearnClassifier(LinearSVC())
    e.train(featuresets[:300])
    print(classify.accuracy(e, featuresets[300:]))
except:
    print(""e"")
try:
    f = SklearnClassifier(SVC(), sparse=False)
    f.train(featuresets[:300])
    print(classify.accuracy(f, featuresets[300:]))
except:",python-getting-started/nlp/classificationTools.py,jessefeinman/FintechHackathon,1
"  settings = {
      'x': None,
      'input_labels_path': None,
      'h5_labels_data_path': None
      }

  def setup(s):
    s.labels = s.load_labels()

  def run(s):
    s.model = sklearn.svm.LinearSVC()
    s.model.fit(s.x, s.labels)

  def load_labels(s):
    with h5py.File(s.input_labels_path) as f:
      return f[s.h5_labels_data_path]



class Predictor(Core):",sparco/job.py,smackesey/sparco,1
"    print ""Done!""
    import IPython
    IPython.embed()
    return df

def fit_models(df, X, y):
   # Classifiers to test
    classifiers = [('logistic_regression', LogisticRegression())] 
                   #('k_nearest_neighbors', KNeighborsClassifier()),
                   #('decision_tree', DecisionTreeClassifier()),
                   #('SVM', LinearSVC()),
                   #('random_forest', RandomForestClassifier()),
                   #('boosting', GradientBoostingClassifier()),
                   #('bagging', BaggingClassifier())]

    ml.build_classifiers(df, X, y, classifiers)
    #ml.test_classifier(df, X, y, classifiers)
 

#-------------------------------------------------------",pipeline/mcps_pipeline.py,BridgitD/school-dropout-predictions,1
"from sklearn.svm import SVC

iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
svm = SVC(kernel='rbf', C=1.0, gamma=100.0, random_state=0)
svm.fit(X_train_std, y_train)
y_pred = svm.predict(X_test_std)

def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
",final/code/sk_learn/svm_iris.py,Mageluer/computational_physics_N2014301040052,1
"            (RandomForestClassifier(n_estimators=100), ""Random forest"")
            ):
        print('=' * 80)
        print(name)
        results.append(benchmark(clf))

    for penalty in [""l2"", ""l1""]:
        print('=' * 80)
        print(""%s penalty"" % penalty.upper())
        # Train Liblinear model
        results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,dual=False, tol=1e-3)))

        # Train SGD model
        results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,penalty=penalty)))

    # Train SGD with Elastic Net penalty
    print('=' * 80)
    print(""Elastic-Net penalty"")
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,penalty=""elasticnet"")))
",OLD/Master Classifier/TextClassifierFNF.py,tpsatish95/Youtube-Comedy-Comparison,1
"np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# figure number
fignum = 1

# fit the model
for name, penality in (('unreg', 1), ('reg', 0.05)):

    clf = svm.SVC(kernel='linear', C=penality)
    clf.fit(X, Y)

    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = a * xx - (clf.intercept_[0]) / w[1]

    # plot the parallels to the separating hyperplane that pass through the",python/sklearn/examples/svm/plot_svm_margin.py,seckcoder/lang-learn,1
"	np.save('trainX', trainX)
	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/mnistBackRandom/arc_cosine.py,akhilpm/Masters-Project,1
"    for i in range(y.shape[0]):
        ret[i] = -1 if y[i] != thres else 1

    return ret


def q15():
    X_train, y_train = load_data('/Users/pjhades/code/lab/ml/train.dat')
    y = set_binlabel(y_train, 0)

    svm = sklearn.svm.SVC(C=0.01, kernel='linear') 
    svm.fit(X_train, y)
    print(linalg.norm(svm.coef_))


def get_error(svm, X, y):
    err = 0
    N = y.shape[0]
    for i in range(N):
        if y[i] != svm.predict(X[i])[0]:",ml2/1.py,pjhades/coursera,1
"                       'vect__analyzer' : ('word', 'char_wb')}
        nb_feature_parameters  = {'vect__ngram_range': ((1,1),(1,2),(1,3)),
                       'vect__analyzer' : ('word', 'char_wb')}
        use_spare_array = True
        use_binary_features = True
        classifiers = ({
            'logistic_regression':(linear_model.LogisticRegression(),
                                   use_spare_array,
                                   not use_binary_features,
                                   concatenate(feature_parameters, {'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),
            'svm_linear':(svm.LinearSVC(tol=1e-6),
                          use_spare_array,
                          not use_binary_features,
                          concatenate(feature_parameters, {'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),
            'svm_gaussian':(svm.SVC(tol=1e-6, kernel='rbf'),
                            use_spare_array,
                            not use_binary_features,
                            concatenate(feature_parameters, {'clf__gamma': [.01, .03, 0.1],
                                                     'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),
            'decision_tree':(tree.DecisionTreeClassifier(criterion='entropy', random_state=RandomState(seed)),",model_analysis.py,chop-dbhi/arrc,1
"from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

def reduce_features_chi2(X, Y):
	X_new = SelectKBest(chi2, k=2).fit_transform(X, Y)
	return X_new

def reduce_features_l1(X, Y):
	lsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(X, y)
	model = SelectFromModel(lsvc, prefit=True)
	X_new = model.transform(X)",src/mod_suggest/feature_reduction.py,Drob-AI/The-Observer,1
"            Y.append(temp);
        
        for i in range(len(X) - 1):
            assert iris.data[i] != X[i]

        trainX = X[:trainCount]
        trainY = Y[:trainCount]
        


        svc = svm.SVC(kernel='linear')
        svc.fit(trainX, trainY)  

        

        predictX = X[-predictCount:]



        expected = Y[-predictCount:]",Scikit/Scikit/exercise_2_2_2.py,TechnicHail/COMP188,1
"from sklearn.svm import SVC

""""""
L1- Based Feature Selection
""""""
def extract_linear_features_indexes(features, labels):
    """"""
    Perform Linear festure selection.
    """"""

    clf = LinearSVC(C=0.01, penalty=""l1"", dual=False)
    clf.fit(features, labels)

    return [i for i, e in enumerate(clf.coef_[0]) if e != 0 and abs(e) > 1e-6]
    
def extract_lasso_features_indexes(features, labels):
    """"""
    Perform Lasso feature selection.
    """"""
",execute.py,krishnasumanthm/Quora_Answer_Classifier,1
"
iris = datasets.load_iris()
rng = np.random.RandomState(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
n_classes = 3


def test_ovr_exceptions():
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    assert_raises(ValueError, ovr.predict, [])

    with ignore_warnings():
        assert_raises(ValueError, predict_ovr, [LinearSVC(), MultinomialNB()],
                      LabelBinarizer(), [])

    # Fail on multioutput data
    assert_raises(ValueError, OneVsRestClassifier(MultinomialNB()).fit,
                  np.array([[1, 0], [0, 1]]),",net-p3/lib/python3.5/site-packages/sklearn/tests/test_multiclass.py,uglyboxer/linear_neuron,1
"print(len(X_test), len(y_test))

#tfidf transformation is automately employed in Pipeline
## tfidf = TfidfVectorizer()
## tfidf.fit_transform(X_train)

# build a pipeline - SVC
from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),
                    ('tfidf', TfidfTransformer()),
                    ('clf', OneVsRestClassifier(LinearSVC(random_state=0)))
                     ])

# fit using pipeline
clf = text_clf.fit(X_train, y_train)

# predict
predicted = clf.predict(X_test)
clf.score(X_test, y_test) 
",reference/movie_review_test.py,Capstone2017/Machine-Learning-NLP,1
"    assert_equal(pipe.get_params(deep=True),
                 dict(svc__a=None, svc__b=None, svc=clf))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",venv/lib/python2.7/site-packages/sklearn/tests/test_pipeline.py,chaluemwut/fbserver,1
"        # summarize components
        print(""Explained Variance: {}"".format(fit.explained_variance_ratio_))
        print(fit.components_)


    def L1(self):
        print(""=""*70)
        print(""L1-Based"")
        print(""=""*70)

        lsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False)
        fit = lsvc.fit(self.X, self.y)

        model = SelectFromModel(lsvc, prefit=True)
        X_new = model.transform(self.X)
        print(""New shape of the matrix: {} old was {}. Removed {} cols"".format(X_new.shape,
                                                                               self.X.shape,
                                                                               self.X.shape[1] - X_new.shape[1]))
        print(X_new)
",immo/scikit/data_analysis.py,bhzunami/Immo,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",sklearn/metrics/tests/test_metrics.py,B3AU/waveTree,1
"    
    ###########
    # Train SVM
    ###########
    if (not exists(conf.modelPath)) | OVERWRITE:
        if VERBOSE: print str(datetime.now()) + ' training liblinear svm'
        if VERBOSE == 'SVM':
            verbose = True
        else:
            verbose = False
        clf = svm.LinearSVC(C=conf.svm.C)
        if VERBOSE: print clf
        clf.fit(train_data, all_images_class_labels[selTrain])
        with open(conf.modelPath, 'wb') as fp:
            dump(clf, fp)
    else:
        if VERBOSE: print 'loading old SVM model'
        with open(conf.modelPath, 'rb') as fp:
            clf = load(fp)
",phow_validate.py,md100play/BirdID,1
"    ----------
    .. [1] `Wikipedia entry on the Hinge loss
            <http://en.wikipedia.org/wiki/Hinge_loss>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
         random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",venv/lib/python2.7/site-packages/sklearn/metrics/metrics.py,chaluemwut/fbserver,1
"                    trainArr.append(tweet['text'])
                    y_train.append(tweet['tags'])
            if len(y_train) == 0 or len(trainArr) == 0 or len([item for sublist in y_train for item in sublist]) == 0:
                return False

        x_train = np.array(trainArr)

        classifier = Pipeline([
            ('vectorizer', CountVectorizer(ngram_range=(1,2))),
            ('tfidf', TfidfTransformer()),
            ('clf', OneVsRestClassifier(LinearSVC()))
            ])

        classifier.fit(x_train, y_train)
        self.classifier = classifier
        return True

    def test(self):
        if self.classifier == None:
            return False",TweetParser.py,Maistho/twitter-parser,1
"    '''
    Perform grid search to determine the best parameters for SVM Classifier
    train_set -- the training set
    train_set_labels -- labels that correspond to the training set

    returns the best determined parameters and the mean score 
    for the classifier obtained with those parameters
    '''
    pipeline = Pipeline([
                        ('vect', TfidfVectorizer(token_pattern=u'\\b\\w+\\b')),
                        ('clf', LinearSVC())
                        ])
    parameters = {""vect__ngram_range"" : ((1,2),), \
                'clf__C': (0.1, 0.3, 0.5, 0.7, 0.9, 1.0 , 1.3, 10), \
                }
    gs_clf = GridSearchCV(pipeline, parameters, n_jobs=2, pre_dispatch=2,verbose=2 )
    # score_func = metrics.f1_score
    gs_clf = gs_clf.fit(train_set, train_set_labels)
    best_parameters, mean_score = get_best_params(gs_clf.grid_scores_)
    for param_name in sorted(parameters.keys()):",okstereotype/pick_best_model_v1.py,lqdc/okstereotype,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = JMI.jmi(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_JMI.py,jundongl/PyFeaST,1
"    """"""
    Implements the detector of prototypic emotions on face images.
    """"""

    #---------------------------------------------
    def __init__(self):
        """"""
        Class constructor.
        """"""

        self._clf = svm.SVC(kernel='rbf', gamma=0.001, C=10,
                                decision_function_shape='ovr',
                                probability=True, class_weight='balanced')
        """"""
        Support Vector Machine with used as the model for the detection of the
        prototypic emotions. The kernel and its parameters were identified by
        running the optimize() method.
        """"""

        self._emotions = OrderedDict([",fsdk/detectors/emotions.py,luigivieira/fsdk,1
"	return prediction

def score(ytrue, ypred):
	t = time()
	acc = metrics.accuracy_score(ytrue, ypred)
	print ""Scoring took {} scs"".format(time() - t)
	print ""Accuracy is {}"".format(acc)
	return acc

def cycle(C):
	clf = svm.SVC(kernel='rbf', C=C)
	clf = classify(clf, features_train, labels_train)
	prediction = predict(clf, features_test)
	score(labels_test, prediction)
	return prediction	

def tuneC():
	C = 0.01
	for i in range(10):
		print ""C is {}"".format(C)",svm/svm_author_id.py,lucasosouza/introML,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",scikit-learn-0.17.1-1/sklearn/tests/test_pipeline.py,RPGOne/Skynet,1
"classLabels = sorted(set([x.split('_')[0] for x in os.listdir(EVAL_DIR) if re.match('[A-Z].+_(train|test)\.txt', x)]))
hwd2_labels = lambda pref, classLabel: map(np.array, zip(*np.genfromtxt(os.path.join(EVAL_DIR, '%s_%s.txt' % (classLabel, pref)), dtype = None)))
slice_kernel = lambda inds1, inds2: all_k[np.ix_(map(allClipsNoExt.index, inds1), map(allClipsNoExt.index, inds2))]

octave.addpath('vlfeat-0.9.19/toolbox/misc')
octave.addpath('vlfeat-0.9.19/toolbox/plotop')

def svm_train_test(train_k, test_k, ytrain, ytest, REG_C):
	ytrain = list(ytrain)

	model = SVC(kernel = 'precomputed', C = REG_C, max_iter = 10000)
	model.fit(train_k, ytrain)

	flatten = lambda ls: list(itertools.chain(*ls))
	train_conf, test_conf = map(flatten, map(model.decision_function, [train_k, test_k]))
	
	rec, prec, trainInfo = octave.vl_pr(ytrain, train_conf)
	rec, prec, testInfo = octave.vl_pr(ytest, test_conf)

	return trainInfo['auc_pa08'], testInfo['auc_pa08']",repro/hollywood-2/classify.py,vadimkantorov/cvpr2014,1
"                               ""Names provided are not unique: \('m1', 'm2', 'm1'\)"",
                               Stacking, _NoFitEstimator,
                               [('m1', _PredictDummy), ('m2', _PredictDummy), ('m1', _PredictDummy)])

    def test_fit(self):
        data = load_iris()
        x = data[""data""]
        y = data[""target""]

        meta = Stacking(LogisticRegression(), [('tree', DecisionTreeClassifier(max_depth=1, random_state=0)),
                                               ('svm', SVC(probability=True, random_state=0))])
        self.assertEqual(2, len(meta))
        meta.fit(x, y)

        p = meta._predict_estimators(x)
        self.assertTupleEqual((x.shape[0], 3 * 2), p.shape)

        self.assertTupleEqual((3, 3 * 2), meta.meta_estimator.coef_.shape)

    def test_fit_sample_weights(self):",tests/test_stacking.py,sebp/scikit-survival,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def processFrame(self, dataURL, identity):
        head = ""data:image/jpeg;base64,""
        assert(dataURL.startswith(head))
        imgdata = base64.b64decode(dataURL[len(head):])
        imgF = StringIO.StringIO()
        imgF.write(imgdata)
        imgF.seek(0)
        img = Image.open(imgF)",demos/web/server.py,sahilshah/openface,1
"INSTRU_NAMES = ['scalpel','retractor','scissors','hemostat','forceps']


def GetClassifier(name):
    """"""
    return the clf based on the @arg name
    the parameters are chosen to be default
    name can be ['linearSVM','rbfSVM','randomforest','decisiontree', 'adaboost']
    """"""
    if name == 'linearSVM':
        clf = svm.SVC(kernel = 'linear', probability = True)
    elif name == 'rbfSVM':
        clf = svm.SVC(kernel = 'rbf', probability = True)
    elif name == 'sigmoidSVM':
        clf = svm.SVC(kernel = 'sigmoid', probability = True)
    elif name == 'polySVM':
        clf = svm.SVC(kernel = 'poly', probability = True)
    elif name == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif name == 'randomforest':",InstrumentRecognition/InstrumentRecognition.py,tian-zhou/Surgical-Instrument-Dataset,1
"        x_train, y_train = get_traindata_single_file(hypes,
                                                     x_files[0],
                                                     y_files[0])

        nb_features = x_train[0].shape[0]
        logging.info(""Input gets %i features"", nb_features)

        # Make model
        from sklearn.svm import LinearSVC, SVC
        from sklearn.tree import DecisionTreeClassifier
        model = SVC(probability=False,  # cache_size=200,
                    kernel=""linear"", C=2.8, gamma=.0073)
        model = LinearSVC(C=2.8)
        model = DecisionTreeClassifier()

        print(""Start fitting. This may take a while"")

        generator = generate_training_data(hypes, x_files, y_files)
        t0 = time.time()
",svm/basic_local_classifier.py,TensorVision/MediSeg,1
"        self.name = self.clf_name + opts

        if self.clf_name == 'naive_bayes':
            self._make_clf(MultinomialNB())
        elif self.clf_name == 'logistic':
            self.binary_counts = True
            self._make_clf(LogisticRegression(C=self.penalty, penalty='l2'))
        elif self.clf_name == 'lin_svc':
            self.binary_counts = True
            self.normalize = True
            self._make_clf(LinearSVC(C=self.penalty))
        else:
            raise NotImplementedError('Classifier %s not supported; choose from:\n'
                                      '%s' % (self.clf_name, self.supported_classifiers))

    def _make_chained(self, column):
        if self.column != column:
            chain = ChainedClassifier(
                clf=SklearnCLF(*self.chain_args),
                column=column,",edxclassify/classifiers/sklearn_clf.py,akshayka/edxclassify,1
"        scaled_X,
        y,
        test_size=0.2,
        random_state=rand_state
    )

    print('Using:',orient,'orientations',pix_per_cell,'pixels per cell and', cell_per_block,'cells per block')
    print('Feature vector length:', len(X_train[0]))
    # Use a linear SVC
    global model
    model = LinearSVC()
    # Check the training time for the SVC
    print(""Training"")
    t=time.time()
    model.fit(X_train, y_train)
    t2 = time.time()
    print(round(t2-t, 2), 'Seconds to train SVC...')
    print(""Trained"")
    # Check the score of the SVC
    print('Test Accuracy of SVC = ', round(model.score(X_test, y_test), 4))",vehicle_tracking/solution/main.py,kashmawy/self_driving_cars,1
"	
	#############
	# Train SVM #
	#############
	if (not exists(conf.modelPath)) | OVERWRITE:
		if VERBOSE: print (str(datetime.now()) + ' training liblinear svm')
		if VERBOSE == 'SVM':
			verbose = True
		else:
			verbose = False
		clf = svm.LinearSVC(C=conf.svmC)
		if VERBOSE: print (clf)
		clf.fit(train_data, all_images_class_labels[selTrain])
		with open(conf.modelPath, 'wb') as fp:
			dump(clf, fp)
	else:
		if VERBOSE: print (""loading old SVM model"")
		with open(conf.modelPath, 'rb') as fp:
			clf = load(fp)
",phow_birdid_multi.py,lbarnett/BirdID,1
"            X_train1.columns = X_train.columns[selector.get_support(True)]
            X_test1 = X_test[X_train1.columns]
        else:
            X_train1, X_test1 = X_train, X_test        
            
        #训练并预测SVC模型
        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]},
                {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},
                 {'kernel': ['sigmoid'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]
        classifier = GridSearchCV(SVC(probability=True), tuned_parameters, cv=5)
        #SVC(kernel=kernel, probability=True)
        classifier.fit(X_train1, y_train)  
        probability = classifier.predict_proba(X_test1)[:,1]
        
        predresult = pd.DataFrame({'target' : y_test, 'probability' : probability})
        
        return predresult

    def SVC_Logistic_trainandtest(self, testsize, cv, feature_sel, varthreshold, nclusters=10, cmethod=None):
        #先用svc过滤，再用logistic评分",allinpay projects/creditscoreSVC/classSVC.py,allinpaybusiness/ACS,1
"                  clf=None,verbose=False):
    """"""
    run classifier for a single dataset
    """"""
    features=data
    if scale:
        features=sklearn.preprocessing.scale(features)
    if shuffle:
        numpy.random.shuffle(labels)
    if not clf:
        clf=sklearn.svm.SVC(C=C)
    skf = sklearn.model_selection.StratifiedKFold(5,shuffle=True)
    pred=numpy.zeros(labels.shape[0])
    for train, test in skf.split(features,labels):
        clf.fit(features[train,:],labels[train])
        pred[test]=clf.predict(features[test,:])
    if verbose:
        print(clf.best_params_)
    acc=sklearn.metrics.accuracy_score(labels, pred)
    return acc",analysis/MVPA/CogNeuro2017/ClassficationAnalysis-attention.py,poldrack/fmri-analysis-vm,1
"from sklearn.svm.classes import LinearSVC

from ..Classifier import Classifier
from ...language.PHP import PHP


class LinearSVCPHPTest(PHP, Classifier, TestCase):

    def setUp(self):
        super(LinearSVCPHPTest, self).setUp()
        self.mdl = LinearSVC(C=1., random_state=0)

    def tearDown(self):
        super(LinearSVCPHPTest, self).tearDown()",tests/classifier/LinearSVC/LinearSVCPHPTest.py,nok/sklearn-porter,1
"
test=pd.read_json('test.json')

test['ingredients_string'] = [' '.join([wnl.lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in test['ingredients']]

Xtest=test.ingredients_string.values

tfidfTest=vec1.transform(Xtest)


svm1=svm.LinearSVC(C=1)

svm1.fit(tfidfTrain,y)

ypred1=svm1.predict(tfidfTest)

test['cuisine']=ypred1

test[['id','cuisine']].to_csv('pred1.csv',index=False)
",submission.py,lingcheng99/Kaggle-what-is-cooking,1
"
y_1 = map(int,data[1])   # classes 2

#print(len(map(int,data[1])))
# set up dataset
n_samples = 72
n_features = 7000


#L1 SVM
l1svc = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

model = SelectFromModel(l1svc, prefit=True)
X_2 = model.transform(X_1)
'''
# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features / 5)
'''",l1svm.py,narendrameena/featuerSelectionAssignment,1
"            self.assertTrue(os.path.isfile(file_name))

    def test_inspect_instance_returns_json_dict(self):
        metrics = self.clipper_inst.inspect_instance()
        self.assertEqual(type(metrics), dict)
        self.assertGreaterEqual(len(metrics), 1)

    def test_model_deploys_successfully(self):
        # Initialize a support vector classifier 
        # that will be deployed to a no-op container
        model_data = svm.SVC()
        container_name = ""clipper/noop-container""
        input_type = ""doubles""
        result = self.clipper_inst.deploy_model(
            self.deploy_model_name, self.deploy_model_version, model_data,
            container_name, input_type)
        self.assertTrue(result)
        model_info = self.clipper_inst.get_model_info(
            self.deploy_model_name, self.deploy_model_version)
        self.assertIsNotNone(model_info)",integration-tests/clipper_manager_tests.py,dubeyabhi07/clipper,1
"n_classes = np.unique(y).shape[0]
#deal with sample_weight
sample_weight = np.ones(y.shape[0])
classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))

###############################################################################
print(""PREPARE CLASSIFICATION"")
#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
if fs_n > 1:
    fs = SelectKBest(f_classif, k=fs_n)
elif fs_n == -1:
    fs = SelectKBest(f_classif, k=1)",JR_toolbox/_skl_king_parallel_20130130.py,cjayb/kingjr_natmeg_arhus,1
"  OBJECTIVE_NAME = 'some_diseases_ver1' #'tuberculum'#'patients_ver1' # e.g. 'BMIgr', 'Sex', 'cl_sleep_interval'
  sample_name = OBJECTIVE_NAME + '_1' # train-test filename
  SEED = 0


  USE_DAY_TIME = True#False #!!!!!!!!!!!!!


  classifiers = [
      (""Dummy"", DummyClassifier(strategy='stratified')), # see http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html
      # (""Linear SVM"", SVC(kernel=""linear"", C=0.025)),
      # (""RBF SVM"", SVC(gamma=2, C=1)),
      #(""Decision Tree"", DecisionTreeClassifier(max_depth=5)),
      #(""Nearest Neighbors"", KNeighborsClassifier(3)),
      # (""AdaBoost"", AdaBoostClassifier()),
      (""Naive Bayes"", GaussianNB()),
      (""LogisticRegression"", LogisticRegression()),
      (""Random Forest"", RandomForestClassifier(n_estimators=100)),
      #(""XGBoost"", xgb.XGBClassifier())
      ] # TODO: xgboost",scripts/pipeline/model_building.py,prikhodkop/ECG_project,1
"    # Split a holdout set
    data_idx, hold_idx = next(iter(StratifiedShuffleSplit(y, 1, test_size = 0.1, random_state=0)))
    X_data, X_hold = X[data_idx], X[hold_idx]
    y_data, y_hold = y[data_idx], y[hold_idx]

    # Test data
    X_test, _ = OttoCompetition.load_data(train=False)
    X_test = scaler.transform(X_test).astype('float')

    # CV
    clf = SVC(**params)
    skf = StratifiedKFold(y_data, n_folds=8, random_state=0)
    parallel = Parallel(n_jobs=8, verbose=True, pre_dispatch='2*n_jobs')
    blocks = parallel(delayed(_fit)(clone(clf), X_data, y_data, train, valid) for train, valid in skf)

    clfs = [c for c, _ in blocks]
    scores = np.array([s for _, s in blocks])
    print(scores)
    
    # Do predictions on test set",otto/svm.py,ldamewood/kaggle,1
"    data[:, 1] = radius * np.sin(theta)

    labels = np.ones(Npts)
    labels[far_pts] = -1

    return data, labels

#------------------------------------------------------------
# Linear model
X, y = linear_model()
clf = svm.SVC(kernel='linear',
              gamma=0.01, coef0=0, degree=3)
clf.fit(X, y)

fig = pl.figure()
ax = pl.subplot(111, xticks=[], yticks=[])
ax.scatter(X[:, 0], X[:, 1], c=y, cmap=pl.cm.bone)

ax.scatter(clf.support_vectors_[:, 0],
           clf.support_vectors_[:, 1],",examples/plot_gui_example.py,astroML/sklearn_tutorial,1
"	#print np.round(features, 1)

	predictions = []

        best_percentage = 0

        #C_range = np.logspace(-2, 10, num=13, base=2)
        #gamma_range = np.logspace(-5, 1, num=7, base=10)
        #param_grid = dict(gamma=gamma_range, C=C_range)
        #cv = StratifiedShuffleSplit(trainingLabels, n_iter=3, test_size=0.31, random_state=42)
        #grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
        #grid.fit(trainingFeatures, trainingLabels)
        ##C_range = np.logspace(-1, 1, num=2, base=2)
        ##gamma_range = np.logspace(-1, 1, num=2, base=10)
        ##param_grid = dict(gamma=gamma_range, C=C_range)
        ##cv = StratifiedShuffleSplit(trainingLabels, n_iter=1, test_size=0.11, random_state=42)
        ##grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
        ##grid.fit(trainingSet, trainingLabels)

        #best_C = grid.best_params_['C']",CrossDataAnalysis.py,JessMcintosh/SonoGestures,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the f-score of each feature
        score = f_score.f_score(X, y)

        # rank features in descending order according to score
        idx = f_score.feature_ranking(score)
",skfeature/example/test_f_score.py,jundongl/scikit-feature,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx, feature_score, subset_score = trace_ratio.trace_ratio(X[train], y[train], num_fea, style='fisher')

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_trace_ratio.py,jundongl/PyFeaST,1
"    def _testHMM_ghmm(self, model, testData, results):
        seqs = self._data.getSequences(testData,self._modelParams)
        #print(""used test seqs: ""+ str(len(seqs)))
        trainSeqs = ghmm.SequenceSet(ghmm.IntegerRange(0,self._k), seqs)
        true_labels = self._data.getSequences(testData,'rating')
        pred_labels = model.viterbi(trainSeqs)
        
        return results
        
    def _trainSVM(self, data):
        model = SVC(**self._models['svm'])
        x = np.array(data[self._data._params])
        y = np.array(data['rating'])
        model.fit(x,y)
        
        return model
        
    def _testSklearn(self, model, testData, results):
        true = testData['rating'].values.tolist()
        test = np.array(testData[self._data._params])",src/judgmentHMM.py,phihes/judgmentHMM,1
"        self.assert_frame_equal(tr, df.loc[['g', 'a', 'e', 'f', 'd', 'h']].reset_index(drop=True))
        self.assert_numpy_array_equal(tr.target.values, np.array([7, 1, 5, 6, 4, 8]))
        self.assert_frame_equal(te, df.loc[['c', 'b']].reset_index(drop=True))
        self.assert_numpy_array_equal(te.target.values, np.array([3, 2]))

    def test_cross_val_score(self):
        import sklearn.svm as svm
        digits = datasets.load_digits()

        df = pdml.ModelFrame(digits)
        clf = svm.SVC(kernel=str('linear'), C=1)
        result = df.cross_validation.cross_val_score(clf, cv=5)
        expected = cv.cross_val_score(clf, X=digits.data, y=digits.target, cv=5)
        self.assert_numpy_array_almost_equal(result, expected)

    def test_permutation_test_score(self):
        import sklearn.svm as svm
        iris = datasets.load_iris()

        df = pdml.ModelFrame(iris)",pandas_ml/skaccessors/test/test_cross_validation.py,sinhrks/pandas-ml,1
"
#Read training data and split into train and test data
data=pd.read_csv('train.csv')
data1=data.values
X=data1[:,1:]
y=data1[:,:1]
y=np.ravel(y)
Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.25)

#Run linear kernel first
svmL1=svm.SVC(kernel='linear',C=0.01)
svmL1.fit(Xtrain,ytrain) 
predL1=svmL1.predict(Xtest) 
print(""Classification report for classifier %s:\n%s\n""
      % (svmL1, metrics.classification_report(ytest,predL1)))

#Run gaussian kernel but the result is poor and running time is long
svmR1=svm.SVC(kernel='rbf',gamma=0.001, C=10000)
svmR1.fit(Xtrain,ytrain)
predR1=svmR1.predict(Xtest)",svm.py,lingcheng99/kagge-digit-recognition,1
"        self.reduce_func = reduce_func
        self.reducer = None
        self.grid_search = None

    def add_estimator(self, name, instance, params):
        self.grid_search.add_estimator(name, instance, params)

    def fit(self, X, y=None):
        if self.default:
            self.grid_search = GridSearchEstimatorSelector(X, y, self.cv)
            self.grid_search.add_estimator('SVC', SVC(), {'kernel': [""linear"", ""rbf""],
                                                          'C': [1, 5, 10, 50],
                                                          'gamma': [0.0, 0.001, 0.0001]})
            self.grid_search.add_estimator('RandomForestClassifier', RandomForestClassifier(),
                                       {'n_estimators': [5, 10, 20, 50]})
            self.grid_search.add_estimator('ExtraTreeClassifier', ExtraTreesClassifier(),
                                       {'n_estimators': [5, 10, 20, 50]})
            self.grid_search.add_estimator('LogisticRegression', LogisticRegression(),
                                       {'C': [1, 5, 10, 50], 'solver': [""lbfgs"", ""liblinear""]})
            self.grid_search.add_estimator('SGDClassifier', SGDClassifier(),",tinylearn.py,llvll/beaconml,1
"    return numpy.ceil(10**6 / n)

if __name__ == '__main__':
    d = load_digits()
    X = d.data
    X = StandardScaler().fit_transform(X)
    y = d.target
    _n = X.shape[0]

    # http://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use
    clf = SVC()
    params = {
            'C': 2**numpy.linspace(-5,15),
            'gamma': 2**numpy.linspace(-15,3),
            'class_weight': [None, 'auto'],
            }
    cv = RandomizedSearchCV(clf, params, n_iter=20, cv=best_cv_num(_n), n_jobs=2, verbose=1)
    cv.fit(X, y)
    print(cv.best_score_)
    print(cv.best_params_)",digits/ml.py,arosh/ml,1
"fig1=pb.figure()
ax1=fig1.add_subplot(111)
ax1.plot(results,label='Predicted')
ax1.plot(people[:-int(percentage_of_testing*len(features))],label='Actual')
ax1.set_title('Linear Regression')
pb.legend(loc='upper left');
from sklearn.metrics import accuracy_score
print ""Linear Regression:"", clf.score(features[int(percentage_of_testing*len(features)):],people[int(percentage_of_testing*len(features)):])


clf=SVC(kernel='rbf')
clf.fit(features[:-int(percentage_of_testing*len(features))], people[:-int(percentage_of_testing*len(features))])
results=clf.predict(features[int(percentage_of_testing*len(features)):])
fig2=pb.figure()
ax2=fig2.add_subplot(111)
ax2.plot(results,label='Predicted')
ax2.plot(people[:-int(percentage_of_testing*len(features))],label='Actual')
ax2.set_title('SVC-RBF')

print ""SVC:"", clf.score(features[int(percentage_of_testing*len(features)):],people[int(percentage_of_testing*len(features)):])",createFeats.py,code-for-india/food_predictor,1
"                ('relword', RelatedWordVectorizer(max_df=0.75, ngram_range=(1, 4),
                                                  sublinear_tf=True)),
                # ('pos', TagVectorizer(max_df=0.75, ngram_range=(1, 4),
                #                       sublinear_tf=True)),
                # ('ner', NERVectorizer(max_df=0.5, ngram_range=(1, 4),
                #                       sublinear_tf=True)),
                # ('custom', CustomFeatures()),
                # ('custom_relword', CustomRelWordFeatures()),
            ])),

            ('clf', LinearSVC()),
        ])
        return model

    def train_model(self):
        """"""
        Train the model with extracted features from all the data

        For a sklearn pipeline example, see:
        http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html",inquire/classification/model.py,jcelliott/inquire,1
"
    def __init__(self, X_whole, y_whole, score_func, k=None):
        self.X_whole = X_whole
        self.y_whole = y_whole

        self.score_func = score_func
        self.k = k
        self.kbest = None
        self.kbest_unfitted = True

        self.svc = SVC(kernel='linear', C=1, verbose=False)  # TODO C=1 linear / rbf ??

    def fit(self, X, y):
        if self.kbest_unfitted:
            self.kbest = SelectKBest(score_func=self.score_func, k=self.k)
            self.kbest.fit(self.X_whole, self.y_whole)
            self.kbest_unfitted = False

        X_new = self.kbest.transform(X)
",scripts/util.py,juanmirocks/LocText,1
"    GENDER_SPECIFIC_SEED_WORDS,
    len([w for w in gender_seed if w in E.words]))
)

gender_seed = set(w for i, w in enumerate(E.words) if w in gender_seed or (w.lower() in gender_seed and i<NUM_TRAINING))
labeled_train = [(i, 1 if w in gender_seed else 0) for i, w in enumerate(E.words) if (i<NUM_TRAINING or w in gender_seed)]
train_indices, train_labels = zip(*labeled_train)
y = np.array(train_labels)
X = np.array([E.vecs[i] for i in train_indices])
C = 1.0
clf = LinearSVC(C=C, tol=0.0001)
clf.fit(X, y)
weights = (0.5 / (sum(y)) * y + 0.5 / (sum(1 - y)) * (1 - y))
weights = 1.0 / len(y)
score = sum((clf.predict(X) == y) * weights)
print(1 - score, sum(y) * 1.0 / len(y))

pred = clf.coef_[0].dot(X.T)
direction = clf.coef_[0]
intercept = clf.intercept_",debiaswe/learn_gender_specific.py,tolga-b/debiaswe,1
"# On recupere independament les données de chaque digit 
list_hog_fd = []
for feature in features:
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Nb digits dans la base de données"", Counter(labels)

# Initialisation classifieur
clf = LinearSVC()

# Initialisation de l'apprentissage
clf.fit(hog_features, labels)

# Export",scikit/creerClassifieur.py,TeKrop/PyDigR,1
"def Analysis():
  test_size = 1000
  invest_amount = 10000 # dollars
  total_invests = 0
  if_market = 0
  if_strat = 0

  X, y, Z = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    invest_return = 0
    market_return = 0
    if clf.predict(X[-x])[0] == y[-x]: # test data
      correct_count += 1
",p20.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"        tfea = TextFeature(train_all[i], train_cut_all[i], train_pos_all[i], train_ner_all[i], 0)
        vec.append(tfea.feature)
        label.append(train_all[i].label)

        if i % 10 == 0:
            print("" cur: "", i)

    print(""training..."")
    # training
    if gamma:
        clf = svm.SVC(gamma=gamma)
    else:
        clf = svm.SVC()
    clf.fit(vec, label)

    # reserve
    joblib.dump(clf, '../Models/svm_model.m')

    result = clf.predict(vec)
    acc, f1 = utils.evaluate(label, result)",Source/run.py,laddie132/RITE_zh-CN,1
"	np.save('trainX', trainX)
	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]
	'''

	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/mnistBackImage/arc_cosine.py,akhilpm/Masters-Project,1
"from titleParse import *
import os
from sklearn import svm

#testStringList = getTitles(""test_data"" + os.sep + ""merged.txt"")
#testStringList = getTitles(""test.txt"")
""""""This is a function designed to extract an attribute vector out of the text of
a Craigslist posting. These attribute vectors will be fed to the SciKit Learn
module to determine the quality of the posting itself.""""""

clf = svm.SVC()

def extractVectorsFromListOfPosts(postList):
    
    def extractVectorFromPost(postText):
        upperCaseText = string.upper(postText)
        count = len(postText)
        whiteCount, letterCount, symbolCount, lowerCaseCount = 0, 0 ,0, 0
        for i in xrange(count):
            if postText[i] in string.whitespace: whiteCount += 1",vectorExtractor.py,bharadwajramachandran/TartanRoof,1
"# For decoding, standardizing is often very important
mask_filename = haxby_dataset.mask_vt[0]
masker = NiftiMasker(mask_img=mask_filename, standardize=True)
func_filename = haxby_dataset.func[0]
masked_timecourses = masker.fit_transform(func_filename)[np.logical_not(resting_state)]

### Classifiers definition

# A support vector classifier
from sklearn.svm import SVC
svm = SVC(C=1., kernel=""linear"")

from sklearn.grid_search import GridSearchCV
# GridSearchCV is slow, but note that it takes an 'n_jobs' parameter that
# can significantly speed up the fitting process on computers with
# multiple cores
svm_cv = GridSearchCV(SVC(C=1., kernel=""linear""),
                      param_grid={'C': [.1, .5, 1., 5., 10., 50., 100.]},
                      scoring='f1')
",examples/decoding/plot_haxby_different_estimators.py,salma1601/nilearn,1
"	# pca.n_components = 20
	# reducedData = pca.fit_transform(allData)


	# #Assing the assumed states
	# states = np.repeat([0,1], 10000)

	# X_train, X_test, y_train, y_test = cross_validation.train_test_split(reducedData, states, test_size=0.2, random_state=0)

	# # searchParams = {'gamma':(1.0/100)*np.logspace(-3, 0, 10), 'nu':np.arange(0.01, 0.2, 0.02)}
	# # clf = grid_search.GridSearchCV(svm.NuSVC(cache_size=2000), searchParams, n_jobs=2)
	# searchParams = {'C':np.linspace(0.1,4,20)}
	# clf = grid_search.GridSearchCV(svm.SVC(cache_size=2000), searchParams)
	# # clf = svm.SVC()

	# clf.fit(X_train, y_train)

	# print clf.score(X_test, y_test)

	# gridScores =  np.reshape([x[1] for x in clf.grid_scores_], (clf.param_grid['nu'].size, clf.param_grid['gamma'].size))",analysis/SSRO.py,matthewware/PyQLab,1
"    # make predictions
    expected = y
    predicted = model.predict(X)
    # summarize the fit of the model
    print(metrics.classification_report(expected, predicted))
    print(metrics.confusion_matrix(expected, predicted))


def svm(X, y):
    # fit a SVM model to the data
    model = SVC()
    model.fit(X, y)
    print(model)
    # make predictions
    expected = y
    predicted = model.predict(X)
    # summarize the fit of the model
    print(metrics.classification_report(expected, predicted))
    print(metrics.confusion_matrix(expected, predicted))
",ml/sklearn_test.py,SquirrelMajik/GRec,1
"            self.assertTrue(os.path.isfile(file_name))

    def test_inspect_instance_returns_json_dict(self):
        metrics = self.clipper_inst.inspect_instance()
        self.assertEqual(type(metrics), dict)
        self.assertGreaterEqual(len(metrics), 1)

    def test_model_deploys_successfully(self):
        # Initialize a support vector classifier 
        # that will be deployed to a no-op container
        model_data = svm.SVC()
        container_name = ""clipper/noop-container""
        input_type = ""doubles""
        result = self.clipper_inst.deploy_model(
            self.deploy_model_name, self.deploy_model_version, model_data,
            container_name, input_type)
        self.assertTrue(result)
        model_info = self.clipper_inst.get_model_info(
            self.deploy_model_name, self.deploy_model_version)
        self.assertIsNotNone(model_info)",integration-tests/clipper_manager_tests.py,dcrankshaw/clipper,1
"
    def train(self):
        x = self.X
        y = self.Y

        if not len(x):
            #TODO: raise and process an exception
            return

        x, y = map(np.array, (x, y))
        clf = svm.SVC(probability=True)
        clf.fit(x, y)
        self.classifier = pickle.dumps(clf)
        self.last_training_set_length = len(self.X)
        self.save()


    def is_trained_recently(self):
        return self.additions_since_last_training() == 0
",recognition/models.py,alexgorin/pyclassif,1
"default_scorers = {
    'accuracy': metrics.accuracy_score
}

default_score_aggreg = [
    np.mean,
    np.min
]

default_classifiers = [
    svm.LinearSVC(random_state=0),
    svm.SVC(random_state=0),
    linear_model.LogisticRegression(),
    tree.DecisionTreeClassifier(),
    naive_bayes.BernoulliNB(),
    naive_bayes.MultinomialNB(),
    naive_bayes.GaussianNB(),
    linear_model.SGDClassifier(),
    linear_model.RidgeClassifier(),
    ensemble.RandomForestClassifier(n_estimators=10)",ml/testing_multiple_classifiers.py,thorwhalen/ut,1
"        words = p.get_words(filter=lambda x: x[""num""], flatten=True)
        
        return X, words


    def model(self, C=1.0):
        # clf = Pipeline([
        #                 ('feature_selection', RandomizedLogisticRegression()),
        #                 ('classification', SVC(probability=True))
        #                ])
        # clf = SVC(C=C, kernel='linear', probability=True)
        clf = LogisticRegression(C=C, penalty=""l1"")
        
        
        return clf




    def predict_population_text(self, text, clf):",bilearn_hybrid.py,ijmarshall/cochrane-nlp,1
"test_dat = []
for line in test_data:
    test_dat.append((line))

# end

##names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
##         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""]
##classifiers = [
##    KNeighborsClassifier(3),
##    SVC(kernel=""linear"", C=0.025),
##    SVC(gamma=2, C=1),
##    DecisionTreeClassifier(max_depth = 5),
##    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
##    AdaBoostClassifier(),
##    NaiveBayesClassifier()]


def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)",ml/mergeallof5.py,john136/exercises,1
"def plot_curve(fpr, tpr, title):
    plt.plot(fpr, tpr)
    plt.plot(fpr, fpr)
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title(title)
    plt.show()


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_6.py,remigius42/code_camp_2017_machine_learning,1
"        r = run(icp, 0.1, CrossSampler(Table('iris'), 4))
        self.assertGreater(r.accuracy(), 0.85)
        self.assertGreater(r.singleton_criterion(), 0.8)

    def test_SVM(self):
        iris = Table('iris')
        tab = Table(iris.X[50:], iris.Y[50:]-1)  # versicolor, virginica
        # clear cases
        train, test = get_instance(tab, 30)
        train, calibrate = next(RandomSampler(train, 2, 1))
        icp = InductiveClassifier(SVMDistance(skl_svm.SVC()), train, calibrate)
        pred = icp(test.x, 0.1)
        self.assertEqual(pred, ['v1'])
        train, test = get_instance(tab, 85)
        train, calibrate = next(RandomSampler(train, 2, 1))
        icp = InductiveClassifier(SVMDistance(skl_svm.SVC()), train, calibrate)
        pred = icp(test.x, 0.1)
        self.assertEqual(pred, ['v2'])
        # border case
        train, test = get_instance(tab, 27)",cp/tests.py,tojojames/MVCP-Orange3,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value",alignak/external_command.py,ddurieux/alignak,1
"                    acc.append(float(cm[i, i])/float(total[i]))


        print ""score : "" + str(score)
        # print metrics.classification_report(y_true, y_pred)
        print ""score norm: "", sum(acc)/float(len(acc))
        print ""----------------------------""

        '''
        yVal = data['Yval'].T.reshape(data['Yval'].shape[1])
        clf = sklearn.svm.SVC(kernel=""linear"", C=1)

        clf.fit(data['Xtrain'], ytrain)
        predict = clf.predict(data['Xval'])

        yVal = data['Yval'].T.reshape(data['Yval'].shape[1])
        print ""Acuracia: "", sklearn.metrics.accuracy_score(yVal, predict)

        cm = confusion_matrix(yVal, predict)
        total = numpy.sum(cm, axis=1)",svm.py,alan-mnix/MLFinalProject,1
"""""""
class sklearn.ensemble.ExtraTreesClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)
""""""
bagging_clf = BaggingClassifier(KNeighborsClassifier(),n_estimators=20,max_samples=0.5, max_features=0.5)
""""""
class sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)
""""""

clf1 = DecisionTreeClassifier(max_depth=4)
clf2 = KNeighborsClassifier(n_neighbors=7)
clf3 = SVC(kernel='rbf', probability=True)
voting_clf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[1,1,5])

clf1 = clf1.fit(X,y)
clf2 = clf2.fit(X,y)
clf3 = clf3.fit(X,y)
voting_clf = voting_clf.fit(X_train_pca, y_train)

boosting_clf = GradientBoostingClassifier(n_estimators=350, learning_rate=0.8,max_depth=3, random_state=0)
""""""",face_recognition_other_ensemble.py,zhangxd12/Lfw_face_recognition_svm_ensemble,1
"
    print(""cross-validation..."")

    cpu_count = 30
    max_learner_count = 40
    fold_count = 100
    rat_scores = dict()
    all_scores = defaultdict(list)
    cvs = cv.StratifiedShuffleSplit(y, n_iter = fold_count, test_size = 0.2)
    
    machine = svm.NuSVC(nu=0.25,
                        kernel='linear',
                        verbose=False,
                        probability=False)
    scores = cv.cross_val_score(
        machine, tmpX, y,
        cv = cvs,
        scoring = 'roc_auc',
        n_jobs = cpu_count,
        verbose=1)",read_data.py,adrinjalali/Network-Classifier,1
"fit_data = instance_a[0] + instance_n[0]
fit_classes = instance_a[1] + instance_n[1]

print(""Training the model...."")

##################################################
##############Train the Support Vector############
######################Machine#####################
##################################################

clf = svm.SVC()
clf.fit(fit_data,fit_classes)

print(""Model has been trained, building test dataset..."")

##################################################
#############Create the validation data###########
##################################################
##################################################
",format_py/n_gram_single.py,doylew/detectionsc,1
"
""""""
SVMの最適なパラメータCを
Cross-validationで求める
""""""

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')

# パラメータCの範囲
C_s = np.logspace(-10, 0, 10)

scores = list()
scores_std = list()

for C in C_s:
    svc.C = C",sklearn/plot_cv_digits.py,sylvan5/PRML,1
"        labels.append(int(category))
      else: 
        test_input_values.append([float(data[""y""])])
        test_labels.append(int(category))
      
      count += 1

  X = np.array(input_values)
  y = np.array(labels)
  
  clf = svm.LinearSVC()
  clf.fit(X, y)

  X_test = np.array(test_input_values)
  y_test = np.array(test_labels)  

  score = clf.score(X_test, y_test) 
  
  return clf, score
",brainsquared/tests/htm/scikit_classifier_trainer.py,CloudbrainLabs/htm-challenge,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.NuSVC(gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='php').export()
print(output)

""""""
<?php

class Brain {",examples/classifier/NuSVC/php/basics.py,nok/sklearn-porter,1
"	from sklearn.metrics import classification_report
	from sklearn.metrics import precision_score
	from sklearn.metrics import recall_score
	from sklearn.svm import SVC
	from sklearn.kernel_approximation import AdditiveChi2Sampler

	chi = AdditiveChi2Sampler()
	chi.fit(hogsH, labels)
	X = chi.fit_transform(hogsH, labels)

	# clf = svm.SVC(kernel='rbf', C=100)
	# clf.fit(X, np.array(labels))
	# print ""Training accuracy: %f""%(clf.score(X, labels)*100.)

	scores = [('precision', precision_score),
	    	('recall', recall_score),]
	for score_name, score_func in scores:

		X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=0)
		tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]},",pyKinectTools/scripts/main_MSR-Dataset.py,colincsl/pyKinectTools,1
"
class LinearSVCHipotesys(Hypothesis):

	def valid_parameters(self, c, loss, penalty, tol, fit_intercept, intercept_scaling):
		if penalty == 'l1' and loss == 'l1':
			return False
		else:
			return True

	def create_hipothesys(self, c, loss, penalty, tol, fit_intercept, intercept_scaling):
		hipotesys = LinearSVC(C=c, loss=loss, penalty=penalty, tol=tol, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, dual=False, verbose=1)
		return hipotesys


class LogisticRegressionHipotesys(Hypothesis):

	def valid_parameters(self, c, loss, penalty, tol, fit_intercept, intercept_scaling):
		if penalty == 'l1' and loss == 'l1':
			return False
		else:",src/titanicDirect.py,rogeralmeida/kaggle-titanic-python,1
"    documents = documents[:-ntesting]

    # SAVE FOR NOW
    print(""Saving Testing Docs for Later..."")
    _pickle.dump(testing, open(""testing-data.p"", 'wb'))

    X, y = documents2feature_vectors(documents)
    Xtest, ytest = documents2feature_vectors(testing)

    print(""Fitting..."")
    clf = svm.SVC(C=0.02, kernel='linear', probability=True)
    clf.fit(X, y)

    # Test
    y_pred = clf.predict(Xtest)
    print(classification_report(ytest, y_pred))

    return clf

",diplomacy/src/external/polite/scripts/train_model.py,MaxStrange/nlp,1
"
def test2():
    from sklearn.svm import SVC
    from sklearn.datasets import load_digits
    from sklmrmr import MRMR

    digits = load_digits()
    X = digits.images.reshape((len(digits.images), -1)).astype(int)
    y = digits.target

    svc = SVC(kernel='linear', C=1)
    mrmr = MRMR(estimator=svc, n_features_to_select=5)
    mrmr.fit(X, y)
    ranking = mrmr.ranking_

    print(ranking)

    return 0

if __name__ == '__main__':",sklmrmr/test.py,veg/sklmrmr,1
"
    def testGenerateRBFGS(self):
        generate = SvcGS.generate()

        learner = generate()
        learner.learnModel(self.X, self.y)
        learner.predict(self.X)
        
    def testSetWeight(self):
        #Try weight = 0 and weight = 1
        svc = SVC()
        svc.setWeight(0.0)
        svc.learnModel(self.X, self.y)

        predY = svc.predict(self.X)
        self.assertTrue((predY == numpy.zeros(predY.shape[0])).all())

        svc.setWeight(1.0)
        svc.learnModel(self.X, self.y)
        predY = svc.predict(self.X)",sandbox/ranking/leafrank/test/SVCTest.py,charanpald/sandbox,1
"    >>> from sklearn.svm import SVC
    >>> from epac import Methods
    >>> from epac.workflow.splitters import CVBestSearchRefitParallel
    >>> X, y = datasets.make_classification(n_samples=12,
    ...                                     n_features=10,
    ...                                     n_informative=2,
    ...                                     random_state=1)
    >>> n_folds_nested = 2
    >>> C_values = [.1, 0.5, 1, 2, 5]
    >>> kernels = [""linear"", ""rbf""]
    >>> methods = Methods(*[SVC(C=C, kernel=kernel)
    ...     for C in C_values for kernel in kernels])
    >>> wf = CVBestSearchRefitParallel(methods, n_folds=n_folds_nested)
    >>> wf.run(X=X, y=y)
    [[{'y/test/pred': array([0, 0, 1, 0, 0, 0]), 'y/train/pred': array([0, 0, 0, 0, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}, {'y/test/pred': array([1, 1, 1, 0, 1, 1]), 'y/train/pred': array([0, 0, 1, 1, 0, 1]), 'y/test/true': array([1, 0, 0, 1, 0, 1])}], [{'y/test/pred': array([0, 1, 1, 0, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 1, 1, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 0, 0, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 1, 1, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 0, 0, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 1, 1, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 0, 0, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 1, 0, 1, 0]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 0, 0, 1, 1]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}, {'y/test/pred': array([0, 1, 1, 0, 1, 0]), 'y/train/pred': array([1, 0, 0, 1, 0, 1]), 'y/test/true': array([0, 0, 1, 1, 0, 1])}]]
    >>> wf.reduce()
    ResultSet(
    [{'key': CVBestSearchRefitParallel, 'best_params': [{'kernel': 'rbf', 'C': 0.1, 'name': 'SVC'}], 'y/true': [1 0 0 1 0 0 1 0 1 1 0 1], 'y/pred': [1 0 0 1 0 0 1 0 1 1 0 1]}])

    """"""",epac/workflow/splitters.py,neurospin/pylearn-epac,1
"
    pipeline = Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler())])
    scaler = pipeline.fit(X_train)
    rescaledX = scaler.transform(X_train)
    
    #c_values = [0.1, 1.0, 100.0, 10000.0, 100000.0]
    c_values = [10000.0, 100000.0]
    kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']
    param_grid = dict(C=c_values, kernel=kernel_values)
    
    model = SVC()
    
    kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
    grid = GridSearchCV(n_jobs=N_JOBS, verbose=10, estimator=model, param_grid=param_grid, scoring=SCORING, cv=kfold)
    
    grid_result = grid.fit(rescaledX, Y_train)
    print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))    
        
    best_idx = grid_result.best_index_
",lib/eda4.py,FabricioMatos/ifes-dropout-machine-learning,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_17_2015.py,magic2du/contact_matrix,1
"    #               learning parameters found using the cross validation set.
    #               You can use model.predict to predict the labels on the cross
    #               validation set. 
    test = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]
    max_score = 0
    for i in xrange(8):
        for j in xrange(8):
            tC = test[i]
            tSigma = test[j]

            model = svm.SVC(C=tC, kernel='rbf', gamma=1.0/tSigma, max_iter=200)
            model.fit(X, y)
            score = model.score(Xval, yval)
            print(score)
            if score > max_score:
                max_score = score
                C = tC
                sigma = tSigma
    # ===========================================
    return C, sigma",solutions/ex6/ex6.py,cameronlai/ml-class-python,1
"ShiftedTarget = numpy.repeat(trainingy,25)

SmallWriting = ShrinkWriting(trainingx[0])
for i in range(1,trainingx.shape[0]):
    SmallWriting = numpy.vstack((SmallWriting,ShrinkWriting(trainingx[i])))

SmallTestx = ShrinkWriting(testx[0])
for i in range(1,testx.shape[0]):
    SmallTestx = numpy.vstack((SmallTestx,ShrinkWriting(testx[i])))

clf = svm.SVC()
clf.fit(trainingx, trainingy)
clf.fit(ShiftedWriting, ShiftedTarget)
clf.fit(SmallWriting, trainingy)
predicty = clf.predict(testx)
predicty = clf.predict(SmallTestx)
print(sum(predicty - testy != 0)/testy.shape[0])
for i in range(0,testy.shape[0]):
    if(predicty[i] != testy[i]):
        print(i,predicty[i],testy[i])",SVM_number.py,ylqk9/SVM_Digits_recog,1
"	
	#############
	# Train SVM #
	#############
	if (not exists(conf.modelPath)) | OVERWRITE:
		if VERBOSE: print (str(datetime.now()) + ' training liblinear svm')
		if VERBOSE == 'SVM':
			verbose = True
		else:
			verbose = False
		clf = svm.LinearSVC(C=conf.svm.C)
		if VERBOSE: print (clf)

		clf.fit(train_data, all_images_class_labels[selTrain])
		with open(conf.modelPath, 'wb') as fp:
			dump(clf, fp)
	else:
		if VERBOSE: print (""loading old SVM model"")
		with open(conf.modelPath, 'rb') as fp:
			clf = load(fp)",birdid_retrain.py,lbarnett/BirdID,1
"DATA_PATH = 'saved_models'


# class L1LinearSVC(LinearSVC):
#     # this is needed as it is not in the piclked data
#     ''' This is how it is done in the algo-tester:
#     http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html'''  # noqa
#     def fit(self, X, y):
#         # The smaller C, the stronger the regularization.
#         # The more regularization, the more sparsity.
#         self.transformer_ = LinearSVC(penalty=""l1"",
#                                       dual=False, tol=1e-3)
#         X = self.transformer_.fit_transform(X, y)
#         return LinearSVC.fit(self, X, y)
#
#     def predict(self, X):
#         X = self.transformer_.transform(X)
#         return LinearSVC.predict(self, X)

",sortekat.py,eiriks/sortekat,1
"

print(""Loading negative data"")
for feat_path in glob(path.join(cfg.neg_fd_path, '*.pkl')):
    fd = joblib.load(feat_path)
    X.append(fd)
    y.append(0)

# Initalize classifier as linear SVM and train on data
print(""Training classifier"")
clf = svm.LinearSVC()
clf.fit(X, y)

# Serialize model to disk
joblib.dump(clf, cfg.model_path)
print(""Classifier saved in {}"".format(cfg.model_path))",car_detector/train.py,CheriPai/car-detector,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_15_2014_server.py,magic2du/contact_matrix,1
"#SVM Classifier

from sklearn import svm

#train model
def train(features,labels,g=""auto"",c=1,k=""linear"",coef0=0,degree=2):
    #define classifier
    if k==""linear"":
        model = svm.LinearSVC(C=c,class_weight=""balanced"")
    elif k==""poly"":
        model=svm.SVC(C=c,kernel=k,degree=degree,coef0=coef0)
    elif k==""rbf"":
        model=svm.SVC(C=c,kernel=k,gamma=g,class_weight=""balanced"",cache_size=1000)

    #fit data
    model.fit(features,labels)

    return model

#predicts labels",classifiers/SVM.py,nlpaueb/aueb.twitter.sentiment,1
"        X_train_vect_red = DepickleMe('svm_train_input.pkl')
        X_test_vect_red = DepickleMe('svm_test_input.pkl')
        
        
    
    
    # prepare svm for content engine
    if not os.path.exists('./pickles/cb_linsvm.pkl'):
        print 'training  LinearSVC'
              
        clf = LinearSVC(C=5.0)          
        clf.fit(X_train_vect_red, y_train)
        PickleMe(clf, 'cb_linsvm.pkl')
    else:
        clf = DepickleMe('cb_linsvm.pkl')
     
    
    
    """"""
    ",RecoDev-Prototype/Evaluation2/algorithm1.py,amirhmoin/recodev,1
"from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from src.multi_class import input_preproc
from src.multi_class import calculate_metrics


def runClassifier(X_train, X_test, y_train, y_test):
    # print y_train

    predictions = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\
        .fit(X_train, y_train).predict(X_test)


    # Metrics...
    precision, recall, f1, accuracy = calculate_metrics.calculateMetrics(predictions, y_test)
    print( ""intermediary results (precision | recall | F1 Score | Accuracy):"" )
    print( ""%.6f %.6f %.6f %.6f"" % (precision, recall, f1, accuracy) )
    return precision, recall, f1, accuracy
",src/multi_class/linear_svc.py,cassinius/right-to-forget-data,1
"            
            if vocab.has_key(term):
                print term, vocab[term], len(x)
                #i = vocab_list.index(term)
                x[vocab_list.index(term)] += 1
        X.append(x)
    print len(X[0]), ""# Features""
    classes = [item[2] for item in training_data]
    
    
    linsvm = svm.LinearSVC()
    Cs = range(1, 20)
    ssvm = GridSearchCV(estimator = linsvm, param_grid = dict(C=Cs), cv = 10)
    print len(X), len(X[0]), len(classes)
    ssvm.fit(X, classes)
    svmm = ssvm.best_estimator_
    
    f = open('svmm_model.pkl', 'w')
    f.write(pickle.dumps(svmm))
    f.close()",svm2/svm_code.py,ezeissler90/CSI_431,1
"                X_train, X_test, y_train, y_test = \
                    train_test_split(X[monkey], y[monkey], test_size=0.1)

                steps = np.linspace(min_nsamples,
                                    X_train.shape[0],
                                    n_steps).astype(int)
                n_train = steps[step]

                X_train, y_train = stratified_sample(X_train, y_train, n_train)
                avg_nsamples += X_train.shape[0]
                clf = GridSearchCV(SVC(),
                                   param_grid,
                                   cv=StratifiedKFold(y_train,
                                                      n_folds=n_folds_gridsearch),
                                   score_func=metrics.accuracy_score,
                                   verbose=0 if verbose else 0, n_jobs=-1)
                clf.fit(X_train, y_train)
                if y_true is None:
                    y_true = y_test
                    y_pred = clf.predict(X_test)",nsamples_performance_supervised.py,bootphon/monkey_business,1
"from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
import urllib.request, json
from sklearn import svm
from sklearn.model_selection import train_test_split
import csv
from . import trans_hist
import operator

clf = svm.SVC(gamma = 0.001, C = 100., probability = True)
data = []
target = []

test_data = []
test_target = []

prods = ['LB','AR','MW','ML','UX']

print(""generating random test data..."")",api/deals/views.py,MoreIsTheNewLess/backend,1
"#test
from time import time
def svm_predict(training_samples, training_labels, test_samples, test_lables, kernel = ""rbf"" , C = 1.0):
	from sklearn.svm import SVC

	clf = SVC(kernel = kernel, C =C)

	t0 = time()
	clf.fit(training_samples,training_labels)
	training_time = round(time()-t0, 3)

	t0 = time()
	pred = clf.predict(test_samples)
	test_time = round(time()-t0, 3)
",src/test.py,amogh3892/Audio-classification-using-Bag-of-Frames-approach,1
"	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(100):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_5_periods{1}D_sample_{0}.txt"".format(i,dim),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_5_periods{1}D_sample_1{0}.txt"".format(str(i).zfill(2),dim)))

	#originally had svm c=496.6 and gamma 0.00767
        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        clf = SVC(C=16.91,gamma=0.00928,probability=True, cache_size=7000)
        args=[str(dim)+ ""Dsin1diff_5_and_5_noCPV_optimised_svm"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),200,6]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/svm_sin/svm_Sin_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"tfv.fit(X_all)
X_all = tfv.transform(X_all)

X = X_all[:lentrain]
X_test = X_all[lentrain:]

## Initialize the standard scaler 
#scl = StandardScaler()

# We will use SVM here..
#svm_model = SVC()

# Create the pipeline 
#clf = pipeline.Pipeline([('svm', svm_model)])

# Create a parameter grid to search for best parameters for everything in the pipeline
#param_grid = {'svm__C': [0.1,1,3],'svm__kernel': ['rbf','linear','poly'],'svm__gamma':[0,0.01,0.1]}

# Initialize Grid Search Model
#model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, ",Bag_of_words/linear.py,tanayz/Kaggle,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = fisher_score.fisher_score(X[train], y[train])

        # rank features in descending order according to score
        idx = fisher_score.feature_ranking(score)
",skfeature/example/test_fisher_score.py,jundongl/scikit-feature,1
"  if not isinstance(test, pd.core.frame.DataFrame):
    test = csv2DF(test, as_mtx=False, toBin=True)

  if smoteit:
    train = SMOTE(train, resample=True)
    # except: set_trace()
  if not tunings:
    if regress:
      clf = SVR()
    else:
      clf = SVC()
  else:
    if regress:
      clf = SVR()
    else:
      clf = SVC()

  features = train.columns[:-1]
  klass = train[train.columns[-1]]
  # set_trace()",src/tools/oracle.py,ai-se/XTREE,1
"  xs = dataset[:][:,1:]

  logging.basicConfig(filename='predict.log',level=logging.DEBUG, format='')

  plt.clf()

  # train model
  plt.subplot(2,2,1)
  plt.title('No weight, C=1')

  clf = svm.SVC(kernel='linear', C=1, probability=True)
  predict(clf, xs, ys)

  plt.subplot(2,2,2)
  plt.title('Weight 1:3, C=1')

  clf = svm.SVC(kernel='linear', C=1, probability=True, class_weight={1: 3})
  predict(clf, xs, ys)

  plt.subplot(2,2,3)",forestfires/predict_linear.py,Josephu/svm,1
"# INFO: # In the future, you might try setting the nan values to the
# mean value of that column, the mean should only be calculated for
# the specific class rather than across all classes, now that you
# have the labels
y = X['wheat_type'].copy()
y = y.map({'canadian': 0, 'kama': 1, 'rosa': 2})
X.drop(labels=['wheat_type'], axis=1, inplace=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)

svc = SVC(kernel=kernel, C=C)

knn = KNeighborsClassifier(n_neighbors=5)

tree = tree.DecisionTreeClassifier(max_depth=2, random_state=2)

benchmark(knn, X_train, X_test, y_train, y_test, 'KNeighbors')
drawPlots(knn, X_train, X_test, y_train, y_test, 'KNeighbors')

benchmark(svc, X_train, X_test, y_train, y_test, 'SVC')",Module6/assignment4.py,Wittlich/DAT210x-Python,1
"Len = np.shape(trainData)[0]
Size = np.size(trainData)

print ""Training data shape is : "", trainData.shape
Width = Len/50000
print Len
print Width*50000
trainData = trainData.reshape((50000, Width))

# Training SVM
SVM = svm.LinearSVC(C=1)
# C=100, kernel='rbf')
print ""Training the SVM""
trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))
#print trainData
SVM.fit(trainData, trainLabel)
print(""Training Score = %f "" % float(100 * SVM.score(trainData, trainLabel)))
#print(""Training Accuracy = %f"" % (SVM.score(trainData, trainLabel) * 100))
eff = {}
eff['train'] = SVM.score(trainData, trainLabel) * 100",conv_cifar/conv_destin.py,tejaskhot/deep-learning,1
"from sklearn import svm
from matplotlib import pyplot as plt
import numpy as np


def fit_svm_and_get_decision_for_requiered_data(X_train, Y_train, X_test,
                                                decision_function_shape=""None"",
                                                kernel=""linear"",
                                                minimum_training_svm_error=0.001):
    clf = svm.SVC(decision_function_shape=decision_function_shape,
                  kernel=kernel)
    clf.fit(X_train, Y_train)

    # Testing time
    scores_test = clf.decision_function(X_test)
    scores_train = clf.decision_function(X_train)

    return scores_train, scores_test
",lib/svm_utils.py,juanka1331/VAN-applied-to-Nifti-images,1
"    best = 0.0
    best_Output = []
    for j in [10**(x) for x in xrange(-3,-2,1)]:
        
        X, y = train_data[:,1::], train_data[:,0]
        x1, y1 = test_data[:,1::], test_data[:,0]
        
        # Set regularization parameter
        for C in range(10,11,1):
            # turn down tolerance for short training time
            #cls = svm.SVC(kernel='poly',degree=3).fit(X,y)
            cls = GradientBoostingClassifier(n_estimators=n_est,max_depth=m_d).fit(X,y)
            #cls = DecisionTreeClassifier().fit(X,y)
            #cls = LogisticRegression(C=C, penalty='l1', tol=j).fit(X, y)
            #cls = LogisticRegression(C=C, penalty='l2', tol=j).fit(X, y) 

            val1 = cls.predict(x1)
            #val1 = cls.predict(x1)
            val2 = val1 #cls.predict(x1)
                        ",titanic/l1_penalty.py,cianmj/kaggle,1
"titanic_test = titanic_test.drop('Survived', 1)

samples = len(titanic)
titanic_s = all_data[all_data['Survived'] >= 0]
titanic = titanic.sample(n=samples)


print(""***************************************************************************"")
#####################################################################################

clf = SVC(kernel='rbf', random_state=42)

samples = len(titanic)
shuffled_data = titanic.sample(n=samples)
x = shuffled_data[features]
y = shuffled_data[""Survived""]

v = titanic_test[features]

c = clf.fit(x, y)",SVM.py,timestocome/titanic-windows,1
"from sklearn.naive_bayes import GaussianNB
naivebayes_model = GaussianNB()
naivebayes_model.fit(X_cropped, y_cropped)
y_validation_predicted = naivebayes_model.predict(X_validation)
print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)


# Start SVM Classification
print ""Performing SVM Classification:""
from sklearn.svm import SVC
svm_model = SVC(kernel='rbf' ,probability=True, max_iter=1000)
svm_model.fit(X_cropped, y_cropped)
y_train_predicted = svm_model.predict(X_train)
print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# plt.show()

y_validation_predicted = svm_model.predict(X_validation)
print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
",Code/Machine_Learning_Algos/training_fullset.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"            model = GradientBoostingClassifier(n_estimators=C)
        elif self.modeltype == 'LOG':
            model = LogisticRegression(C=C)
        elif self.modeltype == 'NN':
            model = MLPClassifier(hidden_layer_sizes=(125,),
                                  activation='relu', alpha=C,
                                  max_iter=200, solver='lbfgs')
        elif self.modeltype == 'RF':
            model = RandomForestClassifier(n_estimators=C)
        elif self.modeltype == 'SVM':
            model = SVC(C=C, decision_function_shape='ovr',
                        kernel='poly')

        self.model = model


if __name__ == '__main__':

    """"""load digit images and labels""""""
    X, y = loaddata('digits.hdf5')",support.py,npaulson/digit_classification,1
"    Xrow = asRowMatrix(features)
    # Split the dataset in two equal parts
    X_train, X_test, y_train, y_test = train_test_split(Xrow, y, test_size=0.5, random_state=0)
    # Define the Classifier:
    scores = ['precision', 'recall']
    # Evaluate the Model:
    for score in scores:
        print(""# Tuning hyper-parameters for %s"" % score)
        print()
    
        clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,
                        scoring='%s_macro' % score)
        clf.fit(X_train, y_train)
    
        print(""Best parameters set found on development set:"")
        print()
        print(clf.best_params_)
        print()
        print(""Grid scores on development set:"")
        print()",py/facerec/svm.py,dashmoment/facerecognition,1
"
try:
    from sklearn import __version__ as sklearnVersion
    svcTakesScaleC = extractVersion(sklearnVersion) < 11
except ImportError, VersionError:
    logger.warning(""Could not import dependency 'sklearn' for SVMs"")
    havesklearn = False
else:
    havesklearn = True

def SVC(*args, **kwargs):
    from sklearn.svm import SVC as _SVC
    # old scikit-learn versions take scale_C as a parameter
    # new ones don't and default to True
    if not svcTakesScaleC and ""scale_C"" in kwargs:
        del kwargs[""scale_C""]
    print(kwargs)
    return _SVC(*args, **kwargs)

_defaultBinSize = 30",lazyflow/operators/opDetectMissingData.py,stuarteberg/lazyflow,1
"normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
X_norm = normalizer.transform(X)

def optimizeAdaBoostSVM(X_norm, y, kFolds=10):
    # grid search 多参数优化
    parameters = {
        'base_estimator__gamma': np.logspace(0, 3, 3),
        'base_estimator__C': np.logspace(0, 3, 3),
        'n_estimators': np.linspace(1, 100, 3, dtype=np.dtype(np.int16)),
    }
    svm = SVC(probability=True, kernel='rbf')
    clf = AdaBoostClassifier(base_estimator=svm)

    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['base_estimator__gamma'], gs.best_params_['base_estimator__C'], gs.best_params_['n_estimators'], gs.best_score_


alpha, C, n_estimators, score = optimizeAdaBoostSVM(X_norm, y, kFolds=10)",finance/AdaboostSVMTest.py,Ernestyj/PyStudy,1
"        :param documents: a list of document to classify.
        """"""
        pass


class SVMClassifier(BaseClassifier):

    def fit(self, documents):
        BaseClassifier.fit(self, documents)

        self.sk_model = SVC().fit(self.text_vectors.values(), self.labels)

    def predict(self, documents):
        unique_word_save = self.vectorizer.unique_words
        self.vectorizer.fit(documents)
        self.vectorizer.unique_words = unique_word_save

        predict_vectors = self.vectorizer.transform()

        return dict(zip(documents,self.sk_model.predict(predict_vectors.values())))",kadot/classifiers.py,the-new-sky/Kadot,1
"        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., n_jobs=1,",sklearn/grid_search.py,smartscheduling/scikit-learn-categorical-tree,1
"

#spot checking of machine learning algorithms begins

""""""
#all models are stored in list 'models'
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('DTC', DecisionTreeClassifier(max_depth = 6, min_samples_split = 4)))
models.append(('SVC', SVC()))
models.append(('RFC', RandomForestClassifier(n_estimators=100)))
models.append(('KNC', KNeighborsClassifier(n_neighbors = 3)))
models.append(('MLP', MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(10, 5), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)))",titanic_predictor.py,Euler10/Titanic-Machine-Learning-from-Disaster,1
"#checking for any missing data.
#total = data.isnull().sum().sort_values(ascending=False)
#percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)
#missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
#missing_data.head()

#Splitting data into training and cross-validation sets.
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.67, random_state=21)

#Spot-checking various multi-label algorithms
linear_svc = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, Y_train)
pred_linear_svc = linear_svc.predict(X_test)
linear_acc = accuracy_score(Y_test, pred_linear_svc)

KNN = OneVsRestClassifier(KNeighborsClassifier(n_neighbors = 3)).fit(X_train, Y_train)
pred_KNN = KNN.predict(X_test)
KNN_acc = accuracy_score(Y_test, pred_KNN)

Decision_tree = OneVsRestClassifier(DecisionTreeClassifier()).fit(X_train, Y_train)
pred_decision_tree = Decision_tree.predict(X_test)",Twitter_Hashtags/tweet_analysis.py,MichaelMKKang/Projects,1
"X = X[order]
y = y[order].astype(np.float)

X_train = X[:.9 * n_sample]
y_train = y[:.9 * n_sample]
X_test = X[.9 * n_sample:]
y_test = y[.9 * n_sample:]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.clf()
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)

    # Circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
",projects/scikit-learn-master/examples/exercises/plot_iris_exercise.py,DailyActie/Surrogate-Model,1
"    samples_ = int(sys.argv[2])

    pred = Predictor(lm.SGDClassifier())
    result_, score_ = pred.prediction(file_name_, samples_)
    print(file_name_, ""score:"", score_, ""上がるぞぉぉぉ"" if result_ == 1 else ""下がるぞぉぉぉ"")

    pred = Predictor(tree.DecisionTreeClassifier())
    result_, score_ = pred.prediction(file_name_, samples_)
    print(file_name_, ""score:"", score_, ""上がるぞぉぉぉ"" if result_ == 1 else ""下がるぞぉぉぉ"")

    pred = Predictor(svm.SVC())
    result_, score_ = pred.prediction(file_name_, samples_)
    print(file_name_, ""score:"", score_, ""上がるぞぉぉぉ"" if result_ == 1 else ""下がるぞぉぉぉ"")",prediction/Predictor.py,SakaiTakao/StockPrediction,1
"        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",scripts/document_classification_20newsgroups.py,giacbrd/ShallowLearn,1
"""""""
groups = [['AD', 'Normal'], ['AD', 'EMCI'], ['AD', 'LMCI'],
          ['LMCI', 'Normal'], ['EMCI', 'LMCI'], ['EMCI', 'Normal']]
score = np.zeros((nb_iter, len(groups)))
for gr in groups:
    g1_feat = corr_feat[idx[gr[0]][0]]
    g2_feat = corr_feat[idx[gr[1]][0]]
    x = np.concatenate((g1_feat, g2_feat), axis=0)
    y = np.ones(len(x))
    y[len(x) - len(g2_feat):] = 0
    svr = SVC(kernel='linear')
    param_grid = {'C': np.logspace(-3,3,7), 'kernel': ['linear']}
    estim = GridSearchCV(cv=None, estimator=svr,
                         param_grid=param_grid, n_jobs=1)
    sss = StratifiedShuffleSplit(y, n_iter=nb_iter, test_size=0.1)
    # 100 runs with randoms 90% / 10% : StratifiedShuffleSplit
    counter = 0
    for train, test in sss:
        Xtrain, Xtest = x[train], x[test]
        Ytrain, Ytest = y[train], y[test]",correlation_baseline_rs_fmri_adni.py,mrahim/adni_rs_fmri_analysis,1
"                    X_train, X_test = community_weighting(X_train, X_test, community_weights)
                else:
                    X_train = normalize(X_train, norm=""l2"")
                    X_test = normalize(X_test, norm=""l2"")

            ############################################################################################################
            # Train model
            ############################################################################################################
            # Train classifier.
            start_time = time.time()
            model = OneVsRestClassifier(svm.LinearSVC(C=C,
                                                      random_state=None,
                                                      dual=False,
                                                      fit_intercept=fit_intercept),
                                        n_jobs=thread_num)

            model.fit(X_train, y_train)
            hypothesis_training_time = time.time() - start_time
            print('Model fitting time: ', hypothesis_training_time)
",reveal_graph_embedding/experiments/utility.py,MKLab-ITI/reveal-graph-embedding,1
"
# Do feature selection/extraction(PCA)
#########################################
decomp = RandomizedPCA(n_components = 25)
train  = decomp.fit_transform(train)
#########################################


# Initialize Classifier(SVC)
#########################################
genderClassifier = SVC()
ageClassifier = SVC()
bothClassifier = SVC()

end = time.time()
#########################################

# genderClassifier.fit(train, genders)
# ageClassifier.fit(train, ages)
",image_classifier.py,chenzeyu/demographic_prediction,1
"
    classifierName = 'Support Vector Machine'
    C = np.logspace(-5.0, 5.0, num=10, endpoint=True, base=2)

    def train(self):

        tuned_parameters = [{'C': self.C}]

        print ('SVM Optimizing. This will take a while')
        start_time = time.time()
        clf = GridSearchCV(LinearSVC(), tuned_parameters,
                           n_jobs=self.threadCount, cv=5)

        clf.fit(self.Xtrain, self.ytrain)
        print('Done with Optimizing. it took ', time.time() -
              start_time, ' seconds')

        self.model = clf.best_estimator_",infodens/classifier/svc_linear.py,rrubino/B6-SFB1102,1
"
Len = np.shape(trainData)[0]
Size = np.size(trainData)

Width = Len/50000
print Len
print Width*50000
trainData = trainData.reshape((50000, Width))

# Training SVM
SVM = svm.LinearSVC(C=1)
# C=100, kernel='rbf')
print ""Training the SVM""
trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))
#print trainData
SVM.fit(trainData, trainLabel)
print(""Training Score = %f "" % float(100 * SVM.score(trainData, trainLabel)))
#print(""Training Accuracy = %f"" % (SVM.score(trainData, trainLabel) * 100))
eff = {}
eff['train'] = SVM.score(trainData, trainLabel) * 100",conv_cifar/scripts/backup.py,Tejas-Khot/deep-learning,1
"from sklearn import preprocessing
from sklearn import svm
from sklearn.externals import joblib
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.model_selection import StratifiedShuffleSplit

class_weight = {0: 0.3, 1: 0.7}
clf = svm.SVC(kernel='linear', C=1.0, decision_function_shape='ovr',
        class_weight=class_weight)

def load_csv():
    '''
    Transform tabular data set into NumPy arrays.
    '''
    df = pd.read_csv('training.csv', sep='\t')

    data = df.ix[:, 6:-1].as_matrix()",training/svm.py,jlonij/dac-training,1
"            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier
            estimator = GradientBoostingClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'linear-svm':
            from sklearn.svm import SVC
            estimator = SVC(probability=True,
                            random_state=self.random_state,
                            kernel='linear',
                            **self.kwargs)
        else:
            raise NotImplementedError

        # Create the different folds
        skf = StratifiedKFold(y, n_folds=self.cv, shuffle=False,
                              random_state=self.random_state)",imblearn/under_sampling/instance_hardness_threshold.py,dvro/imbalanced-learn,1
"        return self.prediction


class SvmSk(Svm):
    def __init__(self, xml_file):
        super().__init__(xml_file)

    def __build_model(self, xml_file):
        svm_params = self.__parse_xml(xml_file)
        if xml_file.get('type') == 'linear':
            self.core = SVMSK.LinearSVC(penalty=svm_params['penalty'], loss=svm_params['loss'], dual=svm_params['dual'],
                                        tol=svm_params['tol'], C=svm_params['C'], multi_class=svm_params['multi_class'],
                                        fit_intercept=svm_params['fit_intercept'],
                                        intercept_scaling=svm_params['intercept_scaling'],
                                        class_weight=svm_params['class_weight'], verbose=svm_params['verbose'],
                                        random_state=svm_params['random_state'], max_iter=svm_params['max_itr'])
        else:
            self.core = SVMSK.SVR(kernel=svm_params['kernel'], degree=svm_params['degree'], gamma=svm_params['gamma'],
                                  coef0=svm_params['coef0'], tol=svm_params['tol'], C=svm_params['C'],
                                  epsilon=svm_params['epsilon'], shrinking=svm_params['shrinking'],",Model/SVM/SVM.py,ADozois/Titanic_challenge,1
"
# How many of each Variable Type do we have
for x in range(1,4):
    print(""Class size: {} {}"".format(x,len(classes[classes == x])))

    # Figure out how many we need to train for accuracy

# Test size
N_test = 5000

clf = svm.LinearSVC()

# X_train, X_test, y_train, y_test = cross_validation.train_test_split (data_svm, classes, test_size=1./3.)
# print(""training set = {} {}"".format(  X_train.shape, y_train.shape ))
# print(""test size = {} {}"".format(X_test.shape, y_test.shape))

# clf.fit(X_train, y_train)
# pred_class = clf.predict(X_test)
# N_match = (pred_class == y_test).sum()
# print(""N_match = {}"".format(N_match))",final_program.py,ElMejorEquipoDeLaSerena/VariableStarsClassification,1
"    ## ================
    X, y = datasets.make_classification(n_samples=options.n_samples,
                                        n_features=options.n_features,
                                        n_informative=options.n_informative)

    ## 2) Build Workflow
    ## =================
    time_start = time.time()
    ## CV + Grid search of a pipeline with a nested grid search
    cls = Methods(*[Pipe(SelectKBest(k=k),
                         SVC(kernel=""linear"", C=C))
                    for C in C_values
                    for k in k_values])
    pipeline = CVBestSearchRefit(cls,
                                 n_folds=options.n_folds_nested,
                                 random_state=random_state)
    wf = Perms(CV(pipeline, n_folds=options.n_folds),
               n_perms=options.n_perms,
               permute=""y"",
               random_state=random_state)",examples/run_somaworkflow_no_gui.py,neurospin/pylearn-epac,1
"ploty = [-6, 6, 100]
plotx = [-6, 6, 100]

X, Y = make_spiral(n_arms=n_classes, noise=.4)

##############################################################################
parameters = {'kernel': ['rbf'],
              'C': [1, 10, 100, 1000, 10000, 100000],
              'gamma': [10 ** x for x in range(-5, 3)],
              'probability':[True]}
svr = svm.SVC()
clf = grid_search.GridSearchCV(svr, parameters)
clf.fit(X, Y.ravel())

svmp = probaproxy(clf.predict_proba)

plt.figure()
point_prob_plot(svmp, X, Y, plotx, ploty)
plt.title('RBF kernel SVM $(\gamma=%d, C=%f)$' % (clf.best_params_['gamma'], clf.best_params_['C']))
plt.savefig('spiral_svm.png')",examples/python/spiral_svm.py,classner/fertilized-forests,1
"### labels_train, and labels_test

### set the random_state to 0 and the test_size to 0.4 so
### we can exactly check your result




###############################################################

clf = SVC(kernel=""linear"", C=1.)
clf.fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################
def submitAcc():",ud421-projects/validation/studentCode.py,sinanh/udacity,1
"    save=(""model"", ""db"")
)

Experiment(
    project=""unit_tests"",
    name=""20_news_emb"",
    data=loader('train', categories=['comp.graphics', 'sci.space', 'alt.atheism']),
    lime_data=[dat[0] for dat in loader('test', emax=5)],
    pipeline=[
        Vectorizer(features=[WordEmbeddings(lang='nl')]),
        Pipe('clf', SVC(kernel='linear', probability=True)),
        Evaluator(scoring='f1_weighted', average='weighted',
                  lime_docs=[dat[0] for dat in loader('test', emax=5)])
    ],
    save=(""model"", ""db"")
)",examples/20news.py,cmry/omesa,1
"	train_data = []
	y = []
	for train_data_index in train_data_indices:
		(data, y_temp) = data_sets[train_data_index]
		train_data.extend(data)
		y.extend(y_temp)
	
	X_train = count_vect.transform(train_data)
	y_train = np.asarray(y)

	clf =svm.SVC()
	
	clf.fit(X_train, y_train)

	# Testing the model
	y_predicted = clf.predict(X_test)

	# Calculating precision, recall and f1-score for each class and get avg
	labels = np.asarray([-1, 1])
	p, r, f1, s = metrics.precision_recall_fscore_support(y_test, y_predicted,",OpinionEngine.py,J-A-S-A/PSA,1
"    
####################################################################
# Dalitz operaton
####################################################################

for i in range(100):
	comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.{0}.0.txt"".format(i), os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.2{0}.1.txt"".format(str(i).zfill(2))))
    
clf = tree.DecisionTreeClassifier('gini','best',46, 100, 1, 0.0, None)
#clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.95,n_estimators=440)
#clf = SVC(C=1.0,gamma=0.0955,probability=True, cache_size=7000)
args=[""dalitz_dt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
#For nn:
#args=[""dalitz_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),71,1]

classifier_eval_simplified.classifier_eval(0,0,args)

",Dalitz_simplified/evaluation_of_optimised_classifiers/dt_Dalitz/dt_Dalitz_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"        this GridSearchCV instance after fitting.

    verbose : integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., loss_func=..., n_jobs=1,",venv/lib/python2.7/site-packages/sklearn/grid_search.py,chaluemwut/fbserver,1
"            ind, c = contrains_color(lines[i])
            if ind is not None:
                d = np.fromstring(lines[i+1], dtype=int, sep="" "")
                D = np.append(D, np.array([d]), axis=0)
                y = np.append(y, ind)

    D = preprocessing.scale(D)
    D_train, D_test, y_train, y_test = cross_validation.train_test_split(
        D, y, test_size=0.2, random_state=42)

    svr = svm.SVC()
    param_grid = dict(gamma=np.logspace(-2, 10, 13), C=np.logspace(-9, 3, 13))
    clf = grid_search.GridSearchCV(svr, param_grid)
    clf.fit(D_train, y_train)

    predictions = clf.predict(D_test)
    cm = np.zeros((len(colors), len(colors)))
    for prediction, truth in zip(predictions, y_test):
        cm[truth, prediction] += 1
",wk6/color.py,Timvanz/uva_statistisch_redeneren,1
"        self.text_features = text_features
        if os.path.exists(""./models/""):
            print(""Prediction models exist. Loading them..."")
            self.load_models()
            print(""Prediction models loaded."")
        else:
            print(""Trained prediction models not found. Training them... (this might take a while)"")
            self.models[""Multinomial Nayve Bayes Classifier""] = SklearnClassifier(MultinomialNB())
            self.models[""Logistic Regression Classifier""] = SklearnClassifier(LogisticRegression())
            self.models[""K-Nearest-Neighbors Classifier""] = SklearnClassifier(KNeighborsClassifier())
            self.models[""Linear SVC Classifier""] = SklearnClassifier(LinearSVC())

            self.train_models()
            self.save_models()
            print(""Training done."")
    
    def predict(self, text_features, algorythm=""Vote Classifier""):
        return self.models[algorythm].classify(text_features)
   
    def train_models(self):",model.py,dgovedarska/who-wrote-me,1
"        if len(result['face']) != 1:
            next

        face = result['face'][0]
        aligned_filename = ""gabor_aligned/"" + os.path.basename(row[0])
        align_faces.CropFace(Image.open(row[0]), eye_left=(face[0],face[1]), eye_right=(face[2],face[1]), offset_pct=(0.08,0.08), dest_sz=(200,200)).save(aligned_filename)
        face_vector = gabor_filter.filter_face(aligned_filename,0, 0, 200, 200)
        X.append(face_vector)
        y.append(row[1])

    clf = svm.SVC()
    clf.fit(X,y)
    result = facedetect.detect_faces(""cohn-kanade-images/S005/001/S005_001_00000006.png"")
    face = result['face'][0]
    aligned_filename = ""gabor_aligned/"" + os.path.basename(row[0])
    align_faces.CropFace(Image.open(row[0]), eye_left=(face[0],face[1]), eye_right=(face[2],face[1]), offset_pct=(0.08,0.08), dest_sz=(200,200)).save(aligned_filename)
    face_vector = gabor_filter.filter_face(aligned_filename,0, 0, 200, 200)

    labels = clf.predict([face_vector])
    print (labels[0])",python/app.py,DiUS/Physiognomy,1
"
        ####################################################


class NuSVC(Sklearn):
    ####################################################
    def __init__(self, context, classifier_name):
        from sklearn.svm import NuSVC

        self.classifier = \
            NuSVC(
                nu=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""nu""],
                cache_size=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][
                    ""cache_size""],
                coef0=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""coef0""],
                degree=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""degree""],
                gamma=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""gamma""],
                kernel=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""kernel""],
                max_iter=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][""max_iter""],
                probability=context[""classifiers""][classifier_name][""learning_algorithm""][""parameters""][",mullpy/classifiers.py,enanablancaynumeros/mullpy,1
"
#Multi-layer perceptron
#MLP is sensitive to feature scaling on the data so lets standardize 
clf_MLP = make_pipeline(preprocessing.StandardScaler(), MLPClassifier(solver= 'lbfgs', alpha = 1e-5, hidden_layer_sizes=(5,2), random_state=1))
scores_MLP = cross_val_score(clf_MLP, X, Y, cv = nfolds)
print(""Multilayer Perceptron scores: "",scores_MLP)
print(""Accuracy: %0.2f (+/- %0.2f)"" %(scores_MLP.mean(), scores_MLP.std()*2))
myscores['MLP'] = scores_MLP.mean()

#Support Vector Machine
clf_SVM = svm.SVC()
scores_SVM = cross_val_score(clf_SVM, X,Y,cv = nfolds)
print(""SVM scores: "",scores_SVM)
print(""Accuracy: %0.2f (+/- %0.2f)"" %(scores_SVM.mean(), scores_SVM.std()*2))
myscores['SVM'] = scores_SVM.mean()
########

print('')
#Special Case: ME!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! (I'm a skinny guy)
",sklearn/classifiers1.py,gnublet/py_explorations,1
"    dis = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        dis[i][i] = 0
    for i in range(n_samples):
        for j in range(i + 1, n_samples):
            dis[i][j] = dis[j][i] = kn(X[i],X[j])
    return dis
def Lsvm_patatune(train_x,train_y):
    tuned_parameters = [
        {'kernel': ['precomputed'], 'C': [0.01, 0.1, 1, 10, 100, 1000]}]
    clf = GridSearchCV(SVC(C=1, probability=True), tuned_parameters, cv=5, n_jobs=1
                       )  # SVC(probability=True)#SVC(kernel=""linear"", probability=True)
    clf.fit(train_x, train_y)
    return clf.best_params_['C']

url = './MyData.csv'
dataframe = pandas.read_csv(url)#, header=None)
array = dataframe.values
X = array[:,1:]
Y = pandas.read_csv('./MyDatalabel.csv')",finalMe/AMKLrbf.py,hongliuuuu/Results_Dis,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_forward.decision_tree_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeature/example/test_decision_tree_forward.py,jundongl/scikit-feature,1
"def LDA(train_x, train_y, test_x, test_y):
    clf = LinearDiscriminantAnalysis()
    clf.fit(train_x,train_y)
    test_error = clf.score(test_x, test_y)
    test_auc = clf.predict_proba(test_x)
    return test_error, test_auc

def Lsvm_patatune(train_x,train_y):
    tuned_parameters = [
        {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000, 10000]}]
    clf = GridSearchCV(SVC(C=1, probability=True), tuned_parameters, cv=5, n_jobs=1
                       )  # SVC(probability=True)#SVC(kernel=""linear"", probability=True)
    clf.fit(train_x, train_y)
    return clf.best_params_['C']


def Lsvm(c,train_x, train_y, test_x, test_y):
    clf = SVC(kernel=""linear"", C=c, probability=True)
    clf.fit(train_x,train_y)
    test_error = clf.score(test_x, test_y)",final7/WeightedMVdis7.py,hongliuuuu/Results_Dis,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_local_test (u-PC's conflicted copy 2015-04-15).py,magic2du/contact_matrix,1
"    X_test = X[X.index >= start_test]
    y_train = y[y.index < start_test]
    y_test = y[y.index >= start_test]

    # Create prediction DataFrame
    pred = pd.DataFrame(index=y_test.index)
    pred[""Actual""] = y_test
    
    # Create and fit the three models    
    print ""Hit Rates:""
    models = [(""LR"", LogisticRegression()), (""LDA"", LDA()), (""SVM"", SVC()), (""SVM-RBF"", SVC(kernel='rbf'))]
    for m in models:
        fit_model(m[0], m[1], X_train, y_train, X_test, pred)",first_try.py,martianstudio/stockanalysis,1
"            trn_kernel_mat = np.delete(kernel_mat, tst_lst, axis=0)
            trn_kernel_mat = np.delete(trn_kernel_mat, tst_lst, axis=1)
            trn_lab = np.delete(labels, tst_lst)

            # Extract testing kernel entries (i.e., all test rows, all training cols)
            tst_kernel_mat = np.delete(kernel_mat, trn_lst, axis=0)
            tst_kernel_mat = np.delete(tst_kernel_mat, tst_lst, axis=1)
            tst_lab = np.delete(labels, trn_lst)

            # Next, train classifier with current C value
            clf = svm.SVC(kernel=""precomputed"", C=1, verbose=False)
            clf.fit(trn_kernel_mat, trn_lab)
            label_hat = clf.predict(tst_kernel_mat)
            score_hat = clf.score(tst_kernel_mat, tst_lab)
            score_list.append(score_hat)

        print ""mAP (%.2f) at C=%.2f"" % (np.mean(score_list)*100,C)
        map_scores.append(np.mean(score_list)*100)

    return map_scores.index(max(map_scores))",Examples/TubeGraphKernels/exputils.py,sumedhasingla/TubeTK,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.SVC(C=1., gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='php').export()
print(output)

""""""
<?php

class Brain {",examples/classifier/SVC/php/basics.py,nok/sklearn-porter,1
"                             sublinear_tf=True,
                             use_idf=True)
train_vectors = vectorizer.fit_transform(train_data)

# Perform classification with SVM, kernel=rbf
classifier_rbf = svm.SVC()
t0 = time.time()
classifier_rbf.fit(train_vectors, train_labels)
t1 = time.time()
time_rbf_train = t1 - t0
logger.info(""Results for SVC(kernel=rbf)"")
logger.info(""Training time: %fs"" % (time_rbf_train))

# Perform classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, train_labels)
t1 = time.time()
time_linear_train = t1 - t0
logger.info(""Results for SVC(kernel=linear)"")",marble-processor-sklearn/src/main/python/server.py,miguelfc/marble,1
"# .. your code here ..
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=7)

#
# TODO: Create an SVC classifier named svc
# Use a linear kernel, and set the C value to C
#
# .. your code here ..
from sklearn.svm import SVC
svc = SVC(C = C, kernel = kernel)

#
# TODO: Create an KNeighbors classifier named knn
# Set the neighbor count to 5
#
# .. your code here ..
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
",Module6/assignment1.py,jeffmkw/DAT210x-Lab,1
"        blend = Layer(self.X, self.y, validation_split=0.2)
        blend.add(LinearRegression())
        blend.add(SVR())
        preds, (X_val, y_val) = blend.predict(self.Xt)
        self.assertEqual(X_val.shape[1], 2)
        self.assertEqual(preds.shape[0], self.Xt.shape[0])
        self.assertAlmostEqual(X_val.shape[0], self.X.shape[0]*0.2)

    def test_clf_pred_shape(self):
        self.stack_clf.add(LogisticRegression())
        self.stack_clf.add(SVC())
        preds, (X_val, y_val)  = self.stack_clf.predict(self.Xt)
        self.assertEqual(X_val.shape[1], 2)
        self.assertEqual(preds.shape[0], self.Xt.shape[0])
        self.assertEqual(X_val.shape[0], self.X.shape[0])

    def test_multipred(self):
        self.stack.add(LinearRegression())
        self.stack.add(SVR())
        preds0, (X_val0, y_val0)  = self.stack.predict(self.Xt)",tests/test_layers.py,jpopham91/berserker,1
"
	#  --------------SVM----------------------------------------
	if 0:
		from sklearn import svm as SVM

		Y = np.zeros(len(p))
		for i in touchInds:
			Y[i] = 1
		Ystart = deepcopy(Y)

		svm = SVM.NuSVC(nu=.2, probability=True)
		# svm = SVM.NuSVC(nu=.5, kernel='poly')

		for i in xrange(10):
			svm.fit(X, Y)
			Y = svm.predict(X)

		probs = svm.predict_proba(X)

		changed = [y for x, y in zip(Y!=Ystart, range(len(Y))) if x]",pyKinectTools/scripts/AMIA_recognitionTests.py,colincsl/pyKinectTools,1
"    kf_n_c = model_selection.KFold( n_splits=n_splits, shuffle=shuffle)
    kf_n = kf5_ext_c.split( xM)
    yV_pred = model_selection.cross_val_predict( clf, xM, yV, cv = kf_n, n_jobs = n_jobs)

    if graph:
        print('The prediction output using cross-validation is given by:')
        jutil.cv_show( yV, yV_pred, grid_std = grid_std)

    return yV_pred

def gs_SVC( X, y, params, n_splits = 5):
    print(""Use gs_param for the more general implementation!"")
    return gs_param( svm.SVC(), X, y, params, n_splits=n_splits)

def gs_param( model, X, y, param_grid, n_splits=5, shuffle=True, n_jobs=-1, graph=False):
    """"""
    gs = gs_param( model, X, y, param_grid, n_splits=5, shuffle=True, n_jobs=-1)

    Inputs
    ======",kgrid.py,jskDr/jamespy_py3,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",imblearn/tests/test_pipeline.py,glemaitre/UnbalancedDataset,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/problems/TR/ORIDSESSVC.py,jpzk/evopy,1
"for org, amino_pattern in mammals.items():
    X.append(amino_pattern)
    Y.append(1)
for org, amino_pattern in aves.items():
    X.append(amino_pattern)
    Y.append(2)
for org, amino_pattern in actinopterygii.items():
    X.append(amino_pattern)
    Y.append(3)

clf = svm.SVC()
clf.fit(X, Y)
",correlation.py,PragyaJaiswal/Protein-Sequence-Parser,1
"            pass

        elif model == 'poly':
            param_grid['degree']=degree_range
            param_grid['coef0']=coef0_range
            param_grid['gamma']=gamma_range

        elif model == 'rbf':
            param_grid['gamma']=gamma_range

        svr = SVC()
        clf = GridSearchCV(svr, param_grid=param_grid, n_jobs=-1, verbose=5)

    # Specified parameter SVMs.
    else:
    	if model == 'linear':
        	clf = SVC(kernel='linear', C=svm_c, tol=svm_tol,
                      max_iter=svm_max_iter)
    	elif model == 'poly':
        	clf = SVC(kernel='poly', C=svm_c, tol=svm_tol,",svm.py,technoapurva/css,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half7.py,mirjalil/ml-visual-recognition,1
"                           memory_level=1)
X = nifti_masker.fit_transform(dataset_files.func)
# Apply our condition_mask
X = X[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",plot_haxby_anova_svm.py,ainafp/nilearn,1
"# along with liquidSVM. If not, see <http://www.gnu.org/licenses/>.
#
'''Drop-in replacements to use liquidSVM in legacy sklearn code.

Where you would else use sklearn.svm.SVC or SVR you can just
use ours. First load some data:
>>> import liquidSVM.data as ld
>>> banana = ld.LiquidData('banana-bc')
Now in sklearn you would do something like
>>> import sklearn.svm as sk
>>> model = sk.SVC(verbose=1)
>>> model.fit(banana.train.data, banana.train.target)
>>> ( model.predict(banana.test.data) != banana.test.target).mean()
You can just replace it with:
>>> import liquidSVM.learn as ll
>>> model = ll.SVC(display=1)
>>> model.fit(banana.train.data, banana.train.target)
>>> ( model.predict(banana.test.data) != banana.test.target).mean()

Be careful to let liquidSVM do its internal cross validation.",bindings/python/liquidSVM/learn.py,liquidSVM/liquidSVM,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    with use_log_level('error'):
        assert_raises(ValueError, gat.score, epochs2)",mne/decoding/tests/test_time_gen.py,hubertjb/mne-python,1
"from sklearn import svm, cross_validation, datasets

iris = datasets.load_iris()
X, y = iris.data, iris.target

model = svm.SVC()
print cross_validation.cross_val_score(model, X, y, scoring='precision')",14_cross_validation.py,halflings/python-data-workshop,1
"# Select quantity of data to use
N_data_total = len(training_labels)
percentage = 0.05
N_to_use = int(percentage * N_data_total)

# Decide how much data to use
X = training_ims[0:N_to_use]
Y = training_labels[0:N_to_use]

# Implement the support vector machine
clf = SVC()
clf.fit(X, Y)
score = clf.score(X, Y) * 100
print 'The score is = ', score",svm_mnist.py,h-mayorquin/mnist_deep_neural_network_BPNNs,1
"	#pre-processing train features
	my_pca=PCA(n_components=0.90,whiten=True)
	pca_train_features=pca.fit_transform(train_features)

	#selecting feature using Random Forest
	rfc=RandomForestClassifier()
	rfc.fit(pca_train_features,train_target)
	final_train_features=rfc.transform(pca_train_features)
	
	#training SVM model
	model=SVC(kernel='rbf')

	#Grid search for model evaluation 
	C_power=[decimal.Decimal(x) for x in list(range(-5,17,2))]                                    
	gamma_power=[decimal.Decimal(x) for x in list(range(-15,5,2))]
	grid_search_params={'C':list(np.power(2, C_power)),'gamma':list(np.power(2, gamma_power))}
	gs=GridSearchCV(estimator=svc,param_grid=grid_search_params,scoring='accuracy',n_jobs=-1,cv=3)

	#fitting the model
	gs.fit(final_train_features,train_target)",Digit Recognizer/main/second_attempt.py,tranlyvu/kaggle,1
"    print ""Pipeline: SVM""
    return Pipeline([
        ('featurizer', FeatureUnion(transformer_list=[
            ('vect-tfidf', Pipeline([
                ('vect', ActionizerCountVectorizer(ngram_range=(1, 4))), 
                ('tfidf', TfidfTransformer(smooth_idf=True, sublinear_tf=False, norm='l1')), 
            ])),
            ('vect', OctileVectorizer())
        ])),
        ('to_dense', DenseTransformer()), 
        ('clf', SVC(C=1000, degree=2, kernel='linear'))
    ])

def multi_scorer(y_true, y_pred):
    f1_score = sklearn.metrics.f1_score(y_true, y_pred)
    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)
    precision = sklearn.metrics.precision_score(y_true, y_pred)
    recall = sklearn.metrics.recall_score(y_true, y_pred)
    mse = sklearn.metrics.mean_squared_error(y_true, y_pred)
",actionizer_sentences.py,chiubaka/actionizer,1
"
    grid = GridSearchCV(model, parameters, verbose=1, n_jobs=-1)
    grid.fit(X[:,1:], Y)
    for p in parameters.keys():
        print 'Gridseach: param %s = %s' % (
            p, str(grid.best_estimator_.get_params()[p]))
    return grid.best_estimator_

def build_pipe():
    scaler = StandardScaler()
    regressor = SVC(kernel='rbf', probability=True, shrinking=True)
    return Pipeline([
        ('scaler', scaler),
        ('reg', regressor),
    ])

Xtrain, Ytrain = load_data()
pipe = build_pipe()
pipe = run_gridsearch(Xtrain, Ytrain, pipe)
run_crossval(Xtrain, Ytrain, pipe)",project2/process.py,jo-m/ml-projects,1
"    pred = model.predict(ds.X_test)
    print('-' * 10)
    print(""Classifier only:"")
    s = scores(ds.Y_test, pred, [""accuracy_score""])
    pprint(s)
    print('-' * 10)
    return s[""accuracy_score""][0]


def init_svc():
    return SVC(kernel=""rbf"", tol=0.001, decision_function_shape=""ovr"")

def assemble_result_df(cmethods, dr_methods, scores_acc):
    import pandas as pd
    df = {}
    n = len(cmethods)
    return pd.DataFrame({
        ""model"": pd.Series(cmethods),
        ""dim reduction"": pd.Series(dr_methods),
        ""accuracy"": pd.Series(scores_acc)",main.py,lpimem/dscc,1
"
        # --- SVM smote
        # Unlike the borderline variations, the SVM variation uses the support
        # vectors to decide which samples are in danger (near the boundary).
        # Additionally it also introduces extrapolation for samples that are
        # considered safe (far from boundary) and interpolation for samples
        # in danger (near the boundary). The level of extrapolation is
        # controled by the out_step.
        if self.kind == 'svm':
            # Store SVM object with any parameters
            self.svm = SVC(random_state=self.random_state, **self.kwargs)",imbalanced-learn-master/imblearn/over_sampling/smote.py,RPGOne/Skynet,1
"def test_classifiers(features, targets, test_features, test_targets, test_name):
    """"""
    Evaluate classification accuracy considering the features/targets for training and validation.
    """"""
    classifiers = {
        ""NB M"" : qc.QuoraMultiNB(features, targets),
        ""NB G"": qc.QuoraGaussianNB(features, targets),
        ""LR"" : qc.QuoraLR(features, targets),
        ""DT"" : qc.QuoraDT(features, targets),
        ""KNN"" : qc.QuoraKNN(features, targets),
        ""SVM"" : qc.QuoraSVC(features, targets),
        ""LDA"" : qc.QuoraLDA(features, targets),
        ""QDA"" : qc.QuoraQDA(features, targets),
        ""RFrst"" : qc.QuoraRandomForest(features, targets),
        ""ABoost"" : qc.QuoraAdaBoost(features, targets),
        ""Nnet"" : nnet.QuoraNnet(features, targets),
        ""ML-LR"" : lr.QuoraMlLR(features, targets),
    }

    make_section(""Test: %s"" % test_name)",quora.py,trein/quora-classifier,1
"
    # a carefully hand-designed dataset lol
    y[7] = 0
    y[27] = 0
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

    for ax, C in zip(axes, [1e-2, 1, 1e2]):
        ax.scatter(X[:, 0], X[:, 1], s=60, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='linear', C=C, tol=0.00001).fit(X, y)
        w = svm.coef_[0]
        a = -w[0] / w[1]
        xx = np.linspace(6, 13)
        yy = a * xx - (svm.intercept_[0]) / w[1]
        ax.plot(xx, yy, label=""C = %.e"" % C, c='k')
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())",mglearn/plot_linear_svc_regularization.py,amueller/advanced_training,1
"        t_datas = self.testing.ix[:, 0:].values.copy()
        self.testing_datas = t_datas.reshape(t_datas.shape[0], 1, 28, 28)

        self.testing_labels = None

    def model_tuning(self):
        # RandomForestClassifier
        #self.model = RandomForestClassifier()

        # SupportVectorMachine
        #self.model = OneVsRestClassifier(LinearSVC(random_state=0))

        # Deep learning
        dims = self.training_datas.shape[1]
        print(dims, 'dims')

        self.model = Sequential()

        self.model.add(Convolution2D(32, 1, 3, 3, border_mode='full'))
        self.model.add(Activation('relu'))",DigitRecognizer/src/digitrecognizer.py,EnsekiTT/kaggle_log,1
"    start_time = time.time()
    X=ODDvec.transform(g_it) #instance-featues matrix
    elapsed_time = time.time() - start_time
    print ""Took %d s"" % (elapsed_time)
    print 'Instances: %d Features: %d with an avg of %d features per instance' % (X.shape[0], X.shape[1],  X.getnnz()/X.shape[0])
    
    print ""Non zero different features %d"" % (len(np.unique(X.nonzero()[1])))
    

    #induce a predictive model
    predictor = LinearSVC(n_iter=150,shuffle=True)

    #predictor = SGDClassifier(n_iter=150,shuffle=True)
    print ""Training SGD classifier optimizing accuracy""

    scores = cross_validation.cross_val_score(predictor, X, y,cv=10, scoring='accuracy')
    import numpy as np
    print('Accuracy: %.4f +- %.4f' % (np.mean(scores),np.std(scores)))
    print ""Training SGD classifier optimizing AUROC""
",scripts/ODDKernel_FeatureExtraction.py,nickgentoo/scikit-learn-graph,1
"
class TestLib1ModelSvm(unittest.TestCase):
    def testSvmLinearKernel(self):
        # reproduces scikit-learn SVC result
        """"""
        import numpy as np
        from sklearn.svm import SVC
	np.random.seed(503)
	X = np.r_[np.random.randn(4,2)+5 - [1,1], np.random.randn(4,2) + [1,1]]
	Y = [0]*4 + [1]*4
	clf = SVC(kernel=""linear"")
	clf.fit(X, Y)

        # using parameters:
	sv     = clf.support_vectors_
	nv     = clf.n_support_
	alpha  = clf.dual_coef_.flatten()
	b  = clf._intercept_ #(reverse sign, weird convention/bug in scikit)
        # reproduces
        print clf.decision_function([0, 1])",titus/test/lib/model/testSvm.py,opendatagroup/hadrian,1
"
# ==============================================================================
# Support vector machines (placed me at 0.97543)
# ==============================================================================
from sklearn.svm.SVC import SVC
from sklearn import preprocessing

scaler = preprocessing.Scaler().fit(features_to_train)
X_scaled = scaler.transform(features_to_train)

vm = SVC(C=1.0, cache_size=5000, kernel='rbf')
vm.fit(X_scaled, targets_to_train)

y_scaled = scaler.transform(features_to_test)
predicted_targets = vm.predict(y_scaled)

# Pretest scores
print 'precision: %0.5f' % metrics.precision_score(pretest_targets, predicted_targets)
print 'f1 score: %0.5f' % metrics.f1_score(pretest_targets, predicted_targets)
",digit.py,jhprks/digitrecognizer,1
"
def test_task():
    # Example from http://scikit-learn.org/stable/modules/feature_selection.html
    from sklearn.datasets import load_iris
    from sklearn.svm import LinearSVC
    iris_data = load_iris()
    # iris_data is sklearn.datasets.base.Bunch
    X, y = iris_data.data, iris_data.target
    print(X.shape)
    clf = Pipeline([
        ('feature_selection', SelectFromModel(LinearSVC(penalty=""l1"", dual=False))),
        ('classification', ExtraTreesClassifier())
    ])
    clf.fit(X, y)
    y_pred = clf.predict(X)
    print(y_pred)",machine_learning/ml.py,den1den/web-inf-ret-ml,1
"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


def run_algorithm(posts, labels, filename):
	if isfile(Constants.SVC_W2V_METRICS + filename + "".txt""):
		print('file exists -- returning')
		return
	post_vectors, labels = __get_vectors(posts, labels, filename)
	X_train, X_test, y_train, y_test = train_test_split(post_vectors, labels, test_size=0.2)
	cv_params = __get_cv_params()
	gs = GridSearch(SVC(), params_grid=cv_params, verbose=1, n_jobs=-1)
	best_params = gs.fit(X_train, y_train)
	print(best_params)
	if best_params['kernel'] == 'linear':
		svc = MySVC(C=best_params['C'], kernel='linear')
	else:
		svc = MySVC(C=best_params['C'], kernel='rbf', gamma=best_params['gamma'])
	svc.fit(X_train, y_train)
	y_pred = svc.predict(X_test)
	output_file = open(Constants.SVC_W2V_METRICS + filename + '.txt', mode='w')",classic/main/w2v_main.py,mister11/otd_master,1
"	df = pd.read_csv('model_pixel_data.csv')
	
	# Extract data 	
	x = np.array(df.drop(['class'], 1))
	y = np.array(df['class'])

	# Make train, dev, test set 
	x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.2)

	# Save the model 
	clf = svm.SVC(kernel='rbf', degree=5)
	clf.fit(x_train, y_train)

	print clf

	# Save the model
	with open('SVMPixelClassifierVersion2.pickle', 'wb') as f:
		pickle.dump(clf, f)

	# Test the accuracy on the test set",Machine Learning Algorithms/Segmentation Pixel Classifier 2/SupportVectoMachines.py,lozuwa/Automatic-Digital-Microscope,1
"
        model = core.raccoon.Raccoon(verbose=2)
        model.fit(Xtrain, ytrain)

        import sklearn.lda
        import sklearn.neighbors
        import sklearn.qda
        import sklearn.svm
        import sklearn.metrics
        import scipy.stats
        predictor = sklearn.svm.SVC()
        param_dist = {'C': scipy.stats.expon(scale=.1), 'gamma': scipy.stats.expon(scale=0.1),
                      'kernel': ['linear', 'rbf']}
        param_dist = {'C': pow(2.0, np.arange(-10, 11)), 'gamma': pow(2.0, np.arange(-10, 11)),
                      'kernel': ['linear', 'rbf']}
        tmpt = model.predict(Xtrain, model=predictor, param_dist=param_dist)
        tmp = model.predict(Xtest, model=predictor, param_dist=param_dist)
        cvs_results.append({'train_result': tmpt, 'test_result': tmp,
                            'Xtrain': Xtrain, 'ytrain': ytrain,
                            'Xtest': Xtest, 'ytest': ytest})",Raccoon/scripts/test.py,adrinjalali/Network-Classifier,1
"Y = []
for x,y in XY:
    X.extend(x)
    Y.extend(y)

print ""RATIO = "", np.sum(Y), len(Y)
    
X = np.array(X,dtype='float')
Y = np.array(Y,dtype='float')

# svc = svm.SVC()

svc = svm.LinearSVC(dual=False,)

svc.fit(X, Y)
print svc.score(X, Y)

joblib.dump(svc, args.output)",wrapping/cython/scripts/learn_mser.py,BioMedIA/irtk-legacy,1
"		for rows in reader:
			labels.append(rows[2])
	return labels

def TrainSVM(data,labels):
	usealgo = 1
	if usealgo == 0:
		from sklearn.linear_model import PassiveAggressiveClassifier
		clf=PassiveAggressiveClassifier(class_weight='balanced',n_jobs=-1,n_iter=15,fit_intercept=True)
	elif usealgo ==1:
		clf = SVC(probability= True,decision_function_shape='ovr',random_state=np.random.randint(1000),kernel=""linear"")

	elif usealgo ==2:
		from sklearn.svm import LinearSVC
		clf = LinearSVC()

	clf.fit(data,labels)
	return clf

def loadTestData(filename):",main.py,NevesLucas/BE562_FinalProject,1
"    else:
        raise AssertionError('AttributeError for voting == ""hard""'
                             ' and with predict_proba not raised')


def test_multilabel():
    """"""Check if error is raised for multilabel classification.""""""
    X, y = make_multilabel_classification(n_classes=2, n_labels=1,
                                          allow_unlabeled=False,
                                          random_state=123)
    clf = OneVsRestClassifier(SVC(kernel='linear'))

    eclf = VotingClassifier(estimators=[('ovr', clf)], voting='hard')

    try:
        eclf.fit(X, y)
    except NotImplementedError:
        return

",projects/scikit-learn-master/sklearn/ensemble/tests/test_voting_classifier.py,DailyActie/Surrogate-Model,1
"y = iris.target
n_classes = np.unique(y).size

# Some noisy data not correlated
random = np.random.RandomState(seed=0)
E = random.normal(size=(len(X), 2200))

# Add noisy data to the informative features for make the task harder
X = np.c_[X, E]

svm = SVC(kernel='linear')
cv = StratifiedKFold(2)

score, permutation_scores, pvalue = permutation_test_score(
    svm, X, y, scoring=""accuracy"", cv=cv, n_permutations=100, n_jobs=1)

print(""Classification score %s (pvalue : %s)"" % (score, pvalue))

###############################################################################
# View histogram of permutation scores",projects/scikit-learn-master/examples/feature_selection/plot_permutation_test_for_classification.py,DailyActie/Surrogate-Model,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_11_14_2014.py,magic2du/contact_matrix,1
"from sklearn.svm import SVC
from sklearn import preprocessing
t = time.time()
#load paths for feature files
with open('SETTINGS.json') as f:
    settings = json.load(f)
path_features=str(settings['features'])
path_trained_model=str(settings['trained_model'])

#set up models
model_fit1=SVC(probability=True,kernel='linear',C=0.0011253,random_state=1)
model_fit2=SVC(probability=True,kernel='linear',C=0.00069519,random_state=1)

#############################
##########Model 1###########
#############################


#############################
#########load data###########",train_model.py,duncan-barrack/kaggle_BCI_challenge,1
"    sum -= model.intercept_
    Y.append(sign(sum))
  return np.array(Y)
#   return np.array([sign(np.sum([alpha*((xx.T.dot(x) + 1)**2) for alpha, x in 
#                                 zip(model.dual_coef_[0], model.support_vectors_)]) + model.intercept_) for xx in XX])


def problem12():
  X = trainSVM[:,:-1]
  Y = trainSVM[:,-1]
  model = SVC(C=1, kernel='poly', degree=2, gamma=1., coef0=1., verbose=True)
  model.fit(X, Y)
#   print 'xx', model.
  print model.dual_coef_, model.support_vectors_
  
  def K(x0, x1):
    return (1 + x0.T.dot(x1))**2
  def _b(A, X):
    return 1 - np.sum(A*np.array([K(x, X[A>0,:]) for x in X]))
  b = _b(model.dual_coef_, model.support_vectors_)",Final/Python/by_Mark_B2/final.py,kirbs-/edX-Learning-From-Data-Solutions,1
"    survcv=np.array(DF.iloc[rp[splitind:nvals],0])
    # note: SVM takes -1 and 1 for single class labels
    survt[~survt]=-1
    survcv[~survcv]=-1
    tset = DF.iloc[rp[0:splitind],1:]
    cvset = DF.iloc[rp[splitind:nvals],1:]
    bestscore=-1
    # loop through the values of c and g
    for c in paramc:
        for g in paramg:
            model=SVC(C=c,gamma=g)
            model=model.fit(tset,survt)
            try:
                scorei=model.score(cvset,survcv)
            except:
                scorei=0 # something went wrong...
            scores[counter]=np.mean(scorei)
            paramholder[counter,0]=c
            paramholder[counter,1]=g
            if scorei>bestscore:",svmmodel.py,kjford/Titanic,1
"from sklearn import svm
import sklearn.neural_network as nn

from naoth.util import *

mybins = np.linspace(0, 1, 33)
  
def learn(X, labels):
  #estimator = Perceptron(n_iter=1000, shuffle=True)
  #estimator = SGDClassifier()
  #estimator = svm.SVC(kernel='rbf')
  estimator = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
  #estimator = nn.MLPClassifier()
  
  #samples = range(3000,10000)
  #print len(labels)
  #estimator.fit(X[samples,:], labels[samples])",Utils/py/ball_detector/ball_patch_learn.py,BerlinUnited/NaoTH,1
"    print ""------------------------------****************""
    X_train = pca.transform(X_train)	# PCA SANK
    X_val = pca.transform(X_val)

    n_features = X_train.shape[1]   
    print ""XX:"", n_features

    g =   1.0/float((3*n_features))
    print g
 
    clf_l = svm.SVC(kernel='linear', C=reg)
    clf = svm.SVC(kernel='rbf', C=reg, gamma=g)
  
    print ""Classifier:""
    print clf, clf.get_params()
    print clf_l, clf_l.get_params()


    print ""Training.""
    clf.fit(X_train, y_train)",python_scripts_from_net/svm.py,sankar-mukherjee/DecMeg2014,1
"

		# standardization the dataset into Gaussians with zero means and unit variances.
		# http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing
		scaler = StandardScaler()
		scaler.fit(X_train)  # Don't cheat - fit only on training data
		X_train = scaler.transform(X_train)
		X_test = scaler.transform(X_test)  # apply same transformation to test data

		# fit the model using SVM with rbf kernel
		self.clf_SVM = svm.SVC(gamma=0.1, probability = True, kernel='rbf',C=2.)
		#self.clf_SVM = svm.SVC()
		self.clf_SVM.fit(X_train, Y_train)
		probas_SVM = self.clf_SVM.fit(X_train, Y_train).predict_proba(X_test)
		#print probas_SVM
	
		# prediction
		# http://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting
		s = pickle.dumps(self.clf_SVM)
		clf2 = pickle.loads(s)",LIWC-train/LIWC_classify.py,YixuanLi/geo-tweet,1
"
    if(False):
        #print ""Testing...""
        #print ""1 = Bread""
        #print ""2 = Nonbread""

        train = np.vstack((breadtrain,nonbreadtrain))
        labels = np.hstack((labelsbtr[:,0],labelsnbtr[:,0]))
        test = np.vstack((breadtest,nonbreadtest))

        lin_svc = svm.LinearSVC(C=1.0).fit(train, labels)
        predictionsSVM = lin_svc.predict(test)

        cfr = RandomForestClassifier(n_estimators=120)
        cfr.fit(train,labels) # train

        gtruth = np.hstack((labelsbte[:,0],labelsnbte[:,0]))
        predictionsRF = cfr.predict(test) # test

        print dirListbte",tests/testcomparison.py,rbaravalle/imfractal,1
"for line in ds.readlines():
    data = [int(x) for x in line.strip().split('\t') if x != '']
    Y.append(data[3:])
    X.append(data[:3])

X=np.array(X)
Y=np.array(Y)
Y=Y.ravel()
X_train, X_test, Y_train, Y_test = train_test_split( X, Y,test_size=0.33)

clf = svm.SVC(gamma=0.001, C=100.)
Y_pred = clf.fit(X_train,Y_train).predict(X_test)

print ""Confusion Matrix""
cm = confusion_matrix(Y_test, Y_pred)
print cm

print ""Accuracy : %f "" %(100*accuracy_score(Y_test, Y_pred))

c1_a = 0",SVM_Classifier/svm.py,uditsharma7/Machine-Learning,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_08_2015_04.py,magic2du/contact_matrix,1
"    plt.show()
    
    from sklearn.lda import LDA
    clf = LDA()
    clf.fit(inputMat[:,1:80].T, outputVec[1:80])    
    fit = clf.predict(inputMat.T)
    err = np.sum(np.abs(fit - outputVec)>0)
    print(err)
    
    # SVM
    clf = sklearn.svm.SVC(kernel='linear', C=1e-1)
    n_samples = inputMat.shape[1]
    cv = sklearn.cross_validation.KFold(n_samples,n_folds=8,shuffle=True)
    scores = sklearn.cross_validation.cross_val_score(clf, inputMat.T, outputVec, cv=cv)
    
    # Predict my data
    # Read in my data file
    myOtu = otu.OTU(r'..\sample_data\01112016.json')
    
    # Get family distribution",code/main.py,isaacgerg/ubiome_longitudinal_analysis,1
"labelClose = np.zeros(len(dataClose))

features = np.vstack((dataOpen,dataClose))
labels = np.append(labelOpen,labelClose)

#test_features = data.features        # features
#test_labels   = data.labels          # labels

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=0)

clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
print clf.score(X_test, y_test) 

#test_features = scaler.transform(test_features)

#clf = SVC(C=1.0,max_iter=100000)
#clf.fit(features,labels)

#print(clf.score(test_features,test_labels))",virtual_sensor/machine_learning/door/svm.py,IoT-Expedition/Edge-Analytics,1
"from sklearn.ensemble import VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB

classificator = VotingClassifier(estimators=[
    ('svm', SVC(kernel='linear')),
    ('naive', MultinomialNB(alpha=.01))]
)

training_set = ['o cruzeiro jogou muito bem', 'parabéns pela vitória cruzeiro',
                'ainda acho o cruzeiro ruim', 'o cruzeiro não jogou bem',
                'cruzeiro jogou contra o sport',
                'primeira rodada o cruzeiro enfrentou o sport']
labels = [1, 1, -1, -1, 0, 0]
",classificacao_votacao.py,cassiobotaro/sentibol,1
"                   kernel_type='rbf', kernel_args={'gamma':80.0}, \
                   learning_rate=0.01, verbose=2)
clf.fit()
pd = clf.predict(x_te)
print('SVI error = {}'.format(np.sum(len(np.where(pd != y_te)[0])) / float(x_te.shape[0])))
plt.figure('Toy Circle')
ax = plt.subplot(222)
ax.set_title('GP with Stochastic Variational Inference')
ax.scatter(x_te[:,0], x_te[:,1], c=[colors[y] for y in pd], s=40)

clf = SVC()
clf.fit(x_tr, y_tr)
pd = clf.predict(x_te)
print('SVM error = {}'.format(np.sum(len(np.where(pd != y_te)[0])) / float(x_te.shape[0])))
plt.figure('Toy Circle')
ax = plt.subplot(223)
ax.set_title('SVM with RBF Kernel')
ax.scatter(x_te[:,0], x_te[:,1], c=[colors[y] for y in pd], s=40)

clf = LogisticRegression()",GPSVI/test/playtoycircle.py,AlchemicalChest/Gaussian-Process-with-Stochastic-Variational-Inference,1
"        for subwords, start, end in subcommands(tokenize_command(sentence)):
            f = feats(subwords, sentence, start, end)

            label = -1
            if ' '.join([w.strip(string.punctuation) for w in subwords]).lower() == correct.lower():
                label = 1

            feature_vectors.append(f)
            labels.append(label)

    clf = svm.LinearSVC()
    clf.fit(feature_vectors, labels)

    return clf

def build_default_model():
    model = build_model(read_data('data/corpora/sendacard_corpus.tsv'))

    def evalmodel(subwords):
        v = model.decision_function(feats(subwords, None, None, None))[0]",text_classification.py,sbirch/webtalk,1
"HOG on person bounding box: 24 ms
HOG on person whole body: 101 ms for 4x4 px/cell and 70 ms for 8x8 px/cell for 24*24 px boxes
HOG per extrema 2-3 ms

It's not computationally efficient to compute hogs everywhere? What about multi-threaded? gpu?
""""""


if 0:
	# Chi Squared Kernel
	svm = SVC(kernel=chi2_kernel)
	svm.fit(features['Gray_HOGs'], labels)
	svm.score(features['Gray_HOGs'], labels)

class MultiChannelClassifier:
	channel_kernels = []
	channel_names = []
	channel_means = []
	channel_data = []
",pyKinectTools/scripts/Pose_tests.py,colincsl/pyKinectTools,1
"import pandas as pd
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import KFold
from sklearn import grid_search
from Four_Factors import *

def svm_model(x,y):
    # SVM modeling
    parameters={'kernel':('linear', 'rbf', 'poly'), 'C':[0.1,.5, 1, 5,10], 'degree':[1,2,3,4]}
    svr=SVC()
    svm=grid_search.GridSearchCV(svr, parameters, cv=kf)
    svm=grid_search.GridSearchCV(svr, parameters)
    svm.fit(x,y)
    print svm.best_score_
    print svm.best_params_
    # ~65%

def ran_forest_model(x,y):
    # Random Forest Modeling",Classification/Winner_Model.py,mprego/NBA,1
"import pandas as pd
import numpy as np
from numpy.random import RandomState
from learn import wrangle, printers, nlp

__author__ = 'Aaron J. Masino'

#SET A RANDOM SEED FOR REPEATABLE RESULTS
__seed__ = 987654321

def load_linearSVC(config, section):
    c = 1.0
    if config.has_option(section,'C'):
        c = float(config.get(section,'C'))
    return svm.LinearSVC(tol=1e-6, C=c)

def load_gaussianSVC(config, section):
    c = 1.0
    gamma = 0.0
    if config.has_option(section,'C'):",model_persist.py,chop-dbhi/arrc,1
"    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS

    # label junk food as -1, the others as +1
    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)

    # split the dataset for model development and final evaluation
    train_data, test_data, target_train, target_test = train_test_split(
        data, target, test_size=.2, random_state=0)

    pipeline = Pipeline([('vect', CountVectorizer()),
                         ('svc', LinearSVC())])

    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
        'svc__loss': ('hinge', 'squared_hinge')
    }

    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)",projects/scikit-learn-master/sklearn/feature_extraction/tests/test_text.py,DailyActie/Surrogate-Model,1
"

def compute_svm_subjects(K, y, n_folds=5):
    """"""
    """"""
    cv = KFold(len(K)/2, n_folds)
    scores = np.zeros(n_folds)
    for i, (train, test) in enumerate(cv):
        train_ids = np.concatenate((train, len(K)/2+train))
        test_ids = np.concatenate((test, len(K)/2+test))
        clf = SVC(kernel='precomputed')
        clf.fit(K[train_ids, :][:, train_ids], y[train_ids])
        scores[i] = clf.score(K[test_ids, :][:, train_ids], y[test_ids])

    return scores.mean()


def permutation_subjects(y):
    """"""Permute class labels of Contextual Disorder dataset.
    """"""",classif_and_ktst.py,emanuele/jstsp2015,1
"
  #C=[0.001,0.01,0,1,1,10,100,1000,10000,100000]
  #gamma=[1.0e-7,1.0e-6,1.0e-5,1.0e-4,0.001,0.01,0.1,1,10,100]

  C=[100000,1000000,10000000]
  gamma=[1.0e-6,1e-7,1.0e-8]

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/t3.py,n7jti/machine_learning,1
"def select_k_best(x, y, k):
    """""" select k best features in dataset """"""

    x_new = SelectKBest(chi2, k=k).fit_transform(x, y)
    return x_new


def get_best_features(x, y):
    """""" finds the optimal number of features """"""

    svc = SVC(kernel=""linear"")
    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2), scoring='log_loss')
    rfecv.fit(x, y)
    return rfecv.n_features_, rfecv.ranking_


def get_features(x, y, n_features_to_select):
    """""" finds the optimal features """"""

    svc = SVC(kernel=""linear"", C=1)",hal/ml/features.py,sirfoga/hal,1
"                              Base.train_y.shape,
                              Base.test_X.shape)

    def evaluate(self):
        """"""
        Model evaluation across multiple classifiers based on accuracy of predictions.
        """"""
        classifiers = [
            xgb.XGBClassifier(**Base.xgb_params),
            KNeighborsClassifier(3),
            SVC(probability=True),
            DecisionTreeClassifier(),
            RandomForestClassifier(),
            AdaBoostClassifier(),
            GradientBoostingClassifier(),
            GaussianNB(),
            LogisticRegression()]

        log_cols = [""Classifier"", ""Accuracy""]
        Base.model_ranking = pd.DataFrame(columns=log_cols)",speedml/model.py,Speedml/speedml,1
"            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'linear-svm':
                from sklearn.svm import LinearSVC
                self.estimator_ = LinearSVC(
                    random_state=self.random_state, **self.kwargs)
            else:
                raise NotImplementedError
        else:
            raise ValueError('Invalid parameter `estimator`. Got {}.'.format(
                type(self.estimator)))

        self.logger.debug(self.estimator_)
",imblearn/ensemble/balance_cascade.py,chkoar/imbalanced-learn,1
"# @todo move into sbfToolchains.py
def removeOption( env, key, option = '-c' ):
	""""""Remove first occurrence of ' option ' from env[key]""""""
	newValue = env[key].replace(' {} '.format(option), ' ', 1)
	env[key] = newValue

### MSVC ###
# export SCONS_MSCOMMON_DEBUG=ms.log
# HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Microsoft\Microsoft SDKs\Windows\v6.0A\InstallationFolder

def createEnvMSVC( sbf, tmpEnv, myTools ):
	myTools += ['msvc', 'mslib', 'mslink']

	dictTranslator = { 'x86-32' : 'x86', 'x86-64' : 'x86_64' }
	targetArch = dictTranslator[ tmpEnv['targetArchitecture'] ]
	if tmpEnv['clVersion'] != 'highest':
		myMsvcVersion = tmpEnv['clVersion']
		myMsvcYear = clVersionNumToYear.get(myMsvcVersion, myMsvcVersion)
		# Tests existance of the desired version of cl
		if myMsvcVersion not in getInstalledCLVersionNum():",src/SConsBuildFramework.py,npapier/sbf,1
"    if output:
        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',
            Instances=n_instances, Results=metrics_results)

if __name__ == '__main__':
    from nltk.classify import NaiveBayesClassifier, MaxentClassifier
    from nltk.classify.scikitlearn import SklearnClassifier
    from sklearn.svm import LinearSVC

    naive_bayes = NaiveBayesClassifier.train
    svm = SklearnClassifier(LinearSVC()).train
    maxent = MaxentClassifier.train

    demo_tweets(naive_bayes)
    # demo_movie_reviews(svm)
    # demo_subjectivity(svm)
    # demo_sent_subjectivity(""she's an artist , but hasn't picked up a brush in a year . "")
    # demo_liu_hu_lexicon(""This movie was actually neither that funny, nor super witty."", plot=True)
    # demo_vader_instance(""This movie was actually neither that funny, nor super witty."")
    # demo_vader_tweets()",env/lib/python3.6/site-packages/nltk/sentiment/util.py,Edu-Glez/Bank_sentiment_analysis,1
"					(RidgeClassifier(tol=1e-2, solver=""sag""), ""Ridge Classifier""),
					(RandomForestClassifier(n_estimators=100), ""Random forest"")):
			print('=' * 80)
			print(name)
			results.append(benchmark(clf))

	for penalty in [""l2""]:
			print('=' * 80)
			print(""%s penalty"" % penalty.upper())
			# Train Liblinear model
			results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty, dual=False, tol=1e-3)))
	# Train NearestCentroid without threshold
	print('=' * 80)
	print(""NearestCentroid (aka Rocchio classifier)"")
	results.append(benchmark(NearestCentroid()))

	print([r[0]+"" ""+str(r[1]) for r in results])
	# make some plots

	return [(r[0],r[1],r[4]) for r in results]",genre_classification.py,sergiooramas/music-genre-classification,1
"    forest.fit(X,Y)
    return forest

def train_clustered_ols(X, Y, k = 20): 
    """"""Cluster data and then train a linear regressor per cluster""""""
    cr = ClusteredRegression(k)
    cr.fit(X, Y)
    return cr 

def train_clustered_svm(X, Y, k = 20, C = 1, verbose = True):
    base_model = LinearSVC(C = C)
    cc = ClusteredClassifier(k = k, base_model = base_model, verbose = verbose)
    cc.fit(X, Y)
    return cc 

def mk_clustered_svm_ensemble(
        num_models = 20, 
        C = 1, 
        k = 20, 
        stacking= False, ",treelearn/recipes.py,capitalk/treelearn,1
"from sklearn.metrics import precision_recall_fscore_support
from sklearn.svm import LinearSVC

from modules.tokenizer import ngrams_tokenizer
from modules.cleaner import clean
from modules.similarity import *

print('PROGRESS: Initializing...')

count_vect = CountVectorizer(preprocessor=clean)
clf = LinearSVC(max_iter=10000)
splitBy = 20
calculations = [
    Cosine(),
    Dice(),
    Jaccard(),
    Overlap(),
]
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
ngrams = 1",birch_svm.py,dwiajik/twit-macet-mining-v3,1
"from sklearn.svm.classes import NuSVC

from ..Classifier import Classifier
from ...language.C import C


class NuSVCCTest(C, Classifier, TestCase):

    def setUp(self):
        super(NuSVCCTest, self).setUp()
        self.mdl = NuSVC(kernel='rbf', gamma=0.001, random_state=0)

    def tearDown(self):
        super(NuSVCCTest, self).tearDown()",tests/classifier/NuSVC/NuSVCCTest.py,nok/sklearn-porter,1
"    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for logistic regression: ', auc)
    
    plot_curve(fpr, tpr, 'Logistic regression ' + str(auc))
    return clf


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_4.py,remigius42/code_camp_2017_machine_learning,1
"        # data_test=test.as_matrix()
        # testX, _testX = train_test_split(data_test/255.,test_size=0.99)
        
        # Random Forest
        # self.clf = RandomForestClassifier()
        
        # Stochastic Gradient Descent
        # self.clf = SGDClassifier()
        
        # Support Vector Machine
        # self.clf = LinearSVC()
        
        # Nearest Neighbors
        # self.clf = KNeighborsClassifier(n_neighbors=13)
        
        
        train = pd.read_csv(""data/train.csv"")
        data_train=train.as_matrix()
        values_train=data_train[:,0]
        images_train=data_train[:,1:]",redigit/clf.py,osgee/redigit,1
"    def predict(self, time_now):
        X, y = extract_feature(self.__data__, self.__poly_kernel__, get_train_instances, time_now)
        self.__model__.fit(X, y)
        pred_X, ub = extract_feature(self.__data__, self.__poly_kernel__, get_pred_instance, time_now)
        y = self.__model__.predict(pred_X)
        predictions = ub[y == 1]
        return predictions, np.ones((len(predictions,)))

def get_model():
    from sklearn.svm import LinearSVC
    return LR(model=LinearSVC(C=10, loss='l1'), alpha=0.7, degree=1)

def sort_by(data, order=['user_id', 'brand_id', 'visit_datetime']):
    actype = np.dtype({
        'names': ['user_id', 'brand_id', 'type', 'visit_datetime'],
        'formats': [np.long, np.long, np.int, np.int]
        })
    typed_data = np.zeros(len(data), dtype=actype)
    for i in range(len(data)):
        typed_data[i] = tuple(data[i])",lr/pred.py,hsinhuang/AliDMCompetition,1
"
def baseline(train_data, train_labels, test_data, test_labels, omit=[]):
    """"""Train various classifiers to get a baseline.""""""
    clf, train_accuracy, test_accuracy, train_f1, test_f1, exec_time = [], [], [], [], [], []
    clf.append(sklearn.neighbors.KNeighborsClassifier(n_neighbors=10))
    clf.append(sklearn.linear_model.LogisticRegression())
    clf.append(sklearn.naive_bayes.BernoulliNB(alpha=.01))
    clf.append(sklearn.ensemble.RandomForestClassifier())
    clf.append(sklearn.naive_bayes.MultinomialNB(alpha=.01))
    clf.append(sklearn.linear_model.RidgeClassifier())
    clf.append(sklearn.svm.LinearSVC())
    for i,c in enumerate(clf):
        if i not in omit:
            t_start = time.process_time()
            c.fit(train_data, train_labels)
            train_pred = c.predict(train_data)
            test_pred = c.predict(test_data)
            train_accuracy.append('{:5.2f}'.format(100*sklearn.metrics.accuracy_score(train_labels, train_pred)))
            test_accuracy.append('{:5.2f}'.format(100*sklearn.metrics.accuracy_score(test_labels, test_pred)))
            train_f1.append('{:5.2f}'.format(100*sklearn.metrics.f1_score(train_labels, train_pred, average='weighted')))",lib/utils.py,mdeff/cnn_graph,1
"    best_parameters = gs.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print(""\t%s: %r"" % (param_name, best_parameters[param_name]))


def grid_search_svc_rbf(X, y):

    pipeline_svc_rbf = Pipeline([
        ('tfidf', TfidfTransformer()),
        ('kbest', SelectKBest()),
        ('clf', SVC())
    ])

    parameters_svc_rbf = {
        'tfidf__use_idf': (True, False),
        'kbest__k': (2000, 3000,),
        'kbest__score_func': (chi2,),
        'clf__kernel': ('rbf',),
        'clf__C': (.1, 1, 10, 100, 1000, 10000),
        'clf__gamma': (.001, .005, .01, .02, .1),",selfgraph/core/grid_search.py,hmartiro/selfgraph,1
"import numpy as np
from sklearn import svm

xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
                     np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

# fit the model
clf = svm.NuSVC()
clf.fit(X, Y)

# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
           origin='lower', cmap=plt.cm.PuOr_r)",projects/scikit-learn-master/examples/svm/plot_svm_nonlinear.py,DailyActie/Surrogate-Model,1
"    print ans_scaled.max(), ans_scaled.argmax()


def k_tree(x, y):
    clf = DecisionTreeClassifier(random_state=241)
    clf.fit(x, y)
    print clf.feature_importances_


def support_vector_classification(x, y):
    svc = SVC(C=100000, kernel='linear', random_state=241)
    svc.fit(x, y)
    return svc


def k_neighbors(x, y):
    neigh = KNeighborsClassifier(n_neighbors=40)
    neigh.fit(x, y)
    return neigh
",sf-crime/main.py,sergiy-evision/math-algorithms,1
"pd.to_pickle(df_Naman.ix[:,:-1], '/home/naman/SNLP/FinalVectors_Naman.pkl')
pd.to_pickle(df_final_full, '/home/naman/SNLP/FinalVectors_Full_D2V.pkl')


X_final=df_final.ix[:,1:-1]
Y_final=df_final.ix[:,-1]


from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score
clf = SVC()
scores_Naman = cross_val_score(clf, X_Naman, Y_Naman, cv=10)
scores_Shankar = cross_val_score(clf, X_Shankar, Y_Shankar, cv=10)
scores_Shankar2 = cross_val_score(clf, X_Shankar, Y_Shankar, cv=10)
scores_Jar = cross_val_score(clf, X_Jar, Y_Jar, cv=10)
scores_final = cross_val_score(clf, X_final, Y_final, cv=10)

scores=np.vstack((scores_Shankar, scores_Shankar2, scores_Jar, scores_Naman)).T
scores=pd.DataFrame(scores, columns=['CharacterNetworks','TopicOverlap','EmotionAnalysis','Doc2Vec'])
",Doc2Vec and Classification/doc2vec_model.py,njordsir/Movie-Script-Analysis,1
"
if __name__ == ""__main__"":
    category = ""airplanes""
    total = time.time()

    clf = RandomForestClassifier(n_estimators = 2000)

    # clf = AdaBoostClassifier(n_estimators = 2000)
    # clf.base_estimator.max_depth = 4

    # clf = LinearSVC(C=100)
    # clf = SVC(C=10)

    dm = CaltechManager()
    vcd = VisualConceptDetection(classifier=clf, datamanager=dm)

    vcd.run(category)
    print ""Total execution time: %f minutes"" % ((time.time() - total) / 60.0)",runClassification.py,peret/visualize-bovw,1
"        X, y, test_size=0.3, random_state=0)
    syncer_obj.add_tag(X, ""samples generated data"")
    syncer_obj.add_tag(x_train, ""training data"")
    syncer_obj.add_tag(x_test, ""testing data"")

    # ANOVA SVM-C
    # 1) anova filter, take 5 best ranked features
    anova_filter = SelectKBest(f_regression, k=5)
    syncer_obj.add_tag(anova_filter, ""Anova filter, with k=5"")
    # 2) svm
    clf = svm.SVC(kernel='linear')
    syncer_obj.add_tag(clf, ""SVC with linear kernel"")
    anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])

    syncer_obj.add_tag(anova_svm, ""Pipeline with anova_filter and SVC"")

    # Fit the pipeline on the training set
    anova_svm.fit_sync(x_train, y_train)
    y_pred = anova_svm.predict(x_test)
    # Compute metrics for the model on the testing set",client/python/samples/sklearn/Pipeline-AnovaFilter.py,mitdbg/modeldb,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.cross_validation import StratifiedKFold
from sklearn.metrics import roc_curve, auc
import matplotlib
matplotlib.style.use('ggplot')


names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""AdaBoost"", ""Naive Bayes""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    DecisionTreeClassifier(max_depth=5),
    AdaBoostClassifier(),
    GaussianNB()]
colors = [""b"", ""g"", ""r"", ""c"", ""m""]

def getSectionCount(macho):
    sectionCount = 0
    for load_cmd, cmd, data in macho.headers[0].commands:
        if hasattr(cmd, ""segname""):",malware_toys/classify_macho.py,tbarabosch/MacRE,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.svm

import submissions
from data import *

svm = sklearn.svm.SVC(kernel=""linear"")
svm.fit(train, target)
pred = svm.predict(test)

submissions.save_csv(pred, ""support_vector_machine.csv"")",titanic/support_vector_machine.py,wjfwzzc/Kaggle_Script,1
"for i in range(1000,2000):
    svm_y_train[i] = 1
print svm_y_train

svm_y_test = numpy.array([0 for i in range(2143)])
for i in range(1210,2143):
    svm_y_test[i] = 1
print svm_y_test


svc = svm.SVC(kernel='linear')
svc.fit(svm_x_train, svm_y_train)

expected = svm_y_test
predicted = svc.predict(svm_x_test)

print(""Classification report for classifier %s:\n%s\n""
      % (svc, metrics.classification_report(expected, predicted)))
print(""Confusion matrix:\n%s"" % metrics.confusion_matrix(expected, predicted))
",sample_impl/doc2vec_sample.py,kitanaisia/SDR,1
"    def extract(self,X):
        Xflat = X.reshape((X.shape[0], -1))
        return self._svm.predict(Xflat) 

    def train(self, X, Y, OriginalX = None):
        Xflat = X.reshape((X.shape[0], -1))
        if self._penalty is None:
            Cs = 10**np.linspace(-1, -4, 10)
            avg_scores = np.zeros(len(Cs))
            for i, C in enumerate(Cs):
                clf = LinearSVC(C=C)
                scores = cross_validation.cross_val_score(clf, Xflat, Y, cv=5)
                avg_scores[i] = np.mean(scores)
                print('C', C, 'scores', scores, 'avg', np.mean(scores))

            Ci = np.argmax(avg_scores)
            C = Cs[Ci]

            clf = LinearSVC(C=C)
            clf.fit(Xflat, Y)",pnet/svm_classification_layer.py,jiajunshen/partsNet,1
"'''Trains SupportVectorMachine and uses it to write predictions.
'''
from sklearn import svm

from utils import load_data, write_predictions

if __name__ == ""__main__"":
    ids, data, labels = load_data()
    clf = svm.SVC().fit(data,labels)
    write_predictions(clf)",winner.py,afoss925/kaggle_schizophrenia_2014,1
"
    
    normalizer = StandardScaler()
    normalizer.fit(X_train)
    X_train_norm = normalizer.transform(X_train)
    X_test_norm = normalizer.transform(X_test)


    classifiers = [
            KNeighborsClassifier(),
            SVC(),
            RandomForestClassifier(),
            GaussianNB(),
            LDA(),
            ]

    names = model_names 
           

    params = [{""n_neighbors"": 3. ** np.arange(5)},",cell_classifier.py,cyanut/cell-classifier,1
"
    # Split up data into randomized training and test sets
    rand_state = np.random.randint(0, 100)
    X_train, X_test, y_train, y_test = train_test_split(
        scaled_X, y, test_size=0.2, random_state=rand_state)

    print('Using:',orient,'orientations',pix_per_cell,
        'pixels per cell and', cell_per_block,'cells per block')
    print('Feature vector length:', len(X_train[0]))
    # Use a linear SVC
    svc = LinearSVC()
    # Check the training time for the SVC
    t=time.time()
    svc.fit(X_train, y_train)
    t2 = time.time()
    print(round(t2-t, 2), 'Seconds to train SVC...')
    # Check the score of the SVC
    print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
    # Check the prediction time for a single sample
    t=time.time()",svm_pipeline.py,JunshengFu/autonomous-driving-vehicle-detection,1
"    features_train = pca.transform(features_train)
    features_test = pca.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_4(RandomizedPCA).py,dikien/Machine-Learning-Newspaper,1
"
# Does a simple gridsearch to optimize gamma and C for the given kernel
def optimizeSVM(vectors, targets, kernel):

    accuracyResults = None
    precisionResults = None
    recallResults = None

    parameters = {'gamma': [0.1, 0.01, 0.001, 0.0001], 'C': [0.1, 1, 10, 100]}

    accuracyTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""accuracy"")
    accuracyTest.fit(vectors, targets)
    accuracyResults = accuracyTest.best_params_

    precisionTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""precision"")
    precisionTest.fit(vectors, targets)
    precisionResults = precisionTest.best_params_

    recallTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""recall"")
    recallTest.fit(vectors, targets)",SVM/SVM.py,xTVaser/thesis2016,1
"	Takes a dataframe of features (X) and a dataframe of the variable to predict (y). 
	Returns a new dataframe comparing each classifier's performace on 
	the given evaluation metrics.
	'''
	rv = pd.DataFrame()

	# Classifiers to test
	classifiers = [('logistic_regression', LogisticRegression()),
					('k_nearest_neighbors', KNeighborsClassifier()),
					('decision_tree', DecisionTreeClassifier()),
					('SVM', LinearSVC()),
					('random_forest', RandomForestClassifier()),
					('boosting', GradientBoostingClassifier()),
					('bagging', BaggingClassifier())]

	index = 0
	for name, clf in classifiers:
		print name

		# Construct K-folds",pipeline/pipeline.py,jmausolf/HIV_Status,1
"		'''
			config: parameter settings
		'''
		self.config = config

	def _create_svm_object(self):
		'''
			create an svm object according to kernel type
		'''
		if self.config['kernel'] == 'linear':
			return SVC(C = self.config['C'], kernel = self.config['kernel'], \
				shrinking = self.config['shrinking'], probability = self.config['probability'], \
				tol = self.config['tol'], cache_size = self.config['cache_size'], \
				class_weight = self.config['class_weight'], verbose = self.config['verbose'], \
				max_iter = self.config['max_iter'], decision_function_shape = self.config['decision_function_shape'], \
				random_state = self.config['random_state'])
		elif self.config['kernel'] == 'poly':
			return SVC(C = self.config['C'], kernel = self.config['kernel'], \
				degree = self.config['degree'], gamma = self.config['gamma'], coef0 = self.config['coef0'], \
				shrinking = self.config['shrinking'], probability = self.config['probability'], \",robust_rescaled_svm.py,FrankTsui/robust_rescaled_svm,1
"
    def platform(self):
        return self._platform

    def isLinux(self):
        return self._platform == 'linux'

    def isMingw(self):
        return self._platform == 'mingw'

    def isMSVC(self):
        return self._platform == 'msvc'

    def isWindows(self):
        return self.isMingw() or self.isMSVC()

    def isSolaris(self):
        return self._platform == 'solaris'

    def isFreebsd(self):",platformHelper.py,TheOneRing/ninja,1
"cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100,
                                   test_size=0.2, random_state=0)

estimator = GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

title = ""Learning Curves (SVM, RBF kernel, $\gamma=0.001$)""
# SVC is more expensive so we do a lower number of CV iterations:
cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,
                                   test_size=0.2, random_state=0)
estimator = SVC(gamma=0.001)
plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

plt.show()",scikit-learn-0.17.1-1/examples/model_selection/plot_learning_curve.py,RPGOne/Skynet,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,joewalter/mne-python,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features on training set
        idx = CFS.cfs(X[train], y[train])

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_CFS.py,jundongl/PyFeaST,1
"iris = datasets.load_iris()

# we only take the first two features. We could
# avoid this ugly slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

# We create an instance of SVM and fit out data.
# We do not scale our data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
linear_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
h = .02  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),",python/ml/scikit/plot_iris.py,dkandalov/katas,1
"
def main():
    dataset = np.recfromcsv(""matches.csv"", delimiter="","")
    participantInfo = np.recfromcsv(""participantInfo.csv"", delimiter="","")
    
    #trainDataEnd = 700
    #testDataEnd = 900
    
    X, Y = formatData(dataset, participantInfo)
    """"""
    clf = svm.SVC()
    clf.fit(XTrain, YTrain)
        
    XTest, YTest = formatData(dataset[trainDataEnd:testDataEnd], participantInfo[trainDataEnd*10:])
    
    YPredicted = []
    for x in XTest:
        YPredicted.append(clf.predict([list(x)]))
    
    print accuracy_score(YTest, YPredicted)",sandbox/svm.py,aodhzhao/LoLMatchPredictor,1
"df.max()
df.min()
type(predictors)

#==============================================================================
# Model fitting part 2?
#==============================================================================

#==============================================================================
# # fit the model
# clf = svm.NuSVC()
# clf.fit(predictors, outcomes)
#
# # plot the decision function for each datapoint on the grid
# Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
# Z = Z.reshape(xx.shape)
#
# plt.imshow(Z, interpolation='nearest',
#            extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
#            origin='lower', cmap=plt.cm.PuOr_r)",Visual_Game/CreatingBaseDF.py,georgetown-analytics/nba-tracking,1
"                continue
            img_resized = mh.imresize(img, (30, 30))
            if img_resized.shape != (30, 30):
                img_resized = mh.imresize(img_resized, (30, 30))
            X.append(img_resized.reshape((900, 1)))
            y.append(target)
    X = np.array(X)
    X = X.reshape(X.shape[:2])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)
    pipeline = Pipeline([
        ('clf', SVC(kernel='rbf', gamma=0.01, C=100))
    ])
    parameters = {
        'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
        'clf__C': (0.1, 0.3, 1, 3, 10, 30),
    }
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=3, verbose=1, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    print 'Best score: %0.3f' % grid_search.best_score_
    print 'Best parameters set:'",MasteringMLWithScikit-learn/8365OS_09_Codes/scratch.py,moonbury/pythonanywhere,1
"from pre_process import pre_proc_all
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import sklearn.svm as svm
import sklearn.metrics as metrics
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def train_by_SVC_RBF(C, gamma, split_train_df, split_train_target_df):
    svc = svm.SVC(C=C, gamma=gamma)
    svc.fit(split_train_df, split_train_target_df)
    
    return svc

    
def predict_by_SVC_RBF(svc, predictor_df, file_path_save_results=None):
    predict_arr = svc.predict(predictor_df)
    
    # Convert to pandas dataframe from numpy array",Titanic/train_n_predict.py,a2takashi/kaggle-challenge,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/constraints_dses_dsessvcal/setup.py,jpzk/evopy,1
"                                                   steps=500)),
        # ('NN 500:200 dropout',
        #  skflow.TensorFlowEstimator(model_fn=dropout_model,
        #                             n_classes=10,
        #                             steps=20000)),
        # ('CNN', skflow.TensorFlowEstimator(model_fn=conv_model,
        #                                    n_classes=10,
        #                                    batch_size=100,
        #                                    steps=20000,
        #                                    learning_rate=0.001)),
        ('SVM, adj.', SVC(probability=False,
                          kernel=""rbf"",
                          C=2.8,
                          gamma=.0073,
                          cache_size=200)),
        ('SVM, linear', SVC(kernel=""linear"", C=0.025, cache_size=200)),
        ('k nn', KNeighborsClassifier(3)),
        ('Decision Tree', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest', RandomForestClassifier(n_estimators=50, n_jobs=10)),
        ('Random Forest 2', RandomForestClassifier(max_depth=5,",ML/mnist/many-classifiers/python.py,MartinThoma/algorithms,1
"# Entrenamiento con 70% de los datos:	data[140:],data[:70],target[140:],target[:70]
# Entrenamiento con 50% de los datos:	data[100:],data[:100],target[100:],target[:100]
X_train, X_test, y_train, y_test = data,data,target,target

C = 1.0  # Parametro del SVM
X = X_train
y = y_train
h = .02 

# Se utilizan 3 tipos de kernels.
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=2, C=C).fit(X, y)

# Titulos para impresion y graficas.
titles = ['SVM con kernel lineal',
          'SVM con kernel RBF',
          'SVM con kernel poligonal (grado 2)']

for i, clf in enumerate((svc, rbf_svc, poly_svc)):",Proyecto Final/ProyectoFinal- Sarcasm/my_sarcasm_is_great.py,Sealos/Sarcasm,1
"
# Split the dataset in two equal parts
x_train, x_test, y_train, y_test = cross_validation.train_test_split_sync(
    X, y, test_size=0.5, random_state=0)

# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5)
clf.fit_sync(x_train, y_train)

print(""The model is trained on the full development set."")
print(""The scores are computed on the full evaluation set."")
y_pred = clf.predict_sync(x_test)
mean_error = SyncableMetrics.compute_metrics(
    clf, precision_score, y_test, y_pred, x_test, '', '')

syncer_obj.sync()",client/python/samples/sklearn/GridSearchCrossValidation.py,mitdbg/modeldb,1
"from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.utils.multiclass import type_of_target

from helpers.log_config import log_to_info


class LinearSVCClassifier(object):
    def classify(self, mp, x_train, y_train, x_test):
        clf = LinearSVC(dual=False, verbose=False, C=1.0)
        log_to_info('Fitting a LinearSVC to labeled training data...')
        clf = clf.fit(x_train, y_train)
        log_to_info('Predicting test value')
        y_test = clf.predict(x_test)
        log_to_info('Done!')

        return y_test",code/classifiers/linear_svc_classifier.py,lukaselmer/hierarchical-paragraph-vectors,1
"        features = []
        labels = []
        for label, path_iter in label_to_paths.items():
            images = list(map(Image.file_to_features, path_iter))
            features.extend(images)
            labels.extend([label] * len(images))
        return np.concatenate(features, axis=0), np.array(labels)

    def fit_model(self):
        self.info('Training classifier ...')
        self.clf = svm.SVC(**self.PARAMS)
        self.clf.fit(*self.train)
        self.info('... done!')

    def validate_classifier(self):
        self.info('Validating classifier ...')
        score = self.clf.score(*self.test)
        self.info('... done; F1 score: {:.2%}'.format(score))

    def save(self, model_filename=get_parent_directory() + DEFAULT_MODEL_FILENAME):",digits/model.py,jeffseif/digits,1
"			else:
				classifierList.append(0)

	inputVector = passageToFeature(filePath)


	print(""Support Vector Machine:\n"")
	from sklearn import svm

	print(""	Creating...\n"")
	train1 = svm.SVC(kernel='rbf')

	print(""	Training...\n"")
	train1.fit(featureList, classifierList)

	print(""	Predicting...\n"")
	print(""	Result: ""+str(train1.predict([inputVector])))


	print(""Nueral Network:\n"")",finalproject.py,Behemyth/ForgeryML,1
"from sklearn.cross_validation import KFold
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.cross_validation import cross_val_score


def find_score(X,y, C_value, length):
   
    kf = KFold(length, n_folds=5, shuffle=True,  random_state=42)   
    svc = SVC(C = C_value, random_state=241, kernel='linear')
    score = cross_val_score(svc, X, y, cv=kf, scoring='accuracy')
    average = reduce(lambda x, y: x + y, score) / len(score) 
   
    return average

def find_best_param_c_by_cross_validation(X, y, length):
    params_c = [math.pow(10, i) for i in range(-5, 6)]
    results = []
    #for c in params_c:",week 3/part 1/task 2/task 1.py,GrimRanger/Coursera,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0706_2015_3.py,magic2du/contact_matrix,1
"model = BernoulliNB()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_1 = round(accuracy_score(Y_test, Y_pred)*100, 2)          #16.15% accuracy

model = linear_model.LogisticRegression()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_2 = round(accuracy_score(Y_test, Y_pred)*100, 2)

model = SVC()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_3 = round(accuracy_score(Y_test, Y_pred)*100, 2)

model = KNeighborsClassifier(n_neighbors = 3)           #How to know what # of neighbors to use? I think this should be 150 or however many pokemon there are?
model.fit(X_train, Y_train)                                     #66.13% accuracy
Y_pred = model.predict(X_test)
acc_4 = round(accuracy_score(Y_test, Y_pred)*100, 2)            #34.13% accuracy with 150 classes.
",Kaggle_Projects/PokemonGO predictions.py,MichaelMKKang/Projects,1
"        C_values = [1, 2, 8, 32, 128, 512, 2048, 8192]
        gamma_values = [0.001,0.01,0.05,0.1,0.3,0.5,0.7]

        #C_values = [2]
        #gamma_values = [0.5]

        best_C = 0
        best_G = 0
        best_percentage = 0

        #clf = svm.SVC(C=2.0,gamma=0.5)

        C_range = np.logspace(-2, 10, num=13, base=2)
        gamma_range = np.logspace(-5, 1, num=7, base=10)
        param_grid = dict(gamma=gamma_range, C=C_range)
        cv = StratifiedShuffleSplit(trainingLabels, n_iter=3, test_size=0.11, random_state=42)
        grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
        grid.fit(trainingSet, trainingLabels)
        #C_range = np.logspace(-1, 1, num=2, base=2)
        #gamma_range = np.logspace(-1, 1, num=2, base=10)",Study/CrossValidate.py,JessMcintosh/EMG-classifier,1
"                count += 1
    else:
        for ele in unique_y:
            y[y == ele] = count
            count += 1

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx = OSFS.OSFS(X[train], y[train])

        # obtain the dataset on the selected features
        selected_features = X[:, idx]
",PyFeaST/example/test_OSFS.py,jundongl/PyFeaST,1
"    AISTATS, 2005.
    """"""

    def __init__(
        self,             gamma_a     = 1.0,  gamma_i = 1.0, gamma_k  = 1.0,
        sparsify = 'kNN', n_neighbors = 10,   radius  = 1.0, reweight = 'rbf',
        t        = None,  normed      = True, p       = 1
    ):

        super(LapSVC, self).__init__(
            estimator   = BinaryLapSVC(), sparsify = sparsify,
            n_neighbors = n_neighbors,    radius   = radius,
            reweight    = reweight,       t        = t,
            normed      = normed
        )

        self.params           = {
            'gamma_a': gamma_a, 'gamma_i': gamma_i, 'gamma_k': gamma_k, 'p': p
        }
",gbssl/laplacian_svm.py,Y-oHr-N/TextCategorization,1
"
# import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

#Train
svc = SVC(probability=True)
svc.fit(X_train, y_train)

y_true = y_test
y_pred = svc.predict(X_test)
y_score = svc.predict_proba(X_test)

#Pickle model
joblib.dump(svc, os.path.join(models_path, 'classifier_without_feature_importances_model.pkl'))
#Pickle y_true",tests/sklearn_scripts/dummy_classification_without_feature_importances.py,edublancas/sklearn-model-evaluation,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Trog Build Dependencies/Python26/Lib/email/test/test_email_renamed.py,DecipherOne/Troglodyte,1
"def evaluate_cross_validation(clf, X, y, K):
    # create a k-fold cross validation iterator
    cv = KFold(len(y), K, shuffle=True, random_state=0)
    # by default the score used is the one returned by score method of the estimator (accuracy)
    scores = cross_val_score(clf, X, y, cv=cv)
    print (scores)
    print (""Mean score: {0:.3f} (+/-{1:.3f})"".format(
        np.mean(scores), sem(scores)))


svc_1 = SVC(kernel='linear')
faces = datasets.fetch_olivetti_faces()
faces.keys()
for i in range(10):
    face = faces.images[i]
    subplot(1, 10, i + 1)
    imshow(face.reshape((64, 64)), cmap='gray')
    axis('off')

",smilely_face detection.py,swarnkarkush21/Smart-Iot-home,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.NuSVC(gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf).export()
# output = Porter(clf, language='java').export()
print(output)

""""""
class Brain {
",examples/classifier/NuSVC/java/basics.py,nok/sklearn-porter,1
"from sklearn.pipeline import Pipeline  # noqa
from sklearn.cross_validation import cross_val_score, ShuffleSplit  # noqa
from mne.decoding import EpochsVectorizer, FilterEstimator  # noqa


scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
vectorizer = EpochsVectorizer()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                              ('scaler', scaler), ('svm', clf)])

data_picks = mne.pick_types(rt_epochs.info, meg='grad', eeg=False, eog=True,
                            stim=False, exclude=raw.info['bads'])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):
",examples/realtime/plot_compute_rt_decoder.py,leggitta/mne-python,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix(y)
    n_samples, n_features = X.shape

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight = RFS.rfs(X[train, :], Y[train, :], gamma=0.1)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",PyFeaST/example/test_RFS.py,jundongl/PyFeaST,1
"from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=7)

#train the tree model
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

tree = DecisionTreeClassifier(max_depth=max_depth, random_state=2)
knn = KNeighborsClassifier(n_neighbors=n_neighbors)
svc = SVC(C=C,kernel=kernel)

for max_depth in range(9,-1, -1):
    print('\nmax_depth = ', max_depth)
    benchmark(tree, X_train, X_test, y_train, y_test, 'Tree')
    #if score < 83.607:
        #print('here is the answer: ', score)

benchmark(knn, X_train, X_test, y_train, y_test, 'KNeighbors')
",Module6/assignment4.py,jeffmkw/DAT210x-Lab,1
"	#if y_cv != None:
		#print 'Accuracy in cv set: %f' % clfr.score(x_cv, y_cv)
	
	return clfr
	
def SupportVectorMachine(x_train, y_train, x_cv, y_cv,sw=None):
	""""""
	Support Vector Machine
	""""""
	#print ""Classifier: Support Vector Machine""
	clfr = SVC(probability=False)
	if sw != None:
		clfr.fit(x_train, y_train,sample_weight=sw)
	else:
		clfr.fit(x_train, y_train)
	#print 'Accuracy in training set: %f' % clfr.score(x_train, y_train)
	#if y_cv != None:
		#print 'Accuracy in cv set: %f' % clfr.score(x_cv, y_cv)
	
	return clfr",source/Classify.py,tbs1980/Kaggle_DecMeg2014,1
"                    print(""%0.3f (+/-%0.03f) for %r""
                          % (mean_score, scores.std() / 2, params))
                print()
                y_pred = gcv.predict(X_val)
                print ""Accuracy score: "" , metrics.accuracy_score(y_val, y_pred, normalize=True)
        elif (algo == 'lsvm' or algo == 'ksvm'):
            tuned_params = [{'kernel':['linear'], 'C': [1, 10, 1000, 10000]},
                    {'kernel':['rbf'], 'gamma':[1e-3, 1e-4], 'C': [1,10,1000,10000]}]
            for score in scores:
                print(""Tuning hyperparameter space for metric %s "" % score)
                gcv = GridSearchCV(svm.SVC(C=1), tuned_params, cv=5, scoring=score)
                gcv.fit(X_train, y_train)
                print()
                print(gcv.best_estimator_)
                print()
                for params, mean_score, scores in gcv.grid_scores_:
                    print(""%0.3f (+/-%0.03f) for %r""
                          % (mean_score, scores.std() / 2, params))
                print()
                y_pred = gcv.predict(X_val)",python_scripts_from_net/submit.py,sankar-mukherjee/DecMeg2014,1
"class EvalLogReg(object):
    """"""docstring for EvalTree""""""
    b_preict = []
    def __init__(self):
        super(EvalLogReg, self).__init__()


  

    def init_classifier(self):
        # clf = svm.SVC(kernel = 'rbf', gamma=self.gamma_value, C=self.c_value)
        # print ""SVM configuration... \n\n"", clf
        # clf = LogisticRegression()
        clf = SGDClassifier(loss=""hinge"", penalty=""l2"")
        return clf



    def fit_train_data(self, clf, a_train, b_train):
        # clf = svm.SVC(kernel = 'rbf', gamma=gamma_value, C=c_value)",implementation/evaluation/logReg.py,imink/UCL_COMPIG15_Project,1
"#           CV           CV (Splitter)
#       /   |   \
#      0    1    2       Folds (Slicer)
#           |
#        Methods         Methods (Splitter)
#    /           \
# SVM(linear)  SVM(rbf)  Classifiers (Estimator)

from sklearn.svm import SVC
from epac import Perms, CV, Methods
perms_cv_svm = Perms(CV(Methods(*[SVC(kernel=""linear""), SVC(kernel=""rbf"")])))
perms_cv_svm.run(X=X, y=y)
perms_cv_svm.reduce()


# Run with soma-workflow for multi-processes
from epac import SomaWorkflowEngine
sfw_engine = SomaWorkflowEngine(tree_root=perms_cv_svm,
                                num_processes=2,
                                )",examples/small_toy.py,neurospin/pylearn-epac,1
"                               scoring=scoring, verbose=verbose, n_jobs=njobs)
    return grid_search


def make_pipeline(feat_un, model_name, feat_dict, cfg):

    if (model_name == 'logistic_regression'):
        model = LogisticRegression(C=1e9, penalty='l1')

    elif (model_name == 'svc'):
        model = SVC()

    elif (model_name == 'naive_bayes'):
        model = MultinomialNB()

    feat_dict = make_param_entries(model_name, 'model', feat_dict, cfg)
    pipe = Pipeline(steps=[('features', feat_un), ('model', model)])

    return pipe, feat_dict
",src/analyze/run_model.py,joelmpiper/bill_taxonomy,1
"import datasets

if not datasets.Quizbowl.loaded:
    datasets.loadQuizbowl()

print '\n\nRUNNING ON EASY DATA\n'
    
print 'training oaa'
X = datasets.QuizbowlSmall.X
Y = datasets.QuizbowlSmall.Y
oaa = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y)
print 'predicting oaa'
oaaDevPred = oaa.predict(datasets.QuizbowlSmall.Xde)
print 'error = %g' % mean(oaaDevPred != datasets.QuizbowlSmall.Yde)

print 'training ava'
ava = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y)
print 'predicting ava'
avaDevPred = ava.predict(datasets.QuizbowlSmall.Xde)
print 'error = %g' % mean(avaDevPred != datasets.QuizbowlSmall.Yde)",projects/p2/quizbowl.py,hal3/ciml,1
"        self.assertIs(df.grid_search.ParameterGrid, gs.ParameterGrid)
        self.assertIs(df.grid_search.ParameterSampler, gs.ParameterSampler)
        self.assertIs(df.grid_search.RandomizedSearchCV, gs.RandomizedSearchCV)

    def test_grid_search(self):
        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                             'C': [1, 10, 100]},
                            {'kernel': ['linear'], 'C': [1, 10, 100]}]

        df = pdml.ModelFrame(datasets.load_digits())
        cv = df.grid_search.GridSearchCV(df.svm.SVC(C=1), tuned_parameters, cv=5)

        with tm.RNGContext(1):
            df.fit(cv)

        result = df.grid_search.describe(cv)
        expected = pd.DataFrame({'mean': [0.97161937, 0.9476906, 0.97273233, 0.95937674, 0.97273233,
                                          0.96271564, 0.94936004, 0.94936004, 0.94936004],
                                 'std': [0.01546977, 0.0221161, 0.01406514, 0.02295168, 0.01406514,
                                         0.01779749, 0.01911084, 0.01911084, 0.01911084],",pandas_ml/skaccessors/test/test_grid_search.py,sinhrks/pandas-ml,1
"


test_frac = 0.10
count = len(X_full)
X = X_full.iloc[-int(count*test_frac):]
y = y_full.iloc[-int(count*test_frac):]
  
print ""X"", X.shape , ""y"", y.shape
  
lsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
  
X_new = model.transform(X)

forest = ExtraTreesClassifier(n_estimators=250,
                              random_state=0)

forest.fit(X, y)
importances = forest.feature_importances_",general studies/get feature importance.py,Diyago/Machine-Learning-scripts,1
"                        n_entities += 1
            logging.info(""Combined {} docs, {} sentences, {} entities"".format(n_docs, n_sentences, n_entities))
            base_result.save(options.models + "".pickle"")
        elif options.action == ""savetocorpus"":
            base_result.corpus.save(options.output + "".pickle"")
        elif options.action == ""train_ensemble"":
            pipeline = Pipeline(
                [
                    #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0001, n_iter=5, random_state=42)),
                    #('clf', SGDClassifier())
                     #('clf', svm.NuSVC(nu=0.01 ))
                    # ('clf', RandomForestClassifier(class_weight={False:1, True:1}, n_jobs=-1, criterion=""entropy"", warm_start=True))
                    #('clf', tree.DecisionTreeClassifier(criterion=""entropy"")),
                     #('clf', MultinomialNB())
                    #('clf', GaussianNB())
                    ('clf', svm.SVC(kernel=""rbf"", degree=2, C=1))
                    #('clf', DummyClassifier(strategy=""constant"", constant=True))
                ])
            print pipeline
            base_result.train_ensemble(pipeline, options.models, options.etype)",src/evaluate.py,AndreLamurias/IBRel,1
"

class TestHostAddrSVCParse(object):
    """"""
    Unit tests for lib.packet.host_addr.HostAddrSVC._parse
    """"""
    @patch(""lib.packet.host_addr.Raw"", autospec=True)
    @patch(""lib.packet.host_addr.HostAddrSVC.__init__"", autospec=True,
           return_value=None)
    def test(self, init, raw):
        inst = HostAddrSVC("""")
        pop = raw.return_value.pop
        pop.return_value = bytes.fromhex(""01 0f"")
        # Call
        inst._parse(""raw"")
        # Tests
        raw.assert_called_once_with(""raw"", ""HostAddrSVC"", inst.LEN)
        ntools.eq_(inst.addr, 0x010f)

",python/test/lib/packet/host_addr_test.py,dmpiergiacomo/scion,1
"    pointList = np.c_[np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)]
    return pointList

def applyFunction(points):
    return np.sign(points[:,1]-points[:,0]+0.25*np.sin(np.pi * points[:,0]))

def doAssignment13():
    experiments = 1000
    gama = 1.5
    numPoints = 100
    clf = svm.SVC(C= np.inf , kernel=""rbf"", coef0=1, gamma=gama)
    Ein0 = 0
    for i in range(experiments):    
        X = getPoints(numPoints)
        y = applyFunction(X)
        clf.fit(X,y)
        #print(clf.score(X,y))
        if(1-clf.score(X,y)==0):
            #print(""here"")
            Ein0 += 1",Final/t17.py,pramodh-bn/learn-data-edx,1
"def make_classifiers(method, balanced, labels, selectors=None, columns=None, random_state=None):
    estimators = {}
    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression()
    elif 'svm' in method:
        estimator = SVC(probability=True)
    elif 'rf' in method:
        estimator = RandomForestClassifier()
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})
",interactome_predict.py,daniaki/ppi_wrangler,1
"		train += [(fixClipName(k), classLabel) for k, v in d.items() if v == '1']
		test += [(fixClipName(k), classLabel) for k, v in d.items() if v == '2']

	return (train, test)

splits = map(read_split, range(3))
slice_kernel = lambda inds1, inds2: all_k[np.ix_(map(allClips.index, inds1), map(allClips.index, inds2))]
REG_C = 1.0

def svm_train_test(train_k, test_k, ytrain, REG_C):
	model = SVC(kernel = 'precomputed', C = REG_C, max_iter = 10000)
	model.fit(train_k, ytrain)

	flatten = lambda ls: list(itertools.chain(*ls))
	train_conf, test_conf = map(flatten, map(model.decision_function, [train_k, test_k]))
	return train_conf, test_conf

def one_vs_rest(SPLIT_IND):
	calc_accuracy = lambda chosen, true: sum([int(true[i] == chosen[i]) for i in range(len(chosen))]) / float(len(chosen))
	partition = lambda f, ls: (filter(f, ls), list(itertools.ifilterfalse(f, ls)))",repro/hmdb-51/classify.py,vadimkantorov/cvpr2014,1
"
    dataset = DataSet(data=numpy.asarray(data),
                      targets=numpy.fromiter([1 if x == 'y' else 0 for x in classificationTargets], numpy.int),
                      target_names=['bad', 'good'], feature_names=headers,
                      regression_targets=numpy.fromiter(regressionTargets, numpy.float), row_labels=labels)

    return dataset


def classifierExperiments(dataset):
    svc = svm.SVC(kernel='linear')

    scores = cross_validation.cross_val_score(svc, dataset.data, dataset.targets, cv=10)

    print ""Linear kernel""
    for score in scores:
        print ""Score: {0}"".format(score)

    print ""Mean {0:.2f} +/- {1:.2f}"".format(scores.mean(), scores.std() / 2)
",training/python/train.py,ktrnka/droidling,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",pylibs/email/test/test_email_renamed.py,j5shi/Thruster,1
"        gr1_idx = data[data.DX_Group == gr[0]].index.values
        gr2_idx = data[data.DX_Group == gr[1]].index.values
        
        gr1_f = X[gr1_idx, :]
        gr2_f = X[gr2_idx, :]
        
        x = X[np.concatenate((gr1_idx, gr2_idx))]
        y = np.ones(len(x))
        y[len(y) - len(gr2_idx):] = 0
    
        estim = svm.SVC(kernel='linear')
        sss = cross_validation.StratifiedShuffleSplit(y,
                                                      n_iter=nb_iter,
                                                      test_size=0.2)
        # 1000 runs with randoms 80% / 20% : StratifiedShuffleSplit
        counter = 0
        for train, test in sss:
            Xtrain, Xtest = x[train], x[test]
            Ytrain, Ytest = y[train], y[test]
            Yscore = estim.fit(Xtrain,Ytrain)",learn_adni_fdg_pet_cluster.py,mrahim/adni_fdg_pet_analysis,1
"                                      stop_words=""english"",
                                      strip_accents=""unicode"",
                                      dtype=np.float32,
                                      decode_error=""replace"")),
            (""scaling"", Normalizer())
        ])

    X = transformer.fit_transform(records)
    y = np.array([r[0][""decision""] for r in records])

    grid = GridSearchCV(LinearSVC(),
                        param_grid={""C"": np.linspace(start=0.1, stop=1.0,
                                                     num=100)},
                        scoring=""accuracy"", cv=5, verbose=3)
    grid.fit(X, y)

    return Pipeline([(""transformer"", transformer),
                     (""classifier"", grid.best_estimator_)])

",inspire/modules/predicter/arxiv.py,jmartinm/inspire-next,1
"    print(""Run"", run)
    train_file = '{}/train_{}.txt'.format(args.input_dir, run)
    test_file = '{}/test_{}.txt'.format(args.input_dir, run)
    out_file = '{}/output_{}.txt'.format(out_dir, run)

    X_train, X_test, Y_train, Y_test, classes_ = get_data(train_file,
                                                          test_file)

    clf = make_pipeline(TfidfVectorizer(analyzer=split,
                                        stop_words=stopwords),
                        LinearSVC())

    clf = OneVsRestClassifier(clf, n_jobs=-1)

    clf.fit(X_train, Y_train)
    Y_pred = clf.predict(X_test)

    print_results(Y_test, Y_pred, classes_, open(out_file, 'w'))",embem/machinelearning/br_classifier.py,NLeSC/embodied-emotions-scripts,1
"print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)

# we create 40 separable points
np.random.seed(0)
X = valid_dataset
Y = valid_labels

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-500, 3000)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",py/plot_separating_hyperplane.py,juanprietob/ExtractMSLesion,1
"print(""------Building models using balanced training data------"")
print(""========================================================"")

# train and test the SVM

parameter_space_bal = {
    'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['auto', 1e-3, 1e-4],
    'C': [0.01, .1, 1, 10, 100, 1000], 'class_weight': [None]}

print(""Building balanced SVM"")
SVM_bal = RandomizedSearchCV(SVC(C=1), parameter_space_bal, cv=10,
        scoring='recall_weighted', iid=True)
print(""fitting balanced SVM"")
SVM_bal.fit(xbaltrain, ybaltrain)

print(""Hyperparameters for balanced SVM found:"")
print(SVM_bal.best_params_)

print(""getting predictions for balanced SVM"")
y_pred_svm_bal = SVM_bal.predict(xtest)",adaboost.py,lkev/wt-fdd,1
"    elapsed_time = time.time() - start_time
    print ""Took %d s"" % (elapsed_time)
    
    print ""Writing on disk Gram matrix""
    start_time = time.time()
    np.savetxt(out_file, gram,fmt='%1.6f')
    elapsed_time = time.time() - start_time
    print ""Took %d s"" % (elapsed_time)
    
    #Learner
    clf = svm.SVC(C=10,kernel='precomputed')
    clf.fit(gram, y)
    scores = cross_validation.cross_val_score(clf, gram, y, cv=10)
    print(""Accuracy: %0.4f (+/- %0.4f)"" % (scores.mean(), scores.std() * 2))
    """"""
    #induce a predictive model
    from sklearn.linear_model import SGDClassifier
    predictor = SGDClassifier()
    
    from sklearn import cross_validation",scripts/ODDKernel_example_Tesselli.py,nickgentoo/scikit-learn-graph,1
"        return LogisticRegression(solver='liblinear', C=self.C, penalty=self.penalty)


class SVMDecisionMaker(_BinaryRelevanceDecisionMaker):
    def __init__(self, C=1., penalty='l2'):
        super(SVMDecisionMaker, self).__init__()
        self.C = C
        self.penalty = penalty

    def _init_model(self):
        return LinearSVC(C=self.C, dual=False, penalty=self.penalty, loss='squared_hinge')


class NaiveBayesDecisionMaker(_BinaryRelevanceDecisionMaker):
    def _init_model(self):
        return GaussianNB()


class PerceptronDecisionMaker(_BinaryRelevanceDecisionMaker):
    def _init_model(self):",src/toolkit/decision.py,matthiasplappert/motion_classification,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,rajul/mne-python,1
"    cv = cross_validation.KFold(n=data.shape[0], n_folds=10, shuffle=True)
    scores = cross_validation.cross_val_score(model, data, labels, cv=cv, scoring=scoring, n_jobs=-1)

    return scores

def fitSVM(data, labels, kernel='rbf'):
    ''' Support Vector Classification. The implementations is a based on libsvm.
    http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    defualt kernel is rbf using one v one approach'''

    model = svm.SVC(C=10, decision_function_shape='ovr', kernel=kernel)
    model = model.fit(data, labels)

    return model

def genSVM(c, kernel):
    ''' Generate an SVM with a specified kernal and C value '''

    clf = svm.SVC(C=c, decision_function_shape='ovr', kernel=kernel)
",SVM-Model.py,sbonner0/DeepTopologyClassification,1
"X = my_data[:,1:36]
y = my_data[:,37]

print(len(X),len(y))

X = X[0:86400*10]
y = y[0:86400*10]

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.7, random_state=0)

#clf = svm.SVC()
clf = svm.LinearSVC()

clf.fit(X_train, y_train)
clf.predict(X_test)

print clf.score(X_test,y_test)",data/tulum2010/svm.py,ppegusii/cs689-final,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_01_2014_server2.py,magic2du/contact_matrix,1
"        train_x = data[:n]
        train_y = target[:n]
        exam_x = data[n:]
        exam_y = target[n:]
    else:
        train_x = data[n:]
        train_y = target[n:]
        exam_x = data[:n]
        exam_y = target[:n]

    svm_clf = svm.SVC(kernel='linear')
    svm_clf.fit(train_x, train_y)

    gmdh = MultilayerGMDH(ref_functions=('linear_cov',),
                          criterion_type='test_bias',
                          feature_names=iris.feature_names,
                          criterion_minimum_width=5,
                          admix_features=True,
                          max_layer_count=50,
                          normalize=True,",examples/gmdh_iris_recognition.py,kvoyager/GmdhPy,1
"    MIN_DF = 2
    vectorizer_binary = CountVectorizer(stop_words=stopWords, min_df=MIN_DF, binary=True, ngram_range=(1, 3))
    #vectorizer_binary = CountVectorizer(stop_words=stopWords, min_df=MIN_DF, binary=True, ngram_range=(1, 2))
    #vectorizer_binary = CountVectorizer(stop_words=stopWords, min_df=MIN_DF, binary=True, ngram_range=(1, 3))
    vectorizer_tfidf = TfidfVectorizer(stop_words=stopWords, min_df=MIN_DF, ngram_range=(1, 3))#, sublinear_tf=True)
    vectorizer = vectorizer_tfidf
    #vectorizer = vectorizer_binary
    print(vectorizer)

    clf = linear_model.LogisticRegression()
    #clf = svm.LinearSVC()
    #clf = linear_model.LogisticRegression(penalty='l2', C=1.2)

    kf = cross_validation.KFold(n=len(target), n_folds=5, shuffle=True)

    # try the idea of calculating a cross entropy score per fold
    cross_entropy_errors_test_by_fold = np.zeros(len(kf))
    cross_entropy_errors_train_by_fold = np.zeros(len(kf))
    precisions_by_fold = np.zeros(len(kf))
    for i, (train_rows, test_rows) in enumerate(kf):",learn1_experiments_tfidfproper.py,ianozsvald/social_media_brand_disambiguator,1
"    # X_test = ch2.transform(X_test)

    # feature_names = [word[i] for i
    #                  in ch2.get_support(indices=True)]
    #

    # for i in feature_names:
    #     print i.encode('utf-8')
    # feature_names = np.asarray(feature_names)
    # print feature_names
    # clf = LinearSVC(penalty=""l1"", dual=False, tol=1e-3)

    # clf.fit(X_train, y_train)
    clf = SGDClassifier(loss=""log"", penalty='l1')
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    prob = clf.predict_proba(X_test[0])
    print prob
    X=[""市场经济复苏，互联网公司蓬勃发展"",""世纪大战终于开启，勇士引得第73胜""]
    Y=['1','0']",TopicalCrawl/TopicalCrawl/classifier/build_dict.py,actlea/TopicalCrawler,1
"            ky = normalize_km(np.dot(y, y.T))
        w = ALIGNF(kernel_list, ky)
        res[:,t] = w
    return res


def svm(train_km, test_km, train_y, test_y):
    # train on train_km and train_y, predict on test_km
    # return prediction
    para_grid ={'C':[1e-2, 1e-1, 1, 10, 100]}
    svc = SVC(kernel='precomputed')
    clf = grid_search.GridSearchCV(svc, para_grid)
    clf.fit(train_km, train_y)
    pred = clf.predict(test_km)
    # for multi-class task, use acc other than F1
    #return f1_score(test_y, pred)
    return accuracy_score(test_y, pred)

def ALIGNFSOFT(kernel_list, ky, y, test_fold, tags):
    # Find best upper bound in CV and train on whole data",svm_code/run_mkl_noise.py,aalto-ics-kepaco/softALIGNF,1
"#feature reduction (on HOG part)
gain, j = mutual_info_classif(data[:, 8:-1], data[:, -1], discrete_features='auto', n_neighbors=3, copy=True, random_state=None), 0
for i in np.arange(len(gain)):
	if gain[i] <= 0.001:
		data = np.delete(data, 8+i-j, 1)
		j += 1

X_train, X_test, y_train, y_test = train_test_split(data[:, 0:-1], data[:, -1], test_size = 0.4, random_state = 0)

start = timer()
clf = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
y_pred = clf.predict(X_test)
end = timer()

print(""Confusion Matrix: \n"")
print(confusion_matrix(y_test, y_pred))

target_names = ['Helmet', 'No Helmet']
print(""\n\nClassification Report: \n"")
print(""Accuracy: %s"" % round(accuracy_score(y_test, y_pred), 4))",Holdout/SVM.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
         verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS",projects/scikit-learn-master/sklearn/metrics/classification.py,DailyActie/Surrogate-Model,1
"
    in_fnames = args.input
    data, target, test, test_names = readDataSet(in_fnames)
    
    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.2, random_state=42)

    #print X_train.shape, y_train.shape, y_train
    #print X_test.shape, y_test.shape, y_test
    #print ""data set balance: "", sum(target), float(len(target)-sum(target)) / len(target)

    #clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    #print clf.score(X_test, y_test)  
    
    """"""
    clf = svm.SVC(kernel='linear', C=1)
    scores = cross_validation.cross_val_score(clf, data, target, cv=10)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    """"""
    '''
    myRF_AUC(data, target)",sklearnClassifiers/simpleClassifier2.py,rampasek/seizure-prediction,1
"            max_depth=max_depth,
            criterion=c)
        for c in [""gini"", ""entropy""]
        for max_depth in [None, 10, 20]
    ]

    models = lr_models  + rf_models

    if args.use_svm_models:
        svm_models = [
            LinearSVC(
                penalty='l1',
                dual=False,
                C=c,
                fit_intercept=intercept,
                class_weight=class_weight)
            for c in Cs
        ]
        models += svm_models
",feature_selection.py,iskandr/mhcpred,1
"acc['svm']=N.zeros(len(ncomp))
acc['rbf']=N.zeros(len(ncomp))
acc['lr']=N.zeros(len(ncomp))
pred={}
for c in range(len(ncomp)):
    pred['svm']=N.zeros(len(train_labels))
    pred['rbf']=N.zeros(len(train_labels))
    pred['lr']=N.zeros(len(train_labels))
    data=N.genfromtxt(melodic_dir+'datarun1_icarun2_%dcomp.txt'%ncomp[c])
    for train,test in loo:
        clf=LinearSVC(C=params['svm']['C'])
        clf.fit(data[train],labels[train])
        pred['svm'][test]=clf.predict(data[test])
        clf=SVC(C=params['rbf']['C'],gamma=params['rbf']['gamma'])
        clf.fit(data[train],labels[train])
        pred['rbf'][test]=clf.predict(data[test])
        clf=LogisticRegression(C=params['lr']['penalty'],penalty='l2')
        clf.fit(data[train],labels[train])
        pred['lr'][test]=clf.predict(data[test])
        ",openfmri_paper/classify_task_ICA_randperm.py,poldrack/openfmri,1
"             feat=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,2))),
            dict(name=""tfidf_ng3"",
             feat=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,3))),
           ]

# classifiers
classifiers = [
               dict(name=""nb"", clf=MultinomialNB()),
               dict(name=""bnb"", clf=BernoulliNB(binarize=0.5)),
               dict(name=""svm"",
                    clf=LinearSVC(loss='l2', penalty=""l2"",
                                  dual=False, tol=1e-3)),
#               dict(name=""forest30"",
#                    clf=RandomForestClassifier(n_estimators=30,
#                                               random_state=123,
#                                               verbose=3))]
              ]

# combinations to repeat
repeat = dict() #data=""2-unbalanced"", feat=""tfidf_ng3"")",python/experiments_acl2013.py,mohamedadaly/labr,1
"def gradient_boosting_classifier(train_x, train_y):
    from sklearn.ensemble import GradientBoostingClassifier
    model = GradientBoostingClassifier(n_estimators=200)
    model.fit(train_x, train_y)
    return model


# SVM Classifier
def svm_classifier(train_x, train_y):
    from sklearn.svm import SVC
    model = SVC(kernel='rbf', probability=True)
    model.fit(train_x, train_y)
    return model

# SVM Classifier using cross validation
def svm_cross_validation(train_x, train_y):
    from sklearn.grid_search import GridSearchCV
    from sklearn.svm import SVC
    model = SVC(kernel='rbf', probability=True)
    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}",knn.py,nanshihui/ipProxyDec,1
"
sklearn_knn_classifier_source = \
    """"""from sklearn.neighbors import KNeighborsClassifier

base_learner = KNeighborsClassifier()
""""""

sklearn_svm_classifier_source = \
    """"""from sklearn.svm import SVC

base_learner = SVC(random_state=8)
""""""

sklearn_gaussian_nb_source = \
    """"""from sklearn.naive_bayes import GaussianNB

base_learner = GaussianNB()
""""""

sklearn_adaboost_classifier_source = \",xcessiv/presets/learnersource.py,reiinakano/xcessiv,1
"    def init_model(self):
        # todo: parameterise hyperparams
        hparams = {
            'C': 0.01,
            'penalty': 'l2',
            'loss': 'hinge',
            'class_weight': 'balanced',
            'fit_intercept': True
        }

        return LinearSVC(**hparams)

    @staticmethod
    def iter_pairwise_instances_with_sampling(docs, sampler, limit):
        toggle = True

        for doc in docs:
            for chain in doc.chains:
                resolution = chain.mentions[0].resolution
                resolution = resolution.id if resolution else None",nel/learn/ranking.py,wikilinks/nel,1
"# l1 data (only 5 informative features)
X_1, y_1 = datasets.make_classification(n_samples=n_samples,
                                        n_features=n_features, n_informative=5,
                                        random_state=1)

# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features / 5)

clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
                       tol=1e-3),
             np.logspace(-2.3, -1.3, 10), X_1, y_1),
            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
                       tol=1e-4),
             np.logspace(-4.5, -2, 10), X_2, y_2)]

colors = ['navy', 'cyan', 'darkorange']
lw = 2
",projects/scikit-learn-master/examples/svm/plot_svm_scale_c.py,DailyActie/Surrogate-Model,1
"from sklearn.dummy import DummyClassifier
from sklearn.metrics import f1_score, accuracy_score
from sklearn.svm import LinearSVC


baseline_classifiers = [DummyClassifier(strategy=""most_frequent""), DummyClassifier(strategy=""stratified"")]
C_range = [0.01, 0.1, 0.3, 1, 3, 10, 15, 30, 50, 80, 100, 300, 1000]


def linear_svg(C):
    return LinearSVC(C=C)

classifiers = baseline_classifiers
classifiers.extend(map(linear_svg, C_range))


def fit(classifier):
    classifier.fit(features_train, label_train)
    predicted = classifier.predict(features_test)
    accuracy = accuracy_score(label_test, predicted)",like_prediction.py,rux-pizza/discourse-analysis,1
"stacking_create_training_set_silk('ensemble_silk_output_raw_n%d.txt' %N,'training_set_silk_n%d.csv' %N, gold_standard_name, N)

#read it and make machine learning on it

data = pd.read_csv('training_set_silk_n%d.csv' %N)

X = data.values[:,2:(N+2)] #x variables
y = np.array(data['y']) #class variables

#fit an SVM with rbf kernel
clf = SVC( kernel = 'rbf',cache_size = 1000)
#parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
gs_rbf.fit(X,y)
#save the output
output = np.reshape(gs_rbf.predict(X),(len(data),1))

#dump it to file",src/old_core/ensemble_silk_stacking.py,enricopal/STEM,1
"
from sklearn.preprocessing import PolynomialFeatures
from sklearn.kernel_approximation import Nystroem
from sklearn.pipeline import Pipeline

names = [""KNN (3)"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""FDA"",
         ""QDA"", ""Linear BPM"", ""Quadratic BPM"", ""Cubic BPM"", ""Nystron BPM""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    BayesPointMachine(),
    Pipeline([('poly2', PolynomialFeatures(degree=2)), ('bpm', BayesPointMachine())]),",plot_classifier_comparison.py,IRC-SPHERE/sklearn-Infer.NET-wrapper,1
"                                  ""my_data_processed/acceleration_train_y.csv"",
                                  ""my_data_processed/acceleration_train_z.csv"",
                                  ""my_data_processed/gyroscope_train_x.csv"",
                                  ""my_data_processed/gyroscope_train_y.csv"",
                                  ""my_data_processed/gyroscope_train_z.csv"")

data = ndarray(shape=(len(data), 39), dtype=float, buffer=np.asanyarray(data))
data = data[:, 0:21]
target = ndarray(shape=(len(target),), dtype=int, buffer=np.asanyarray(target))

clf = svm.SVC()
clf.fit(data, target)

target, data = load_data_enhanced(""my_data_processed/activity_test.csv"",
                                  ""my_data_processed/acceleration_test_x.csv"",
                                  ""my_data_processed/acceleration_test_y.csv"",
                                  ""my_data_processed/acceleration_test_z.csv"",
                                  ""my_data_processed/gyroscope_test_x.csv"",
                                  ""my_data_processed/gyroscope_test_y.csv"",
                                  ""my_data_processed/gyroscope_test_z.csv"")",Activity_and_Context_Recognition/Classifier/test_dataset_enhanced_acceleration.py,VizLoreLabs/LCI-FIC2-SE,1
"# change endianess
def i_SETEND(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    internals['endianstate'] = -1 if i.set_bigend else 1

# event hint
def i_SEV(i,fmap):
    fmap[pc] = fmap(pc+i.length)

# supervisor call
def i_SVC(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    logger.info('call to supervisor is unsupported')

def i_SWP(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    Rt,Rt2,Rn = i.operands
    data = fmap(__mem(Rn,32))
    fmap[__mem(Rn,32)] = fmap(Rt2)
    fmap[Rt] = data",amoco/arch/arm/v7/asm.py,bdcht/amoco,1
"
'''
scores = cv.cross_val_score(
    a, tmpX, y,
    cv=5,
    scoring = 'roc_auc',
    n_jobs = 1,
    verbose=1)
print(np.average(scores))

machine = svm.NuSVC(nu=0.25,
    kernel='linear',
    verbose=False,
    probability=False)
machine.fit(tmpX[:60,], y[:60])
threshold = np.min(np.abs(machine.coef_)) + (np.max(np.abs(machine.coef_)) - np.min(np.abs(machine.coef_))) * 0.8
np.arange(machine.coef_.shape[1])[(abs(machine.coef_) > threshold).flatten()]


local_X = tmpX[train,]",plot_ratboost.py,adrinjalali/Network-Classifier,1
"from load_datasets import *
import argparse
import collections
    
def get_classifier(name, vectorizer):
  if name == 'logreg':
    return linear_model.LogisticRegression(fit_intercept=True)
  if name == 'random_forest':
    return ensemble.RandomForestClassifier(n_estimators=1000, random_state=1, max_depth=5, n_jobs=10)
  if name == 'svm':
    return svm.SVC(probability=True, kernel='rbf', C=10,gamma=0.001)
  if name == 'tree':
    return tree.DecisionTreeClassifier(random_state=1)
  if name == 'neighbors':
    return neighbors.KNeighborsClassifier()
  if name == 'embforest':
    return embedding_forest.EmbeddingForest(vectorizer)

def main():
  parser = argparse.ArgumentParser(description='Evaluate some explanations')",data_trusting.py,marcotcr/lime-experiments,1
"            maxWeights[wx > maxWeights] = wx
          returnDict[target] = predictions
      elif self.partitionPredictor == 'svm':
        partitions = self.__amsc[index].Partitions(self.simplification)
        labels = np.zeros(self.X.shape[0])
        for idx,(key,indices) in enumerate(partitions.iteritems()):
          labels[np.array(indices)] = idx
        # In order to make this deterministic for testing purposes, let's fix
        # the random state of the SVM object. Maybe, this could be exposed to the
        # user, but it shouldn't matter too much what the seed is for this.
        svc = svm.SVC(probability=True,random_state=np.random.RandomState(8),tol=1e-15)
        svc.fit(self.X,labels)
        probabilities = svc.predict_proba(featureVals)

        classIdxs = list(svc.classes_)
        if self.blending:
          weightedPredictions = np.zeros(len(featureVals))
          sumW = 0
          for idx,key in enumerate(partitions.keys()):
            fx = self.__amsc[index].Predict(featureVals,key)",framework/SupervisedLearning.py,joshua-cogliati-inl/raven,1
"        },
    'expt_5': { 
        'note': 'scaled knn',
        'name': 'scaled knn',
        'pl': Pipeline([ ('scaling', StandardScaler()), 
                        ('knn', KNeighborsClassifier(n_jobs=-1)) ]) 
        },
    'expt_6': { 
        'note': 'rbf kernel SVM', 
        'name': 'rbf kernel SVM', 
        'pl': Pipeline([ ('rbf-svm', SVC(kernel='rbf')) ]) 
        },
    'expt_7': { 
        'note': 'scaled rbf kernel SVM',
        'name': 'Portable popcorn machine',
        'pl': Pipeline([ ('scaling', StandardScaler()), 
                        ('rbf-svm', SVC(kernel='rbf', cache_size=1000)) ]) 
        },
    'expt_8': { 
        'note': 'default decision tree',",models.py,jrmontag/classifier-comp-year2,1
"    NN_params = {}
    
    if choice=='log':
        params = log_params
        model = SGDClassifier()
    elif choice=='huber':
        params = huber_params
        model = SGDClassifier()
    elif choice=='svm':
        params = SVM_params
        model = svm.SVC(C=1)
    elif choice=='rf':
        params = RF_params
        model = RandomForestClassifier(n_estimators=1000, bootstrap=False)
        #clf = RandomForestClassifier(n_estimators=1000, bootstrap=False)
    
    # Set up Grid Search
    print ""Grid search...""
    clf = GridSearchCV(model, params, n_jobs=2, scoring='f1')
    clf.fit(X, y)",modelMake.py,momiah/cvariants_opencv,1
"        mask = np.isnan(d).any(axis=1)
        d = d[~mask]
        d = np.real(d)
        d = preprocessing.scale(d)
        l = datamatrix.loc[:,'gesture']
        l = l.values
        l = l[~mask]
        self.train_data, self.test_data, self.train_labels, self.test_labels = train_test_split(
            d, l, test_size=0.33, random_state=42
        )
        self.clf = svm.SVC(decision_function_shape='ovo')

    def train(self):
        self.clf.fit(self.train_data, self.train_labels)
        #self.clf_model = self.clf.decision_function([[1]])

    def report(self):
        prediction = self.clf.predict(self.test_data)
        # if we have the gestures in constants, add target_names=Constants.gesture_names
        print(classification_report(self.test_labels, prediction))",analysis/old/cassicication_old.py,joergsimon/gesture-analysis,1
"
# ..
# .. dimension reduction ..
pca = decomposition.RandomizedPCA(n_components=150, whiten=True)
pca.fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# ..
# .. classification ..
clf = svm.SVC(C=5., gamma=0.001)
clf.fit(X_train_pca, y_train)

print 'Score on unseen data: '
print clf.score(X_test_pca, y_test)

",faces1.py,theidentity/lekha_OCR_1.0,1
"    `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_

    This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.
    Only supports L2 regularization currently.

    >>> from pyspark.sql import Row
    >>> from pyspark.ml.linalg import Vectors
    >>> df = sc.parallelize([
    ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),
    ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()
    >>> svm = LinearSVC(maxIter=5, regParam=0.01)
    >>> model = svm.fit(df)
    >>> model.coefficients
    DenseVector([0.0, -0.2792, -0.1833])
    >>> model.intercept
    1.0206118982229047
    >>> model.numClasses
    2
    >>> model.numFeatures
    3",python/pyspark/ml/classification.py,apache/spark,1
"import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel=""linear"")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data
clf.fit(features_train, labels_train)


#### store your predictions in a list named pred",p5/svm/svm_quizzes.py,stefanbuenten/nanodegree,1
"        if self.features is None:
            return
        try:
            QApplication.setOverrideCursor(QCursor(Qt.WaitCursor))
            self.crossValidation()
        finally:
            QApplication.restoreOverrideCursor()

    def crossValidation(self):

        clf = svm.SVC(kernel=""rbf"", C=self.regConst.value(),
                      gamma=self.gamma.value())


        self.showMessage(""metric\tmean std"".upper())
        for sm in self.ScoringMethods:
            scores = cross_validation.cross_val_score(
                clf, self.features, self.labels, cv=self.kfold, scoring=sm)
            txt = ""%s:\t %0.2f +/-%0.2f"" %(sm.title(), scores.mean(), scores.std())
            self.showMessage(txt)",cat/gui/crossvalidationdlg.py,rhoef/afw,1
"    q = int(raw_input())

    for _ in range(q):
        line = raw_input().strip().split()
        params.append(map(lambda x: float(x.split(':')[1]), line[1:]))
        names.append(line[0])

    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
    # params = sel.fit_transform(params)

    # clf = svm.LinearSVC(max_iter = 3000, dual = False)
    clf = RandomForestClassifier(min_samples_split = 4, criterion = ""entropy"")

    params_normalized = preprocessing.normalize(params, axis = 0)

    params_scaled = preprocessing.scale(params_normalized)

    clf.fit(params_scaled[:-q], annotations)

    ans = clf.predict(params_scaled[-q:])",ai/Machine-Learning/Quora-Answer-Classifier/main.py,m00nlight/hackerrank,1
"        #                                            steps=500)),
        # ('NN 500:200 dropout',
        #  skflow.TensorFlowEstimator(model_fn=dropout_model,
        #                             n_classes=10,
        #                             steps=20000)),
        # ('CNN', skflow.TensorFlowEstimator(model_fn=conv_model,
        #                                    n_classes=10,
        #                                    batch_size=100,
        #                                    steps=20000,
        #                                    learning_rate=0.001)),
        ('SVM, adj.', SVC(probability=True,
                          kernel=""rbf"",
                          C=2.8,
                          gamma=.0073,
                          cache_size=200)),
        # ('SVM, linear', SVC(probability=True,
        #                     kernel=""linear"",
        #                     C=0.025,
        #                     cache_size=200)),
        ('k nn (k=3)', KNeighborsClassifier(3)),",ML/data-driven/blood-donation/main.py,MartinThoma/algorithms,1
"        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')

# Specify the indices of the samples that we want to mark on the resulting plots.
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

######################Needed from previous section##############################
from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))

plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
",self_practice/Chapter 3 Support Vector Machine.py,wei-Z/Python-Machine-Learning,1
"from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA



def apply_algorithm(paras, data):
    X = data[""X""]
    y = data[""y""]

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    elif paras['clf'] == 'lr':
        clf = linear_model.LogisticRegression(C=0.5)",python/change_features/methods.py,Healthcast/RSV,1
"            self._crossvalidation.crossvalidate(\
                scaled_cv_feasibles, scaled_cv_infeasibles)

        # @todo WARNING maybe rescale training feasibles/infeasibles (!) 
        fvalues = [f.getA1() for f in self._selected_feasibles]
        ivalues = [i.getA1() for i in self._selected_infeasibles]

        points = ivalues + fvalues
        labels = [-1] * len(ivalues) + [1] * len(fvalues) 

        self._clf = svm.SVC(\
            kernel = 'linear',\
            C = self._best_parameter_C, \
            tol = 1.0)
        self._clf.fit(points, labels)

        self.logger.log()
        return True

    def distance_to_hp(self, x):",evopy/metamodel/dses_svc_linear_meta_model.py,jpzk/evopy,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value",shinken/external_command.py,fpeyre/shinken,1
"i_STTRB  = i_STR
i_STTRH  = i_STR
i_STUR   = i_STR
i_STURB  = i_STR
i_STURH  = i_STR

def i_SMC(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    ext('EXCEPTION.EL3 %s'%i.imm,size=pc.size).call(fmap)

def i_SVC(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    ext('EXCEPTION.EL1 %s'%i.imm,size=pc.size).call(fmap)

def i_SYS(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    logger.warning('semantic undefined for %s'%i.mnemonic)

def i_SYSL(i,fmap):
    fmap[pc] = fmap[pc]+i.length",amoco/arch/arm/v8/asm64.py,bdcht/amoco,1
"
### set the random_state to 0 and the test_size to 0.4 so
### we can exactly check your result
features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(
    iris.data, iris.target, test_size=0.4, random_state=0)



###############################################################

clf = SVC(kernel=""linear"", C=1.)
clf.fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################
def submitAcc():
    return clf.score(features_test, labels_test)",src/model-evaluation-and-validation/train-test-split-sklearn.py,bond-/udacity-ml,1
"    """"""
    from sklearn.base import clone
    from sklearn.utils import check_random_state
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import check_cv

    if clf is None:
        scaler = StandardScaler()
        svc = SVC(C=1, kernel='linear')
        clf = Pipeline([('scaler', scaler), ('svc', svc)])

    info = epochs_list[0].info
    data_picks = pick_types(info, meg=True, eeg=True, exclude='bads')

    # Make arrays X and y such that :
    # X is 3d with X.shape[0] is the total number of epochs to classify
    # y is filled with integers coding for the class to predict
    # We must have X.shape[0] equal to y.shape[0]",mne/decoding/time_gen.py,agramfort/mne-python,1
"
### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier
from sklearn.pipeline import Pipeline

svc_ovo = OneVsOneClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

svc_ova = OneVsRestClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

### Cross-validation scores ###################################################
from sklearn.cross_validation import cross_val_score",plot_haxby_multiclass.py,ainafp/nilearn,1
"
    """"""

    def __init__(self, **kwargs):
        """"""Class constructor.

        Args:
          kwargs (dict): keyword arguments to be forwarded

        """"""
        explicit_clf = LinearSVC(C=DFLT_EXP_C, **DFLT_PARAMS)
        self.explicit = WangExplicitSenser(a_clf=explicit_clf, **kwargs)
        implicit_clf = LinearSVC(C=DFLT_IMP_C, **DFLT_PARAMS)
        self.implicit = WangImplicitSenser(a_clf=implicit_clf, **kwargs)",dsenser/wang/wang.py,WladimirSidorenko/DiscourseSenser,1
"            0, 100, size=(2000, 4)), columns=list('ABCD'))
        y = pd.DataFrame(np.random.randint(
            0, 100, size=(2000, 1)), columns=['output'])

        # Add tag for dataframe
        syncer_obj.add_tag(X, ""digits-dataset"")
        syncer_obj.clear_buffer()

        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                             'C': [10, 100]}]
        clf = GridSearchCV(SVC(), tuned_parameters, cv=3)
        y = y.values.ravel()
        clf.fit_sync(X, y)
        events = syncer_obj.sync()
        self.grid_search_event = events[0]

    def test_gridcv_event(self):
        utils.validate_grid_search_cv_event(self.grid_search_event, self)
        self.assertEquals(self.grid_search_event.numFolds, 3)
        best_fit_event = self.grid_search_event.bestFit",client/python/modeldb/tests/sklearn/testGridSearchEvent.py,mitdbg/modeldb,1
"    kind = 'regular'
    nn_k = 'rnd'
    smote = SMOTE(random_state=RND_SEED, kind=kind, k_neighbors=nn_k)
    assert_raises_regex(ValueError, ""has to be one of"",
                        smote.fit_sample, X, Y)


def test_sample_regular_with_nn_svm():
    kind = 'svm'
    nn_k = NearestNeighbors(n_neighbors=6)
    svm = SVC(random_state=RND_SEED)
    smote = SMOTE(
        random_state=RND_SEED, kind=kind, k_neighbors=nn_k, svm_estimator=svm)
    X_resampled, y_resampled = smote.fit_sample(X, Y)
    X_gt = np.array([[0.11622591, -0.0317206], [0.77481731, 0.60935141],
                     [1.25192108, -0.22367336], [0.53366841, -0.30312976],
                     [1.52091956, -0.49283504], [-0.28162401, -2.10400981],
                     [0.83680821, 1.72827342], [0.3084254, 0.33299982],
                     [0.70472253, -0.73309052], [0.28893132, -0.38761769],
                     [1.15514042, 0.0129463], [0.88407872, 0.35454207],",imblearn/over_sampling/tests/test_smote.py,scikit-learn-contrib/imbalanced-learn,1
"end = time.time()
print ""Time Taken for Feature Extraction : "", end-start

train_feature_list = [train_data_bag,train_data_tfidf]
test_feature_list = [test_data_bag,test_data_tfidf]
feature_names = [""Bag Of Words"", ""Tf-Idf""]

#Initializing the classifiers 

rf = RandomForestClassifier(n_estimators=51,random_state=1)
svm = SVC(kernel=""linear"",probability=True)
mnb = MultinomialNB(fit_prior=True)
ada = AdaBoostClassifier(random_state=1)

#Creating an estimator list for Voting Classifiers 

classifier_names = [""Random Forests"",""SVM"",""Multinomial NB"",""Adaboost""]
classifiers = [rf,svm,mnb,ada]
estimator_list = zip(classifier_names,classifiers)
",PizzaTextModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_09_2015_01.py,magic2du/contact_matrix,1
"
    `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_

    This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.

    >>> from pyspark.sql import Row
    >>> from pyspark.ml.linalg import Vectors
    >>> df = sc.parallelize([
    ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),
    ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()
    >>> svm = LinearSVC(maxIter=5, regParam=0.01)
    >>> model = svm.fit(df)
    >>> model.coefficients
    DenseVector([0.0, -0.2792, -0.1833])
    >>> model.intercept
    1.0206118982229047
    >>> model.numClasses
    2
    >>> model.numFeatures
    3",python/pyspark/ml/classification.py,MLnick/spark,1
"    return feats


def train_regression(feats, gold):
    w = linalg.lstsq(feats, gold)
    return w[0]


def train_svm(feats, gold):
    labeled = []
    clf = svm.SVC()
    clf.fit(feats, gold)
    return clf
    for i, f in enumerate(feats):
        feat_dict = {}
        for j, feat in enumerate(f):
            feat_dict[j] = feat
        labeled.append([feat_dict, gold[i]])
    return SklearnClassifier(Pipeline()).train(labeled, max_iter=30)
",src/twitter_regression.py,recski/semeval,1
"        # No need to reverse, we have done that in feaure level
        #x, y = y, x
        y_type = type_map.loc[name][""A type""]
    else:
        # normal direction
        y_type = type_map.loc[name][""B type""]


   
    if(y_type == ""Binary"" or y_type == ""Categorical""):
        clf = SVC()
    else:
        clf = SVR()

    x_arr = np.array(x, ndmin=2)
    X = x_arr.transpose()


    scaler = preprocessing.StandardScaler().fit(X)      
    X  = scaler.transform(X)",features/feature_functions.py,ssamot/causality,1
"data_test = np.swapaxes(data_test, 0, 1)

# Check dimensions
print 'Train labels: ', labels_train.shape
print 'Train data: ', data_train.shape
print 'Test labels: ', labels_test.shape
print 'Test data: ', data_test.shape
sys.stdout.flush()

# Train SVM using train data
clf = SVC(kernel='linear', C=1.0)
print 'Training SVM...'
sys.stdout.flush()
t0 = time.time()
clf.fit(data_train, labels_train)
print (""Training SVM: %.2f seconds"" %(time.time()-t0))
sys.stdout.flush()


# Test SVM",handcrafted_classif/colorHistSVM.py,imatge-upc/affective,1
"    def predict_proba(self, X):
        return self.estimator.predict_proba(X)[:,1]


class Blender(BaseEstimator):
    '''
    !NOTE: This stack-estimator only for binary classification ! 
    !NOTE: коэффиициенты получаем из минимизации log-Loss (кроссэнтропия)
    
    Example, how to use:
    blndr = Blender([XGB, SVC(with probas=True), RF, KNN])
    blndr.fit(X,y)
    blndr.predict_proba(X_test)
    
    # or with your weights:
    blndr = Blender([XGB, SVC(with probas=True), RF, KNN], with_own_weights = True)
    blndr.fit(X,y)
    w = np.array([0.5, 0.2, 0.2, 0.1])  #sum(w)==1
    blndr.predict_proba(X_test, weights = w)
    '''",Python/ML/class for binary blending & stack (logloss min)/Blending.py,Amir14111/sandbox,1
"if __name__ == '__main__':
    path = 'corpus'
    cyber_label = 'cyber'
    noncyber_label = 'noncyber'

    # build_fcs will take all the .sc.xz files in path/cyber_label and
    # path/noncyber_label as the labeled corpus.
    cyber_fcs, noncyber_fcs = build_fcs(path, cyber_label, noncyber_label)

    # Initialize the classifier with a custom sklean classifier
    c = Clacy(svm.SVC(kernel='linear', C=1))
    # Load in the data
    c.load_corpus(cyber_fcs, noncyber_fcs, ['bowNP'])
    # Cross-validate on the data
    scores = c.cv()
    print np.mean(scores)
    print scores
    # Train on the full corpus
    c.train()
    # Make some predictions",clacy/clacy.py,diffeo/clacy,1
"
        # weight components in FeatureUnion
        transformer_weights={
            'subject': 0.8,
            'body_bow': 0.5,
            'body_stats': 1.0,
        },
    )),

    # Use a SVC classifier on the combined features
    ('svc', SVC(kernel='linear')),
])

# limit the list of categories to make running this exmaple faster.
categories = ['alt.atheism', 'talk.religion.misc']
train = fetch_20newsgroups(random_state=1,
                           subset='train',
                           categories=categories,
                           )
test = fetch_20newsgroups(random_state=1,",projects/scikit-learn-master/examples/hetero_feature_union.py,DailyActie/Surrogate-Model,1
"    grid_search.transform(X)

    # Test exception handling on scoring
    grid_search.scoring = 'sklearn'
    assert_raises(ValueError, grid_search.fit, X, y)


@ignore_warnings
def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)
    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')
    grid_search.fit(X, y)

    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
                                        scoring='accuracy')
    # smoketest grid search",projects/scikit-learn-master/sklearn/model_selection/tests/test_search.py,DailyActie/Surrogate-Model,1
"    x14 = df_train['y_diff'].values
    x15 = df_train['z_diff'].values

    # still at 0.74 accuracy

    print ""here in run_model: {}"".format(np.where(df_train['x_rolling_average'].isnull())[0])

    y = df_train['state'].values
    X = np.column_stack([x1, x2, x3, x4, x5, x6, x10, x11, x12, x13, x14, x15])

    clf = svm.SVC(kernel='linear', C=1)
    knn = KNeighborsClassifier(n_neighbors=5)
    rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)

    # figure out optimal n_estimators",app/science3.py,ChristopherGS/sensor_readings,1
"n.variable('configure_args', ' '.join(sys.argv[1:]))
env_keys = set(['CXX', 'AR', 'CFLAGS', 'LDFLAGS'])
configure_env = dict((k, os.environ[k]) for k in os.environ if k in env_keys)
if configure_env:
    config_str = ' '.join([k + '=' + configure_env[k] for k in configure_env])
    n.variable('configure_env', config_str + '$ ')
n.newline()

CXX = configure_env.get('CXX', 'g++')
objext = '.o'
if platform.isMSVC():
    CXX = 'cl'
    objext = '.obj'

def src(filename):
    return os.path.join('src', filename)
def built(filename):
    return os.path.join('$builddir', filename)
def doc(filename):
    return os.path.join('doc', filename)",configure.py,TheOneRing/ninja,1
"            t = json.loads(line)
            features = ExtractFromItem(t)
            X_test_boolean.append(features['boolean'].values())
            X_test_numeric.append(features['numeric'].values())
            unlabeled.append((t, features))
    return X_test_boolean, X_test_numeric


classifiers = {
    'Nearest Neighbors': KNeighborsClassifier(3),
    'RBF SVM': SVC(gamma=2, C=1),
    'Decision Tree': DecisionTreeClassifier(max_depth=5),
    'Random Forest': RandomForestClassifier(max_depth=5,
                                            n_estimators=10, max_features=1),
    'AdaBoost': AdaBoostClassifier(),
    'Gaussian Naive Bayes': GaussianNB(),
    'Bernoulli Naive Bayes': BernoulliNB(),
    'LDA': LDA()
}
#val_ratio_datasets",python/mlAlgorithms/classifier.py,veksev/cydi,1
"X_train, y_train = load_svmlight_file(
    ""data/wise2014-train.libsvm"", dtype=np.float32, multilabel=True)

X_test, y_test = load_svmlight_file(
    ""data/wise2014-test.libsvm"", dtype=np.float32, multilabel=True)

print(""Binarizing."")
lb = MultiLabelBinarizer()
y_train = lb.fit_transform(y_train)
#http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html
clf = OneVsRestClassifier(LinearSVC(loss='l2', penalty='l2', tol=1e-3,
                                    dual=False), n_jobs=2)

print(""Performing cross validation."")
cv = KFold(y_train.shape[0], n_folds=3, shuffle=True, random_state=42)
scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=cv)
print(""CV scores."")
print(scores)
print(""F1: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
",basic_clf.py,kastnerkyle/kaggle-wise2014,1
"        step, which will always raise the error.

    Examples
    --------
    >>> from sklearn import svm, datasets
    >>> from jupyter_utils.spark import SparkGridSearchCV
    >>> from pyspark.sql import SparkSession
    >>> spark = ...
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = SparkGridSearchCV(spark.sparkContext, svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    SparkGridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",jupyter_utils/spark.py,Stibbons/jupyter_utils,1
"X_dev, y_dev = load_data_set(args.phn_set1,args.phn_set2, args.data_path, 'dev')

tuned_parameters = [{'kernel':['rbf'], 'gamma':[1e-2,1e-3,1e-4,1e-5,1e-1],
                     'C':[.1,1,10,.01,100]}]


print 'commencing training'
error_values = []
for gamma_id, gamma in enumerate([1e-4]):
    for C_id, C in enumerate([100]):
        clf = SVC(C=C,gamma=gamma,kernel='rbf',tol=0.00001,verbose=True)
        clf.fit(X_train,y_train)
        s = clf.score(X_dev,y_dev)
        print ""C=%g\tgamma=%g\tscore=%g"" % (C,gamma,s)
        error_values.append((gamma,C,s))
        if gamma_id == 0 and C_id ==0:
            best_score = s
            print ""updated best score to %g"" % best_score
            best_C = C
            best_gamma = gamma",local/load_compare_phone_sets_spec_kernel.py,markstoehr/phoneclassification,1
"    results=clf.predict(features[int(percentage_of_testing*len(features)):])
    fig1=pb.figure()
    ax1=fig1.add_subplot(111)
    ax1.plot(results,label='Predicted')
    ax1.plot(people[:-int(percentage_of_testing*len(features))],label='Actual')
    ax1.set_title('Linear Bayesian Ridge')
    pb.legend(loc='upper left');
    from sklearn.metrics import accuracy_score
    print ""Linear BayesianRidge:"", clf.score(features[int(percentage_of_testing*len(features)):],people[int(percentage_of_testing*len(features)):])
def SVC_Poly(features,people,percentage_of_testing):
    clf = SVC(kernel='poly', degree=3)
    clf.fit(features[:-int(percentage_of_testing*len(features))], people[:-int(percentage_of_testing*len(features))])
    results=clf.predict(features[int(percentage_of_testing*len(features)):])
    fig1=pb.figure()
    ax1=fig1.add_subplot(111)
    ax1.plot(results,label='Predicted')
    ax1.plot(people[:-int(percentage_of_testing*len(features))],label='Actual')
    ax1.set_title('SVC- Poly')
    pb.legend(loc='upper left');
    from sklearn.metrics import accuracy_score",createFeatsLowD.py,code-for-india/food_predictor,1
"                         (p, q, p_len, q_len, p_len + q_len))

        X = list(self.by_domain_data[p])
        X.extend(self.by_domain_data[q])
        y = [p] * p_len
        y.extend([q] * q_len)

        pipeline = Pipeline([
                (""vert"", TfidfVectorizer(min_df = 1, binary = False, ngram_range = (1, 1),
                                         tokenizer = Tokenizer())),
                (""svm"", LinearSVC(loss='l2', penalty=""l1"",
                                  dual=False, tol=1e-3)),
                ])

        if self.cv > 0:
            _logger.info(""Doing grid search on %d fold CV"" % self.cv)
            params = {
                ""svm__C"": [1, 10, 50, 100, 500, 1000],
                }
            grid = GridSearchCV(pipeline, params, cv=self.cv, verbose=50)",model/ensemble/nb_svm/train_svm.py,luanjunyi/cortana,1
"print len(trainArticles)
print len(testArticles)
listOfYears = []


testArticleLookupDict = {}
for title in range(0,len(testArticles)):
    #print (eval(testArticles[title])['title'])
    testArticleLookupDict[eval(testArticles[title])['title']] = title

clf = svm.SVC(probability=True)#tree.DecisionTreeClassifier()
titles = []
weights = []

G = nx.Graph()#G is an empty graph


#A
def getArticle(article):
    singleSets = []",tmp.py,JFriel/honours_project,1
"                             frequencies.''', 
                             'auto', {dict, 'auto'})

        max_iter = param('''Hard limit on iterations within solver, or -1 for
                         no limit.''', -1)

        import numpy as np
        X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
        y = np.array([1, 1, 2, 2])
        from sklearn.svm import SVC
        clf = SVC()
        clf.fit(X, y) 
        SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
                gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
                shrinking=True, tol=0.001, verbose=False)
        print(clf.predict([[-0.8, -1]]))

        '''
        http://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html
        decision_function(X)	    Distance of the samples X to the separating hyperplane.",_graveyard/oldhotspotter/ideas/matching_graph.py,Erotemic/hotspotter,1
"              'br', 'p', 'DOC', 'cms', 'html', 'htm', 'index']

"""""" Tries to figure out the topic for articles
""""""

class L1LinearSVC(LinearSVC):

    def fit(self, X, y):
        # The smaller C, the stronger the regularization.
        # The more regularization, the more sparsity.
        self.transformer_ = LinearSVC(penalty=""l1"",
                                      dual=False, tol=1e-3)
        X = self.transformer_.fit_transform(X, y)
        return LinearSVC.fit(self, X, y)

    def predict(self, X):
        X = self.transformer_.transform(X)
        return LinearSVC.predict(self, X)

",consumer/predict_nlp.py,konfabproject/konfab-consumer,1
">>> from sklearn.grid_search import GridSearchCV
>>> from sklearn.metrics import classification_report


>>> if __name__ == '__main__':
>>>     data = fetch_mldata('MNIST original', data_home='data/mnist')
>>>     X, y = data.data, data.target
>>>     X = X/255.0*2 - 1
>>>     # X = scale(X)
>>>     X_train, X_test, y_train, y_test = train_test_split(X, y)
>>>     # clf = SVC(kernel='rbf', C=2.8, gamma=.0073)
>>>     pipeline = Pipeline([
>>>         ('clf', SVC(kernel='rbf', gamma=0.01, C=100))
>>>     ])
>>>     print X_train.shape
>>>     parameters = {
>>>         'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
>>>         'clf__C': (0.1, 0.3, 1, 3, 10, 30),
>>>     }
>>>     grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1, scoring='accuracy')",MasteringMLWithScikit-learn/8365OS_09_Codes/ch9.py,moonbury/pythonanywhere,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = DISR.disr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_DISR.py,jundongl/PyFeaST,1
"import threading
import os.path

# basic parameters
my_kernel = 'linear'
my_max_iteration = 250000
my_test_size = 0.3
my_random_state = 42

def classifyModel(trainingData, trainingLabel, kernel='linear', max_iter=-1):
    clf = svm.SVC(kernel=kernel, max_iter=max_iter).fit(trainingData, trainingLabel)
    return clf
def testingWithModel(testData, testLabel, model):
    error_count = 0.0
    result = model.predict(testData)
    for i, la in enumerate(result):
        if la != testLabel[i]:
            error_count += 1
    return error_count/result.shape[0]
def classify(trainingData, trainingLabel, testData, testLabel, kernel='linear', max_iter=-1):",src/script/new/moment.py,changkun/AugmentedTouch,1
"aetst = preprocessing.normalize(aetst)

aptrn = preprocessing.normalize(aptrn)
aptst = preprocessing.normalize(aptst)

if doPCA:
    pcatrn = preprocessing.normalize(pcatrn)
    pcatst = preprocessing.normalize(pcatst)
    #pass

clf = svm.LinearSVC()
clf.fit(aetrn,trnlabs)
aepred = clf.predict(aetst)
cmae = np.zeros((2,2))
for p,t in zip(aepred,tstlabs):
    cmae[t,p] += 1
aewr = np.trace(cmae)/np.sum(cmae)
aeuwr = np.sum(np.diag(cmae)/np.sum(cmae,axis=1))/2
print aewr,aeuwr
",src/encode.py,shahmohit/pynca,1
"import numpy as np


def train_classifier(clf_name, labels, examples, num_samples=None, optimize=False, cross_validate=False):
    if optimize:
        print 'Training the classifier w/optimization...\n'
        clf = optimize_classifier(opt_method='grid', clf_name=clf_name)
    else:
        print 'Training the classifier w/o optimization...\n'
        if clf_name == 'svm':
            clf = svm.SVC(kernel='poly', degree=2)
        elif clf_name == 'tree':
            clf = tree.DecisionTreeClassifier()
        elif clf_name == 'forest':
            clf = ensemble.RandomForestClassifier(criterion='entropy', max_depth=10, max_features='auto',
                                                  n_estimators=40)
        elif clf_name == 'adaboost':
            clf = ensemble.AdaBoostClassifier(algorithm='SAMME.R', n_estimators=100, learning_rate=0.2)
        elif clf_name == 'gradientboost':
            clf = ensemble.GradientBoostingClassifier(learning_rate=0.2, n_estimators=150)",01_assignment/classify_mnist.py,grantathon/computer_vision_machine_learning,1
"SVM = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42)

SVM_scores = cross_validation.cross_val_score(SVM, texts_tfidf, labels, cv=5)
print SVM_scores


# GNB = GaussianNB()
# GNB_scores = cross_validation.cross_val_score(GNB, texts_tfidf.toarray(), labels, cv=5)
# print GNB_scores

# SVM_Linear = svm.LinearSVC()
# SVM_Linear_scores = cross_validation.cross_val_score(SVM_Linear, texts_tfidf.toarray(), labels, cv=5)
# print SVM_Linear_scores


# pickle.dump(SVM,open('SVM.p','wb'))


SVM_rbf = svm.SVC(decision_function_shape='ovr', kernel = 'rbf')
SVM_rbf_scores = cross_validation.cross_val_score(SVM_rbf, texts_tfidf, labels, cv=5)",Python_Projects/NLP/keyword assignment/TrashBin/preprosessing.py,YangLiu928/NDP_Projects,1
"    X = preprocessing.scale(X)
    return X, y


def train_and_test(X, y, percents):
    test_size = int(round(len(X) * 0.05))
    print('Size of test-set: ' + str(test_size))

    print('Training the model')
    # Try both with kernel = 'linear' and 'rbf'
    # clf = svm.SVC(kernel='linear', C = 0.01)
    clf = svm.LinearSVC(C = 1.0, dual = False)
    # clf = ensemble.RandomForestClassifier(n_jobs=-1,
    #     n_estimators=200,
    #     criterion='entropy')
    clf.fit(X[:-test_size], y[:-test_size])

    correct_count = 0
    stock_percent = 0
    result_percent = 0",modules/stock_analyzer.py,dasovm/olga,1
"
    transformer = preprocessing.MinMaxScaler().fit(to_float(flatten(Xs)))
    Xs_train = transformer.transform(to_float(Xs_train))
    Xs_test = transformer.transform(to_float(Xs_test))

    if attribute_count is not None:
        Xs_test, Xs_train = _eliminate_features(Xs_test, Xs_train, attribute_count, ys_train)
        Xs_test = flatten(Xs_test)
        Xs_train = flatten(Xs_train)

    clf = SVC(**SVC_parameters)
    # clf = LinearSVC(class_weight='auto')
    clf.fit(to_float(Xs_train), ys_train)

    ys_pred = clf.predict(to_float(Xs_test))
    predicted_class = list(ys_pred)
    actual_class = ys_test

    print ""%d, %.3f"" % (test_index[0], accuracy_score(actual_class, predicted_class))
",source/classification/gridsearch.py,jschavem/facial-expression-classification,1
"
def load_labels(label_file):
    """"""Return a NumPy array containing the labels in *labels_file*.""""""
    return array([int(label_line) for label_line in label_file])


def classify(features, labels, folds=5):
    """"""Perform supervised classification of the sentences described by
    *features* using the provided *labels*, and return a NumPy array
    with the classifier's predictions.""""""
    clf = GridSearchCV(SVC(), {'C': 10.0 ** arange(-2, 4)},
                       cv=folds, n_jobs=-1)
    clf.fit(features[:labels.shape[0]], labels)
    return clf.predict(features)


def main():
    import argparse
    parser = argparse.ArgumentParser(
        description='An evaluator for translation hypotheses.',",hw3-evaluation/evaluate.py,query/mt-submissions,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_04_07_2015_parallel.py,magic2du/contact_matrix,1
"def classificationError(Y, Y_in):
  return sum(Y != Y_in) / float(len(Y))

def prepareOneVsAll(data, class1):
    return data[:,1:], (data[:,0] == class1)*2. - 1.

def prepareOneVsOne(data, class1, class2):
    sel = np.logical_or(data[:,0] == class1, data[:,0] == class2)
    return data[sel,1:], (data[sel,0] == class1)*2. - 1.

def polySVC(X, Y, C=.01, Q=2, coef0=1, verbose=False):
  clf = svm.SVC(kernel='poly', C=C, degree=Q, coef0=coef0, gamma=1., verbose=verbose)
  clf.fit(X, Y)
  return classificationError(Y, clf.predict(X)), len(clf.support_vectors_), clf

def problem2_4():
  train, test = getData()
  for class1 in (1, 3, 5, 7, 9):
#   for class1 in (0, 2, 4, 6, 8):
    res = polySVC(*prepareOneVsAll(train, class1))",Final/Python/by_Mark_B2/hw8.py,nobel1154/edX-Learning-From-Data-Solutions,1
"    n_correct = 0
    for i in range(len(Y_true)):
        n_labels += len(Y_true[i])
        for label in Y_pred[i]:
            if label in Y_true[i]:
                n_correct += 1
    return float(n_correct) / n_labels


def test_ovr_exceptions():
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    assert_raises(ValueError, ovr.predict, [])


def test_ovr_fit_predict():
    # A classifier which implements decision_function.
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    pred = ovr.fit(iris.data, iris.target).predict(iris.data)
    assert_equal(len(ovr.estimators_), n_classes)
",python/sklearn/sklearn/tests/test_multiclass.py,seckcoder/lang-learn,1
"    Examples
    --------

    from sklearn import datasets
    from sklearn.svm import SVC

    iris = datasets.load_iris()
    X = iris.data[:, [0,2]]
    y = iris.target

    svm = SVC(C=1.0, kernel='linear')
    svm.fit(X,y)

    plot_decision_region(X, y, clf=svm, res=0.02, cycle_marker=True, legend=1)

    plt.xlabel('sepal length [cm]')
    plt.ylabel('petal length [cm]')
    plt.title('SVM on Iris')
    plt.show()
",mlxtend/evaluate/decision_regions.py,YoungKwonJo/mlxtend,1
"ys = np.array([-1]*N)
sidx = random.sample(np.where(ytrue == 0)[0], supevised_data_points/2)+random.sample(np.where(ytrue == 1)[0], supevised_data_points/2)
ys[sidx] = ytrue[sidx]

Xsupervised = Xs[ys!=-1, :]
ysupervised = ys[ys!=-1]

# compare models
lbl = ""Purely supervised SVM:""
print lbl
model = sklearn.svm.SVC(kernel=kernel, probability=True)
model.fit(Xsupervised, ysupervised)
evaluate_and_plot(model, Xs, ys, ytrue, lbl, 1)


lbl =  ""S3VM (Gieseke et al. 2012):""
print lbl
model = scikitTSVM.SKTSVM(kernel=kernel)
model.fit(Xs, ys)
evaluate_and_plot(model, Xs, ys, ytrue, lbl, 2)",examples/compare_rbfsvm_methods.py,carloslds/semisup-learn,1
"
    print ""Training...""

    # commented out to delete .toarray() option, because len(data_X[0]) is not defined
    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.2.2.py,totuta/deep-supertagging,1
"		#""RBFSVM"",
		""DecisionTree"",
		""RandomForest"",
		""AdaBoost"",
		#""NaiveBayes"",
		#""LDA"",
		#""QDA""]
	]	
classifiers = [
		#KNeighborsClassifier(5),
		#SVC(kernel=""linear"", C=0.025),
		#SVC(gamma=2, C=1),
		DecisionTreeClassifier(max_features=""sqrt""),
		RandomForestClassifier(n_estimators=20,max_features=""sqrt""),
		AdaBoostClassifier(),
		#GaussianNB(),
		#LDA(),
		#QDA()]
		]
",ErrorCorrectionClassifier.py,raunaq-m/MultiRes,1
"from sklearn.lda import LDA
from sklearn.qda import QDA
import pdb

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
""""""
pdb.set_trace()",test.py,malimome/game-auth,1
"HUID login


URLS:
 * Proxied url example: http://www.nature.com.ezp-prod1.hul.harvard.edu/nature/journal/vaop/ncurrent/full/nature14131.html
 * Login page: https://www.pin1.harvard.edu/cas/login?service=https%3A%2F%2Fwww.pin1.harvard.edu%2Fpin%2Fauthenticate%3F__authen_application(...)



Login page form:
<form id=""fm1"" action=""/cas/login?service=https%3A%2F%2Fwww.pin1.harvard.edu%2Fpin%2Fauthenticate%3F__authen_application%3DHUL_ACC_MGMT_SVC(...)"" method=""post"">
    <fieldset class=""inner"" id=""multi-choice"">
        <input id=""compositeAuthenticationSourceType1"" type=""radio"" name=""compositeAuthenticationSourceType"" value=""PIN"" checked tabindex=""1""/>
        <input id=""compositeAuthenticationSourceType2"" type=""radio"" name=""compositeAuthenticationSourceType"" value=""HMS ECOMMONS"" tabindex=""2""/>
        <input id=""compositeAuthenticationSourceType3"" type=""radio"" name=""compositeAuthenticationSourceType"" value=""ADID"" tabindex=""3""/>
    </fieldset>
    <fieldset class=""outer"">
        <fieldset class=""inner"" id=""form-field"">
            <input id=""username"" name=""username"" class=""required"" tabindex=""11"" accesskey=""u"" type=""text"" value="""" size=""40"" maxlength=""71"" autocomplete=""off""/>
        </fieldset>",ezfetcher/login_adaptors/HUID_lib.py,scholer/ezfetcher,1
"        assert np.all((kgsites) == np.array(sites)), (set(kgsites) - set(sites), set(sites) - set(kgsites))

        genos1kg = np.array(genos1kg[:, idxs])

        assert genotype_matrix.shape[1] == genos1kg.shape[1]
    log.info(""loaded and subsetted thousand-genomes genotypes (shape: %s) in %.1f seconds"" % 
            (genos1kg.shape, time.time() - t0))

    t0 = time.time()
    clf = make_pipeline(RandomizedPCA(n_components=4, whiten=True, copy=True),
                    svm.SVC(C=2, probability=True))
    background_target = np.array([int(x) for x in _str.split(""|"")])

    clf.fit(genos1kg, background_target)
    log.info(""ran randomized PCA on thousand-genomes samples at %d sites in %.1f seconds"" 
             % (genos1kg.shape[1], time.time() - t0))

    ipops = ""AFR AMR EAS EUR SAS UNKNOWN"".split()

    t0 = time.time()",peddy/pca.py,brentp/peddy,1
"    # Unserupervised clustering
    clf = KMeans(2)
    predict = clf.fit_predict(train)
    clf_mean = np.array(clf.cluster_centers_).mean(0)
    
    im_predict = clf.predict(im.reshape([-1, n_channels])).reshape([480,640])
    if clf.cluster_centers_[0][0] < clf.cluster_centers_[1][0]:
        predict = predict == 0
    # im_predict = clf.predict(im.reshape([-1, 3])[:,1:2]).reshape([480,640])

    svm = LinearSVC()
    svm.fit(train - clf_mean, predict)
    clf_w = svm.coef_

    im_predict = (np.dot((im - clf_mean), clf_w) > 0)[:,:,0]

    return clf_mean, clf_w

# TEST: Predict foreground objects
def get_foreground(im, clf_mean, clf_w, min_area=500):",costar_predicator/predicator_8020_module/src/predicator_8020_module/utils_8020.py,ready-robotics/costar_stack,1
"    total = sum(scoreList)
    for score in scoreList:
        outputList.append(score/total)
    return outputList


def trainInfer(doc_train, label_train, doc_test, label_test, classifier, trainProbFlag):
    if classifier == 'NaiveBayes':
        model = MultinomialNB()
    elif classifier == 'SVM':
        model = svm.SVC(probability=True)
    elif classifier == 'MaxEnt':
        model = LogisticRegression()
    else:
        print 'Ensemble Model Error!'
        sys.exit()

    model.fit(doc_train, label_train)
    labelList = model.classes_
    testProbs = model.predict_proba(doc_test)",individualModels.py,renhaocui/ensembleTopic,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,christianbrodbeck/mne-python,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1, norm_trace=False)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)


def test_ajd():
    """"""Test if Approximate joint diagonalization implementation obtains same
    results as the Matlab implementation by Pham Dinh-Tuan.
    """"""",mne/decoding/tests/test_csp.py,Eric89GXL/mne-python,1
"    def __init__(self, path, etype, **kwargs):
        super(EnsembleModel, self).__init__(path, etype=etype, **kwargs)
        self.basedir = ""models/ensemble/""
        self.goldstd = kwargs.get(""goldstd"")
        self.data = {}
        self.offsets = []
        self.pipeline = Pipeline(
            [
                #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0001, n_iter=5, random_state=42)),
                #('clf', SGDClassifier())
                # ('clf', svm.NuSVC(nu=0.01 ))
                ('clf', RandomForestClassifier(class_weight={False:1, True:1}, n_jobs=-1, criterion=""entropy"", warm_start=True))
                # ('clf', tree.DecisionTreeClassifier(criterion=""entropy"")),
                # ('clf', MultinomialNB())
                # ('clf', GaussianNB())
                #('clf', svm.SVC(kernel=""rbf"", degree=2, C=1)),
                #('clf', svm.SVC(kernel=""linear"", C=2))
                #('clf', DummyClassifier(strategy=""constant"", constant=True))
            ])
",src/classification/ner/ensemble.py,AndreLamurias/IBEnt,1
"	import os
	folder=os.path.dirname(os.path.abspath(__file__))
	

	#folder=matfile.root.folder[:]
	#pdb.set_trace()
	#upload class identity - array of int's indicate label

	classNum = np.amax(labels) #number of classes
	#set up classifier
	clf = svm.SVC()
	clf.decision_function_shape='ovo'
	clf.kernel='linear'
	#train classifier
	clf.fit(rawData,labels)
	#query classifier
	predict_class=clf.predict(test_data)
	
	#output parameters of classifier
	coefs=clf.coef_",Compiled/SVMPredict.py,jacobbaron/Neuron_Segmentation,1
"from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

from ut.util.log import printProgress

default_classifiers = [
        KNeighborsClassifier(3),
        SVC(kernel=""linear"", C=0.025),
        SVC(gamma=2, C=1),
        DecisionTreeClassifier(max_depth=5),
        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
        AdaBoostClassifier(),
        GaussianNB(),
        LDA(),
        QDA()]

",stats/classification/explore.py,thorwhalen/ut,1
"
#get the dataset
faces = fetch_olivetti_faces()
print faces.DESCR

#plot the faces
print_faces(faces.images, faces.target, 20)

#split data
X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.25, random_state=0)
svc_1 = SVC(kernel='linear')

#using cross validation to evaluate on the training data
evaluate_cross_validation(svc_1, X_train, y_train, 5)

#training and testing
train_and_evaluate(svc_1, X_train, X_test, y_train, y_test)


",python/sklearn/learning_sklearn/svm_classification.py,qingkaikong/useful_script,1
"            cm = confusion_matrix(y_test, y_pred)

            # accuracy
            results[data][model_class_name_map[model]][""acc""].append(np.sum(cm.diagonal()) * 100.0 / np.sum(cm))
        
            pass
        pass

    # Linear SVC
    print ""model is LinearSVC.""
    model_ = LinearSVC()
    st = time.time()
    model_.fit(X_train, y_train)
    et = time.time()
    y_pred = model_.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    for epoch in epochs:  # add the same results to all epochs
        results[data][""LinearSVC""][""acc""].append(np.sum(cm.diagonal()) * 100.0 / np.sum(cm))
        results[data][""LinearSVC""][""elapsed""].append(et - st)
        pass",cw/evaluate_sparse_data.py,kzky/python-online-machine-learning-library,1
"list_hog_fd = []
for feature in letter_image_features:
    print ""feature"", feature
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier
joblib.dump(clf, ""digits_cls_alpha7.pkl"", compress=3)


# LATER:",handwritingRecognition/generateClassifierAlpha2.py,forrestgtran/TeamX,1
"features,labels = extract_features(list_images)

pickle.dump(features, open('features', 'wb'))
pickle.dump(labels, open('labels', 'wb'))

features = pickle.load(open('features'))
labels = pickle.load(open('labels'))

# run a 10-fold CV SVM using probabilistic outputs.
X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=0)
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)

# probabalistic SVM
clf =  sklearn.calibration.CalibratedClassifierCV(svm)
clf.fit(X_train, y_train)
y_pred = clf.predict_proba(X_test)


k_fold = KFold(len(labels),n_folds=10, shuffle=False, random_state=0)",fish/Inception3-SVM-Classifier.py,sysid/kg,1
"#X, Y = datasets.load_svmlight_file('glass.scale_binary')
#X, Y = datasets.load_svmlight_file('Data/heart_scale')
#X, Y = datasets.load_svmlight_file('Data/w8a')

#X, Y = datasets.load_svmlight_file('toy_2d_16.train')

C=1

from sklearn import svm

#clf = svm.SVC(C=C,kernel='linear',verbose=True)
clf = svm.SVC(C=C,kernel='rbf',gamma=1.0,verbose=True)
t0=time.clock()
svm_m= clf.fit(X,Y)
t1=time.clock()

print '\nTrains Takes: ', t1-t0
#print 'alpha\n',clf.dual_coef_.toarray()

#print 'nSV=',clf.n_support_",examples/svm_example.py,ksirg/pyKMLib,1
"from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from sklearn.grid_search import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

from pipeline_grid_search import PipelineGridSearchCV

pipe = Pipeline([
    (""pca"", PCA()),
    (""svm"", SVC()),
    ])

cv_params = dict([
    ('pca__n_components', [100,200,300]),
    ('svm__C', [1,10,100,1000]),
])

X, y = make_classification(n_samples=1000, n_features=1000)
",examples/example.py,tkerola/pipeline_grid_search,1
"
  #C=[0.001,0.01,0,1,1,10,100,1000,10000,100000]
  #gamma=[1.0e-7,1.0e-6,1.0e-5,1.0e-4,0.001,0.01,0.1,1,10,100]

  C=[10000,100000,1000000]
  gamma=[1e-7,1.0e-8]

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/tune-rain-five2.py,n7jti/machine_learning,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also
    --------",sklearn/svm/classes.py,smartscheduling/scikit-learn-categorical-tree,1
"def NBAccuracy(features_train, labels_train, features_test, labels_test):
    """""" compute the accuracy of your Naive Bayes classifier """"""
    ### import the sklearn module for GaussianNB
    #from sklearn.naive_bayes import GaussianNB
    #clf = GaussianNB()
    from sklearn.svm import SVC
    clf = SVC(kernel=""linear"")
    

    ### fit the classifier on the training features and labels
    clf.fit(features_train, labels_train)

    ### calculate and return the accuracy on the test data
    ### this is slightly different than the example,
    ### where we just print the accuracy
    ### you might need to import an sklearn module",terrain-data/classify.py,askldjd/udacity-machine-learning,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED",shinken/external_command.py,xorpaul/shinken,1
"	#set the timer
	start = time.time()

	trainX = np.load('trainX_feat.npy')
	testX = np.load('testX_feat.npy')
	trainY = np.load('trainY_feat.npy')
	testY = np.load('testY_feat.npy')
	print('\n!!! Data Loading Completed !!!\n')
	
	#clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print(accuracy_score(testY, pred))
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",UMKL/exploratory/kernel.py,akhilpm/Masters-Project,1
"    ----------
    .. [1] `Wikipedia entry on the Hinge loss
            <http://en.wikipedia.org/wiki/Hinge_loss>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',
         penalty='l2', random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/metrics/classification.py,RPGOne/Skynet,1
"
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
def learn(training_data, training_labels, show_score=False, store=False):

    print (""Start Learning...."")
    
    clf = SVC(kernel='linear', probability=True, C=1)

    clf.fit(training_data, training_labels)

    print (""Done Learning."")
    
    
    if store:
        print (""Pickling classifier..."")
        pickle.dump(clf, open(path_config.CLASSIFIER_PICKLING_FILE, 'wb'))",src/train_classifier.py,kep1616/data_programming,1
"#On the digits dataset, plot the cross-validation score of a SVC estimator with an linear kernel as a function of parameter C (use a logarithmic grid of points, from 1 to 10).


#import numpy as np
#from sklearn import cross_validation, datasets, svm

#digits = datasets.load_digits()
#X = digits.data
#y = digits.target

#svc = svm.SVC(kernel='linear')
#C_s = np.logspace(-10, 0, 10)

import os
import numpy as np
from sklearn import cross_validation, datasets, svm, neighbors, linear_model
        

class mydata:
    def __init__(self):",Scikit/Scikit/exercise_2_3_1.py,TechnicHail/COMP188,1
"    """"""
        Esta función devuelve un diccionario con
        los clasificadores que vamos a utilizar y
        una rejilla de hiperparámetros
    """"""
    clfs = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) 
            }

    grid = { 
    'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},",Tareas/tarea_3/temp.py,rsanchezavalos/compranet,1
"  N = len(y)

  print 'Training... ( N =', N, ')'
  shuffler = cross_validation.ShuffleSplit(N, 1, 0.1)
  for train_idx, test_idx in shuffler:
    X_train = X[train_idx,:]
    X_test = X[test_idx,:]
    y_train = y[train_idx]
    y_test = y[test_idx]
    
    clf = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.003, verbose=False)
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    print 'Correct rate:', or_ml.correctRate(pred, y_test)

    gg = np.meshgrid(xrange(0, 180, 1), xrange(0, 256, 1))
    dd = np.column_stack((gg[0].flat, gg[1].flat))
    pred = clf.predict(dd)
    pred2d = pred.reshape(256, 180)
    joblib.dump(pred2d, opt.outfile)",or_lib/scripts/color_train.py,Beautiful-Flowers/object_recognizer,1
"  # Need to build 2D array for X, Y, Z
  for c in cs:
    powc = np.power(10,c)
    f1_scores.append([])
    accuracies.append([])
    cs_array.append([])
    gs_array.append([])
    for g in gammas:
      powg = np.power(10,g)

      clf = svm.SVC(kernel='rbf', C=powc, gamma=powg)
      clf.fit(xs[:TRAIN_SIZE], ys[:TRAIN_SIZE])

      ys_predicted = clf.predict(xs[TRAIN_SIZE:]) # Predict

      accuracy = clf.score(xs[TRAIN_SIZE:], ys[TRAIN_SIZE:])
      #f1_score = metrics.f1_score(ys[TRAIN_SIZE:], ys_predicted, average='micro')
      accuracies[-1].append(accuracy)
      cs_array[-1].append(c)
      gs_array[-1].append(g)",forestfires/optimize_c_gamma.py,Josephu/svm,1
"pipeline.add(normalise)
pipeline.add(cut_samples, [onset])
pipeline.add(pool)

# Run pipeline
X_train = pipeline.run(X_train)
X_test = pipeline.run(X_test)

# Create classifier
from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train.ravel())

# Create submission
prediction = model.predict(X_test)
io.create_submission(y_test.ravel(), prediction, ""submission_pooling.csv"")",examples/pooling.py,wohlert/agnosia,1
"            self._crossvalidation.crossvalidate(\
                scaled_cv_feasibles, scaled_cv_infeasibles)

        # @todo WARNING maybe rescale training feasibles/infeasibles (!) 
        fvalues = [f.getA1() for f in self._selected_feasibles]
        ivalues = [i.getA1() for i in self._selected_infeasibles]

        points = ivalues + fvalues
        labels = [-1] * len(ivalues) + [1] * len(fvalues) 

        self._clf = svm.SVC(kernel = 'linear', C = self._best_parameter_C, tol = 1.0)
        self._clf.fit(points, labels)

        # Update new basis of meta model
        self._normal = self.get_normal()

        self.logger.log()
        return True

    def distance_to_hp(self, x):",evopy/metamodel/cma_svc_linear_meta_model.py,jpzk/evopy,1
"    # Does it need to be numpy?        
    return returnList

def makeSVM(X, y, names, direction):
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    '''
    Using sklearn SVM.SVC - Support Vector Classifier
    If no kernel is supplied then it defaults to the RBF kernel
    '''
    clf = SVC(probability=True, verbose=False)
    scores = cross_val_score(clf, X, y, cv=nIterations) # K-fold cross valildation, K = nIterations
    avgScore = np.mean(scores)
    # unnecessary because of cross_val_score magic!!!! AMAZING
#    clf.fit(X_train, y_train) 
#    y_pred = clf.predict(X_test)
#    score = clf.score(X_test, y_test)

    print('The SVM, moving in the {} direction, using {} and {} scored an average of {} \n'.format(direction, names[0], names[1], avgScore))
    print(79 * '_')",FYP/main.py,matt123miller/Learning-Python-ML,1
"    labels_train_pos = np.ones((data_train_pos.shape[0], 1))
    data_train_neg = load_data(new_train_data_path, type=""neg/"")
    labels_train_neg = np.zeros((data_train_neg.shape[0], 1))
    x_train = np.concatenate((data_train_pos, data_train_neg), axis=0)
    y_train = np.concatenate((labels_train_pos, labels_train_neg), axis=0)

    x_train_features = hog_extraction(x_train)

    print(""Apprentissage."")
    error, clf = cross_validation_svm(x_train_features, y_train, N=3)
    # error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.05), N=5)
    # error, clf = cross_validation(x_train_features, y_train, AdaBoostClassifier(n_estimators=50))
    # error, clf = cross_validation(x_train_features, y_train, RandomForestClassifier(), N=0)

    window_w = SIZE_TRAIN_IMAGE[0]
    window_h = window_w

    print(""Predictions sur les données de test."")
    # The images in which a face is to detect.
    test_images = [given_data_test_path + file_name for file_name in os.listdir(given_data_test_path)]",tests.py,jjerphan/SY32FacialRecognition,1
"
			# print('Inverse transformation !!!')
			# print(test_vector)
			# inv_trans = dict_vectorizer.inverse_transform(test_vector_transformed)

			# fit LinearSVC
			# multi label binarizer to convert iterable of iterables into processing format
			mlb = MultiLabelBinarizer()
			y_enc = mlb.fit_transform(train_data_labels_list)

			train_vector = OneVsRestClassifier(svm.SVC(probability=True))
			classifier_rbf = train_vector.fit(train_data_trasformed, y_enc)

			# test_vecc = cnt_vectorizer.fit_transform(X[:, 0])
			# # todo use pickle to persist
			# test_vector_reshaped = np.array(test_vector.ravel()).reshape((1, -1))
			prediction = classifier_rbf.predict(test_vector_transformed)


			print(""Predicted usernames: \n"")",src/predict.py,rajikaimal/emma,1
"                ave_score = np.average(scores)
                print(""Average: %d 日, %s, Prediction: %f, Score: %f"" %
                      (s, f, ave_predict, ave_score))
                results.append(ave_predict)
                score_results.append(ave_score)
        return results, score_results

    print(""Decision Tree"")
    dt_preds, dt_scores = do_prediction(Predictor.Predictor(tree.DecisionTreeClassifier()), 10)
    print(""SVM"")
    svm_preds, svm_scores = do_prediction(Predictor.Predictor(svm.SVC(kernel='rbf')), 5)
    print(""SGD"")
    sgd_preds, sgd_scores = do_prediction(Predictor.Predictor(lm.SGDClassifier()), 10)

    print(""Decision Tree"")
    print('\t'.join([str(r) for r in dt_preds]))
    print('\t'.join([str(r) for r in dt_scores]))

    print(""SVM"")
    print('\t'.join([str(r) for r in svm_preds]))",RunPredict.py,SakaiTakao/StockPrediction,1
"import numpy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
import scipy.sparse

from preprocessing import normalize, micro_tokenize
import conf

class NBSVM_predictor(object):
    def __init__(self, kernel, class_weight, C):
        self.svm = SVC(
            kernel=kernel,
            class_weight=class_weight,
            C=C,
            random_state=conf.SEED,
        )
        self.r = None

    def fit(self, X, y):
        # NBSVM as described in ""Baselines and bigrams: simple, good",experiments/src/evaluate_nbsvm.py,OFAI/million-post-corpus,1
"from sklearn.pipeline import Pipeline  # noqa
from sklearn.cross_validation import cross_val_score, ShuffleSplit  # noqa
from mne.decoding import EpochsVectorizer, FilterEstimator  # noqa


scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
vectorizer = EpochsVectorizer()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                              ('scaler', scaler), ('svm', clf)])

data_picks = mne.pick_types(rt_epochs.info, meg='grad', eeg=False, eog=True,
                            stim=False, exclude=raw.info['bads'])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):
",plot_compute_rt_decoder.py,jpirsch/myBCI,1
"
def makePrediction(para,temLogNumPerTW,timeWindow,labelTWL):
	traingSetSize=int(math.floor(timeWindow*para['trainingSetPercent']))
	print('%d timewindows are treated as training dataset!'%traingSetSize)
	trainX=np.array(temLogNumPerTW[0:traingSetSize])
	trainY=np.array(labelTWL[0:traingSetSize])
	# clf=svm.SVC(C=1.0, cache_size=5000, class_weight=None, coef0=0.0,
 #    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
 #    max_iter=-1, probability=False, random_state=None, shrinking=True,
 #    tol=0.001, verbose=False)
	clf=svm.LinearSVC(penalty='l1', tol=0.0001, C=1, dual=False, fit_intercept=True,
	intercept_scaling=1, class_weight='balanced',  max_iter=1000)
	clf=clf.fit(trainX,trainY.ravel())

	testingX=temLogNumPerTW[traingSetSize:]
	testingY=labelTWL[traingSetSize:]
	# testingX = trainX
	# testingY = trainY
	prediction=list(clf.predict(testingX))
	print np.count_nonzero(prediction)",SVM/SVM_SOSP.py,cuhk-cse/loglizer,1
"while ((0 in Y_test) & (1 in Y_test) & (2 in Y_test) & (3 in Y_test) & (4 in Y_test) & (5 in Y_test) & (6 in Y_test)) == False:
	test_size *= 1.02
	X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=test_size, random_state=0)
	print ""Couldn't detect all labels in testing data. Regenerating testing data...""

# Daten skalieren
max_abs_scaler = MaxAbsScaler()
X_train = max_abs_scaler.fit_transform(X_train)

# SVM Klassifizierung
svm = svm.LinearSVC()
svm.fit(X_train, Y_train) 
X_test = max_abs_scaler.transform(X_test)
Y_pred = svm.predict(X_test)

target_names = [""Bahnticket"", ""Bewirtungsbeleg"", ""Flugticket"", ""Hotelrechnung"", ""Reisekostenabrechnung"", ""Taxi-Beleg"", ""Sonstiges""]
print classification_report(Y_test, Y_pred, target_names=target_names)

print 'SVM Done.\nAccuracy: %f' % accuracy_score(Y_test, Y_pred)
",latex/tex/code/classification.py,manu183/Cloudbasierte-Klassifizierung-von-Belegen,1
"

def test_SVC_predict_from_file():
    from b4msa.classifier import SVC
    from b4msa.textmodel import TextModel
    from b4msa.utils import read_data_labels
    import os
    fname = os.path.dirname(__file__) + '/text.json'
    X, y = read_data_labels(fname)
    t = TextModel(X)
    c = SVC(t)
    c.fit_file(fname)
    y = c.predict_file(fname)
    for i in y:
        assert i in ['POS', 'NEU', 'NEG']


def test_SVC_predict():
    from b4msa.classifier import SVC
    from b4msa.textmodel import TextModel",b4msa/tests/test_classifier.py,INGEOTEC/b4msa,1
"y = y[order].astype(np.float)

X_train = X[:.9 * n_sample]
y_train = y[:.9 * n_sample]
X_test = X[.9 * n_sample:]
y_test = y[.9 * n_sample:]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    print (fig_num)
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.clf()
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)

    # Circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
",sklearnLearning/statisticalAndSupervisedLearning/plot_iris_exercise.py,zhuango/python,1
"        k = int(len(i)*ratio)
        train_indices += (random.Random(seed).sample(i,k=k))
    #find the unused indices
    s = np.bincount(train_indices,minlength=n_samples)
    mask = s==0
    test_indices = np.arange(n_samples)[mask]
    return train_indices,test_indices
def Lsvm_patatune(train_x,train_y):
    tuned_parameters = [
        {'kernel': ['precomputed'], 'C': [0.01, 0.1, 1, 10, 100, 1000]}]
    clf = GridSearchCV(SVC(C=1, probability=True), tuned_parameters, cv=5, n_jobs=1
                       )  # SVC(probability=True)#SVC(kernel=""linear"", probability=True)
    clf.fit(train_x, train_y)
    return clf.best_params_['C']

def kn(X,Y):
    d = np.dot(X,Y)
    dx = np.sqrt(np.dot(X,X))
    dy = np.sqrt(np.dot(Y,Y))
    if(dx*dy==0):",AMKL/ACal7rbf.py,hongliuuuu/Results_Dis,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ll_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeast/example/test_ll_l21.py,jundongl/scikit-feast,1
"        #data, labels = prepare_searchlight_mvpa_data(images, conditions,
        #                                                    random=RandomType.UNREPRODUCIBLE)

        # the following line is an example to leaving a subject out
        #epoch_info = [x for x in epoch_info if x[1] != 0]

    num_subjs = int(sys.argv[5])
    # create a Searchlight object
    sl = Searchlight(sl_rad=1)
    mvs = MVPAVoxelSelector(data, mask, labels, num_subjs, sl)
    clf = svm.SVC(kernel='linear', shrinking=False, C=1)
    # only rank 0 has meaningful return values
    score_volume, results = mvs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        score_volume = np.nan_to_num(score_volume.astype(np.float))
        io.save_as_nifti_file(score_volume, mask_image.affine,
                                   'result_score.nii.gz')
        seq_volume = np.zeros(mask.shape, dtype=np.int)
        seq = np.zeros(len(results), dtype=np.int)",examples/fcma/mvpa_voxel_selection.py,IntelPNI/brainiak,1
"dim = 300
x_p = np.random.multivariate_normal(np.ones(dim) * 1, np.eye(dim), num_p)
x_n = np.random.multivariate_normal(np.ones(dim) * 1.5, np.eye(dim), num_n)
x = np.vstack([x_p, x_n])
y = np.array([1.] * num_p + [-1.] * num_n)

# Hyper parameters
cost = 1e0
max_iter = 10000000

clf_mdsvm = csvc.SVC(C=cost, kernel='poly', max_iter=max_iter)
clf_sklearn = svm.SVC(C=cost, kernel='poly', max_iter=max_iter, shrinking=False, gamma=1.)

t = time.time()
clf_mdsvm.fit(x, y)
print 'MDSVM: {} sec'.format(time.time() - t)

t = time.time()
clf_sklearn.fit(x, y)
print 'scikit-learn: {} sec'.format(time.time() - t)",benchmarks/bench_poly_svc.py,sfujiwara/mdsvm,1
"# Load data set and target values
data, target = make_classification(
    n_samples=1000,
    n_features=45,
    n_informative=12,
    n_redundant=7
)

def svccv(C, gamma):
    val = cross_val_score(
        SVC(C=C, gamma=gamma, random_state=2),
        data, target, 'f1', cv=2
    ).mean()

    return val

def rfccv(n_estimators, min_samples_split, max_features):
    val = cross_val_score(
        RFC(n_estimators=int(n_estimators),
            min_samples_split=int(min_samples_split),",bayes_opt/sklearn_example.py,vsmolyakov/opt,1
"        print ""Usage: %s [username[:password]@]<address> [protocol list...]"" % sys.argv[0]
        print ""Available protocols: %s"" % ATSVC.KNOWN_PROTOCOLS.keys()
        print ""Username and password are only required for certain transports, eg. SMB.""
        sys.exit(1)

    import re

    username, password, address = re.compile('(?:([^@:]*)(?::([^@]*))?@)?(.*)').match(sys.argv[1]).groups('')

    if len(sys.argv) > 2:
        dumper = ATSVC(sys.argv[2:], username, password)
    else:
        dumper = ATSVC(username = username, password = password)
    dumper.play(address)",packages/python/impacket-0.9.9.9/examples/atsvc.py,sipdbg/sipdbg,1
"trainData = preprocessing.scale(trainData)
print('preprocessing over')
# Split the dataset in two equal parts
X_train, X_test, y_train, y_test = train_test_split(
    trainData, trainTarget, test_size=0.2, random_state=123)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

clf = svm.LinearSVC()
#clf = svm.SVC(gamma=0.001, C=100) #this gives improved results

clf.fit(X_train,y_train)
#print('training over')
#print(le.inverse_transform(clf.predict(X_test)))
score = clf.score(X_test, y_test)
print('score')
print(score)
",Python/Basic/apply_clf_svm.py,wahibhaq/android-speaker-audioanalysis,1
"
class TestPermCV(unittest.TestCase):

    def test_perm_cv(self):
        X, y = datasets.make_classification(n_samples=20, n_features=5,
                                            n_informative=2)
        n_perms = 3
        n_folds = 2
        rnd = 0
        # = With EPAC
        wf = Perms(CV(SVC(kernel=""linear""), n_folds=n_folds,
                      reducer=ClassificationReport(keep=True)),
                   n_perms=n_perms, permute=""y"",
                   random_state=rnd, reducer=None)
        r_epac = wf.run(X=X, y=y)
        # = With SKLEARN
        from sklearn.cross_validation import StratifiedKFold
        clf = SVC(kernel=""linear"")
        r_sklearn = [[None] * n_folds for i in xrange(n_perms)]
        perm_nb = 0",epac/tests/test_workflow.py,neurospin/pylearn-epac,1
"import datetime

import app.analytics.filterSentences as fl
import networkx as nx
import matplotlib.pyplot as plt
G=nx.DiGraph()

np.seterr(divide='ignore',invalid='ignore')

listOfYears = []
clf = linear_model.Perceptron(n_iter=90)#svm.SVC(probability=True)
probs = []
titles = []
trainData = eval(open('trainDoubleSet','r').readlines()[0])
testData = open('testDoubleSet','r').readlines()
#
#C
def train(features):
    features = [item for item in features if len(item[0]) != 0]
    feats = [item[0] for item in features]",perceptron.py,JFriel/honours_project,1
"    print(len(neg_vec))

    # Merge positive and negative feature vectors and generate their corresponding labels.
    vec = np.array(pos_vec + neg_vec)
    vec_label = np.array([1] * len(pos_vec) + [0] * len(neg_vec))

    # ##############################################################################
    # Classification and accurate analysis.

    # Using 10-fold cross-validation to evaluate the performance of the predictor.
    clf = svm.LinearSVC()
    scores = cross_validation.cross_val_score(clf, vec, y=vec_label, cv=10)
    print('Per accuracy in 10-fold CV:')
    print(scores)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

    # ###############################################################################
    # Classification and ROC analysis.

    # Run classifier with cross-validation and plot ROC curves",repDNA/example/example1.py,liufule12/repDNA,1
"nuts = ImageSet(data_path + '/data/supervised/nuts')
nut_blobs = [n.invert().findBlobs()[0] for n in nuts]
for n in nut_blobs:
    tmp_data.append([n.area(), n.height(), n.width()])
    tmp_target.append(1)

dataset = np.array(tmp_data)
targets = np.array(tmp_target)

print('Training Machine Learning')
clf = LinearSVC()
clf = clf.fit(dataset, targets)
clf2 = LogisticRegression().fit(dataset, targets)

print('Running prediction on bolts now')
untrained_bolts = ImageSet(data_path + '/data/unsupervised/bolts')
unbolt_blobs = [b.findBlobs()[0] for b in untrained_bolts]
for b in unbolt_blobs:
    ary = [b.area(), b.height(), b.width()]
    name = target_names[clf.predict(ary)[0]]",SimpleCV/examples/machine-learning/machine-learning_nuts-vs-bolts.py,tpltnt/SimpleCV,1
"    [LogisticRegression(random_state=42)],
    [LogisticRegression(random_state=42, multi_class='multinomial', solver='lbfgs')],
    [LogisticRegression(random_state=42, fit_intercept=False)],
    [LogisticRegressionCV(random_state=42)],
    [RidgeClassifier(random_state=42)],
    [RidgeClassifierCV()],
    [SGDClassifier(random_state=42)],
    [SGDClassifier(random_state=42, loss='log')],
    [PassiveAggressiveClassifier(random_state=42)],
    [Perceptron(random_state=42)],
    [LinearSVC(random_state=42)],
    [OneVsRestClassifier(SGDClassifier(random_state=42))],
])
def test_explain_linear(newsgroups_train, clf):
    assert_explained_weights_linear_classifier(newsgroups_train, clf)


@pytest.mark.parametrize(['clf'], [
    [LogisticRegression(random_state=42)],
    [SGDClassifier(random_state=42)],",tests/test_sklearn_explain_weights.py,TeamHG-Memex/eli5,1
"'''
from sklearn import svm
from sklearn.externals import joblib

FILENAME = 'clf.pkl'

try:
    clf = joblib.load(FILENAME)
except:
    import asl
    clf = svm.SVC(gamma=0.0001, C=50, probability=True)
    clf.fit(asl.data, asl.target)
    joblib.dump(clf, FILENAME)",leap-motion-client/classifier.py,kasirajanss93/hackaz-gspeak,1
"clf_tree = tree.DecisionTreeClassifier(max_depth=10)
clf_tree.fit(features_train, target_train)
cal_score(""DECISION TREE CLASSIFIER"",clf_tree, features_test, target_test)
	
#LOGISTIC REGRESSION 
logreg = LogisticRegression(C=3)
logreg.fit(features_train, target_train)
cal_score(""LOGISTIC REGRESSION"",logreg, features_test, target_test)
#predictions = logreg.predict(test)
# SUPPORT VECTOR MACHINES 
clf = svm.SVC(kernel = 'linear')
clf.fit(features_train, target_train)
cal_score(""LINEAR KERNEL"",clf, features_test, target_test)
#print clf.kernel
#for sigmoid kernel
clf= svm.SVC(kernel='rbf', C=2).fit(features_train, target_train)
cal_score(""SVM RBF KERNEL"",clf, features_test, target_test)		
#predictions = clf.predict(test)
#Lasso 
clf = linear_model.Lasso(alpha=.1)",opencosmics.py,sidgan/opencosmics,1
"    train=[]
    test=[]
    for i in indices:
        if (indices[i]==fold):
            test.append(i-1)
        else:
            train.append(i-1)
    #generated train and test lists, incuding indices of the examples in training/test
    #for the specific fold. Indices starts from 0 now
    
    clf = svm.SVC(C=c, kernel='precomputed')
    #print dat.km[0]
    gram = [[] for x in xrange(0,len(dat.km))]
    train_gram = [] #[[] for x in xrange(0,len(train))]
    test_gram = []# [[] for x in xrange(0,len(test))]
    r=-1
    for row in dat.km:
        r+=1
        gram[r]=[0 for i in range(0,len(dat.km))]           
        for key,element in row.iteritems():",scripts/cross_validation.py,nickgentoo/scikit-learn-graph,1
"    x_train, y_train = features.make_arrays(spam_train, ham_train)
    x_cv, y_cv = features.make_arrays(spam_cv, ham_cv)
    x_test, y_test = features.make_arrays(spam_test, ham_test)
    _log_time(t)
    return x_train, y_train, x_cv, y_cv, x_test, y_test


def use_svm(x_train, y_train, x_cv, y_cv, x_test, y_test):
    t = time.time()
    logging.info(""Training SVM classifier"")
    clf = svm.SVC()
    clf.fit(x_train, y_train)
    t = _log_time(t)
    logging.info(""Predicting CV set"")
    pred_cv = clf.predict(x_cv)
    t = _log_time(t)
    print ""CV error rate: {:.2f}%"".format(np.abs(pred_cv-y_cv).mean()*100)
    logging.info(""Predicting test set"")
    pred_test = clf.predict(x_test)
    _log_time(t)",ml_spam/main.py,pomalley/ml_spam,1
"       in_model = list_inputs
       list_features.discard('id')
       in_modelF = list_features
       #X = df[list(in_model)]
       X = df[list(in_modelF)]   # exclude 'id'
       y = df[output_var]
       start_time = time.time() #start time to calculate speed
       modelSVM = svm.SVC(probability=True, class_weight=""auto"")
       #kernel='poly', degree=3, C=1.0  #kernel='rbf', gamma=0.7, C=1.0
       #modelSVM = svm.SVC(kernel='poly', degree=3, C=1.0,probability=True, class_weight=""balanced"")
       #modelSVM = svm.SVC(kernel='linear')
       #modelSVM = svm.SVC(probability=True, class_weight=""auto"")
       #modelSVM = svm.SVC(probability=True)
       resultSVM = modelSVM.fit(X, y) 
       elapsed_timeSVM = time.time() - start_time  # end time for Algorithm
       pred_SVM = resultSVM.predict(X)
       timeAlg.append(elapsed_timeSVM)
       gini_score_SVM = 2*roc_auc_score(df[output_var], pred_SVM)-1
       giniAlg.append(gini_score_SVM)
",ex3groupf/FA_GroupF_Final_A1A41A6_H1H5.py,LJohnnes/iembdfa,1
"from sklearn import svm

if __name__ == ""__main__"":
    X , y = make_blobs(n_samples = 1000, centers = 10, random_state=123)
    y = np.take([True, False], (y < 5))
    
    lin_svc = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
                  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
                  max_iter=-1, probability=False, random_state=None, shrinking=True,
                  tol=0.001,verbose=False)
    rbf_svc = svm.SVC(C=1.0, kernel='rbf', gamma=0.7)
    poly_svc = svm.SVC(C=1.0, kernel='poly', degree=3)
    
    #palette = itertools.cycle(seaborn.color_palette(n_colors = 10))
    scores_lin = []
    scores_rbf = []
    scores_poly = []
    lin_roc_auc_scorer = []
    rbf_roc_auc_scorer = []
    poly_roc_auc_scorer = []",kernel_selection.py,lidalei/DataMining,1
"    train_data = iter(data[1:-1])
    test_data = iter([data[0], data[-1]])

    # label junk food as -1, the others as +1
    y = np.ones(len(data))
    y[:6] = -1
    y_train = y[1:-1]
    y_test = np.array([y[0], y[-1]])

    pipeline = Pipeline([('vect', CountVectorizer(min_df=1)),
                         ('svc', LinearSVC())])

    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
        'svc__loss': ('l1', 'l2')
    }

    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)",python/sklearn/sklearn/feature_extraction/tests/test_text.py,seckcoder/lang-learn,1
"        self.framesInTheSlidingWindow = int(self.featuresPerSeries/3)
        self.learningProblem = learningProblem
        #This is the path from where the experiment results will be read
        self.readPath = readPath
        self.writePath = writePath
        self.slidingWindow = slidingWindow
        #print ""features per series"",2*self.featuresPerSeries
        self.numberOfFeatures = 6
        self.test_size = test_size
        if learningProblem != ""regression"":
            self.clf = SVC(C=1.6,gamma=0.002)
        else:
            self.clf = SVR(kernel='rbf',C=1.2, epsilon=1.38)
        self.flagPredict = flag
        if flag:
            self.writePath = './tuft_real_data/17June/extractedFeatures/'

    def outlierDetection(self,listItem):
        #Reference http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm
        listItem = copy.deepcopy(listItem)",predict_on_test_videos.py,haramoz/RND-ss14,1
"from sklearn.pipeline import Pipeline
from sklearn.cross_validation import cross_val_score, ShuffleSplit

from mne.decoding import ConcatenateChannels, FilterEstimator

scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
concatenator = ConcatenateChannels()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                              ('scaler', scaler), ('svm', clf)])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):

    print(""Just got epoch %d"" % (ev_num + 1))

    if ev_num == 0:",examples/realtime/plot_compute_rt_decoder.py,effigies/mne-python,1
"
# Constants
k_folds = 3 # k-fold cross-validation

def train_classifier(x, y, lb):
    # Convert to binary array
    # http://stackoverflow.com/a/34276057/2578205
    Y = lb.fit_transform(y)

    if k_folds >= 2:
        clf = CalibratedClassifierCV(LinearSVC(), cv=k_folds, method='sigmoid')
    else:
        clf = LinearSVC()

    classifier = Pipeline([
        ('vectorizer', CountVectorizer(
            min_df=0,
            ngram_range=(1,3),
            analyzer='word',
            # stop_words='english',",src/classifier/classifier.py,Glavin001/IssueBot,1
"        (KNeighborsClassifier(n_neighbors=10), 'kNN'),
        (RandomForestClassifier(n_estimators=100), 'Random forest')):
    print('_' * 80)
    print(name)
    results.append(benchmark(clf, *bm_data))

for penalty in ['l2', 'l1']:
    print('_' * 80)
    print('%s penalty' % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3),
                             *bm_data))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty),
                             *bm_data))

# Train SGD with Elastic Net penalty",News/NewsClassification.py,craymichael/News,1
"    assert_equal(pipe.get_params(deep=True),
                 dict(svc__a=None, svc__b=None, svc=clf))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)
    # Smoke test the repr:
    repr(pipe)
",venv/lib/python2.7/site-packages/sklearn/tests/test_pipeline.py,GbalsaC/bitnamiP,1
"
data = []
target = []

for x in xrange(1000):
	data.append([float(x), x/2.0, x/3.0, x/4.0, x/5.0, x/6.0, x/7.0, x/8.0, x/9.0, x/10.0])
	target.append(float(x))

target[-6] = 1

clf = svm.SVC(gamma=.001, C=100)

stop = -5

clf.fit(data[:stop], target[:stop])

# print data[1]
# print target

print clf.predict(data[stop:])",Tests/moreMLpractice.py,hubwayPredict/main,1
"        'RandomForestClassifier': RandomForestClassifier(),
        'RidgeClassifier': RidgeClassifier(),
        'GradientBoostingClassifier': GradientBoostingClassifier(),
        'ExtraTreesClassifier': ExtraTreesClassifier(),
        'AdaBoostClassifier': AdaBoostClassifier(),


        'SGDClassifier': SGDClassifier(),
        'Perceptron': Perceptron(),
        'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),
        'LinearSVC': LinearSVC(),

        # Regressors
        'LinearRegression': LinearRegression(),
        'RandomForestRegressor': RandomForestRegressor(),
        'Ridge': Ridge(),
        'LinearSVR': LinearSVR(),
        'ExtraTreesRegressor': ExtraTreesRegressor(),
        'AdaBoostRegressor': AdaBoostRegressor(),
        'RANSACRegressor': RANSACRegressor(),",auto_ml/utils_models.py,ClimbsRocks/auto_ml,1
"plt.xscale('log')
# plt.savefig('./figures/regression_path.png', dpi=300)
plt.show()

#############################################################################
print(50 * '=')
print('Section: Dealing with the nonlinearly'
      'separable case using slack variables')
print(50 * '-')

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
# plt.tight_layout()
# plt.savefig('./figures/support_vector_machine_linear.png', dpi=300)",code/optional-py-scripts/ch03.py,1iyiwei/pyml,1
"    features_train = ll.transform(features_train)
    features_test = ll.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_4(LocallyLinearEmbedding).py,dikien/Machine-Learning-Newspaper,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = reliefF.reliefF(X[train], y[train])

        # rank features in descending order according to score
        idx = reliefF.feature_ranking(score)
",skfeature/example/test_reliefF.py,jundongl/scikit-feature,1
"def test(w,data):
    x=rescale(do_feature_vector(data))
    result = x*w
    return result

def do_svm(test,train):
    test_x = do_feature_vector(test)
    test_y = classify(test)
    train_x = do_feature_vector(train)
    train_y = classify(train)
    svc = svm.SVC().fit(train_x,train_y)
    print (svc.predict(test_x) - test_y)


def main(loops, c_data):
    global CONFIG_DATA
    CONFIG_DATA = config(c_data)
    loop = loops
    if loop < 1:
        loop = 1",src/linear_least_squares.py,pemami4911/Numerical-Analysis-Semester-Project,1
"
import numpy as np


def main():
    """"""Orchestrate the retrival of data, training and testing.""""""
    data = get_data()

    # Get classifier
    from sklearn.svm import SVC
    clf = SVC(probability=False,  # cache_size=200,
              kernel=""rbf"", C=2.8, gamma=.0073)

    print(""Start fitting. This may take a while"")

    # take all of it - make that number lower for experiments
    examples = len(data['train']['X'])
    clf.fit(data['train']['X'][:examples], data['train']['y'][:examples])

    analyze(clf, data)",ML/mnist/svm/python.py,MartinThoma/algorithms,1
"    #####################################################
    training_data = prune_nonnumber(training_data)
    #for row in training_data:
        #del row[0]
    #####################################################
    threshold = int(len(training_labels) * 0.8)
    validation_data = np.array(training_data[threshold:], float)
    validation_labels = np.array(training_labels[threshold:], float)
    training_data = np.array(training_data[:threshold], float)
    training_labels = np.array(training_labels[:threshold], float)
    clf = ensemble.GradientBoostingClassifier(n_estimators=1000, max_depth=2) #ensemble.ExtraTreesClassifier()#ensemble.RandomForestClassifier() #tree.DecisionTreeClassifier()  #skl.linear_model.SGDClassifier(penalty='elasticnet') #svm.NuSVC() #GaussianNB()
    clf.fit(training_data, training_labels)
    validation_results = clf.predict(validation_data)
    fpr, tpr, thresholds = metrics.roc_curve(validation_labels, validation_results, pos_label=1)
    auc = metrics.auc(fpr,tpr)
    print auc 
    testing_data = read_csv(""test.csv"", category_dict, False, False)
    """"""
    for row in testing_data:
        del row[9]",classify.py,AmrARaouf/StumbleUpon-Classification,1
"# also load the iris dataset
iris = datasets.load_iris()
rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    # Test parameters on classes that make use of libsvm.
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    # Check consistency on dataset iris.",434-MachineLearning/final_project/linearClassifier/sklearn/svm/tests/test_svm.py,neale/CS-program,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",.meteor/dev_bundle/python/Lib/email/test/test_email_renamed.py,sicknarlo/df_site,1
"    for delta_g, one_fv, two_fv, label in train_data:
        train_fvs.append(map(operator.sub, one_fv, two_fv))
        train_labels.append(label)
        train_fvs.append(map(operator.sub, two_fv, one_fv))
        train_labels.append(not label)
    # Now that we have our data, build our classifier.
    logging.info('Training binary classifier')
    scaler = StandardScaler()
    scaler.fit(train_fvs)
    train_fvs = scaler.transform(train_fvs)
    clf = GridSearchCV(LinearSVC(),
                       {'C': 10.0 ** np.arange(-2, 4)},
                       cv=5)  # Merely importing TextBlob causes a hang
                              # in multiprocessing (n_jobs=-1).  ???
    clf.fit(train_fvs, train_labels)
    logging.info('Best cross-validation hyperparameters: %s',
                 ', '.join('{}={}'.format(k, v)
                           for k, v in clf.best_params_.iteritems()))
    logging.info('Reranking testing data')
    best_hyps = []",hw4-reranking/rerank.py,query/mt-submissions,1
"        # selector to remove zero-variance features
        self.var_selector = VarianceThreshold()

        #scaler = StandardScaler(with_mean=False)
        #selector = SelectKBest(chi2, k=n_features)
        #combined_features = FeatureUnion([('selector', selector)])
        #self.pipeline = Pipeline([('vectorizer', feat_vectorizer), ('features', combined_features), ('scaler', scaler), ('model', self.model)])

        # Choose model type
        if (model_type == ""linear_svm""):
            self.model = eval(""LinearSVC("" + model_params + "")"")
        elif (model_type == ""svm""):
            self.model = eval(""SVC("" + model_params + "")"")
        elif (model_type == ""knn""):
            self.model = eval(""KNeighborsClassifier("" + model_params + "")"")
        elif (model_type == ""ridge_classifier""):
            self.model = eval(""RidgeClassifier("" + model_params + "")"")
        elif (model_type == ""ridge_regression""):
            self.classification = False
            self.model = eval(""Ridge("" + model_params + "")"")",mlfix/scripts/model.py,varisd/school,1
"np.random.seed(1)

# Combine Earth with LogisticRegression in a pipeline to do classification
earth_classifier = Pipeline([('earth', Earth(max_degree=3, penalty=1.5)),
                             ('logistic', LogisticRegression())])

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA"", ""Earth""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA(),
    earth_classifier]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",examples/plot_classifier_comp.py,scikit-learn-contrib/py-earth,1
"#test_size= 0.2 # selecting number of samples

#X_train, X_test, y_train, y_test = cross_validation.train_test_split(data[0],data[1], test_size=0.2, random_state=1) # use it for subsampling
'''
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
'''
#L1 SVM
clf = LinearSVC(penalty='l1', dual=False, C=c)
scores = cross_validation.cross_val_score(clf, data[0], data[1], cv=10)

print(scores)
print(""L1 SVM \n  Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))


#L2 SVM trained on all the features
clf = LinearSVC(penalty='l2',dual=False,C=c)
scores = cross_validation.cross_val_score(clf, data[0], data[1], cv=10)",crossValidation.py,narendrameena/featuerSelectionAssignment,1
"    """"""

    def __init__(self, mvp, preproc_pipeline=None, clf=None,
                 mask_type='unilateral', voting='soft', weights=None):

        self.mvp = mvp
        self.voting = voting
        self.mask_type = mask_type

        if clf is None:
            clf = SVC(C=1.0, kernel='linear', probability=True,
                      decision_function_shape='ovo')
        self.clf = clf

        # If no preprocessing pipeline is defined, we'll assume that at least
        # scaling and minor (univariate) feature selection is desired.
        if preproc_pipeline is None:
            scaler = StandardScaler()
            transformer = SelectAboveCutoff(1, fisher_criterion_score)
            preproc_pipeline = Pipeline([('transformer', transformer),",skbold/estimators/roi_voting_classifier.py,lukassnoek/skbold,1
"    features_train = iso.transform(features_train)
    features_test = iso.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_4(isomap).py,dikien/Machine-Learning-Newspaper,1
"                           memory_level=1)
X = nifti_masker.fit_transform(dataset_files.func)
# Restrict to non rest data
X = X[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",plot_haxby_grid_search.py,abenicho/isvr,1
"X_train, X_test, Y_train, Y_test = train_test_split(irisDataset.data,
                                                    irisDataset.target,
                                                    test_size=0.3,
                                                    random_state=0)
#Shaping for numpy use
X_train.shape, Y_train.shape
X_test.shape, Y_test.shape

# Initializing classifier
#clf = tree.DecisionTreeClassifier()
clf = SVC(C=1, kernel='linear')
# Training classifier
clf.fit(X_train, Y_train)
prediction = clf.predict(X_test)
score = accuracy_score(Y_test, prediction)
print('Score: {}'.format(score))",Classifiers/detectFlower.py,pawanrai9999/SimpleMachines,1
"        data = Raw(raw, self.NAME, self.LEN)
        self.addr = struct.unpack(""!H"", data.pop(self.LEN))[0]

    def pack(self):  # pragma: no cover
        return struct.pack(""!H"", self.addr)

    def is_mcast(self):  # pragma: no cover
        return self.addr & self.MCAST

    def multicast(self):
        return HostAddrSVC(self.addr | self.MCAST, raw=False)

    def anycast(self):
        return HostAddrSVC(self.addr & ~self.MCAST, raw=False)

    def __str__(self):
        s = ""0x%02x"" % (self.addr & ~self.MCAST)
        if self.is_mcast():
            return s + "" M""
        return s + "" A""",python/lib/packet/host_addr.py,klausman/scion,1
"
from utils import *

from scipy.stats import mannwhitneyu

def loo(X, labels):
    label_encoder = LabelEncoder()
    int_labels = label_encoder.fit_transform(labels)
    print(int_labels)

    clf = SVC(kernel='linear')#, probability=True)
    nb = X.shape[0]
    loo = LeaveOneOut(nb)

    silver, gold = [], []
    for train, test in loo:
        print('.')
        X_train, X_test = X[train], X[test]
        y_test = [int_labels[i] for i in test]
        y_train = [int_labels[i] for i in train]",src/tales_analysis.py,mikekestemont/grimm,1
"
        # ASSESSING GINI INDEX FOR EACH INVIVIDUAL IN THE INITIAL POOL

        X_train=df[var_model]
        Y_train=df[output_var]

        ######
        # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.
        #####
        if ""SVM"" in methods:
            svc = svm.SVC(probability = True)
            model= svc.fit(X_train,Y_train)
            Y_predict=model.predict(X_train)
        ######
        # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.
        #####


        ######
        # CHANGE_HERE - START: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)",iembdfa/GeneticFeature.py,ccbrandenburg/financialanalyticsproject,1
"
def calculate_desciptors_pos(im):
    fd = hog(im, orientations, pixels_per_cell, cells_per_block, visualize, normalize)
    return fd


def train():
    fds = []
    labels = []
    clf_type = 'LIN_SVM'
    clf = LinearSVC()
    clf.fit(fds, labels)
    joblib.dump(clf, model_path)


if __name__ == '__main__':
    src_img = eva.load_image(Const.image1)
    calculate_desciptors_pos(src_img)",object-detector/extract_feature.py,Esmidth/DIP,1
"LABELED = 'Labeled'
TRAIN = 'Train'
TEST = 'Test'

class Classifier(BiPlot):
    '''
    To hold methods and data to support classification of measurements in a STOQS database.
    See http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html
    '''
    classifiers = { 'Nearest_Neighbors': KNeighborsClassifier(3),
                    'Linear_SVM': SVC(kernel=""linear"", C=0.025),
                    'RBF_SVM': SVC(gamma=2, C=1),
                    'Decision_Tree': DecisionTreeClassifier(max_depth=5),
                    'Random_Forest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                    'AdaBoost': AdaBoostClassifier(),
                    'Naive_Bayes': GaussianNB(),
                    'LDA': LDA(),
                    'QDA': QDA()
                  }
    def getActivity(self, mpx, mpy):",stoqs/contrib/analysis/classify.py,danellecline/stoqs,1
"vTrain[:, 4] = vFamP
vTrain[:, 5] = vFamS

# Finally normalize all items: zscore --> crop outliers (top 5%) --> min-max
for v in np.arange(np.shape(vTrain)[1]):
	vTrain[:, v] = (vTrain[:, v] - np.mean(vTrain[:, v])) / np.std(vTrain[:, v])
	vTrain[np.where(vTrain[:, v]) > optSD, v] = optSD
	vTrain[:, v] = (vTrain[:, v] - np.min(vTrain[:, v])) / (np.max(vTrain[:, v]) - np.min(vTrain[:, v]))

## Train the classifiers
clf = svm.SVC(kernel='rbf', gamma=optGamma, C=optC).fit(vTrain, vSurv)

####################################################################
##
## import the test data and process

data = np.genfromtxt('test.csv', delimiter="","", 
	                              skip_header=1, 
	                              missing_values='', 
	                              filling_values='inf',",titanic/titanic_analysis.py,josephdviviano/intro-data-science,1
"### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()
# features_train = features_train[:len(features_train)/100] 
# labels_train = labels_train[:len(labels_train)/100] 



#########################################################
### your code goes here ###
from sklearn.svm import SVC
clf = SVC(kernel=""rbf"", C=10000)
clf.fit(features_train, labels_train)
preds = clf.predict(features_test)

from sklearn.metrics import accuracy_score
t0 = time()
accuracy_score(labels_test, preds)
#########################################################

",svm/svm_author_id.py,selva86/python-machine-learning,1
"    print ""x url shape"",x_url.shape
    print ""t url shape"",t_url.shape
    label = np.array(label)

    clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=False,C=1.0,tol=0.0001,class_weight=None, random_state=None, intercept_scaling=1.0)

    clf1 = LogisticRegression(penalty='l2',dual=True,fit_intercept=False,C=1.0,tol=0.0001,class_weight=None, random_state=None, intercept_scaling=1.0)

    clf2 = SGDClassifier(loss=""log"", penalty=""l2"",alpha=0.0001,fit_intercept=False)#sgd 的训练结果也不错

    clf3 = SVC(C=1.0,gamma=0.3,probability=True)

    clf4 = SVC(kernel='sigmoid',degree=9,gamma=0.3,probability=True)

    print ""训练content lr""
    clf.fit(x_content,label)
    pred0 = clf.predict_proba(t_content)[:,1]

    print ""训练content sgd""
    clf2.fit(x_content,label)",combine_final.py,ezhouyang/class,1
"    random.shuffle(indices)
    X = numpy.array([X[i] for i in indices])
    Y = numpy.array([Y[i] for i in indices])

    ind = int(Y.shape[0] * 0.7)

    X_tr, X_te = X[:ind], X[ind:]
    Y_tr, Y_te = Y[:ind], Y[ind:]

    def classifier():
      return svm.LinearSVC(C=0.8)
    
    def train():
      c = classifier()
      c.fit(X, Y)

      dirname = os.path.dirname(sys.argv[0])

      filename = '{}/svm_{}_{}.pkl'.format(dirname, year, time.time())
      with open(filename, 'w') as f:",vision/data/Torpedoes/svm.py,cuauv/software,1
"  train_inputs.append(map(float, inputs))
print 'Done. Time taken: %f secs.\n' % (time()-t)

print 'Create classifier'
t = time()
clf = None

# No preprocessing for SVMs
# Otherwise, scale inputs (preprocessing to make more amenable for machine learning)
if classifier == 1: # Support vector machines
  clf = SVC()
elif classifier == 2: # Gaussian Naive Bayes
  train_inputs = preprocessing.scale(np.array(train_inputs))
  clf = GaussianNB()
elif classifier == 3: # Multinomial Naive Bayes
  clf = MultinomialNB()
elif classifier == 4: # Stochastic gradient descent with logistic regression
  train_inputs = preprocessing.scale(np.array(train_inputs))
  clf = SGDClassifier(loss='log')
print 'Done. Time taken: %f secs.\n' % (time()-t)",predict.py,onkursen/predicting-terrorist-attacks,1
"        self.assert_numpy_array_almost_equal(result[0], expected[0])
        self.assert_numpy_array_almost_equal(result[1], expected[1])
        self.assert_numpy_array_almost_equal(result[2], expected[2])

    def test_validation_curve(self):
        digits = datasets.load_digits()
        df = pdml.ModelFrame(digits)

        param_range = np.logspace(-2, -1, 2)

        svc = df.svm.SVC(random_state=self.random_state)
        result = df.learning_curve.validation_curve(svc, 'gamma',
                                                    param_range)
        expected = lc.validation_curve(svm.SVC(random_state=self.random_state),
                                       digits.data, digits.target,
                                       'gamma', param_range)

        self.assertEqual(len(result), 2)
        self.assert_numpy_array_almost_equal(result[0], expected[0])
        self.assert_numpy_array_almost_equal(result[1], expected[1])",pandas_ml/skaccessors/test/test_learning_curve.py,pandas-ml/pandas-ml,1
"
dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

ab=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1, n_estimators=10, random_state=1)

dt=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')

gb=GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=25, presort='auto', random_state=1, subsample=1.0, verbose=0, warm_start=False)

rf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False)
",scripts/histograms/non-normalised-ml.py,jmrozanec/white-bkg-classification,1
"    C_range = [  1.00000000e-02,   1.00000000e-01,   1.00000000e+00,
         1.00000000e+01,   1.00000000e+02,   1.00000000e+03,
         1.00000000e+04]
    gamma_range = [1.00000000e-06,   1.00000000e-05,   1.00000000e-04,
         1.00000000e-03,   1.00000000e-02,   1.00000000e-01,
         1.00000000e+00,   1.00000000e+01,   1.00000000e+02]
    param_grid = dict(gamma=gamma_range, C=C_range)
    [train_x, train_y] = parseDataFile('trainingDataFull.csv', featsSelect)
    cv = StratifiedShuffleSplit(train_y, n_iter=5, test_size=0.2, random_state=42)
    print 'starting grid search'
    grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)
    grid.fit(train_x, train_y)
    print(""The best parameters are %s with a score of %0.2f"" % (grid.best_params_, grid.best_score_))",tuneC.py,ekyauk/BeepBoop,1
"

class SupportVectorMachine(PersonalClassifier):
    def __init__(self, data_set, labels):
        super(SupportVectorMachine, self).__init__(data_set, labels)

    def train(self, kernel='linear'):
        x = self.data_set
        y = self.labels

        clf = svm.SVC(kernel=kernel, gamma=10)
        clf.fit(x, y)
",personal_classifier/support_vector_machine.py,BavoGoosens/Gaiter,1
"
def json_from_file(relative_path):
    with open(os.path.dirname(os.path.abspath(__file__)) + ""/"" + relative_path) as fs:
        return json.load(fs)

# Loading sensors' datas
places = json_from_file(""../sensors/all_places_infos.json"")

X   = pd.DataFrame()
y   = pd.DataFrame()
clf = svm.SVC(kernel = 'linear', C = 1)

for place in places:
    try:
        df = pd.read_csv(""dataset_sensor-"" + str(place[""id""]) + "".csv"", index_col = ""Date"")
    except:
        continue

    if (df.shape[0] < 250):
        continue",modeles/prediction/learn-classification.py,DavidBruant/6element,1
"            p['from_text_poplar_characters_rate'],
            p['to_text_poplar_characters_rate']]
        

def svn_hit(p, e):
    return int(p['from_encode'] == e['from_encode'] and p['to_encode'] == e['to_encode'])

def svm_classify(potential_infos, lang, encodings):
    svm_key = lang + ""_"" + """".join(sorted(encodings))
    if svm_key not in SVMS:
        SVMS[svm_key] = svm.SVC()
        X, Y = svm_learn_data(lang, encodings)
        SVMS[svm_key].fit(X, Y)
    return SVMS[svm_key].predict(map(svm_args, potential_infos))",garbled_characters_detecter/svm_classifyer.py,yankay/garbled-characters-detecter,1
"        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
        print('=' * 80)
        print(name)
        results.append(benchmark(clf))

    for penalty in [""l2"", ""l1""]:
        print('=' * 80)
        print(""%s penalty"" % penalty.upper())
        # Train Liblinear model
        results.append(benchmark(LinearSVC(loss='l2', 
                                           penalty=penalty,
                                           dual=False, 
                                           tol=1e-3)))

        # Train SGD model
        results.append(benchmark(SGDClassifier(alpha=.0001, 
                                               n_iter=50,
                                               penalty=penalty)))
",python/sklearn/examples/general/classification_of_text_documents_using_sparse_features.py,kwailamchan/programming-languages,1
"from sklearn.pipeline import make_pipeline
from sklearn.grid_search import GridSearchCV

pipeline = make_pipeline(CountVectorizer(),
                         LinearSVC())
pipeline.fit(text_train, y_train)
print(""Pipeline test score: %f""
      % pipeline.score(text_test, y_test))
visualize_coefficients(pipeline.named_steps['linearsvc'],
                       pipeline.named_steps['countvectorizer'].get_feature_names())

param_grid = {'linearsvc__C': 10. ** np.arange(-3, 3)}

grid_search = GridSearchCV(pipeline, param_grid=param_grid)",solutions/text_pipeline.py,amueller/pydata-amsterdam-2016,1
"    set_c1_spiketrains(validation_ddict)
    # Let the simulation run to ""fill"" the layer pipeline with spikes
    sim.run(40)
    clear_data(layer_collection['C2'])
    print('>>>>>>>>> Extracting data samples for validation <<<<<<<<<')
    validation_samples = extract_data_samples(validation_image_count)
    sim.reset()

    print('Fitting SVM model onto the training samples')

    clf = svm.SVC(kernel='linear')
    clf.fit(training_samples, training_labels)

    logfile = open('log_final/{}.log'.format(plb.Path(args.weights_from).stem), 'a')

    print('Predicting the categories of the validation samples')
    predicted_labels = clf.predict(validation_samples)
    print('============================================================',
          file=logfile)
    print('Epoch', epoch, file=logfile)",classify-images.py,roberthangu/snn_object_recognition,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",jyhton/Lib/email/test/test_email_renamed.py,p4datasystems/CarnotKE,1
"
    Returns
    -------
    is_parameter: bool
        Whether the parameter was found to be a a named parameter of the
        estimator's fit method.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in signature(estimator.fit).parameters


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",sklearn/utils/validation.py,kevin-coder/scikit-learn-fork,1
"	cv 		= cross_validation.LeaveOneOut(n=len(y))
	scores 	= cross_validation.cross_val_score(clf, X, y, cv=cv)
	true 	= sum(scores)
	acc 	= sum(scores)/len(scores)'''

	cv 		= cross_validation.LeaveOneOut(n=len(y))
	y_pred 	= [0] * len(y)
	for train, test in cv:
		X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
		clf 	= naive_bayes.GaussianNB()
		#clf 	= svm.LinearSVC()
		#clf = tree.DecisionTreeClassifier()
		#clf = RandomForestClassifier(n_estimators=10)
		#clf = NearestCentroid()

		clf.fit(X_train, y_train)
		pred 	= clf.predict(X_test)[0]
		ind 	= test[0]
		y_pred[ind] = pred
	y 		= y.tolist()",scripts/predict_rivalry_net/src/predict_rivalry.py,nbir/gambit-scripts,1
"scoring = 'accuracy'

#evaluating different algorithms

models = []
models.append(('LR',LogisticRegression()))
models.append(('LDA',LinearDiscriminantAnalysis()))
models.append(('KNN',KNeighborsClassifier()))
models.append(('CART',DecisionTreeClassifier()))
models.append(('NB',GaussianNB()))
models.append(('SVM',SVC()))

results = []
names = []

for name, model in models:
    kfold = cross_validation.KFold(n=num_instances,n_folds=num_folds,random_state=seed)
    cv_results = cross_validation.cross_val_score(model,X_train,Y_train,cv=kfold,scoring=scoring)
    results.append(cv_results)
    names.append(name)",ml_python.py,harishkrao/Machine-Learning,1
"
features, class_outputs = get_training('../joined_matrix_split.txt', 'LARCENY/THEFT') 
# 0:27 category  28:38= supervisor district, 39 = count 311, 
# 40:76 counts of 911 categories, 77=count of 911 aggregate
#class_outputs = preprocessing.binarize(class_outputs)
#features_10k, class_10k = features[:10000, 0:40], class_outputs[:10000]
#test_features_10k, test_class_10k = features[10000:20000, 0:40], class_outputs[10000:20000]

features_10k, class_10k, test_features_10k, test_class_10k = get_equal_training_test(features, class_outputs)

#call_models = [svm.LinearSVC(), svm.SVC(), tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
#all_models = [tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
all_models = [svm.SVR()]
opt_models = dict()
for i,model in enumerate(all_models):
    param_grid = None
    opt_model = train(features_10k, class_10k, test_features_10k, test_class_10k, model, params_grid = param_grid)
    (accuracy, precision, recall) = get_classification_metrics(test_features_10k, test_class_10k, opt_model)
    opt_models[opt_model] = (accuracy, precision, recall) 
",classificationSpecific/train_classification_specific.py,JamesWo/cs194-16-data_manatees,1
"import sklearn.naive_bayes
import sklearn.svm
import sklearn.metrics
import fusion
import sklearn.datasets
import sklearn.linear_model

data = sklearn.datasets.load_iris()

clf = fusion.FusionClassifier([sklearn.svm.SVC(probability=True), sklearn.svm.SVC(probability=True), sklearn.naive_bayes.MultinomialNB()])

clf2 = sklearn.naive_bayes.MultinomialNB()

clf2.fit(data.data, data.target)
clf.fit(data.data, data.target)

y = clf.predict(data.data)
y2 = clf2.predict(data.data)
",toy.py,alan-mnix/MLFinalProject,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",venv/lib/python2.7/site-packages/sklearn/metrics/tests/test_metrics.py,chaluemwut/fbserver,1
"
# Fine-tuning machine learning models via grid search
# Tuning hyperparameters via grid search
'''
The approach of grid search is quite simple, it's brute-force exhaustive search
paradigm where we specify a list of values for different hyperparameters, and the
computer evaluates the model performance for each combination of those to obtain
the optimal set:'''
from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC
pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])
param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_grid = [{'clf__C': param_range,
                           'clf__kernel': ['linear']},
                         {'clf__C': param_range,
                          'clf__gamma': param_range,
                          'clf__kernel': ['rbf']}]
gs = GridSearchCV(estimator=pipe_svc,
                                    param_grid=param_grid,
                                    scoring='accuracy',",self_practice/Chapter 6 Part I.py,wei-Z/Python-Machine-Learning,1
"                    self.logger(""%d / %d fitting FCE for feature %d"" % (i, self.features.shape[0], f))

                fce.fit(X, f, fit_rdcs=False, fit_gp=True)
        else:
            raise Exception(""FCE_type unknown"")
            

        if self.verbose > 0:
            self.logger(""Done."")

    def predict(self, X, model=sklearn.svm.SVC(), param_dist=None):
        X = X.view(np.ndarray)
        if X.ndim == 1:
            X = X.reshape(1, -1)

        #X = self.normalizer.transform(X)

        if self.dynamic_features == False:
            if self.verbose > 0:
                self.logger(""training the model"")",Raccoon/core/raccoon.py,adrinjalali/Network-Classifier,1
"                                                                 ['hinge', 'squared_hinge'],
                                                                 ['l1', 'l2'],
                                                                 [True, False],
                                                                 [True, False]):
    features = input_data.drop('class', axis=1).values.astype(float)
    labels = input_data['class'].values

    try:
        # Create the pipeline for the model
        clf = make_pipeline(StandardScaler(),
                            LinearSVC(C=C,
                                      loss=loss,
                                      penalty=penalty,
                                      dual=dual,
                                      fit_intercept=fit_intercept,
                                      random_state=324089))
        # 10-fold CV score for the pipeline
        cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
        accuracy = accuracy_score(labels, cv_predictions)
        macro_f1 = f1_score(labels, cv_predictions, average='macro')",model_code/LinearSVC.py,rhiever/sklearn-benchmarks,1
"        total[1] += 1
        if total[0] > 0.9:
            pca = PCA(n_components=total[1])
            break
    pca.fit(data_train)
    data_train = pca.transform(data_train)


    # Create and fit a svm classifier
    from sklearn import svm
    clf = svm.SVC()
    clf.fit(data_train, answer_train)
    data_test = pca.transform(data_test)
    print(np.sum(clf.predict(data_test) == answer_test))

    # Create and fit a nearest-neighbor classifier
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier()
    knn.fit(data_train, answer_train)
    KNeighborsClassifier(algorithm='auto',n_neighbors=5,weights='uniform') # try different n_neighbors",model.py,samyachour/EKG_Analysis,1
"### Construct a Pipeline
pipe = {}

pipe['Decision Tree'] = make_pipeline(Imputer(), 
                                      scaling,
                                      DecisionTreeClassifier())

pipe['SVM'] = make_pipeline(Imputer(),
                            scaling,
                            SelectKBest(), 
                            SVC(kernel='rbf'))

pipe['Naive Bayes'] = make_pipeline(Imputer(),
                                    scaling,
                                    SelectKBest(),
                                    GaussianNB())

pipe['Random Forest'] = make_pipeline(Imputer(),
                                      scaling, 
                                      RandomForestClassifier())",P5-Identify Fraud from Enron Email/poi_id.py,slimn/Data-Analyst,1
"	y_casual_train = y_casual[:nTrain]
	y_regis_train = y_regis[:nTrain]
	y_total_train = y_total[:nTrain]
	Xtest = X[nTrain:,:]
	y_casual_test = y_casual[nTrain:]
	y_regis_test = y_regis[nTrain:]
	y_total_test = y_total[nTrain:]
	
	#linear
	#param_grid = {'C': [1, 5, 10, 100],}
	#clf = GridSearchCV(SVC(kernel='linear'), param_grid,n_jobs=-1)
	#clf = SVC(kernel='poly')
	#clf.fit(Xtrain,ytrain)
	#pred = clf.predict(Xtest)
	#print ""best estimator = "",clf.best_estimator_
	#print ""RMSE poly = "", rmsle(ytest, pred)

	#new stuff
	clf_regis = SVR(kernel='poly')
	clf_regis.fit(Xtrain,y_regis_train)",svm/test_poly_svm.py,agadiraju/519finalproject,1
"    log2g = np.logspace(-15, 5, 5, base=2).tolist()

    tuned_parameters = [
        {
            'kernel': ['linear','rbf'],
            'gamma': log2g,
            'C': log2c,
        },
    ]

    clf = grid_search.GridSearchCV(svm.SVC(random_state=7), tuned_parameters, scoring='roc_auc', cv=5, verbose=10, n_jobs=5)
    clf.fit(x_train, y_train)
    print ''
    print 'with these params ' + str(clf.best_params_) + ' we got accuracy of ' + str(clf.best_score_)
    return clf


# ------------------------------------------------------------
# ------------------------------------------------------------
",modules/localization/train_localization.py,allansp84/license-plate,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",sklearn/tests/test_pipeline.py,jjx02230808/project0223,1
"        See SVC.__init__ for details.
    coef0 : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    degree : int
        Degree of kernel, if kernel is polynomial.
        See SVC.__init__ for details.
    """"""

    def __init__(self, C, kernel='rbf', gamma = 1.0, coef0 = 1.0, degree = 3):
        estimator = SVC(C=C, kernel=kernel, gamma = gamma, coef0 = coef0,
                degree = degree)
        super(DenseMulticlassSVM,self).__init__(estimator)

    def fit(self, X, y):
        """"""
        .. todo::

            WRITEME
        """"""",pylearn2/models/svm.py,CKehl/pylearn2,1
"    seed = 7
    scoring = 'accuracy'

    # Spot Check Algorithms
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC()))

    # evaluate each model in turn
    results = []
    names = []
    for name, model in models:
        kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)",core/demo/BeyondCompare.py,god99me/RandomRoughForest,1
"        If ``'False'``, the ``cv_results_`` attribute will not include training
        scores.


    Examples
    --------
    >>> from sklearn import svm, datasets
    >>> from sklearn.model_selection import GridSearchCV
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",scikit-learn-0.18.1/sklearn/model_selection/_search.py,RPGOne/Skynet,1
"
    accuracy = metrics.accuracy_score(y_test, predict)
    if not accuracyonly:
        # can print out precision recall and f1
        print metrics.classification_report(y_test, predict)
    return accuracy


def test_baseline(X_labeled, y_labeled, X_test, y_test):

    clf_SVM = SVC(kernel='linear', probability=True)
    # clf_SVM = MultinomialNB()
    print '\nstart testing baseline :/'

    print 'svm'
    clf_SVM.fit(X_labeled, y_labeled)
    predict = clf_SVM.predict(X_test)
    accuracy_bl_svm = evaluation(y_test, predict)

    return accuracy_bl_svm",run.py,silbertmonaphia/ml,1
"def fit_model(train_features, train_labels, svm_clf = False, RandomForest = False, nb = False, lasso = False, kneighbor=False):
    #Input: SVM, RandomForest, and NB are all boolean variables and indicate which model should be fitted
    #SVM: Linear Support Vector Machine
    #RandomForest: Random Forest, we set the max_depth equal to 50 because of prior tests
    #NB: LDA representation using an ngram range of (1,1)
    #train_features: Train reviews that have been transformed into the relevant features
    #train_labels: Labels for the training reviews, transformed into a binary variable
    #Output: A fitted model object

    if svm_clf == True:
        clf = svm.LinearSVC()
        clf.fit(train_features, train_labels)
    elif RandomForest == True:
        clf = RandomForestClassifier(max_depth = 100, max_leaf_nodes=50, criterion='entropy')
        clf.fit(train_features, train_labels)
    elif nb == True:
        clf = GaussianNB()
        clf.fit(train_features, train_labels)
    elif lasso == True:
        clf = Lasso()",machine_learning/yelp_ml_p3.py,georgetown-analytics/yelp-classification,1
"
# Does a simple gridsearch to optimize gamma and C for the given kernel
def optimizeSVM(vectors, targets, kernel):

    accuracyResults = None
    precisionResults = None
    recallResults = None

    parameters = {'gamma': [0.1, 0.01, 0.001, 0.0001], 'C': [0.1, 1, 10, 100]}

    accuracyTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""accuracy"")
    accuracyTest.fit(vectors, targets)
    accuracyResults = accuracyTest.best_params_

    precisionTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""precision"")
    precisionTest.fit(vectors, targets)
    precisionResults = precisionTest.best_params_

    recallTest = GridSearchCV(svm.SVC(kernel=kernel), parameters, scoring=""recall"")
    recallTest.fit(vectors, targets)",SVM/SVM.py,xTVaser/web-threat-thesis,1
"            # Fit a per-column scaler
            self.Scaler = StandardScaler().fit(featureList)
            # Apply the scaler
            print(""Normalizing feature list"")
            X_total = self.Scaler.transform(featureList)
            y_total = np.asarray(labelList, dtype=np.uint8)
            rand_state = np.random.randint(0, 100)
            X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=rand_state)
            print(""Training classifier"")
            # Use a linear SVC (support vector classifier)
            self.Classifier = LinearSVC()
            # Train the SVC
            self.Classifier.fit(X_train, y_train)
            print(""Test accuracy %.4f"" % (self.Classifier.score(X_test, y_test)))
            print(""Saving classifier data for fast access"")
            classifierData[""Scaler""] = self.Scaler
            classifierData[""Classifier""] = self.Classifier
            pickle.dump(classifierData, open(self.CLASSIFIER_DATA_FILE, ""wb""))

    def Normalize(self, featureList):",classifier.py,vernor1/vehicle_detection,1
"
n_features = X.shape[1]

C = 1.0
kernel = 1.0 * RBF([1.0, 1.0])  # for GPC

# Create different classifiers. The logistic regression cannot do
# multiclass out of the box.
classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),
               'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),
               'Linear SVC': SVC(kernel='linear', C=C, probability=True,
                                 random_state=0),
               'L2 logistic (Multinomial)': LogisticRegression(
                   C=C, solver='lbfgs', multi_class='multinomial'),
               'GPC': GaussianProcessClassifier(kernel)
               }

n_classifiers = len(classifiers)

plt.figure(figsize=(3 * 2, n_classifiers * 2))",projects/scikit-learn-master/examples/classification/plot_classification_probability.py,DailyActie/Surrogate-Model,1
"    NN_params = {}
    
    if choice=='log':
        params = log_params
        model = SGDClassifier()
    elif choice=='huber':
        params = huber_params
        model = SGDClassifier()
    elif choice=='svm':
        params = SVM_params
        model = svm.SVC(C=1)
    elif choice=='rf':
        params = RF_params
        model = RandomForestClassifier(n_estimators=1000, bootstrap=False)
        #clf = RandomForestClassifier(n_estimators=1000, bootstrap=False)
    
    # Set up Grid Search
    print ""Grid search...""
    clf = GridSearchCV(model, params, n_jobs=2, scoring='f1')
    clf.fit(X[:255], y[:255]) # Grid search only on part of the data",baseline.py,momiah/cvariants_theano,1
"    X_train = X[X.index < start_test]
    X_test = X[X.index >= start_test]
    y_train = y[y.index < start_test]
    y_test = y[y.index >= start_test]
   
    # Create the (parametrised) models
    print(""Hit Rates/Confusion Matrices:\n"")
    models = [(""LR"", LogisticRegression()), 
              (""LDA"", LDA()), 
              (""QDA"", QDA()),
              (""LSVC"", LinearSVC()),
              (""RSVM"", SVC(
              	C=1000000.0, cache_size=200, class_weight=None,
                coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
                max_iter=-1, probability=False, random_state=None,
                shrinking=True, tol=0.001, verbose=False)
              ),
              (""RF"", RandomForestClassifier(
              	n_estimators=1000, criterion='gini', 
                max_depth=None, min_samples_split=2, ",SAT eBook/chapter11/forecast.py,Funtimezzhou/TradeBuildTools,1
"    
    learners = [DecisionTreeClassifier(class_weight='auto', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, random_state=42, splitter='best')]

    for i in xrange(len(learners)):

        ############################################################################     

        #learner = GaussianNB()
        #learner = DecisionTreeClassifier()
        #learner = RandomForestClassifier()
        #learner = SVC()
        #learner = LogisticRegression()

        learners[i] = learners[i].fit(X_train,y_train)

        y_pred = learners[i].predict(X_test)

        ############################################################################

        print learners[i]",python/py/quant/avg_clf_train.py,austinjalexander/sandbox,1
"
f = h5py.File('stack_MCIsMCIc_W.mat')
labels = f[""labels""]; labels=np.array(labels).flatten()
stack = f[""stack_MCIsMCIc_W""]; stack=np.array(stack).astype(np.float32).T;
imgsize = f[""imgsize""]; imgsize=np.array(imgsize).flatten().astype(np.int32)
del f

os.chdir(workdir)

# Define SVM linear voter
clf=svm.SVC(kernel='linear')


# Change labels to 0, 1
ul=np.unique(labels)
l=labels
l[l==ul[0]]=0
l[l==ul[1]]=1
labels==l
",lib/Clasif_atlas_no_def3_CV_regiones_W.py,juanka1331/VAN-applied-to-Nifti-images,1
"        Callable object that returns a scalar score; greater is better.

    Examples
    --------
    >>> from sklearn.metrics import fbeta_score, make_scorer
    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
    >>> ftwo_scorer
    make_scorer(fbeta_score, beta=2)
    >>> from sklearn.model_selection import GridSearchCV
    >>> from sklearn.svm import LinearSVC
    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
    ...                     scoring=ftwo_scorer)
    """"""
    sign = 1 if greater_is_better else -1
    if needs_proba and needs_threshold:
        raise ValueError(""Set either needs_proba or needs_threshold to True,""
                         "" but not both."")
    if needs_proba:
        cls = _ProbaScorer
    elif needs_threshold:",projects/scikit-learn-master/sklearn/metrics/scorer.py,DailyActie/Surrogate-Model,1
"import numpy as np
from sklearn import svm
from sklearn.datasets import load_svmlight_file
from sklearn.metrics import accuracy_score
import time
dataset_X, dataset_Y = load_svmlight_file('cod-rna')
dataset_X= dataset_X.toarray()
for i in range(0,1):
	C= 10.0**i
	model = svm.SVC(C=C, kernel='rbf', gamma =1.0, tol=0.001, cache_size=256)
	start = time.time()
	model.fit(dataset_X, dataset_Y)
	end = time.time()

	score = 1-accuracy_score(dataset_Y, model.predict(dataset_X))
	print (C, end-start, score, flush=True)",examples/Benchmark/scicit-learn/Kernel_CSvm.py,Shark-ML/Shark,1
"        pass


class MultipClassifiers(Classifier):

    def __init__(self, doc2vec):
        super(MultipClassifiers, self).__init__(doc2vec)
        self.classifiers = [
            #KNeighborsClassifier(3), #*
            #KNeighborsClassifier(6), #*
            SVC(kernel=""linear"", C=0.025, probability=True),
            #SVC(gamma=2, C=1), #*
            #GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),#need memory
            #DecisionTreeClassifier(max_depth=5),
            #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            #MLPClassifier(alpha=1),
            #AdaBoostClassifier(),
            #GaussianNB(),
            #QuadraticDiscriminantAnalysis() #*
            ]",main.py,VNGResearch/doc2vec,1
"        solver = ['lbfgs']
        max_iter = [1000]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(mlp, dict(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter, early_stopping=[False]), X, y)
        f = open('output/religiousity.mlp.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates SVM classifier
        svm = SVC()
        kernel = ['linear', 'rbf', 'poly', 'sigmoid']
        Cs = np.logspace(-3, 4, 8) # C = [0.001, 0.01, .., 1000, 10000]
        gamma = np.logspace(-3, 4, 8) # gamma = [0.001, 0.01, .., 1000, 10000]
        degree = [2, 3]
        coef0 = [0.0]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(svm, dict(kernel=kernel, C=Cs, gamma=gamma, degree=degree, coef0=coef0), X, y)
        f = open('output/religiousity.svm.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):",tests/test_religiousity.py,fberanizo/author-profiling,1
"y_train=np.asarray(y_train)

gamma = 1 / np.median(pdist(X_train, 'euclidean'))
C = compute_crange(rbf_kernel(X_train, gamma=gamma))

# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': list(2 ** np.arange(-3, 4.) * gamma), 'C': list(C),
                     'class_weight': [{1: int((y_train == -1).sum() / (y_train == 1).sum())}]},
                    ]

clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy', n_jobs=10, verbose=True)
clf.fit(X_train, y_train)

best = clf.best_estimator_
print(""Best estimator has training accuracy of %.4g"" % clf.best_score_)
svmfile = 'svm_version2/svm'
print(""Writing to svm file: "",svmfile)
joblib.dump(best, svmfile)

",train_SVM_append.py,cajal/pupil-tracking,1
"# # Cross validation with 100 iterations to get smoother mean test and train
# # score curves, each time with 20% data randomly selected as a validation set.
# cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)

# estimator = GaussianNB()
# plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

# title = ""Learning Curves (SVM, RBF kernel, $\gamma=0.001$)""
# # SVC is more expensive so we do a lower number of CV iterations:
# cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
# estimator = SVC(gamma=0.001)
# plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

# plt.show()",plot_learning_curve.py,bahmanh/DocumentClassification,1
"            ##############
        try:
            past = np.argsort(self.body[""time""])[::-1][:self.interval * self.step]
            self.lastprob = np.mean(clf.predict_proba(self.csr_mat[past])[:,pos_at])
            # self.lastprob = np.mean(np.array(prob)[order][:self.step])
        except:
            pass

    ## Train model ##
    def train(self,pne=True):
        clf = svm.SVC(kernel='linear', probability=True)
        poses = np.where(np.array(self.body['code']) == ""yes"")[0]
        negs = np.where(np.array(self.body['code']) == ""no"")[0]
        left = poses
        decayed = list(left) + list(negs)
        unlabeled = np.where(np.array(self.body['code']) == ""undetermined"")[0]
        try:
            unlabeled = np.random.choice(unlabeled,size=np.max((len(decayed),self.atleast)),replace=False)
        except:
            pass",src/util/mar.py,fastread/src,1
"    for fold in folds:
        kf.append((fold[0], fold[1]))
    return kf

def classify_final_test(data, features, data_to_classify, cost, g, svm_kernel):
    ##make all data becoming part of the training
    train_set, _ = utils.cut_dataset(range(len(data)), [], data, features)
    _, test_set = utils.cut_dataset([], range(len(data_to_classify)), data_to_classify, features)
    
    if svm_kernel == ""linear"":
        clf = svm.LinearSVC(C=cost) #used for gissete and madelon
    else:
        clf = svm.SVC(C=cost, gamma=g, kernel='rbf')  
    #
    clf.fit(train_set.fts_values, train_set.fts_pred)
    predictions = clf.predict(test_set.fts_values)
    preds = []
    for i in predictions:
        #print i
        preds.append(int(i))",MITWS_shared_memory/classification_part.py,JSilva90/MITWS,1
"
names = [""Nearest Neighbors"",
        ""Linear SVM"",
        ""Decision Tree"",
        ""Naive Bayes"",
        ""Neural Network""
]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    DecisionTreeClassifier(max_depth=5),
    GaussianNB(),
    # MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=1e-05,
    #    batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
    #    epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',
    #    learning_rate_init=0.001, max_iter=200, momentum=0.9,
    #    nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
    #    tol=0.0001, validation_fraction=0.1, verbose=False,
    #    warm_start=False)",classifier.py,Abhijith1995/tumor_classifier,1
"    y_test = [int(line.strip()) for line in open(test_label)]
    X_test = np.asarray(X_test)
    #test_X = preprocessing.normalize(test_X, norm='l2')
    y_test = np.asarray(y_test)

    
    cca = CCA(n_components=100)
    (X_cca, U_cca) = cca.fit_transform(X_training, U[:n])
    X_test_cca = cca.predict(X_test)
    
    svr = SVC()
    svr.fit(X_cca, y_training)    
    pred = svr.predict(X_test_cca)
    
    print pred
    print test_y
    print accuracy_score(y_test, pred)
    with open(test_file + '.cca.2.pred', 'w') as output:
        for p in pred:
            print >>output, p",src/matlab/spectral/ccaQE.py,mriosb08/palodiem-QE,1
"
scaler = StandardScaler().fit(x_train)
x_train = scaler.transform(x_train)

test = pd.read_csv('C:/Users/sound/Desktop/Kaggle/Leaf Classfication/Data/test.csv')
test_ids = test.pop('id')
x_test = test.values
scaler = StandardScaler().fit(x_test)
x_test = scaler.transform(x_test)

svc = SVC(probability=True)
svc.fit(x_train, y_train)
y_test = svc.predict_proba(x_test)

submission = pd.DataFrame(y_test, index=test_ids, columns=le.classes_)",Leaf Classfication/Leaf Classfication in SVM/code/Leaf Classfication in SVM.py,0Steve0/Kaggle,1
"    return newsample


if __name__ == ""__main__"":
    in_data = np.genfromtxt(""features.train"", dtype=float)
    out_data = np.genfromtxt(""features.test"", dtype=float)
    clist = [0.0001, 0.001, 0.01, 0.1, 1.0]
    num = 1.0
    vs = 5.0
    c = 0.001
    clf = SVC(C = c, kernel = 'rbf', gamma=1.0)  
    sample = classifyPoints(num, in_data, vs)
    X = np.c_[sample[:,0], sample[:,1]]
    y = sample[:,2]
    '''kf = cross_validation.KFold(len(y), n_folds=10, indices=False)
    print(kf)
    for train, test in kf:
        print(""%s %s"" % (train, test))'''
    ss = cross_validation.ShuffleSplit(len(y), test_size=0.1, random_state=0, indices=False)
    print(ss)",Week 8/cv.py,pramodh-bn/learn-data-edx,1
"    cross validation for multiple learners
    """"""
    for idx, learner in enumerate(learners):
        print(""======= {} ======="".format(str(learner_names[idx])))
        cross_validation_sklearner(learner, learner_names[idx], X, Y, K)


if __name__ == ""__main__"":
    book_y, book_X = load_training_data(""training_data/feature_avg_filtered.txt"")
    learner_lr = linear_model.LogisticRegression()
    learner_svm = svm.SVC(probability=True)
    learner_rf = ensemble.RandomForestClassifier(n_estimators=50)
    learner_dt = tree.DecisionTreeClassifier()
    learner_gbc = ensemble.GradientBoostingClassifier(n_estimators=50)
    learner_ada = ensemble.AdaBoostClassifier(n_estimators=50)
    cross_validation_multi_learners((learner_lr, learner_svm, learner_rf, learner_dt, learner_gbc),
                                    (""lr"", ""svm"", ""rf"", ""dt"", ""gbc""), book_X, book_y)",predict_age.py,wangleye/age_risk_estimation_book,1
"from sklearn.utils.testing import assert_warns
from sklearn.utils.testing import skip_if_32bit

iris = datasets.load_iris()
data, y = iris.data, iris.target
rng = np.random.RandomState(0)


def test_transform_linear_model():
    for clf in (LogisticRegression(C=0.1),
                LinearSVC(C=0.01, dual=False),
                SGDClassifier(alpha=0.001, n_iter=50, shuffle=True,
                              random_state=0)):
        for thresh in (None, "".09*mean"", ""1e-5 * median""):
            for func in (np.array, sp.csr_matrix):
                X = func(data)
                clf.set_params(penalty=""l1"")
                clf.fit(X, y)
                X_new = assert_warns(
                    DeprecationWarning, clf.transform, X, thresh)",projects/scikit-learn-master/sklearn/feature_selection/tests/test_from_model.py,DailyActie/Surrogate-Model,1
"        Value to assign to the score if an error occurs in estimator fitting.
        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",skutil/utils/fixes.py,tgsmith61591/skutil,1
"
    for gamma in gamma_grid:
        rp = MultiEGKSampler(gamma, n_sample=n_sample, normalize=normalize)
        scores = defaultdict(float)
        for idx_train, idx_test in StratifiedKFold(label):
            X_train = rp.fit_transform(means[idx_train], covs[idx_train])
            X_test = rp.transform(means[idx_test], covs[idx_test])
            l_train = label[idx_train]
            l_test = label[idx_test]
            for C in c_grid:
                clf = LinearSVC(C=C)
                clf.fit(X_train, l_train)
                l_predict = clf.predict(X_test)
                accuracy = np.mean(l_predict == l_test)
                scores[C] += accuracy

        best_C_score, best_C = max((score, C)
                                   for (C, score) in scores.iteritems())
        #print gamma, scores
        if best_C_score > best_score:",random_mvegk.py,mlds-lab/egk,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = reliefF.reliefF(X[train], y[train])

        # rank features in descending order according to score
        idx = reliefF.feature_ranking(score)
",skfeast/example/test_reliefF.py,jundongl/scikit-feast,1
"    with open(filename, ""w"") as f: 
        for i, line in enumerate(data):
            for datum in line:
                f.write(str(datum) + ""\t"")
            f.write(str(clas[i]) + ""\n"")

    print(""Done!"")


def train(data, cla):
    # clf = svm.LinearSVC(C=2.0)
    clf = svm.SVC(C=2, kernel=""rbf"", gamma=3, cache_size = 1000, probability=False)
    clf.fit(data, cla)
    # print clf.score(data, cla)
    return clf


def preprocess(data, cla):
    # print ""\nScaling and normalizing data...""
    data = preprocessing.scale(data)",tools/train_svm_from_csv.py,fmgvalente/agent,1
"     TrainSamplesClass3 = getClassExamples(train, 3)
     TrainLabelsClass3 = np.zeros(2500)
     TrainLabelsClass3.fill(3)

     TrainSamplesClass4 = getClassExamples(train, 4)
     TrainLabelsClass4 = np.zeros(2500)
     TrainLabelsClass4.fill(4)

     trainD = np.concatenate([TrainSamplesClass0, TrainSamplesClass1, TrainSamplesClass2, TrainSamplesClass3, TrainSamplesClass4], axis=0)
     trainL = np.concatenate([TrainLabelsClass0, TrainLabelsClass1, TrainLabelsClass2, TrainLabelsClass3, TrainLabelsClass4], axis=0)
     clf = svm.SVC(cache_size=7000)
     start = time.clock()
     clf.fit(trainD, trainL)
     calc_time = time.clock() - start
     wall_time = time.time() - start
     print([calc_time, wall_time])
     #dec_Fx = clf.decision_function(train['data'])
     pred_results = clf.predict(test['data'][:350000,:])",Python/hyperSpecSvm.py,Crobisaur/HyperSpec,1
"                label_number[item] = len(number_label)
                number_label.append(item)

        training_data = [ ]
        training_number = [ ]
        for i in xrange(len(labels)):
            if labels[i] is not None:
                training_data.append(data[i])
                training_number.append(label_number[labels[i]])
        
        clf = svm.SVC(probability=True)
        clf.fit(training_data, training_number)
        
        for i in xrange(len(work.index)):
            result = Classification()
            image_measure = work.get_measure(i)
            result.columns = number_label
            result.log_p = clf.predict_log_proba(image_measure.data)
            result.call = [
                number_label[max(xrange(len(number_label)),key=lambda i:item[i])]",semiautocount/classify.py,Victorian-Bioinformatics-Consortium/semiautocount,1
"            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'linear-svm':
                from sklearn.svm import SVC
                self.estimator_ = SVC(probability=True,
                                      random_state=self.random_state,
                                      kernel='linear',
                                      **self.kwargs)
            else:
                raise NotImplementedError
        else:
            raise ValueError('Invalid parameter `estimator`. Got {}.'.format(
                type(self.estimator)))
",imblearn/under_sampling/prototype_selection/instance_hardness_threshold.py,scikit-learn-contrib/imbalanced-learn,1
"        # If the name is given, then pop it
        name = kwargs.pop('name', None)
        if name is None:
            # If the name of the matcher is give, then create one.
            # Currently, we use a constant string + a random number.
            self.name = 'SVM'+ '_' + get_ts()
        else:
            # Set the name of the matcher, with the given name.
            self.name = name
        # Set the classifier to the scikit-learn classifier.",py_entitymatching/matcher/svmmatcher.py,anhaidgroup/py_entitymatching,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",logisland-plugins/logisland-scripting-processors-plugin/src/main/resources/nltk/parse/transitionparser.py,mariemat/log-island,1
"    classifier.add(LSTM(units=units, return_sequences=True, activation='sigmoid', input_shape=(None, input_shape[2])))
    for i in range(1, layers):
        classifier.add(LSTM(units=units, return_sequences=True, activation='sigmoid'))
        if with_dropout:
            classifier.add(Dropout(rate=dropout_rate))
    classifier.add(Dense(units=1))
    classifier.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
    return classifier

def build_svc():
    return SVC()",titanic/models.py,davidvartanian/machine-learning-tests,1
"        sw_interpolation=sw_interpolation,
        include_original_img=include_original_img,
        n_jobs=n_jobs,
        verbosity=verbosity,
        temp_folder=temp_folder,
        random=random)

    nb_filters = len(randConvCoord.get_filters())

    #--SVM--
    baseClassif = LinearSVC(verbose=verbose, random_state=rand_state)

    #--Classifier
    classifier = uClassifier(coordinator=randConvCoord,
                             base_classifier=baseClassif,
                             n_estimators=nb_trees,
                             max_depth=max_depth,
                             min_samples_split=min_samples_split,
                             min_samples_leaf=min_samples_leaf,
                             n_jobs=n_jobsEstimator,",examples/cifar10/rcbaggedCifar.py,jm-begon/randconv,1
"    if m == ""GBC"" or m == ""GBDT"":
        from sklearn.ensemble import GradientBoostingClassifier
        n = int(args.param) if args.param else 10
        return GradientBoostingClassifier(
            n_estimators=n)
    if m == ""LRL1"":
        from sklearn.linear_model import LogisticRegression
        return LogisticRegression(penalty='l1', C=0.01)
    if m == ""SVC"":
        from sklearn.svm import SVC
        return SVC(kernel=args.param, probability=True)
    if m == ""KNN"":
        from sklearn.neighbors import KNeighborsClassifier
        return KNeighborsClassifier()
    if m == ""XGB"":
        from .xgbwrapper import XGBWrapper
        return XGBWrapper()
    if m == ""XGBBin"":
        from .xgbwrapper import XGBBinary
        if not args.param: args.param = '0.3'",kagura/getmodel.py,nishio/kagura,1
"""""""

# データを作成
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

pl.figure(figsize=(13, 5))
for i, name, penalty in ((1, 'unregularized', 1), (2, 'regularized', 0.05)):
    # 分類器を学習
    clf = svm.SVC(kernel='linear', C=penalty)
    clf.fit(X, Y)

    # 分類境界を求める
    w = clf.coef_[0]
    a = - w[0] / w[1]
    b = clf.intercept_[0]
    xx = np.linspace(-5, 5)
    yy = a * xx - (b / w[1])
",sklearn/plot_svm_margin.py,sylvan5/PRML,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)",mne/decoding/tests/test_csp.py,choldgraf/mne-python,1
"          np.random.multivariate_normal(mean2, cov, N / 4),
          np.random.multivariate_normal(mean4, cov, N / 4)]
t = [-1] * (N / 2) + [1] * (N / 2)

pl.figure(figsize=(18, 5))
no = 1

# それぞれのカーネルでSVMを学習
for kernel in ('linear', 'poly', 'rbf'):
    # 分類器を訓練
    clf = svm.SVC(kernel=kernel)
    clf.fit(X, t)

    pl.subplot(1, 3, no)
    cmap1 = ListedColormap(['red', 'blue'])
    cmap2 = ListedColormap(['#FFAAAA', '#AAAAFF'])

    # 訓練データをプロット
    pl.scatter(X[:, 0], X[:, 1], c=t, zorder=10, cmap=cmap1)
    pl.scatter(clf.support_vectors_[:, 0],",sklearn/nonlinear_svm.py,TenninYan/Perceptron,1
"
from mimicus import config
from mimicus.classifiers.sklearn_SVC import sklearn_SVC
from mimicus.tools import datasets

class sklearn_SVC_Test(unittest.TestCase):
    '''
    Tests for the sklearn_SVC class.
    '''
    def setUp(self):
        self.svc = sklearn_SVC()
    
    def test_constructor(self):
        _ = sklearn_SVC()
        _ = sklearn_SVC()
        _ = sklearn_SVC()
    
    def test_fit(self):
        X, y, _ = datasets.csv2numpy(config.get('sklearn_SVC_test', 'traindata'))
        self.svc.fit(X, y)",mimicus/test/sklearn_SVC_test.py,srndic/mimicus,1
"autoencoder.fit(training_set, training_set,
                epochs=2000, batch_size=10, verbose=0)

training_auto = encoder.predict(training_set)
test_auto = encoder.predict(test_set)
auto_norm = np.amax(training_auto)
training_auto = training_auto / auto_norm  # normalizing for better SVM
test_auto = test_auto / auto_norm

# Support vector machine with resized images
svm_resize = svm.SVC(kernel='rbf', C=5000)
svm_resize.fit(training_resize, training_labels)

svm_resize_predict = svm_resize.predict(test_resize)

print(""\nSVM with resized images\n"")
print(""Training set confusion matrix:\n%s""
      % metrics.confusion_matrix(training_labels,
                                 svm_resize.predict(training_resize)))
print(""Test set confusion matrix:\n%s""",Exercises/07_image_reduce_and_classical_svm.py,peterwittek/qml-rg,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix(y)
    n_samples, n_features = X.shape

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight = RFS.rfs(X[train, :], Y[train, :], gamma=0.1)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeast/example/test_RFS.py,jundongl/scikit-feast,1
"# print('y-test', len(y_test), y_test)

train_proba_predictions = rf1.predict_proba(X_test)
# print('proba_predictions', len(train_proba_predictions), train_proba_predictions)
print('y-test', len(y_test), y_test)
ll = log_loss(y_test, train_proba_predictions)
print(""Log Loss: {}"".format(ll))


# SVC
# svc = SVC(probability = True)
# svc.fit(X_train, y_train)
# #Y_pred = svc.predict(X_test)
# acc_svc = round(svc.score(X_train, y_train) * 100, 2)
# print('SVC- accuracy: ', acc_svc)

# calculate predictions with probabilities
# prediction_p = svc.predict_proba(X_test)
# plot_data(y_test, prediction_p[:,1], ""SVC"")
",examples/leaf/leaf-febi.py,remigius42/code_camp_2017_machine_learning,1
"y = []
for data_set in data_sets:
	(data, y_temp) = data_set
	train_data.extend(data)
	y.extend(y_temp)

count_vect = CountVectorizer(ngram_range=(1, 2), vocabulary=feature_set)
X_train = count_vect.transform(train_data)
y_train = np.asarray(y)

clf = svm.SVC()
clf.fit(X_train, y_train)

############################################################
# Step 2: Find out the polarity of each sentence of revice #
############################################################

review = input(""Please enter a review: "")
review = preprocess_document(review)
",ProductReviewAnalysisApp.py,J-A-S-A/PSA,1
"from sklearn import cross_validation, svm, metrics, cluster, tree, ensemble
import kkeras


def clst( X_train, y_train, X_test, y_test, nb_classes):
	model = tree.DecisionTreeClassifier()
	model.fit( X_train, y_train)
	dt_score = model.score( X_test, y_test)
	print( ""DT-C:"", dt_score)

	model = svm.SVC( kernel = 'linear')
	model.fit( X_train, y_train)
	sv_score = model.score( X_test, y_test)
	print( ""SVC:"", sv_score)

	model = kkeras.MLPC( [X_train.shape[1], 30, 10, nb_classes])
	model.fit( X_train, y_train, X_test, y_test, nb_classes)
	mlp_score = model.score( X_test, y_test)
	print( ""DNN:"", mlp_score)
",kcell_r1.py,jskDr/jamespy_py3,1
"X = convert2memmap(X)
y = convert2memmap(y)

Xy = dict(X=X, y=y)

## 2) Build two workflows respectively
## =======================================================

from sklearn.svm import SVC
from epac import CV, Methods
cv_svm_local = CV(Methods(*[SVC(kernel=""linear""),
                            SVC(kernel=""rbf"")]),
                  n_folds=3)
cv_svm_swf = CV(Methods(*[SVC(kernel=""linear""),
                          SVC(kernel=""rbf"")]),
                n_folds=3)

## 3) Run two workflows using local engine and soma-workflow
## =========================================================
",examples/run_a_big_matrix.py,neurospin/pylearn-epac,1
"#Convert features and answers into arrays:
testFeatures = np.asarray(testFeatures)
testAnswers = np.asarray(testAnswers)
finalFeatures = np.asarray(finalFeatures)
finalAnswers = np.asarray(finalAnswers)
cvFeatures = np.asarray(cvFeatures)
cvAnswers = np.asarray(cvAnswers)

#Creation of the learners:
lrLearner = LogisticRegression(penalty='l2', dual=False, C=10000.0)
svmLearner = svm.SVC(C=5, kernel='poly', degree=4, probability=True)
knnLearner = neighbors.KNeighborsClassifier(n_neighbors=250, algorithm='auto')

# Finding Training and Test Errors (Sequential)
# lrTrainingError, lrTestingError, lrIndices = findTrainerError(lrLearner, 'LogReg', finalFeatures, finalAnswers, testFeatures, testAnswers)
# svmTrainingError, svmTestingError, svmIndices = findTrainerError(svmLearner, 'SVM', finalFeatures, finalAnswers, testFeatures, testAnswers)
# knnTrainingError, knnTestingError, knnIndices = findTrainerError(knnLearner, 'kNN', finalFeatures, finalAnswers, testFeatures, testAnswers)

#Finding Errors (in parallel):
lrLearner, svmLearner, knnLearner, lrTrainingError, lrTestingError, lrIndices, svmTrainingError, svmTestingError, svmIndices, knnTrainingError, knnTestingError, knnIndices = findTrainerErrorParallel(lrLearner, svmLearner, knnLearner, finalFeatures, finalAnswers, testFeatures, testAnswers)",stockMarket/learning/learning.py,seba-1511/stockMarket,1
"def quickScore(estimator, trainFeatures, trainTargets, val):
	feat_train, feat_test, tar_train, tar_test = cross_validation.train_test_split(trainFeatures, trainTargets, train_size=val)
	return getScore(estimator, feat_train, tar_train, 3)

'''
def score(estimator, test):
	return estimator.decision_function(test)
	'''

def compareMultiClass(trainFeatures, trainTargets):
	rbfsvm = SVC(C=1, kernel='rbf',gamma=2)
	polysvm = SVC(C=1, kernel='poly',degree=3)

	p1v1 = onevsone(polysvm )
	r1v1 = onevsone(rbfsvm)

	r1va = onevsall(rbfsvm)
	p1va = onevsall(polysvm)

	print ""Polynomial SVM 1v1:""",runsvm.py,bpteich/MulticlassSVMProject,1
"                X_valid,y_valid = valid_set
                X_test,y_test = test_set
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_05202015.py,magic2du/contact_matrix,1
"
  star = read_image(root + ""stars/%d.png"" % i)
  X.append(star)
  y.append(1)

train_percentage = 0.75
num_classes = 3
yb = label_binarize(y, classes=[i for i in range(num_classes)])
X_train, X_test, y_train, y_test = train_test_split(X, yb, test_size=(1-train_percentage))

clf = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=False))
clf.fit(X_train, y_train)

y_score = clf.fit(X_train, y_train).decision_function(X_test)

precision = {}
recall = {}
avg_precision = {}

",svm.py,darylsew/potato,1
"
    `intercept_` : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also
    --------",sklearn/svm/classes.py,loli/sklearn-ensembletrees,1
"        step = max(min(0.99, step), 0.1)

        if num_features < 1:
            num_features = 1
        elif num_features > len(training_features.columns):
            num_features = len(training_features.columns)

        if len(training_features.columns.values) == 0:
            return input_df.copy()

        estimator = SVC(kernel='linear')
        selector = RFE(estimator, n_features_to_select=num_features, step=step)
        try:
            selector.fit(training_features, training_class_vals)
            mask = selector.get_support(True)
            mask_cols = list(training_features.iloc[:, mask].columns) + non_feature_columns
            return input_df[mask_cols].copy()
        except ValueError:
            return input_df[non_feature_columns].copy()
",tpot/tpot.py,pronojitsaha/tpot,1
"                    ""i like london better than new york""])
y_train = [[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[1],[1]]
X_test = np.array(['nice day in nyc',
                   'welcome to london',
                   'hello welcome to new york. enjoy it here and london too'])
target_names = ['New York', 'London']

classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

classifier.fit(X_train, y_train)
predicted = classifier.predict(X_test)
for item, labels in zip(X_test, predicted):",ML/sklearn_test.py,jeffzhengye/pylearn,1
"from sklearn import svm

xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

t0 = time.time()

# fit the model
clf = svm.NuSVC()
clf.fit(X, Y)

# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
t1 = time.time()
print('time taken = %.3f' % (t1 - t0))

plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto', origin='lower',",semester2/task12/example3.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"gram_list = list(reader)
gram_all = np.array(gram_list).astype('float')

nb_lines = gram_all.shape[0]
nb_lines_train = int(nb_lines*percentage_train)

gram_train = gram_all[0:nb_lines_train,0:nb_lines_train]
gram_test = gram_all[nb_lines_train:,0:nb_lines_train]

for c in [0.01, 0.1, 1, 10, 100]:
	model = svm.SVC(C = c, kernel='precomputed')

	model.fit(gram_train, y[0:nb_lines_train])
	print ""model fitted""
        print("" c : ""+str(c))

	y_pred = model.predict(gram_test)
	error_train = mean_absolute_error(y[nb_lines_train:], y_pred)
	print error_train",cheminfo/prediction_molecules_properties_kernel/script_svc.py,jajoe/machine_learning,1
"    this is the code to accompany the Lesson 2 (SVM) mini-project

    use an SVM to identify emails from the Enron corpus by their authors

    Sara has label 0
    Chris has label 1
    eg:
    from sklearn import svm
    x = [[0,1],[1,1]]
    y = [0,1]
    clf = svm.SVC()
    clf.fit(x,y)
    SVC(C=1.0, cache_size = 200, class_weight = None, coef0 = 0.0, degree = 3,
    gamma = 0.0, kernel = 'rbf', max_iter = 1, probability = False,
    random_state = None, shrinking = True, tol = 0.001, verbose = False)
""""""

import sys
from time import time
sys.path.append(""../tools/"")",svm/svm_hemant.py,CoderHam/Machine_Learning_Projects,1
"from sklearn import svm
import numpy as np
from .features import extract_features
from .classes import convert_to_integers, convert_to_strings

class GazeClassifier(object):

    def __init__(self):
        # Load default pretrained state
        # TODO
        self.learner = svm.SVC()

    def fit(self, pointlistlist, classes):

        # Convert pointlists to numeric features
        fl = list([extract_features(pl) for pl in pointlistlist])

        # Convert classes to integers
        cl = list(convert_to_integers(classes))
",gazeclassifier/gazeclassifier.py,infant-cognition-tampere/gazeclassifier-py,1
"    train_features = training features (both positive and negative)
    train_labels = corresponding label vector
    svm_eps = eps of svm
    svm_C = C parameter of svm
    classifier_type = liblinear or libsvm""""""

    #do normalization
    (train_features, train_labels), train_mean, train_std, trace = normalize([trainXy],
                                                                              trace_normalize=trace_normalize)
    if classifier_type == 'liblinear':
        clf = sklearn_svm.LinearSVC(**kwargs)
    if classifier_type == 'libSVM':
        clf = sklearn_svm.SVC(**kwargs)
    elif classifier_type == 'LRL':
        clf = LogisticRegression(**kwargs)
    elif classifier_type == 'MCC':
        clf = CorrelationClassifier(**kwargs)
    elif classifier_type.startswith('svm.'):
        ct = classifier_type.split('.')[-1]
        clf = getattr(sklearn_svm,ct)(**kwargs)",simffa/classifier.py,yamins81/simffa,1
"X_train = X[:90]
Y_train = Y[:90]
X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X,Y, test_size = 0.2)

#print X_train.shape
#print Y_train.shape

clf = DecisionTreeClassifier()
clf_fitted=clf.fit(X_train, Y_train)
#print clf_fitted.score(X_test, Y_test)
clf=svm.LinearSVC(C=1)
clf_fitted2=clf.fit(X_train, Y_train)
print clf_fitted2.score(X_test, Y_test)

scores = cross_validation.cross_val_score(clf,X,Y,cv=5)
print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std()*2))",MachinLearningTest.py,stephenmylabathula/GitTest,1
"        seed = 7
        scoring = 'accuracy'

        # Spot Check Algorithms
        models = []
        models.append(('LR', LogisticRegression()))
        models.append(('LDA', LinearDiscriminantAnalysis()))
        models.append(('KNN', KNeighborsClassifier()))
        models.append(('CART', DecisionTreeClassifier()))
        models.append(('NB', GaussianNB()))
        models.append(('SVM', SVC()))
        models.append(('RF',  RandomForestClassifier(n_estimators=100)))

        # evaluate each model in turn
        results = []
        names = []
        for name, model in models:
            kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
            cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
            results.append(cv_results)",core/demo/main.py,god99me/RandomRoughForest,1
"      
  l = len(data)
  X = data[:,1:]
  y = data[:,0]

  train_x = X[:-validation_size]
  train_y = y[:-validation_size]
  test_x  = X[-validation_size:]
  test_y  = y[-validation_size:]
  # Create and fit the svm classifier
  svc = svm.SVC(kernel='linear')
  svc.fit(train_x, train_y)
  predict_labels = svc.predict(test_x)
  print(""predicted labels :"")
  print(predict_labels)
  print(""real labels :"")
  print(test_y)
  print(""mean deviation :"")
  predict_label_np = np.array(predict_labels)
  test_y_np = np.array(test_y)",cheminfo/prediction_molecules_properties_2/sk_svm.py,jajoe/machine_learning,1
"    df = sm.datasets.get_rdataset(""Guerry"", ""HistData"").data
    smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=df).fit()


def test_scikit_learn(df):

    sklearn = pytest.importorskip('sklearn')  # noqa
    from sklearn import svm, datasets

    digits = datasets.load_digits()
    clf = svm.SVC(gamma=0.001, C=100.)
    clf.fit(digits.data[:-1], digits.target[:-1])
    clf.predict(digits.data[-1:])


def test_seaborn():

    seaborn = pytest.importorskip('seaborn')
    tips = seaborn.load_dataset(""tips"")
    seaborn.stripplot(x=""day"", y=""total_bill"", data=tips)",lib/python2.7/site-packages/pandas/tests/test_downstream.py,ammarkhann/FinalSeniorCode,1
"    print ""Classification problem:\n"", len(set(yy)), 'classes.', len(XX), ""instances.""
    return XX, np.array(yy).astype(int)



def perform_class(X, y, iterations=1):
    scores = []
    for i in range(iterations):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42+iterations)
        parameters = {'C':[0.01, 0.1, 1, 10, 100]}
        clf_acc = GridSearchCV(svm.LinearSVC(), parameters, n_jobs=3, cv=3, refit=True, scoring = 'accuracy')
        clf_acc.fit(X_train, y_train)
        scores.append([metrics.accuracy_score(y_test, clf_acc.predict(X_test)), metrics.f1_score(y_test, clf_acc.predict(X_test),average='micro')])
    acc = np.mean([x[0] for x in scores]), np.std([x[0] for x in scores])
    mif = np.mean([x[1] for x in scores]), np.std([x[1] for x in scores])
    return acc, mif

",senLDA/functions.py,balikasg/topicModelling,1
"

class ErrorGenBadHost(ErrorGenBase):
    CLASS = SCMPClass.ROUTING
    TYPE = SCMPRoutingClass.BAD_HOST
    DESC = ""bad host""

    def _build_pkt(self):
        pkt = super()._build_pkt()
        pkt.set_payload(IFIDPayload.from_values(77))
        pkt.addrs.dst.host = HostAddrSVC(99, raw=False)
        return pkt


class ErrorGenBadVersion(ErrorGenBase):
    DESC = ""bad version""

    def _send_pkt(self, spkt, first_hop):
        next_hop, port = self._get_next_hop(spkt)
        raw = bytearray(spkt.pack())",python/integration/scmp_error_test.py,dmpiergiacomo/scion,1
"#plt.show()
plt.savefig(image_directory+'iris_setosa_versicolor.png')

# SVM: Plot the margin


nX = preprocessing.scale(X)
nX1 = [ r[0] for r in nX ]
nX2 = [ r[1] for r in nX ]

clf = svm.SVC(kernel='linear')
clf.fit(nX, y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",Lecture18/notebooks/Lecture18.py,kephale/TuftsCOMP135_Spring2016,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_11_2014_server.py,magic2du/contact_matrix,1
"	#set the timer
	start = time.time()

	trainX = np.load('trainX_feat.npy')
	testX = np.load('testX_feat.npy')
	trainY = np.load('trainY_feat.npy')
	testY = np.load('testY_feat.npy')
	print('\n!!! Data Loading Completed !!!\n')
	
	clf = KNeighborsClassifier(n_neighbors=25, weights='distance')
	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithTreeFS/mnistBackImage/kernel.py,akhilpm/Masters-Project,1
"    n_numeric = len([c.TYPE for c in columns if c.TYPE is Types.NUMERICAL and c.CATEGORIES is None])
    C = [0.1, 1, 10, 100, 1000]
    gamma = ['auto', 1, 0.1, 0.001, 0.0001]
    parameters = dict(svm__C=C,
                      svm__gamma=gamma)

    # create model pipeline
    ns = NumericScaler(n_numeric)
    rf = RandomForestClassifier() #random_state=2)
    rfe = feature_selection.RFE(rf)
    svm = SVC(kernel='rbf') #, random_state=17)
    pipe = Pipeline(steps=[('ns', ns),
                           ('rfe', rfe),
                           ('svm', svm)])

    # run grid search with 10-fold validation
    clf = GridSearchCV(pipe, parameters, cv=10, verbose=1)
    clf.fit(data, labels)
    pred = clf.predict(data)
",src/model.py,nikhilnrng/german-credit-risk,1
"print ""{0:d}-fold ross-validation for {1:d} C values..."".format(n_folds, len(c_values))
for c in c_values:
    fold_scores = []
    for fold in np.unique(cv_folds):
        fold_K_train = K_train[cv_folds != fold][:, cv_folds != fold]
        fold_y_train = y_train[cv_folds != fold]
        fold_K_test = K_train[cv_folds == fold][:, cv_folds != fold]
        fold_y_test = y_train[cv_folds == fold]

        ""Fitting with C={0:.4f}""
        estimator = SVC(C=c, kernel=""precomputed"").fit(fold_K_train, fold_y_train)
        fold_scores.append(estimator.score(fold_K_test, fold_y_test))
    cv_score = np.mean(fold_scores)
    print ""... cv score: {0:.4f}"".format(cv_score)
    if cv_score > best_c_score:
        best_c = c
        best_c_score = cv_score
print ""The best C value is C = {0:.4f} with cv score = {1:.4f}"".format(best_c, best_c_score)

print ""Re-training with best hyperparameter values...""",exercises/code/applications.antibiotics.svm.py,aldro61/microbiome-summer-school-2017,1
"print('Scaling training data between [0, 1]')
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
#X_test = scaler.fit_transform(X_test)

# Then split into training/testing data
X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.10, random_state=42)

# Start up SVC classifier (default params)
print('Fitting SVC model')
svc = SVC(C=1.0, kernel='sigmoid', gamma='auto', cache_size=5000)
svc.fit(X_train, Y_train)

print('accuracy:', svc.score(X_test, Y_test))",svm.py,get9/kaggle_scripts,1
"def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact by computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.93) than that the non
    # shuffling variant (around 0.81).

    X, y = digits.data[:600], digits.target[:600]
    model = SVC(C=10, gamma=0.005)

    n_splits = 3

    cv = KFold(n_splits=n_splits, shuffle=False)
    mean_score = cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.92, mean_score)
    assert_greater(mean_score, 0.80)

    # Shuffling the data artificially breaks the dependency and hides the",phy/lib/python2.7/site-packages/sklearn/model_selection/tests/test_split.py,marcsans/cnn-physics-perception,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",python/ml/scikit/svm_gui.py,dkandalov/katas,1
"
    # plot training data
    colors = [""black"" if label == -1.0 else ""grey"" for label in knowns]
    pyplot.scatter(data[:, 0], data[:, 1], color=colors)
    pyplot.savefig(""ex2-train-"" + str(numPoints) + ""-"" + kernelFunc)
    # pyplot.show()
    pyplot.close()

    t0 = time()
    # fit the model
    # clf = svm.SVC(kernel='linear')
    clf = svm.NuSVC()
    clf.fit(data, knowns)
    score = clf.score(data, knowns)
    t1 = time()
    print(str(numPoints) + ""-"" + kernelFunc)
    print('time taken = %.3f' % (t1 - t0))
    print('Known data: %5.2f%% correct' % (score))

    # print(""\nSupport vector machine prediction boundaries\n"")",semester2/task12/exercise2.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"def i_SETEND(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    internals['endianstate'] = 1 if i.set_bigend else 0
    exp.setendian(-1 if i.set_bigend else +1)

# event hint
def i_SEV(i,fmap):
    fmap[pc] = fmap(pc+i.length)

# supervisor call
def i_SVC(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    logger.info('call to supervisor is unsupported')

def i_SWP(i,fmap):
    fmap[pc] = fmap(pc+i.length)
    Rt,Rt2,Rn = i.operands
    data = fmap(mem(Rn,32))
    fmap[mem(Rn,32)] = fmap(Rt2)
    fmap[Rt] = data",amoco/arch/arm/v7/asm.py,alex-bender/amoco,1
"    features, labels, vectorizer, selector, le, features_data = preprocess(""pkl/article_2_people.pkl"", ""pkl/lable_2_people.pkl"", k)
    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_1.py,dikien/Machine-Learning-Newspaper,1
"    # data_X_test = X_test[:int(DATA_SIZE*0.8)]
    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]

    print ""Train data size: "" + str(int(DATA_SIZE*0.8))
    print ""Test  data size: "" + str(len(X_target)-int(DATA_SIZE*0.8))


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))
    regr = OneVsRestClassifier(linear_model.LinearRegression())

    # train the model using the training data
    # regr.fit(X_train, X_target)
    regr.fit(data_X_train, data_Y_train)

    # predict to test
    print ""Predicting...""
    test_result = regr.predict(data_X_test)",DST_v1.02.py,totuta/deep-supertagging,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_server_test_0513.py,magic2du/contact_matrix,1
"def test_kfold_can_detect_dependent_samples_on_digits():  # see #2372
    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact by computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.93) than that the non
    # shuffling variant (around 0.81).

    X, y = digits.data[:600], digits.target[:600]
    model = SVC(C=10, gamma=0.005)

    n_folds = 3

    cv = KFold(n_folds=n_folds, shuffle=False)
    mean_score = cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.92, mean_score)
    assert_greater(mean_score, 0.80)

    # Shuffling the data artificially breaks the dependency and hides the",projects/scikit-learn-master/sklearn/model_selection/tests/test_split.py,DailyActie/Surrogate-Model,1
"    # for each k-element iteration
    for train_index, test_index in kf:
        X_train = X.ix[X.index[train_index]]
        X_test = X.ix[X.index[test_index]]
        y_train = y.ix[y.index[train_index]]
        y_test = y.ix[y.index[test_index]]

        # In this instance only use the 
        # Radial Support Vector Machine (SVM)
        print(""Hit Rate/Confusion Matrix:"")
        model = SVC(
            C=1000000.0, cache_size=200, class_weight=None,
            coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
            max_iter=-1, probability=False, random_state=None,
            shrinking=True, tol=0.001, verbose=False
        )

        # Train the model on the retained training data
        model.fit(X_train, y_train)
",Document/szse/Quantitative Trading/sat-ebook-and-full-source-20150618/algo-ebook-full-source-code-20150618/chapter16/k_fold_cross_val.py,Funtimezzhou/TradeBuildTools,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",scripts/plot_classifier_comparison.py,JohnAshburner/ISBI,1
"                   'Blue_merged', 'all']:
        print '-'*20
        print nowstr(), monkey
        print '-'*20
        train, _ = get_train_test_files(monkey)
        X_train, y_train, labels = _load_files_labels(monkey, filelist=train,
                                          under_sample=True,
                                          under_sampling_ratio=UNDERSAMPLING_RATIO)
        acoustic_pipeline = Pipeline([('data', FeatureUnion([('audio', AudioLoader()),
                                                             ('vad', VADLoader())])),
                                      ('svm', SVC(kernel='rbf', gamma=1e-5, C=20))])
        paramdist = {'svm__C': np.logspace(0, 4, 100),
                     'data__vad__stacksize': scipy.stats.randint(11, 51),
                     'data__vad__cut': [True, False],
                     'data__audio__stacksize': scipy.stats.randint(11, 51),
                    }

        acoustic_clf = RandomizedSearchCV(acoustic_pipeline, paramdist, n_iter=1000,
                                          verbose=1,
                                          cv=3, n_jobs=40)",train_am.py,bootphon/monkey_business,1
"#                                      model3.decision_function(fannie_test)))
# print('Now it is SVM time')
# with open('SVC.dill', 'wb') as f:
#     dill.dump(model3, f)
# print('Now let\'s grid search SVM parameters')

# # Grid search of SVC
# cv = ShuffleSplit(len(fannie_train), 1, test_size=0.2)
# model3_2 = Pipeline([
#     ('features', features),
#     ('LinearSVC', LinearSVC(C=1))
# ])
# search_model = GridSearchCV(model3_2,
#                             {'LinearSVC__C': np.logspace(-2, 2, 5)}, cv=3,
#                             scoring='roc_auc')
# search_model.fit(fannie_train, status_train)

# print('best parameters are: ', search_model.best_params_)
# print('Best score we can get: ', search_model.best_score_)
",processing/learning.py,DigitalPig/SmartUnderwriter,1
"categories = ['course', 'department', 'faculty', 'other', 'project', 'staff', 'student']
vectorizer = CountVectorizer(input='filename', stop_words='english', encoding='latin1')
y = []

for file_path in file_list:
	y.append(file_path.split('/')[3])

file_train, file_test, y_train, y_test = train_test_split(file_list, y, test_size=0.3)

x_train = vectorizer.fit_transform(file_train)
clf = SVC(kernel='linear')
clf.fit(x_train,y_train)
#multiclass_logistic_regression.fit(x_train, y_train)

# start testing
x_test = vectorizer.transform(file_test)
y_hat = clf.predict(x_test)
print accuracy_score(y_test,y_hat)
",SVM/SVM_multiclass.py,ML-SWAT/Web2KnowledgeBase,1
"   #print x[:20]
   X = np.array(x)
   YN = np.array(yn)
   countj = 0
   for i in range(len(yn)):
       if yn[i] == 1.0:
           countj +=1
   print j, countj
   #insert code for quadratic programming SVM
   # fit the model
   clf = SVC(C = 0.01, kernel = 'poly', degree = 2, gamma = 1.0, coef0 = 1.0)
   clf.fit(X, YN)
   #print 'dual_coef_ holds the product yn_i * alpha_i'
   lagcoef = list(np.array(clf.dual_coef_).reshape(-1,))
   #print lagcoef
   #print 'w from svc'
   #print clf.coef_
   print 'support vectors from SLSQP'
   supvec = clf.support_vectors_
   #print supvec",Homework_8/Python/hw8.py,kirbs-/edX-Learning-From-Data-Solutions,1
"norm_trn_data0 = norm_trn_data[:Ntrn/2,:]
norm_trn_data1 = norm_trn_data[Ntrn/2:,:]
norm_tst_data = (tst_data - tst_data.mean(axis=0)) / np.sqrt(tst_data.var(axis=0,ddof=1))
N = tst_data.shape[0]
D = trn_data.shape[1]

trn_labels = np.hstack(( np.zeros(Ntrn/2), np.ones(Ntrn/2) ))
tst_labels = np.hstack(( np.zeros(N/2), np.ones(N/2) ))
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)

output = {}
output['ldaerr'] = (1-sklda.score(norm_tst_data, tst_labels))
output['knnerr'] = (1-skknn.score(norm_tst_data, tst_labels))
output['svmerr'] = (1-sksvm.score(norm_tst_data, tst_labels))
",tests/test_poisson.py,binarybana/samcnet,1
"        'params': [
            {
                'solver': ['liblinear'],
                'penalty': ['l2'],
                'random_state': [77]
            },
        ]
    },
    {
        'name': 'SVM',
        'model': SVC(),
        'params': [
            {
                'kernel': ['poly'],
                'degree': [5],
                'random_state': [77]
            },
        ]
    },
    {",scripts/classifier.py,yohanesgultom/knowledge-extractor-id,1
"        #         fig = sns_plot.figure
        #         fig.savefig(args.outbase+elt+'.'+vplot_label+'.png')


        for elt in ['L1', 'ALU', 'SVA']:
            clf = RandomForestClassifier()
            #clf = LinearDiscriminantAnalysis()
            #clf = AdaBoostClassifier()
            #clf = GaussianNB()
            #clf = KNeighborsClassifier()
            #clf = SVC(gamma=2, C=1)

            elt_data = data[data.Superfamily==elt]

            x = elt_data[vplot_labels]
            y = elt_data['KnownGood']

            clf.fit(x,y)
            score = clf.score(x,y)
            sys.stderr.write('%s score: %f\n' % (elt, score))",scripts/autofilter.py,adamewing/tebreak,1
"    transformed = matrix.dot(matrix_w)
    print transformed.shape
    #Compute cov matrix
    if os.path.isfile('svm.model'):
        print 'Loading Model file...'
        #Load models from file
        with open('svm.model', 'rb') as file:
            Z = pickle.load(file)
    else:
        #Start to train SVM
        Z = OneVsRestClassifier(SVC(kernel=""rbf"")).fit(transformed, labels)
        with open('svm.model', 'wb') as file:
            pickle.dump(Z, file)


    recData = transformed.dot(matrix_w.T) + matrix.mean(axis=1)[:, None]
    j = Image.fromarray(recData[0].reshape((32,32)))
    newdoc = Document(docfile = ""documents/pca.png"")
    j = j.convert('L')
    j.save(""myproject/media/documents/pca.png"");",myproject/myproject/myapp/views.py,motian12ps/Bigdata_proj_yanif,1
"predictors_tr = tfidftr

targets_tr = traindf['cuisine']

predictors_ts = tfidfts


# classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
# classifier = RandomForestClassifier(n_estimators=200)
classifier = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05) #0.69328

classifier=classifier.fit(predictors_tr,targets_tr)",deep_learn/whatiscooking/test1.py,zhDai/CToFun,1
"def classification_test1():
	'''
	In the case of the digits dataset, the task is to predict, given an image, which digit it represents.
	'''
	iris = datasets.load_iris()
	digits = datasets.load_digits()
	print(digits.data)
	print(digits.data[:-1])
	print(digits.target)
	print(digits.images[0])
	clf = svm.SVC(gamma=0.001, C=100.)
	fit_result = clf.fit(digits.data[:-1], digits.target[:-1])
	print(fit_result)
	result = clf.predict(digits.data[-1:])
	print(result)

def model_persistence_test1():
	'''
	It is possible to save a model in the scikit by using Python’s built-in persistence model, namely pickle.
	'''",learn-python2.7/scikit-learn/scikit-sample.py,JoshuaMichaelKing/MyLearning,1
"        kernel = param.strip()

    # train
    name = 'svn_' + param + '_ts-' + str(x_tr.shape[0])
    path = fio.get_classifier_file(name)
    if os.path.exists(path):
        print('      loading...')
        clf = joblib.load(path)
    else:
        print('      computing...')
        clf = svm.SVC(kernel=kernel, degree=degree)
        clf.fit(x_tr, y_tr)
        joblib.dump(clf, path)

    # score
    y_pr = clf.predict(x_te)
    score_tr = clf.score(x_tr, y_tr)
    cross_tr = np.mean(cross_validation.cross_val_score(clf, x_tr, y_tr, cv=3))
    score_te = clf.score(x_te, y_te)
    cross_te = np.mean(cross_validation.cross_val_score(clf, x_te, y_te, cv=3))",svm/test_svm.py,dwettstein/pattern-recognition-2016,1
"FILL_WITH_MEAN = ['officers_age', 'transit_time', 'car_time','agesqrd','complainant_age']

#estimators
CLFS = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'AB': AdaBoostClassifier(
                    DecisionTreeClassifier(max_depth=1),
                    algorithm=""SAMME"",
                    n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(
                    learning_rate=0.05,
                    subsample=0.5,
                    max_depth=6,
                    n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'KNN': KNeighborsClassifier(n_neighbors=3),
        'test': DecisionTreeClassifier(),",pipeline/pipeline.py,ladyson/police-complaints,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(100):
		comp_file_list.append((os.environ['learningml']+""/GoF/data/gaussian_same_projection_on_each_axis/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.1_{0}.txt"".format(i,dim),os.environ['learningml']+""/GoF/data/gaussian_same_projection_on_each_axis/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.075_{0}.txt"".format(i,dim)))
	
	#Earlier for SVM we had C=496.6 and gamma=0.00767

        clf = SVC(C=290.4,gamma=0.0961,probability=True, cache_size=7000)

        ####################################################################


	classifier_eval.classifier_eval(name=(str(dim)+""Dgaussian_same_projection_redefined__0_1__0_075_optimised_svm""),comp_file_list=comp_file_list,clf=clf)

",learningml/GoF/optimisation_and_evaluation/not_automated/svm_gaussian_same_projection/svm_Gaussian_same_projection_evaluation_of_optimised_classifiers.py,weissercn/learningml,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIFS.mifs(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_MIFS.py,jundongl/scikit-feast,1
"sel, rawdata, normdata = get_data(data_yj, params)

norm_trn_data = normdata.loc[sel['trn'], sel['feats']]
norm_tst_data = normdata.loc[sel['tst'], sel['feats']]
tst_data = rawdata.loc[sel['tst'], sel['feats']]

t1 = time()
#################### CLASSIFICATION ################
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, sel['trnl'])
skknn.fit(norm_trn_data, sel['trnl'])
sksvm.fit(norm_trn_data, sel['trnl'])
errors['lda'] = (1-sklda.score(norm_tst_data, sel['tstl']))
errors['knn'] = (1-skknn.score(norm_tst_data, sel['tstl']))
errors['svm'] = (1-sksvm.score(norm_tst_data, sel['tstl']))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",exps/fig_jk.py,binarybana/samcnet,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 4.5,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/fitness_cmaessvc/setup.py,jpzk/evopy,1
"#from sklearn.externals import joblib

format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# if want to output log to a file instead of outputting log to the console,
# replace ""stream=sys.stdout"" with ""filename='fcma.log'""
logging.basicConfig(level=logging.INFO, format=format, stream=sys.stdout)
logger = logging.getLogger(__name__)

def example_of_aggregating_sim_matrix(raw_data, labels, num_subjects, num_epochs_per_subj):
    # aggregate the kernel matrix to save memory
    svm_clf = svm.SVC(kernel='precomputed', shrinking=False, C=1)
    clf = Classifier(svm_clf, num_processed_voxels=1000, epochs_per_subj=num_epochs_per_subj)
    rearranged_data = raw_data[num_epochs_per_subj:] + raw_data[0:num_epochs_per_subj]
    rearranged_labels = labels[num_epochs_per_subj:] + labels[0:num_epochs_per_subj]
    clf.fit(list(zip(rearranged_data, rearranged_data)), rearranged_labels,
            num_training_samples=num_epochs_per_subj*(num_subjects-1))
    predict = clf.predict()
    print(predict)
    print(clf.decision_function())
    test_labels = labels[0:num_epochs_per_subj]",examples/fcma/classification.py,lcnature/brainiak,1
"        showSaveReport(report);
    
    return peaks
    
#########################################################################################
##################################################################################
###################################################################################
###Test for runEvaluation###
##Initialize the classifier
#clf = KNeighborsClassifier(1);
clf = svm.SVC(kernel=""linear"")
#clf = LogisticRegression
clfName = 'svc_linear'
#clfName = 'RandomForest'
###classifier =DecisionTreeClassifier();
#classifier = RandomForestClassifier();
##################################to run
dirPath = 'C:\\Users\\LLP-admin\\workspace\\weka\\token-experiment-dataset\\';
fname = 'user_' + str(0)
fmt = '.csv'",simpleBatch.py,cocoaaa/ml_gesture,1
"    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_1(self):
        x = array([[1, 1], [2, 2], [3, 3]])
        y = array([0, 1, 2])
        measure = SVCDistanceNCMeasure()
        cp = ConformalSVC(measure)
        cp.fit(x, y)
        cp.calibrate(x, y)
        cp.predict_cf(x)
        predicted_l, credibility, confidence = cp.predict_cf(x)
        self.assertEqual(0, predicted_l[0])
        self.assertEqual(1, predicted_l[1])
        self.assertEqual(2, predicted_l[2])
        self.assertAlmostEqual(0.66666667, credibility[0])
        self.assertAlmostEqual(1, credibility[1])",tests/predictors/SVCTest.py,SergioGonzalezSanz/conformal_predictors,1
"
            # Split up data into randomized training and test sets
            rand_state = np.random.randint(0, 100)
            X_train, X_test, y_train, y_test = train_test_split(
                scaled_X, y, test_size=0.2, random_state=rand_state)

            print('Using:', self.orient, 'orientations', self.pix_per_cell,
                  'pixels per cell and', self.cell_per_block, 'cells per block')
            print('Feature vector length:', len(X_train[0]))
            # Use a SVC
            svc = LinearSVC()
            # Check the training time for the SVC
            t = time.time()
            svc.fit(X_train, y_train)
            t2 = time.time()
            print(round(t2 - t, 2), 'Seconds to train SVC...')
            # Check the score of the SVC
            print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
            # Check the prediction time for a single sample
            t = time.time()",Term01-Computer-Vision-and-Deep-Learning/P5-Vehicle-Detection/detection_functions/Vehicle_Classification.py,Raag079/self-driving-car,1
"    return svm_objects



def multi_sgd_train((data, patches_label, partition)):
    data = data.reshape((data.shape[0], -1))
    assert data.shape[0]==patches_label.shape[0]
    from sklearn import linear_model
    from sklearn import svm
    clf = linear_model.SGDClassifier(n_iter=2)
    #clf = svm.LinearSVC()
    partitionLabel = [partition[i] for i in patches_label]
    clf.fit(data, partitionLabel)
    #print(partition)
    print(np.mean(clf.predict(data) == partitionLabel))
    return clf

def calcScore((x, y, svmCoef, svmIntercept, data)):
    #print(""inside Calcscore"")
    #print(svmCoef.shape)",pnet/randomPartitionSVMLayer.py,jiajunshen/partsNet,1
"from sklearn import svm, cross_validation
from sklearn.datasets import make_hastie_10_2
data = make_hastie_10_2(1000, 1234)

n_iterations = 20
for i in range(n_iterations):
    # Get suggested new experiment
    job = scientist.suggest()

    # Perform experiment using 5 fold cross validation.
    learner = svm.SVC(kernel='rbf', **job)
    accuracy = cross_validation.cross_val_score(
            learner, data[0], data[1], cv=5).mean()

    # Inform scientist about the outcome
    scientist.update(job, accuracy)
    scientist.report()",examples/hastie_10_2_rbf_svm.py,schevalier/Whetlab-Python-Client,1
"from sklearn.grid_search import GridSearchCV
from multiprocessing import cpu_count
from filteropt import create_pipeline
from sklearn.externals import joblib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def grid_search(grid, inputs, targets, threads=cpu_count(), verbose=3):
    estimator = svm.SVC(cache_size=200)
    model = GridSearchCV(estimator, grid, n_jobs=threads, verbose=verbose)
    model.fit(inputs, targets)
    return model.best_estimator_


def generate_sets(dataset, w=11, N=5000):
    process(dataset, create_pipeline())
    return extractor.extract(dataset, w=w, N=N)
",grid.py,dominiktomicevic/pedestrian,1
"#print (""Transform OK!"")
#print (""Let's train the model"")
#print(""train_data"",train_data.shape)
#print(""train_label"",train_label.shape)
#
##trian_data = preprocessing.normalize(train_data, norm='l2')
##test_data = preprocessing.normalize(test_data, norm='l2')
#
##knn = neighbors.KNeighborsClassifier()
#logistic = linear_model.LogisticRegression()
##clf = svm.SVC(probability = True)
##clf = tree.DecisionTreeClassifier()
#
#
##print('KNN score: %f' % knn.fit(train_data, train_label).score(valid_data, valid_label))
##result = knn.fit(train_data, train_label).predict_proba(test_data)
#new_train_data = train_data[0:4000,:]
#new_train_label = train_label[0:4000]
#result = logistic.fit(new_train_data, new_train_label).predict_proba(train_data)
##clf.fit(train_data, train_label)",liao/refine_dataset.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"        clf = LinearCSVMC(C=1, probability=1, enable_ca=['probabilities'])
    elif clf_type == 'GNB':
        clf = GNB()
    elif clf_type == 'LDA':
        clf = LDA()
    elif clf_type == 'QDA':
        clf = QDA()
    elif clf_type == 'SMLR':
        clf = SMLR()
    elif clf_type == 'RbfSVM':
        sk_clf = SVC(gamma=0.1, C=1)
        clf = SKLLearnerAdapter(sk_clf, enable_ca=['probabilities'])
    elif clf_type == 'GP':
        clf = GPR()
    else:
        clf = LinearCSVMC(C=1, probability=1, enable_ca=['probabilities'])
    
    ############## Feature Selection #########################
    if f_sel == 'True':
        logger.info('Feature Selection selected.')",mvpa_itab/main_wu.py,robbisg/mvpa_itab_wu,1
"    pl.show()

def main():
    # import some data to play with
    XTmp, Y = datasets.load_svmlight_file(""./SVMTrain.txt"")
    X = XTmp.toarray()
    XTestTmp, YTest = datasets.load_svmlight_file(""./SVMTest.txt"")
    XTest = XTestTmp.toarray()


    clf = svm.NuSVC(nu=0.1, kernel='rbf', degree=3, gamma=0.0, coef0=0.0)
    #clf = svm.SVC(kernel='rbf', gamma=1e-4, C=1000)
    #clf = svm.SVC(C=1.0, kernel='linear', probability=True)
    #clf = svm.SVC(gamma=0.1)
    #clf = svm.SVC(kernel='poly', gamma=0.001)

    # we create an instance of SVM Classifier and fit the data.
    print 'Training data and scoring it'
    #probas = clf.fit(X,Y).predict_proba(X)
    ''' Score computed on one sample '''",kinect/pySVM/ClassifierSVM.py,hackliff/domobot,1
"    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    
    sc=[]
    for train_index, test_index in kf:
        #print(""TRAIN:"", train_index, ""TEST:"", test_index)
    
        #generated train and test lists, incuding indices of the examples in training/test
        #for the specific fold. Indices starts from 0 now
        
        clf = svm.SVC(C=c, kernel='precomputed')
        train_gram = [] #[[] for x in xrange(0,len(train))]
        test_gram = []# [[] for x in xrange(0,len(test))]
        #compute training and test sub-matrices
        index=-1    
        for row in gram:
            index+=1
            if index in train_index:
                train_gram.append([gram[index,i] for i in train_index])
            else:",scripts/cross_validation_ICML16_norm.py,nickgentoo/scikit-learn-graph,1
"                            target_height=options.pyxit_target_height,
                            interpolation=options.pyxit_interpolation,
                            transpose=options.pyxit_transpose,
                            colorspace=options.pyxit_colorspace,
                            fixed_size=options.pyxit_fixed_size,
                            n_jobs=options.pyxit_n_jobs,
                            verbose=options.verbose)

    if options.svm:
        if options.svm == SVM_LIBSVM:
            svm = SVC(probability=True, C=options.svm_c, kernel=""linear"")
        if options.svm == SVM_LIBLINEAR:
            svm = LinearSVC(C=options.svm_c)
        if options.svm == SVM_LRL1:
            svm = LogisticRegression(penalty=""l1"", C=options.svm_c)
        if options.svm == SVM_LRL2:
            svm = LogisticRegression(penalty=""l2"", C=options.svm_c)
        if options.svm == ET:
            svm = ExtraTreesClassifier(n_estimators=1000,
                                       max_features=""sqrt"",",cytomine-datamining/algorithms/pyxit/pyxitstandalone.py,cytomine/Cytomine-python-datamining,1
"
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.lda import LDA
from sklearn.feature_selection import SelectKBest
X, y = datasets.make_classification(n_samples=12, n_features=10,
                                    n_informative=2)

from epac import Methods, Pipe

self = Methods(*[Pipe(SelectKBest(k=k), SVC(kernel=kernel, C=C)) for kernel in (""linear"", ""rbf"") for C in [1, 10] for k in [1, 2]])
self = Methods(*[Pipe(SelectKBest(k=k), SVC(C=C)) for C in [1, 10] for k in [1, 2]])


import copy
self.fit_predict(X=X, y=y)
self.reduce()
[l.get_key() for l in svms.walk_nodes()]
[l.get_key(2) for l in svms.walk_nodes()]  # intermediary key collisions: trig aggregation
",doc/future_bottumup.py,neurospin/pylearn-epac,1
"import glob
import numpy as np
import timeit
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn import metrics
import pickle
from sklearn.externals import joblib

clf = OneVsRestClassifier(SVC(kernel='poly', degree=3))
f = None
try:
    s = joblib.load('svm.pkl')
except:
    print(""new clf"")
    s = clf

parameters = {
    ""estimator__C"": [1,2,4,8],",svm.py,Guitar-Machine-Learning-Group/guitar-transcriber,1
"        p.text(0, 14, str(target[i]))
        p.text(0, 60, str(i))
    plt.show()


#print_faces(faces.images, faces.target, 400)

#

from sklearn.svm import SVC
#svc_1 = SVC(kernel='linear')

from sklearn.cross_validation import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.25, random_state=33)

from sklearn.cross_validation import cross_val_score, KFold
from scipy.stats import sem

def evaluate_cross_validation(clf, X, y, K):
    cv = KFold(len(X), K, shuffle=True, random_state=33)",c2-SVM.py,chongguang/scikitLearning,1
"#     plt.semilogx(param_values, test_scores, alpha=0.4, lw=2, c='g')
    plt.plot(param_values, train_scores, alpha=0.4, lw=2, c='b')
    plt.plot(param_values, test_scores, alpha=0.4, lw=2, c='g')
    plt.xlabel(param_name + "" values"")
    plt.ylabel(""Mean cross validation accuracy"")
    # return the training and testing scores on each parameter value
    return train_scores, test_scores

def optimizeSVM(X_norm, y, kFolds=10):
    clf = pipeline.Pipeline([
        ('svc', svm.SVC(kernel='rbf')),
    ])
    # grid search 多参数优化
    parameters = {
        'svc__gamma': np.logspace(0, 3, 20),
        'svc__C': np.logspace(0, 3, 10),
        # 'svc__gamma': np.linspace(0, 50, 20),
        # 'svc__C': np.linspace(0.001, 30, 10),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)",finance/SVMTest.py,Ernestyj/PyStudy,1
"            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier
            estimator = GradientBoostingClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'linear-svm':
            from sklearn.svm import SVC
            estimator = SVC(probability=True,
                            random_state=self.random_state, **self.kwargs)
        else:
            raise NotImplementedError

        # Create the different folds
        skf = StratifiedKFold(y, n_folds=self.cv, shuffle=False,
                              random_state=self.random_state)

        probabilities = np.zeros(y.shape[0], dtype=float)",imblearn/under_sampling/instance_hardness_threshold.py,dvro/UnbalancedDataset,1
"    plt.axis('tight')

    x_min = XTest[:, 0].min()-2
    x_max = XTest[:, 0].max()+2
    y_min = XTest[:, 1].min()-2
    y_max = XTest[:, 1].max()+2

    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]

    if (kernel == 'linear'):
        clf = SVC(kernel='linear')
        clf.fit(XTrain[:,0:2], yTrain)
    else:
        clf = SVC(kernel='rbf', C=1.0, gamma=0.0)
        clf.fit(XTrain[:,0:2], yTrain)

    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])
    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    plt.pcolormesh(XX, YY, Z > 0, cmap=cmap_light)",modelling/visplots.py,M-R-Houghton/euroscipy_2015,1
"    for (preprocessor, C, gamma) in itertools.product(
                preprocessor_list,
                [0.01, 0.1, 0.5, 1., 10., 50., 100.],
                [0.01, 0.1, 0.5, 1., 10., 50., 100., 'auto']):
        features = input_data.drop('class', axis=1).values.astype(float)
        labels = input_data['class'].values

        try:
            # Create the pipeline for the model
            clf = make_pipeline(preprocessor,
                                SVC(C=C,
                                    gamma=gamma,
                                    kernel='rbf',
                                    random_state=324089))
            # 10-fold CV score for the pipeline
            cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
            accuracy = accuracy_score(labels, cv_predictions)
            macro_f1 = f1_score(labels, cv_predictions, average='macro')
            balanced_accuracy = balanced_accuracy_score(labels, cv_predictions)
        except KeyboardInterrupt:",preprocessor_model_code/SVCRBF.py,rhiever/sklearn-benchmarks,1
"import seaborn as sns
from sklearn.preprocessing import LabelEncoder

'Use SKLL!? https://skll.readthedocs.org/en/latest/run_experiment.html#quick-example'
'''
 enhanced confusion matrix + weights for RF:
http://stackoverflow.com/questions/24123498/recursive-feature-elimination-on-random-forest-using-scikit-learn
'''

'Works!'
def PlotPerfPercentFeatures(X,y,est=LinearSVC()):
    '''
    Performance of a classifier (default: SVM-Anova)
    varying the percentile of features selected (F-test) .

    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py
    '''
    transform = SelectPercentile(f_classif)

    clf = Pipeline([('anova', transform), ('est', est)])",ProFET/feat_extract/OutPutRes.py,ddofer/ProFET,1
"  a, b, c, d = score1 * test1Count, score0 * test0Count, (1.0 - score1) * test1Count, (1.0 - score0) * test0Count
  tss = (a * b - c * d) / ((a + c) * (b + d))
  print(""TSS                  "" + str(tss))

# Train random forests with the best parameters found by randomized search

rf = RandomForestClassifier(n_estimators = 100, bootstrap = True, min_samples_leaf = 10, criterion = 'entropy', max_depth = None)
rf.fit(X, y)
score_and_show(rf, ""\nRandom Forest"")

support = svm.SVC()
support.fit(X, y)
score_and_show(support, ""\nSVM"")
",1_Netflix/Supervised Learning/rf.py,PhDP/Articles,1
"    classifier = bm.fit(data_train, target_train)
    predictedNB = classifier.predict(data_test)
    evaluate_model(target_test, predictedNB)
    # print mean_squared_error(target_test,predictedNB)


    data_train, data_test, target_train, target_test = cross_validation.train_test_split(dataSVM, target, test_size=0.3,
                                                                                         random_state=37)
    calculateMajorityClass(target_train)
    print ""Using SVM ""
    svm = SVC(probability=False,random_state=33,kernel='linear',shrinking=True)
    classifier = svm.fit(data_train, target_train)
    predicted = classifier.predict(data_test)
    evaluate_model(target_test, predicted)
    # print mean_squared_error(target_test,predicted)


def evaluate_model(target_true, target_predicted):
    print classification_report(target_true, target_predicted)
    print ""The accuracy score is {:.2%}"".format(accuracy_score(target_true, target_predicted))",PosNeg.py,akshaykamath/ReviewPredictionYelp,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load_ten_fold.py,magic2du/contact_matrix,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,kingjr/mne-python,1
"    plt.ylabel('Score')
    plt.semilogx(p_range, train_scores_mean, label='Training score', color='#E29539')
    plt.semilogx(p_range, test_scores_mean, label='Cross-validation score', color='#94BA65')
    plt.legend(loc='best')
    plt.savefig('figures/val_curve.png', transparent=True)

def visual_gridsearch(model, X, y):
    C_range = np.logspace(-2, 10, 5)
    gamma_range = np.logspace(-5, 5, 5)
    param_grid = dict(gamma=gamma_range, C=C_range)
    grid = GridSearchCV(SVC(), param_grid=param_grid)
    grid.fit(X, y)

    scores = [x[1] for x in grid.grid_scores_]
    scores = np.array(scores).reshape(len(C_range), len(gamma_range))

    plt.figure(figsize=(8, 6))
    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)
    plt.imshow(scores, interpolation='nearest', cmap=ddlheatmap)
    plt.xlabel('gamma')",diagnostics/pycon/code/vizmkr.py,rebeccabilbro/viz,1
"



def main():

    # Supported classifier models
    n_neighbors = 3
    models = {
        'nb' : naive_bayes.GaussianNB(),
        'svm-l' : svm.SVC(),
        'svm-nl' : svm.NuSVC(),
        'tree' : tree.DecisionTreeClassifier(),
        'forest': AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"",n_estimators=200),
        'knn-uniform' : neighbors.KNeighborsClassifier(n_neighbors, weights='uniform'),
        'knn-distance' : neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
    }

    userInput = getUserInput(models)
    appDf = loadAppData(userInput['file'])",appClassifierBenchmark.py,seekshreyas/obidroid,1
"df.max()
df.min()
type(predictors)
    
#==============================================================================
# Model fitting part 2?
#==============================================================================

#==============================================================================
# # fit the model
# clf = svm.NuSVC()
# clf.fit(predictors, outcomes)
# 
# # plot the decision function for each datapoint on the grid
# Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
# Z = Z.reshape(xx.shape)
# 
# plt.imshow(Z, interpolation='nearest',
#            extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
#            origin='lower', cmap=plt.cm.PuOr_r)",ModelFitting/single_SVM.py,kizzen/Baller-Shot-Caller,1
"
    # Create the TF-IDF vectoriser and transform the corpus
    vectorizer = TfidfVectorizer(min_df=1)
    X = vectorizer.fit_transform(corpus)
    return X, y

def train_svm(X, y):
    """"""
    Create and train the Support Vector Machine.
    """"""
    svm = SVC(C=1000000.0, gamma=0.0, kernel='rbf')
    svm.fit(X, y)
    return svm


if __name__ == ""__main__"":
    # Create the list of Reuters data and create the parser
    files = [""data/reut2-%03d.sgm"" % r for r in range(0, 22)]
    parser = ReutersParser()
",01_aat-ebook-full-source-code-20160430/chapter13/reuters-svm.py,UpSea/midProjects,1
"        plt.clf()
        plt.figure()
        # for each set
        for number in range(1, 6):


            outputfile = get_output_path(typen)
            inputfile = get_input_path(number, typen)

            X, Y  = get_data_here(inputfile)
            svc = SVC()
            rfecv = RFECV(estimator=svc,  cv=StratifiedKFold(Y, 2), step=0.3,
                      scoring='accuracy', estimator_params={'kernel': 'linear'})
            rfecv.fit(X, Y)
            print(""Optimal number of features : %d"" % rfecv.n_features_)

            plt.ylim(0, 3)
            plt.xlim(1, 5.5)
            plt.xlabel(""Number of features selected"")
            plt.ylabel(""Cross validation score (nb of misclassifications)"")",MLNet-2.0/plotting/feature_cross_validation/plot.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"  test_labels = test_dataset[:, FEATURE_NUMBER]
  test_features = test_dataset[:, 0:FEATURE_NUMBER]

# Define the model
classifiers = [
  DecisionTreeClassifier(max_depth=5),
  MLPClassifier(algorithm='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, learning_rate_init=0.001, batch_size=64, max_iter=100, verbose=False),
  MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),
  MLPClassifier(algorithm='adam', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),
  KNeighborsClassifier(2),
  SVC(kernel=""linear"", C=0.025),
  SVC(gamma=2, C=1),
  RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
  AdaBoostClassifier(),
  GaussianNB(),
  LinearDiscriminantAnalysis(),
  QuadraticDiscriminantAnalysis()]

if len(sys.argv) > 1:
  classifier_index = int(sys.argv[1])",sklearn_exmaples/cancer_classifier.py,tobegit3hub/deep_recommend_system,1
"#Create a Prediction Matrix
testData = generateData(dirImagesTest, preprocessingFun=getMinorMajorRatio, RGB=False, dims=[25, 25])

#Divide data for validation
XTrain, XTest, yTrain, yTest = cross_validation.train_test_split(
    Xdata[0:10000, :], yLabels[0:10000], test_size=0.4, random_state=0)

# Validate a linear SVM classification model
print(""Fitting the classifier to the training set"")
t0 = time()
lin_clf = svm.LinearSVC(verbose=1)
param_grid = {'C': [1.0, 3.0, 10.0, 30.0, 100]}
clf = GridSearchCV(lin_clf, param_grid)
clf = clf.fit(XTrain, yTrain)
print(""done in %0.3fs"" % (time() - t0))
print(""Best estimator found by grid search:"")
print(clf.best_estimator_)

#Train classifier on full data
fullDataLinClf = svm.LinearSVC(C=1.0, verbose=1)",dataSciBowlMain.py,wacax/NationalDataSciBowl,1
"                Xs.append(FindConditions(data, i, var))     # Find conditions for day 1
            X.append(Xs)                             
            
            y1 = PercentChange(data, i+1)                   # Find the stock price movement for day 2
            if y1 > 0: y.append(1)                          # If it went up, classify as 1
            else:      y.append(0)                          # If it went down, classify as 0
                
        XX = vstack(X)                                      # Convert to numpy array
        yy = hstack(y)                                      # Convert to numpy array
    
        model = SVC(C=self.C, gamma=self.gamma, probability=True)
        model.fit(XX, yy)
        
        # ====================== #
        #         Testing        #
        # ====================== #       
        
        testDay = testStart                                  
        while (testDay < testEnd):
            ",clairvoyant/Portfolio.py,anfederico/Clairvoyant,1
"  with open(clf_fn, ""wb"") as f:
    pickle.dump(clf2, f)
    print ""classifier saved to file.""
  with open(clf_fn, ""rb"") as f:
    clf2 = pickle.load(f)
    print ""classifier loaded from file.""
    pred_labels2 = clf2.predict(test_feats)
    score2 = metrics.accuracy_score(test_labels, pred_labels2)
    print ""restored logistic classifier score: {}"".format(score2)

  clf3 = svm.SVC()
  clf3.fit(train_feats, train_labels)
  pred_labels3 = clf3.predict(test_feats)
  score3 = metrics.accuracy_score(test_labels, pred_labels3)
  print ""svm score: {}"".format(score3)

  return

  # train tf linear classifier; similar as finetuning the last layer.
  model_params = commons.ModelParams(",deepmodels/tf/samples/img_classifier.py,flyfj/deepmodels,1
"        with open(""input/Dataset/YAll_POS.processed"") as f:
            yAll = pickle.load(f)
    
    xTrain, xTest, yTrain, yTest = cross_validation.train_test_split(XAll, yAll, test_size=0.2)

#### SVC 
    classifierTime = time()
    print ""Training Classifier""
    svc80Exists = os.path.exists(""input/Dataset/svc80_POS.classifier"")
    if not svc80Exists:
        svcClassifier = OneVsRestClassifier(LinearSVC()).fit(xTrain, yTrain)
        print ""Training classifier took : ""+str(time()- classifierTime)
        with open(""input/Dataset/svc80_POS.classifier"", 'w') as f:
            pickle.dump(svcClassifier, f)
        print ""Classifier dumped on disk""
    else:
        print ""Loading SVC 80 Classifier from pickle""
        with open(""input/Dataset/svc80_POS.classifier"") as f:
            svcClassifier = pickle.load(f)
",Task1/CreateClassifiersusingPOSTagger.py,njetty/Yelp-Review-Analysis,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_03_2014_server_2.py,magic2du/contact_matrix,1
"

# In[286]:

print(np.isnan(y))


# In[287]:

#TRAIN
clf = OneVsRestClassifier(SVC(kernel='linear'))
clf.fit(X, y)


# #will need to check later on for optimization purposes, now seems the issue is somewhere else.
# 
# from sklearn.model_selection import GridSearchCV
# from sklearn import svm
# 
# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}",main_test.py,pedrogogo/SMC_Th,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_15_2014.py,magic2du/contact_matrix,1
"    vote1 = learnSVM(trainFeat, trainClass, testFeat)
    vote2 = learnKNN(trainFeat, trainClass, testFeat)
    vote3 = learnMaxEnt(trainFeat, trainClass, testFeat)
    res = [0] * len(testFeat)
    for i in range(0, len(testFeat)):
        if vote1[i] + vote2[i] + vote3[i] > 1:
            res[i] = 1
    return res

def SVM(trainFeat, trainClass, testFeat):
    model = svm.SVC()
    model.fit(trainFeat, trainClass)
    return model.predict(testFeat)

def KNN(trainFeat, trainClass, testFeat):
    model = neighbors.KNeighborsClassifier(15, weights='distance')
    model.fit(trainFeat, trainClass)
    return model.predict(testFeat)

def MaxEnt(trainFeat, trainClass, testFeat):",learnModels.py,metzzo/Paraphrase_Identification,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",tools/python_26/Lib/email/test/test_email_renamed.py,q3df/recordsystem2,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the f-score of each feature
        score = f_score.f_score(X, y)

        # rank features in descending order according to score
        idx = f_score.feature_ranking(score)
",PyFeaST/example/test_f_score.py,jundongl/PyFeaST,1
"    train, labels, test, _, _ = utils.load_data()

    # polynomial features
    poly_feat = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)
    train = poly_feat.fit_transform(train, labels)
    test = poly_feat.transform(test)

    print train.shape

    # feature selection
    feat_selector = LinearSVC(C=0.0001, penalty='l1', dual=False)
    train = feat_selector.fit_transform(train, labels)
    test = feat_selector.transform(test)

    print train.shape

    clf = XGBoost(max_iterations=4800, max_depth=12, min_child_weight=4.9208250938262745, row_subsample=.9134478530382129,
                  min_loss_reduction=.5132278416508804, column_subsample=.730128689911957, step_size=.009)

",otto/model/model_11_xgboost_poly/xgboost_poly.py,ahara/kaggle_otto,1
"    grid = ParameterGrid({""max_samples"": [0.5, 1.0],
                          ""max_features"": [1, 2, 4],
                          ""bootstrap"": [True, False],
                          ""bootstrap_features"": [True, False]})

    for base_estimator in [None,
                           DummyClassifier(),
                           Perceptron(),
                           DecisionTreeClassifier(),
                           KNeighborsClassifier(),
                           SVC()]:
        for params in grid:
            BaggingClassifier(base_estimator=base_estimator,
                              random_state=rng,
                              **params).fit(X_train, y_train).predict(X_test)


def test_sparse_classification():
    # Check classification for various parameter settings on sparse input.
",projects/scikit-learn-master/sklearn/ensemble/tests/test_bagging.py,DailyActie/Surrogate-Model,1
"from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE

# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

# Create the RFE object and rank each pixel
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_.reshape(digits.images[0].shape)

# Plot pixel ranking
import pylab as pl
pl.matshow(ranking)
pl.colorbar()
pl.title(""Ranking of pixels with RFE"")",exemplo/scikt_plot_rfe_digits.py,ebertti/nospam,1
"# Label vector.
Y = np.zeros(X.shape[0])
Y[500:] = 1

# Plot the generated data.
fig,ax = plt.subplots(1,1,figsize=(5,5))
ax.plot(X0[:,0], X0[:,1], 'o', label=""Class 0"", alpha=0.6)
ax.plot(X1[:,0], X1[:,1], 's', label=""Class 1"", alpha=0.6)

# Create and fit the Support Vector Classifier.
svc = LinearSVC()
svc.fit(X,Y)

# Create function for drawing the decision boundary.
line = lambda t: -(svc.intercept_[0] +\
                   svc.coef_[0,0]*t) / svc.coef_[0,1]
t = np.linspace(-4,4,101)

# Plot the determined boundary between classes.
ax.plot(t, line(t), '-', label=""Decision Boundary"")",research/talks/gss-python/code/sklearn-example.py,notmatthancock/notmatthancock.github.io,1
"from sklearn.svm import SVC


X, y = datasets.make_classification(n_samples=500,
                                    n_features=200000,
                                    n_informative=2,
                                    random_state=1)



methods = Methods(*[SVC(C=1, kernel='linear'), SVC(C=1, kernel='rbf')])   

data = {""X"":X, 'y':y, ""methods"": methods}

# X = np.random.random((500, 200000))

def map_func(data):
  from sklearn.cross_validation import StratifiedKFold
  from sklearn import svm, cross_validation
  kfold = StratifiedKFold(y=data['y'], n_folds=3)",test/bug_joblib/test_joblib_2000fts.py,neurospin/pylearn-epac,1
"
                for i in range(len(test)):
                    localDFList[testIdx[i]] = curDFList[i]
            assert(countTruth(lambda x: np.isnan(x), localDFList) == 0)

            #--- train SVC!!
            #- prepare X
            X = np.hstack(
                [np.matrix(globalDFList).T,
                 np.matrix(localDFList).T])
            metaClassifier = LinearSVC(
                loss='l2', penalty='l1', dual=False, tol=1e-6,
                random_state=0)
            metaClassifier.fit(X, y)
            trainLog += ""Trained a meta classifier:\n""
            coef = metaClassifier.coef_[0]
            trainLog += ""  coef for (global, local): (%.6f, %.6f)\n"" % (
                coef[0], coef[1])
            trainLog += ""  intercept: %.6f\n"" % (metaClassifier.intercept_)
",dssg/classifier.py,dssg/ushine-learning,1
"    preds_mean = kfolder.predict(X, vote_function=lambda x: numpy.mean(x, axis=0))
    # Now let's compare this with mean prediction:
    assert mean_squared_error(y, preds) > mean_squared_error(y, preds_mean)


def test_folding_classifier():
    base_ada = SklearnClassifier(AdaBoostClassifier())
    folding_str = FoldingClassifier(base_ada, n_folds=2)
    check_folding(folding_str, True, True, True)

    base_svm = SklearnClassifier(SVC())
    folding_str = FoldingClassifier(base_svm, n_folds=4)",tests/test_folding.py,vkuznet/rep,1
"    auc_score_,fpr_,tpr_,precision_,recall_,precision_scores_,recall_scores_,average_scores_,MCC_=[],[],[],[],[],[],[],[],[]
    if clf_ is 'logistic':
        clf=Pipeline([('scaler',StandardScaler()),
                        ('estimator',LogisticRegressionCV(Cs=np.logspace(-3,3,7),
                          max_iter=int(1e4),
                          tol=1e-4,
                          scoring='roc_auc',solver='sag',cv=5,#class_weight={1:10,0:1}))])
                          class_weight={1:weights*np.count_nonzero(Y)/len(Y),0:1-(np.count_nonzero(Y)/len(Y))}))])
    elif clf_ == 'svm':
        clf=Pipeline([('scaler',StandardScaler()),
                        ('estimator',SVC(C=1.0,kernel=kernel,
                          max_iter=int(1e4),
                          tol=1e-4,
#                          class_weight={1:10,0:1},
                          class_weight={1:weights*np.count_nonzero(Y)/len(Y),0:1-(np.count_nonzero(Y)/len(Y))},
                          probability=True,random_state=12345))])
    elif clf_ == 'RF':
        clf=Pipeline([('scaler',StandardScaler()),
                      ('estimator',RandomForestClassifier(n_estimators=50,
                                                          class_weight={1:weights*np.count_nonzero(Y)/len(Y),0:1-(np.count_nonzero(Y)/len(Y))},))])",eegPipelineFunctions.py,adowaconan/Spindle_by_Graphical_Features,1
"
print ""FIN SEPARATION, CREATION ARRAYS""

X = np.array(TrainVal2)
y = np.array(TrainFam2)
Xtest = np.array(TestVal)
ytest = np.array(TestFam)

print ""FIN CREATION DES ARRAYS, LANCEMENT DES ALGORITHMES""
print ""DEBUT LIN-SVC""
lin_svc = svm.LinearSVC(C=1.0).fit(X, y)
lin_svcPredict = lin_svc.predict(Xtest)
print ""FIN LIN-SVC, DEBUT NEAREST-CENTROID""
near_centroid = NearestCentroid(shrink_threshold=None).fit(X, y)
near_centroidPredict = near_centroid.predict(Xtest)
print ""FIN NEAREST-CENTROID, DEBUT K-NN UNIFORME, K=15""
knn_uni = neighbors.KNeighborsClassifier(15, weights='uniform').fit(X, y)
knn_uniPredict = knn_uni.predict(Xtest)
print ""FIN K-NN UNIFORME, DEBUT K-NN DISTANCE, K=15""
knn_dist = neighbors.KNeighborsClassifier(15, weights='distance').fit(X, y)",Tests/multi-algo.py,oubould/TestsML,1
"
    print 'BiClassifier '
    base_lr = BiClassifier('test3', feature_select=False, need_converter=False)
    base_lr.train(X_labeled, y_labeled)
    y_pred = base_lr.predict(X_test)
    print classification_report(y_test, y_pred)



    # print 'SVM'
    # base_svm = LinearSVC()
    # base_svm.fit(X_labeled, y_labeled)
    # y_pred = base_lr.predict(X_test)
    # print classification_report(y_test, y_pred)
    #
    # print 'SVM CoTraining'
    # svm_co_clf = CoTrainingClassifier(LinearSVC(), u=N_SAMPLES//10)
    # svm_co_clf.fit(X1, X2, y)
    # y_pred = svm_co_clf.predict(X_test[:, :N_FEATURES // 2], X_test[:, N_FEATURES // 2:])
    # print classification_report(y_test, y_pred)",TopicalCrawl/TopicalCrawl/classifier/sklearn_cotraining/sklearn_cotraining/test.py,actlea/TopicalCrawler,1
"    for file in glob.glob(sys.argv[1]+'.mat'):
        data = scipy.io.loadmat(file)

        #print(""\nTreinando SVM multi classe..."")
        #print(""\n""+file.split(""gram_"")[1].split(""_"")[0])
        ytrain = data['Ytrain'].T.reshape(data['Ytrain'].shape[1])

        x_train, x_val, y_train, y_val = cross_validation.train_test_split(data['Xtrain'], ytrain, test_size=0.2, random_state=0)
        tuned_parameters = []

	#cl = fusion.PerClassFusionClassifier([sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True), sklearn.svm.SVC(C=1, gamma=1e-3, probability=True),sklearn.svm.SVC(C=1, gamma=1e-3, probability=True),sklearn.svm.SVC(C=1, gamma=1e-3, probability=True)])

	cl = fusion.PerClassFusionClassifier(create_svms())

        #print "" -- TRAINNING: grid search with 5 fold cross-validation""

	#cl.fit(x_train, y_train)

	#print cl.predict(x_val)	
",class_specific_fusion.py,alan-mnix/MLFinalProject,1
"                                                 n_clusters_per_class=2)
    return X, y


# ANOVA SVM-C
def createANOVASVM():
    
    # anova filter, take 3 best ranked features
    anova_filter = SelectKBest(f_regression, k=3)   
    # svm
    clf = svm.SVC(kernel='linear')

    anova_svm = Pipeline([('anova', anova_filter), \
                          ('svm', clf)])

    return anova_svm


def predict(X, y, anova_svm):
    anova_svm.fit(X, y)",examples/scikit-learn/examples/general/pipeline_anova_svm.py,KellyChan/python-examples,1
"
    Uses SelectKBest from scikit-learn.feature_selection.

    Parameters
    ----------
    kernel : str
        Kernel for SVM (default: 'linear')
    k : int
        How many voxels to select (from the k best)
    **kwargs
        Arbitrary keyword arguments for SVC() initialization.

    Returns
    -------
    ftest_svm : scikit-learn Pipeline object
        Pipeline with f-test feature selection and svm.
    """"""

    ftest_svm = Pipeline([
        ('scaler', StandardScaler()),",skbold/pipelines/mvpa_pipelines.py,lukassnoek/skbold,1
"          % (mean_score, scores.std() / 2, params))

grid_scores = DataFrame(clf.grid_scores_)
grid_scores.to_csv(""grid_scores_nusvc.csv"")

print(""Detailed classification report:"")
y_true, y_pred = y_test, clf.predict(X_test)
print(classification_report(y_true, y_pred))

# Make a model with the best parameters
estimator = NuSVC(kernel='rbf', gamma=clf.best_estimator_.gamma,
                  nu=clf.best_estimator_.nu)
                # C=clf.best_estimator_.C)

# Plot the learning curve to find a good split
title = 'NuSVC'
plot_learning_curve(estimator, title, X_train, y_train, cv=cv, n_jobs=4)
p.savefig(""supervised_learning_nusvc.pdf"")

# Find a good number of test samples before moving on",project_code/Machine Learning/supervised_specs.py,e-koch/Phys-595,1
"def svm_classify(data, C):
    """"""
    trains a linear SVM on the data
    input C specifies the penalty factor of SVM
    """"""
    train_data, _, train_label = data[0]
    valid_data, _, valid_label = data[1]
    test_data, _, test_label = data[2]

    print('training SVM...')
    clf = svm.SVC(C=C, kernel='linear')
    clf.fit(train_data, train_label.ravel())

    p = clf.predict(test_data)
    test_acc = accuracy_score(test_label, p)
    p = clf.predict(valid_data)
    valid_acc = accuracy_score(valid_label, p)

    return [test_acc, valid_acc]
",utils.py,VahidooX/DeepCCA,1
"                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            print(""trying to get svm lock"")
            svm_lock.acquire()
            print(""got svm lock"")
            # with probability
            svm = GridSearchCV(SVC(C=1, probability=True), param_grid, cv=5, refit=True).fit(X, y)
            # without probability
#            svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)
            # without parameter search
#            svm = SVC(C=1, kernel='linear', probability=True).fit(X, y)
            print(""finished svm training"")                        
            svm_lock.release()
            print(""released svm lock"")                                    
            print(""successfully trained svm {} people:{}"".format(svm, people))                    
",server/openface-server/cloudlet-demo-openface-server.py,cmusatyalab/faceswap,1
"mean1 = np.array([-1, 2])
mean2 = np.array([1, -1])
cov = np.array([[1.0, 0.8], [0.8, 1.0]])

np.random.seed(500)
X = np.r_[np.random.multivariate_normal(mean1, cov, N / 2),
          np.random.multivariate_normal(mean2, cov, N / 2)]
t = [0] * (N / 2) + [1] * (N / 2)

# 線形SVMを学習
clf = svm.SVC(kernel='linear', C=2)
clf.fit(X, t)

# 訓練データをプロット
cmap = ListedColormap(['red', 'blue'])
pl.scatter(X[:, 0], X[:, 1], c=t, cmap=cmap)

# サポートベクトルを強調
pl.scatter(clf.support_vectors_[:, 0],
           clf.support_vectors_[:, 1],",sklearn/linear_svm.py,TenninYan/Perceptron,1
"
def knn_model(X, y):
    model = KNeighborsClassifier(10)

    model.fit(X, y)

    return model


def svm_model(X, y):
    model = SVC()

    model.fit(X, y)

    return model


def init(data_file):
    X, y = load_data(data_file)
",server/utils/ml.py,SquirrelMajik/GRec,1
"RANDOM_STATE = 42

# Generate a dataset
X, y = datasets.make_classification(n_classes=2, class_sep=2,
                                    weights=[0.1, 0.9], n_informative=10,
                                    n_redundant=1, flip_y=0, n_features=20,
                                    n_clusters_per_class=4, n_samples=5000,
                                    random_state=RANDOM_STATE)

pipeline = pl.make_pipeline(os.SMOTE(random_state=RANDOM_STATE),
                            LinearSVC(random_state=RANDOM_STATE))

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    random_state=RANDOM_STATE)

# Train the classifier with balancing
pipeline.fit(X_train, y_train)

# Test the classifier and get the prediction",examples/evaluation/plot_classification_report.py,chkoar/imbalanced-learn,1
"from __future__ import unicode_literals
from __future__ import with_statement

import sklearn.svm

import data
import process.bag_of_words
import submissions

if __name__ == '__main__':
    svm = sklearn.svm.LinearSVC(random_state=process.seed)
    svm.fit(process.bag_of_words.train, data.target)
    pred = svm.predict(process.bag_of_words.test)

    submissions.save_csv(pred, '{file_name}.csv'.format(file_name=__file__[:-3]))",word2vec_nlp_tutorial/support_vector_machine.py,wjfwzzc/Kaggle_Script,1
"import pylab as pl
from sklearn import svm, datasets

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

h = .02  # step size in the mesh

clf = svm.SVC(C=1.0, kernel='linear')

# we create an instance of SVM Classifier and fit the data.
clf.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))",examples/svm/plot_svm_iris.py,B3AU/waveTree,1
"# Extract the hog features
list_hog_fd = []
for feature in features:
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier",handwritingRecognition/generateClassifier.py,forrestgtran/TeamX,1
"from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


# classification models
classifiers = {'K-Nearest Neighbors (Braycurtis norm)':
               KNeighborsClassifier(n_neighbors=3, algorithm='auto',
                                    metric='braycurtis'),
               'Random Forest':
               RandomForestClassifier(n_estimators=80, n_jobs=1),
               'SVM': SVC(gamma=2, C=1),
               'Linear Support Vector Machine': SVC(kernel=""linear"", C=0.025),
               'Decision Tree': DecisionTreeClassifier(max_depth=5),
               'Ada Boost': AdaBoostClassifier(n_estimators=80,
                                               learning_rate=0.4),
               'Naive Bayes': GaussianNB(),
               }
vc = VotingClassifier(estimators=list(classifiers.items()), voting='hard')

",src/utils_classification.py,ThorbenJensen/wifi-locator,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED",shinken/external_command.py,bclermont/shinken,1
"
    Returns:
        best_params -- values for C and gamma that gave the highest accuracy with
                       cross-validation
        best_score -- highest accuracy with cross-validation
    """"""
    C_range = np.logspace(-2, 10, 13)
    gamma_range = np.logspace(-9, 3, 13)
    param_grid = dict(gamma=gamma_range, C=C_range)
    cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)
    grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
    grid.fit(X, y)
    print(""The best parameters are %s with a score of %0.2f""
        % (grid.best_params_, grid.best_score_))
    return grid.best_params_, grid.best_score_

def uniqify_filename_list(filename_list, idfun=None):
   # based on code by Peter Bengtsson
   # https://www.peterbe.com/plog/uniqifiers-benchmark
   if idfun is None:",finch_helper_funcs.py,NickleDave/hybrid-deep-finch,1
"##########################################################################
##  Test for ROC-AUC Curve
##########################################################################

class ROCAUCTests(VisualTestCase):

    def test_roc_auc(self):
        """"""
        Assert no errors occur during ROC-AUC integration
        """"""
        model = LinearSVC()
        model.fit(X,y)
        visualizer = ROCAUC(model, classes=[""A"", ""B""])
        visualizer.score(X,y)


##########################################################################
##  Test for Classification Report
##########################################################################
",tests/test_classifier.py,jkeung/yellowbrick,1
"from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

classifiers = { 'logistic_regression' : LogisticRegression(),
                'svm': SVC(),
                }",ml/classifiers.py,cavestruz/MLPipeline,1
"print 'logistic'
L = LogisticRegression(C = 50.0)
L.fit(Xtr, ytr)
print L.score(Xte,yte)
print L.score(Xtr,ytr)
print getf1(L, Xte, yte)

#too slow
'''
print 'svm'
S = SVC(C = 10.0)
S.fit(Xtr,ytr)
print S.score(Xte,yte)
print S.score(Xtr,ytr)
'''

print 'forest'
R = RandomForestClassifier()
R.fit(Xtr,ytr)
print R.score(Xte,yte)",baseline_t.py,ricsoncheng/sarcasm_machine,1
"
    def mapping_target(self, label):
        if label not in self.target_mapping:
            self.target_names.append(label)
            self.target_mapping[label] = self.target_index
            self.target_index += 1
        return self.target_mapping[label]


CLASSIFIERS = {
    'svm': LinearSVC(),
    'sgd': SGDClassifier(alpha=.0001, n_iter=50, penalty='l2'),
    'nb': MultinomialNB(alpha=.01),
}


def classifier(name):
    if name in CLASSIFIERS: return CLASSIFIERS[name]
    return CLASSIFIERS['sgd']
",document-classification/test.py,nicholasding/pythonml,1
"# main function to test the SVC Model
from matplotlib import pyplot as plt
import utils
from isvc import iSVC
import numpy as np
import isvc_gui as gui

__author__ = 'morgan'

clf = iSVC(display=True, gamma=1)
# generate data
X, y = utils.gen_noise_gauss(100)
n = np.size(X, 0)
big_x, big_y = utils.getBoxbyX(X, grid=30)
big_xy = np.c_[big_x.reshape(big_x.size, 1), big_y.reshape(big_x.size, 1)]
# add supervised labels
y_semisupervised = np.zeros((n, 1))
n_perm = np.random.permutation(n)  # random partition for labeling
",Python/test_isvc.py,feuerchop/IndicativeSVC,1
"        else:
            X, y = datasets.make_classification(n_samples=self.n_samples,
                                                n_features=self.n_features,
                                                n_informative=2,
                                                random_state=1)
            Xy = dict(X=X, y=y)
        ## 2) Building workflow
        ## =======================================================
        from sklearn.svm import SVC
        from epac import CV, Methods
        cv_svm_local = CV(Methods(*[SVC(kernel=""linear""),
                                    SVC(kernel=""rbf"")]), n_folds=3)

        cv_svm = None
        if self.is_swf:
            # Running on the cluster
            from epac import SomaWorkflowEngine
            mmap_mode = None
            if self.memmap:
                mmap_mode = ""r+""",doc/memory_benchmark/test_memmapping.py,neurospin/pylearn-epac,1
"hdim_deep2 = 300
nb_epoch = 40
batch_size = 100
dimx = 392
dimy = 392
lamda = 0.02
loss_type = 2 # 1 - l1+l2+l3-L4; 2 - l2+l3-L4; 3 - l1+l2+l3 , 4 - l2+l3

def svm_classifier(train_x, train_y, valid_x, valid_y, test_x, test_y):
    
    clf = svm.LinearSVC()
    #print train_x.shape,train_y.shape
    clf.fit(train_x,train_y)
    pred = clf.predict(valid_x)
    va = accuracy_score(np.ravel(valid_y),np.ravel(pred))
    pred = clf.predict(test_x)
    ta = accuracy_score(np.ravel(test_y),np.ravel(pred))
    return va, ta

def split(train_l,train_r,label,ratio):",corrnet/DeepLearn_cornet.py,GauravBh1010tt/DeepLearn,1
"        clf = grid_search.GridSearchCV(est2, lm1, cv=4, n_jobs=n_jobs,
                                       verbose=0)
    elif cl == 'lr1g':
        est1 = lm.LogisticRegression(
                    penalty='l1', dual=False, tol=0.0001,
                    C=1, fit_intercept=True, intercept_scaling=1.0,
                    class_weight=class_weight, random_state=random_state)
        clf = grid_search.GridSearchCV(est1, lm1, cv=4, n_jobs=n_jobs,
                                       verbose=0)
    elif cl == 'svmL':
        clf = svm.LinearSVC(C=1.0, loss='l2', penalty='l2', dual=True,
                            verbose=0, class_weight=class_weight)
    elif cl == 'svmL1':
        clf = svm.LinearSVC(C=1.0, loss='l2', penalty='l1', dual=False,
                            verbose=0, class_weight=class_weight)
    elif cl == 'svmL2':
        clf = svm.LinearSVC(C=1.0, loss='l1', penalty='l2', verbose=0,
                            class_weight=class_weight)
    elif cl == 'svmL1g':
        # est3 = svm.SVC(kernel='linear',verbose=0)",kgml/classifier.py,orazaro/kgml,1
"##   SVM Training   ##
######################

'''
#1st svm fit
print(""Svm Classification with first 500 features of layer fc7"")
C=1.0

print(""\tfitting svc..."")
sttime= time.clock()
svc = svm.SVC(kernel='linear', C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting rbf_svc..."")
sttime= time.clock()
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting poly_svc..."")
sttime= time.clock()",Region-to-Image_Matching/example/extract_train_test_DBH_flower.py,SelinaChe/Complex-Object-Detection-StackGAN,1
"    print 'convex optimizer solved'
    if np.allclose(np.ravel(mysolution['x']), np.ravel(solution['x'])):
        print 'EQUAL!!!'
    else:
        print 'WROng!!!'

def test_mysvm():
    X, y = testData()
    X = np.concatenate([X, y.reshape((len(y), 1))], axis=1)
    X = [list(x.ravel()) for x in X]
    hw6.svm_q1(X, mysvm.SVC(mysvm.SMO, mysvm.Kernel('linear')))
    hw6.svm_q1(X, svm.SVC())


def testData(y_ones=False):
    # Create the dataset
    rng = np.random.RandomState(1)
    X = rng.rand(100, 2)
    y = np.asarray([0] * 50 + [1] * 50)
    X[y == 1] += 2.0",Tests/hw6_tests.py,alliemacleay/MachineLearning_CS6140,1
"                                        random_state=1)
    print ""memm_local pt2""
    X = convert2memmap(X)
    y = convert2memmap(y)
    Xy = dict(X=X, y=y)
    ## 2) Build two workflows respectively
    ## =======================================================
    print ""memm_local pt3""
    from sklearn.svm import SVC
    from epac import CV, Methods
    cv_svm_local = CV(Methods(*[SVC(kernel=""linear""),
                                SVC(kernel=""rbf"")]),
                      n_folds=3)
    print ""memm_local pt4""
#    from epac import LocalEngine
#    local_engine = LocalEngine(cv_svm_local, num_processes=2)
#    cv_svm = local_engine.run(**Xy)
    cv_svm_local.run(**Xy)
    print cv_svm_local.reduce()
    print ""memm_local pt5""",doc/memory_benchmark/benchmark_mem.py,neurospin/pylearn-epac,1
"
def plt_subplots(nrow=1, ncol=1, sharex=False, sharey=False, figsize=(10,10)):
    """""" Initialize subplots. """"""
    fig,axarr = plt.subplots(nrow, ncol, sharex=sharex, sharey=sharey,figsize=figsize)
    axgen = (e for e in np.array(axarr).ravel())
    return fig,axgen

def plot_decision_line(clf, X, y, names=None):
    """""" Plot decision line

    Plot decision line for the linear model (svm.SVC(kernel= ""linear""))
    
    Parameters
    ----------
    clf: (BaseEstimator, ClassifierMixin)
        classifier

    X: array-like, shape=(n_samples,n_features) 
        train data
",kgml/plot.py,orazaro/kgml,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_21_2015.py,magic2du/contact_matrix,1
"# classify using SVM
def SVM_Classify(trainDataPath, trainLabelPath, testDataPath, testLabelPath, kernelType):
    trainData = np.array(trainDataPath)
    trainLabels = trainLabelPath
    testData = np.array(testDataPath)
    testLabels = testLabelPath

    if kernelType == ""HI"":

        gramMatrix = histogramIntersection(trainData, trainData)
        clf = SVC(kernel='precomputed')
        clf.fit(gramMatrix, trainLabels)

        predictMatrix = histogramIntersection(testData, trainData)
        SVMResults = clf.predict(predictMatrix)
        correct = sum(1.0 * (SVMResults == testLabels))
        accuracy = correct / len(testLabels)

        print ""SVM (Histogram Intersection): "" +str(accuracy * 100)+ ""% ("" +str(int(correct))+ ""/"" +str(len(testLabels))+ "")""
",recognition/classification.py,bvnayak/image_recognition,1
"        src_addr = SCIONAddr.from_values(ISD_AS(""1-1""), HostAddrIPv4(""127.0.0.1""))
        connector = self._setup_connector()
        # Call
        ntools.eq_(connector._resolve_dst_addr(src_addr, dst_addr),
                   host_info.return_value)
        # Tests
        ntools.assert_false(connector.get_service_info.called)
        host_info.assert_called_once_with([HostAddrIPv4(""127.0.0.2"")], SCION_UDP_EH_DATA_PORT)

    def test_with_different_ases(self):
        dst_addr = SCIONAddr.from_values(ISD_AS(""1-2""), HostAddrSVC(0, raw=False))
        src_addr = SCIONAddr.from_values(ISD_AS(""1-1""), HostAddrIPv4(""127.0.0.1""))
        connector = self._setup_connector(svc_info_desc=(""bs"", ""bs1""))
        # Call
        ntools.eq_(connector._resolve_dst_addr(src_addr, dst_addr), None)
        # Tests
        ntools.assert_false(connector.get_service_info.called)


class TestSCIONDConnectorTryCache:",python/test/lib/app/sciond_test.py,Oncilla/scion,1
"    #Finding for SVM:
    print 'Finding for SVM...'
    kernels = ['linear', 'rbf', 'poly', 'sigmoid'] #['sigmoid', 'rbf', 'linear'] #['poly'] #
    regul = [pow(5, x) for x in xrange(0, 4)]
    degree = xrange(1, 5)
    for k in kernels:
        for reg in regul:
            if k == 'poly':
                for deg in degree:
                    print k, ':', reg, ':', deg
                    svmLearner = svm.SVC(C=reg, kernel=k, degree=deg)
                    svmLearner.fit(finalFeatures, finalAnswers)
                    score = svmLearner.score(cvFeatures, cvAnswers)
                    if svmBestParam[0] < score:
                        svmBestParam = [score, reg, k, deg]
            else:
                svmLearner = svm.SVC(C=reg, kernel=k)
                svmLearner.fit(finalFeatures, finalAnswers)
                score = svmLearner.score(cvFeatures, cvAnswers)
                if svmBestParam[0] < score:",stockMarket/learning/findBestParameters.py,seba-1511/stockMarket,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(1,101):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

        clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        args=[str(dim)+ ""Dgauss_dt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/dt_gauss/dt_Gauss_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"# also load the iris dataset
iris = datasets.load_iris()
rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    # Test parameters on classes that make use of libsvm.
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[-0.25, .25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    # Check consistency on dataset iris.",projects/scikit-learn-master/sklearn/svm/tests/test_svm.py,DailyActie/Surrogate-Model,1
"    data[:, 1] = radius * np.sin(theta)

    labels = np.ones(Npts)
    labels[far_pts] = -1

    return data, labels


def plot_linear_model():
    X, y = linear_model()
    clf = svm.SVC(kernel='linear',
                  gamma=0.01, coef0=0, degree=3)
    clf.fit(X, y)

    fig = pl.figure()
    ax = pl.subplot(111, xticks=[], yticks=[])
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=pl.cm.bone)

    ax.scatter(clf.support_vectors_[:, 0],
               clf.support_vectors_[:, 1],",unit_20/parallel_ml/notebooks/figures/svm_gui_frames.py,janusnic/21v-python,1
"  X = np.array(data_df[features].values)
  y = ( data_df[""Status""]
        .replace(""underperform"",0)
        .replace(""outperform"",1)
        .values.tolist()
  )
  return X,y

def Analysis():
  X, y = Build_Data_Set()
  clf = svm.SVC(kernel=""linear"", C= 1.0)
  clf.fit(X,y)
  w = clf.coef_[0]
  a = -w[0] / w[1]
  xx = np.linspace(min(X[:, 0]), max(X[:, 0]))
  yy = a * xx - clf.intercept_[0] / w[1]

  h0 = plt.plot(xx,yy, ""k-"", label=""non weighted"")

  plt.scatter(X[:, 0],X[:, 1],c=y)",p13.py,cleesmith/sentdex_scikit_machine_learning_tutorial_for_investing,1
"        data[i] = re.sub('[^AZERTYUIOPQSDFGHJKLMWXCVBNazertyuiopqsdfghjklmwxcvbn]', '', data[i]).lower()
        
# Delcaration des pre-processing / classifier
ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=0)
transformer = TfidfTransformer()

# Chaque paramètres représente la meilleur performance de l'ensemble
classifiers = [
    SGDClassifier(alpha = 1e-4),
    DecisionTreeClassifier(max_depth=None),
    SVC(gamma=2, C=1),
    RandomForestClassifier(n_estimators=60),
    AdaBoostClassifier()]
    

Result = numpy.empty((0,3), float)

# Boucle sur les classifiers
for clf in classifiers:
",BIM_Classifier_TextOnly.py,HugoMartin78/bim-classifier,1
"def main():
#    train = csv_io.read_data(""../Data/train.csv"")
#    target = [x[0] for x in train]
#    train = [x[1:] for x in train]
#    test = csv_io.read_data(""../Data/test.csv"")

    dataset = np.genfromtxt(open('../Data/train.csv','r'), delimiter=',', dtype='f8')[1:]
    target = np.array([x[0] for x in dataset])
    train = np.array([x[1:] for x in dataset])

    cfr = svm.SVC(probability=True)

    #Simple K-Fold cross validation. 5 folds.
    cv = cross_validation.KFold(len(train), k=5, indices=False)

    results = []
    for traincv, testcv in cv:
        cfr.fit(train[traincv], target[traincv])
        probas = cfr.predict_proba(train[testcv])
        results.append(evalfun.llfun(target[testcv], [x[1] for x in probas]) )",BioResponse/Benchmarks/svm_benchmark.py,francis-liberty/kaggle,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_26_2015_parallel.py,magic2du/contact_matrix,1
"# Youtube video tutorial: https://www.youtube.com/channel/UCdyjiB5H8Pu7aDTNVXTTpcg
# Youku video tutorial: http://i.youku.com/pythontutorial

""""""
Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.
""""""
from __future__ import print_function
from sklearn import svm
from sklearn import datasets

clf = svm.SVC()
iris = datasets.load_iris()
X, y = iris.data, iris.target
clf.fit(X, y)

# method 1: pickle
import pickle
# save
with open('save/clf.pickle', 'wb') as f:
    pickle.dump(clf, f)",DeepLearnMaterials/tutorials/sklearnTUT/sk11_save.py,MediffRobotics/DeepRobotics,1
"        this GridSearchCV instance after fitting.

    verbose : integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., n_jobs=1,",ambra/grid_search.py,vene/ambra,1
"

def margins_and_hyperplane():
    #gen some data
    np.random.seed(0)
    n = 20
    X = (np.vstack((np.ones((n,2))*np.array([0.5,1]), 
        np.ones((n,2))*np.array([-0.5,-1]))) + np.random.randn(2*n,2)*0.3)
    Y = np.hstack((np.ones(n), np.zeros(n)))

    clf = svm.SVC(kernel='linear')
    clf.fit(X, Y)

    # Note the following code comes from a scikit learn example...
    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xs = np.linspace(-2, 2)
    ys = a * xs - (clf.intercept_[0]) / w[1]
    ",classification/util.py,NICTA/MLSS,1
"# print(""GOT "",text_clf.predict(data_test[0]))
# print(""GOT "",text_clf.predict_proba(data_test[0]))
# sys.exit()


if True:
    # Create the pipeline
    text_clf = Pipeline([('vect', vectorizer),
                         #('clf', MultinomialNB()),
                         ('clf', RandomForestClassifier(n_estimators=100)),
                         #('clf', LinearSVC()),
                         #('clf', SGDClassifier()),
                         #('clf', SVC(probability=True)), # probability slows method but allows retrievel of probabilities
                         ])
    
    t0 = time()
    text_clf.fit(data_train, target_train)
    train_time = time() - t0
    print(""train time: %0.3fs"" % train_time)
    ",sklearn_separate.py,linucks/textclass,1
"            else:
                wb = input
            # Normalize the input vector
            x = wb/np.linalg.norm(wb)
            h = np.tanh(self.W.dot(x))
        return np.tanh(np.dot(np.array(h), self.v))

# Class that uses Scikit-Learn's implementation of SVM to predict labels
class svm():
    def __init__(self):
        # self.clf = SVC(kernel='rbf')
        self.clf = NuSVC()

    def train(self, inputs):
        # Parameters:
        #     inputs: An array of Input objects containing input vectors along with their corresponding labels.

        # Creates lists to use for fitting model
        X = []
        Y = []",Backpropagator.py,amagoon/Neural-Network-Tools,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 5, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[5.0, 5.0]]),
        sigma = 1.0,
        beta = 0.9,
        meta_model = meta_model) 

    return method
",tests/TR2_test.py,jpzk/evopy,1
"    num_train_records = num_records * 2 / 3
    train_input_values = input_values[:num_train_records]
    test_input_values = input_values[num_train_records:]
    train_labels = labels[:num_train_records]
    test_labels = labels[num_train_records:]

    X = np.array(train_input_values)
    y = np.array(train_labels)

    if self.classifier_type == ""svm"":
      clf = svm.LinearSVC()
    else:
      raise ValueError(""Classifier type is '%s' but can only be: %s""
                       % VALID_CLASSIFIER_TYPES)

    clf.fit(X, y)

    X_test = np.array(test_input_values)
    y_test = np.array(test_labels)
",brainsquared/modules/classifiers/SKLearnClassifier.py,CloudbrainLabs/htm-challenge,1
"	kpca.fit(kpca_train)

	kernel_train = arc_cosine(trainX, trainX[0:1000])
	kernel_test = arc_cosine(testX, trainX[0:1000])

	trainX_kpca = kpca.transform(kernel_train)
	testX_kpca = kpca.transform(kernel_test)
	print testX_kpca.shape

	#fit the svm model and compute accuaracy measure
	clf = svm.SVC(kernel=arc_cosine)
	clf.fit(trainX_kpca, trainY)

	pred = clf.predict(testX_kpca)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

if __name__ == '__main__':",kpcaWithTreeFS/mnistKPCA.py,akhilpm/Masters-Project,1
"    try:
    	feature.append(combine_embedding(op,embeddings[nodes[0]],embeddings[nodes[1]]))
    except KeyError:
	continue
    label.append(training_set[edge])
feature_np = np.asarray(feature)
label_np = np.asarray(label)


#x,residuals,rank,s = np.linalg.lstsq(feature_np,label_np)
clf = svm.SVC()
clf.fit(feature_np,label_np)
train_error = evaluate_perf(feature_np,clf,label_np)


#Get test data and evaluate performance
feature_test = []
label_test = []
for edge in testing_set.keys():
    nodes = edge.split('-')",Initial_Test/linkPrediction.py,abhi252/GloVeGraphs,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_backward.decision_tree_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",PyFeaST/example/test_decision_tree_backward.py,jundongl/PyFeaST,1
"             [1.5,1.8],
             [8,8],
             [1,0.6],
             [9,11]])

# our output or target
y = [0,1,0,1,0,1]


# our svm classifer
clf = svm.SVC(kernel='linear', C=1.0)

# learn
clf.fit(X, y)

# visualize
w = clf.coef_[0]
print(w)

a = -w[0] / w[1]",ScikitlearnPlayground/LinearClassifierSVM.py,eboreapps/Scikit-Learn-Playground,1
"	#set the timer
	start = time.time()

	trainX = np.load('trainX_feat.npy')
	testX = np.load('testX_feat.npy')
	trainY = np.load('trainY_feat.npy')
	testY = np.load('testY_feat.npy')
	print('\n!!! Data Loading Completed !!!\n')
	
	#clf = KNeighborsClassifier(n_neighbors=25, weights='distance')
	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithUVFS/mnistBackImage/kernel.py,akhilpm/Masters-Project,1
"        from sklearn.ensemble import ExtraTreesClassifier
        extratrees = ExtraTreesClassifier(n_estimators = 100) #1000 trees
        extratrees = extratrees.fit(datatrain,alivetrain)
        Output = extratrees.predict(datatest)

    #-------------
    # SUPPORT VECTOR MACHINES
    elif choose_method == 6:
        print '\nRunning SVM Classifier...\n'
        from sklearn import svm
        clf = svm.SVC()
        clf.fit(datatrain,alivetrain)
        Output = clf.predict(datatest)

    #-------------
    # NAIVE BAYES
    elif choose_method == 7:
        print '\nRunning Gaussian Naive Bayes...\n'
        from sklearn.naive_bayes import GaussianNB
        gnb = GaussianNB()",bikerides.py,jesford/bike_sharing,1
"    np.savetxt(filename, output, fmt='%d', delimiter=',',
               header='Id,Solution', comments='')


X_train = load_data('train.csv')
y_train = load_data('trainLabels.csv')
X_test = load_data('test.csv')

#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, train_size=0.33)
X_train, X_test = perform_pca((X_train, X_test), 12)
clf = svm.SVC(kernel='rbf', C=10000000, probability=True)
X_train, y_train = semi_supervised_set(clf, X_train, y_train, X_test, 0.85)

clf = svm.SVC(kernel='rbf', C=10000000)
clf.fit(X_train, y_train)
#print(clf.score(X_test, y_test))

write_result(clf.predict(X_test), 'submission_pca_svm_semi.csv')",scikit-learn/semisupervised.py,aufziehvogel/kaggle,1
"import numpy as np
import os
from nltk.tokenize import RegexpTokenizer
from collections import Counter
import sys
import constituentretrofit_fixed_word2vec_native as consfit


class EvaluateSentimentVec(object):
    def __init__(self):
        self.lin_clf = svm.LinearSVC()
        self.tokenizer = RegexpTokenizer(r""[\w'-]+"")
        self.word2vec = {}
        self.dim = 0
        self.documentPos = []
        self.documentNeg = []

    def loadVector(self, path):
        print 'loading vector...'
        vocab, self.word2vec, self.dim = consfit.readWordVectors(path)",evaluate_sentiment_vec_native.py,sidenver/ConstituentRetrofit,1
"from numpy import genfromtxt
features_test = genfromtxt('d:/CODE/ml-crops/preproc/dataset/features_train.csv', delimiter=',')
classes_test = genfromtxt('d:/CODE/ml-crops/preproc/dataset/classes_train.csv', delimiter=',')

features_train = genfromtxt('d:/CODE/ml-crops/preproc/dataset/features_test.csv', delimiter=',')
classes_train = genfromtxt('d:/CODE/ml-crops/preproc/dataset/classes_test.csv', delimiter=',')

###############################################
# perform SVM classification
from sklearn.svm import SVC
clf = SVC(kernel='linear')
fit_start_time = time()
clf.fit(features_train, classes_train)
fit_end_time = time()
print ""\nTraining time : "", round(fit_end_time - fit_start_time, 3), ""s""

###############################################
# predict
predict_start_time = time()
classes_predicted = clf.predict(features_test)",SVM/svm_ml_crops.py,stefan-contiu/ml-crops,1
"  columns = list(x.columns)
  for col in columns:
    x,sc_hash = scale_col(x, col, sc_hash)

  pdb.set_trace()

  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
  
  # SVC
  from sklearn.svm import SVC
  svc = SVC()
  svc.fit(x_train, y_train)
  y_pred_svm = svc.predict(x_test)

  # Validation
  from sklearn.metrics import confusion_matrix
  confusion_matrix(y_test, y_pred_svm)
  # predicted  0      1    Total:
  # real 0 [7590, 1240]     8830
  #      1 [2715, 2187]     4902 ",min_hypo1.py,cgrohman/ponies,1
"			vf = MatrixUtils.standardize_vector(vf,v_avg,v_var)		
		if normalize:
			vm = MatrixUtils.normalize_vector(vm,v_min,v_max)		
			vf = MatrixUtils.normalize_vector(vf,v_min,v_max)		
		X.append(vm)
		y.append(0)
		X.append(vf)
		y.append(1)	
	
	# SVM & RF
	rbf = svm.SVC(kernel='rbf', C=10000, gamma=0.1)	
	#rbf = svm.SVC(kernel='rbf', C=10, gamma=10)	
	#rbf = svm.LinearSVC()
	#rbf = linear_model.SGDClassifier()
	#rbf = linear_model.Perceptron(n_iter=4, shuffle=True)
	rbf.fit(X, y)  
	
	# whole vectors:
	#rf = RandomForestClassifier(n_estimators=rf_n_estimators)
	#",train_and_test_binary.py,jimbotonic/df_nlp,1
"
base_learners = [RandomForestClassifier(n_estimators=500,
                                        max_depth=10,
                                        min_samples_split=50,
                                        max_features=0.6),
                 LogisticRegression(C=1e5),
                 GradientBoostingClassifier()]

for clf in estimators.values():
    clf.add([RandomForestClassifier(), LogisticRegression(), MLPClassifier()])
    clf.add_meta(SVC())


times, scores = {k: [] for k in estimators}, {k: [] for k in estimators}
for i in range(5000, xtrain.shape[0] + 5000, 5000):
    for name, clf in estimators.items():

        t0 = perf_counter()
        clf.fit(xtrain[:i], ytrain[:i])
        times[name].append(perf_counter() - t0)",benchmarks/ensemble_comp.py,flennerhag/mlens_dev,1
"			train.append(oriData[i])
			g_train.append(userMap[userId][0])
			a_train.append(userMap[userId][1])
		elif userId in targets:
			user.append(userId)
			test.append(oriData[i])

	train = np.array(train)
	test = np.array(test)

	genderClassifier = SVC()
	ageClassifier = SVC()

	genderClassifier.fit(train, g_train)
	ageClassifier.fit(train, a_train)

	gP = genderClassifier.predict(test)
	aP = ageClassifier.predict(test)

	result = []",image_classifier_lib.py,chenzeyu/demographic_prediction,1
"        
#         train_set = X[row_selected, :]
#         train_label = Y[row_selected]
#         test_set = X[row_unselected, :]
#         test_label = Y[row_unselected]
        
        train_set, train_label, test_set, test_label = get_train_test_rate(x, y, rate=row_selected_rate)
        
        
        # fit a SVM model to the data
        model = svm.LinearSVC(loss='hinge')
        model.fit(train_set, train_label)
        # print(model)
        # make predictions
        expected = test_label
        predicted = model.predict(test_set)
        
        sum_accuracy += accuracy(expected, predicted)
    avg_accuracy = sum_accuracy / cnt
    return avg_accuracy",evaluate/evaluate.py,xjchensz/LSFS,1
"			dsz+=1
		x[rcnt,dic[big_tweet][0]]=dic[big_tweet][1]
	rcnt+=1


print ""done""

#save = (y,x,dic)
#pickle.dump(save,open(""small_save.p"",""wb""),1)

# clf = svm.LinearSVC().fit(x[10000:-10000],y[10000:-10000])
# # pickle.dump((clf,dic,shape(x)[1]),open(""small_model.p"",""wb""),1)
# print ""done""
# score  = clf.score(x[10000:-10000],y[10000:-10000])
# print ""train "",score
# score  = clf.score(x[:10000],y[:10000])
# print ""neg "",score
# score  = clf.score(x[-10000:],y[-10000:])
# print ""pos "",score
",load.py,satwantrana/twitter-sentiment-predictor,1
"    #                                                 mask2)
    epochs_per_subj = int(sys.argv[5])
    num_subjs = int(sys.argv[6])
    # the following line is an example to leaving a subject out
    #vs = VoxelSelector(labels[0:204], epochs_per_subj, num_subjs-1, raw_data[0:204])
    # if using all subjects
    vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data)
    # if providing two masks, just append raw_data2 as the last input argument
    #vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data, raw_data2=raw_data2)
    # for cross validation, use SVM with precomputed kernel
    clf = svm.SVC(kernel='precomputed', shrinking=False, C=10)
    results = vs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        logger.info(
            'correlation-based voxel selection is done'
        )
        #print(results[0:100])
        mask_img = nib.load(mask_file)
        mask = mask_img.get_data().astype(np.bool)",examples/fcma/voxel_selection.py,lcnature/brainiak,1
"	                           max_features=0.2, 
	                           n_jobs=4,
	                           random_state=1).fit(X_train, y_train)
	#y_pred = clf.predict(X_test)

	return clf

def support_vector(X_train, y_train):
	""""""
	""""""
	clf = SVC(kernel='linear')
	clf.fit(X_train, y_train)
	
	return clf

def main(filenames, filename_pheno, phenos):
	""""""
	""""""
	ids, X = [], []
	snps = None",methods.py,celiacintas/prediction_tests,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_04_2014_server.py,magic2du/contact_matrix,1
"    print(e)
    exit(""Could not load omesa. Please update the path in this file."")

Experiment(
    project=""unit_tests"",
    name=""gram_experiment"",
    data=[CSV(""n_gram.csv"", data=""intro"", label=""label"")],
    pipeline=[
        Vectorizer(features=[Ngrams(level='char', n_list=[3])]),
        Pipe('scaler', MaxAbsScaler()),
        Pipe('clf', SVC(kernel='linear'),
             parameters={'C': np.logspace(-2.0, 1.0, 10)}),
        Evaluator(scoring='f1', average='micro',
                  lime_docs=CSV(""n_gram.csv"", data=""intro"", label=""label"")),
    ],
    save=(""log"", ""model"", ""db"")
)

Experiment(
    project=""unit_tests"",",examples/n_gram.py,cmry/omesa,1
"    train_features = [(build_features(inputs_train[i], i, words)) for i in range(len(inputs_train))]
    valid_features = [(build_features(inputs_valid[i], i, words)) for i in range(len(inputs_valid))]
    test_features  = [(build_features(inputs_test[i], i, words)) for i in range(len(inputs_test))]
    print('extracted features for all tweets')

    return np.array(train_features), np.array(valid_features), np.array(test_features)

def train_model(features, targets):
    print(""Fitting the classifier to the training set"")
    t0 = time()
    clf = SVC()
    clf.fit(features, targets)
    print(""done in %0.3fs"" % (time() - t0))
    return clf

def grid_search_train_model(features, targets):

    print(""Fitting the classifier to the training set using grid search"")
    t0 = time()
    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],",SVM/train_sklearn_SVM.py,RamaneekGill/Twitter-Sentiment-Analysis,1
"        user_id = requestData['user_id']
        #svc_id = requestData['svc_id']
        #action_type = requestData['action_type']
        action_type = 'new_instance'
        apply_time_limit = requestData['apply_time_limit']
        svc_type = requestData['svc_type']
        sub_type = requestData['sub_type']
        svc_label = requestData['svc_label']
        svc_params = requestData['svc_params']

        svc = models.SVC(project_id=project_id, user_id=user_id, svc_label=svc_label,
                             svc_type=svc_type, sub_type=sub_type,
                             apply_time_limit=apply_time_limit,
                             svc_params=jsonStr(svc_params),
                             status=models.SVC_STATUS_CHOICES[0][0], info=models.SVC_STATUS_CHOICES[0][1])
        svc.save()
        svc_id = svc.id
        logger.info(""svc id: "" + str(svc_id) + "" saved success"")

        task_id = marathonLib.task_submit(project_id, user_id, svc_id, action_type, svc_type, sub_type, svc_label, svc_params)",applications/app/views.py,junneyang/scheduler-mgnt,1
"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA


names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"",""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
    
    ",scikit_code/multi_classifier_plot.py,chakpongchung/RIPS_2014_BGI_source_code,1
"			acc_best = acc_best_in_search
			feats_best = feats_best_in_search
			print(np.argwhere(feats_best)[:, 0])
			print(acc_best)
		else:
			break

	return feats_best

def use_features_for_classification(data, labels, feats_test):
	clf = NuSVC(nu = 0.1, probability = False, decision_function_shape = 'ovo')
	clf.fit(normalize(data[0][:, feats_test]), labels[0])
	preds = clf.predict(normalize(data[1][:, feats_test]))
	return labels[1], preds
	
# trains and tests an SVM
def test_features(data, labels, feats_test):

	labels_true, labels_pred = use_features_for_classification(data, labels, feats_test)
	conf_matrix = confusion_matrix(labels_true, labels_pred)",feature_selector.py,bbenligiray/greedy-face-features,1
"  N = len(y)

  print 'Training...'  
  shuffler = cross_validation.ShuffleSplit(N, 1, opt.ratio)
  for train_idx, test_idx in shuffler:
    X_train = X[train_idx,:]
    X_test = X[test_idx,:]
    y_train = y[train_idx]
    y_test = y[test_idx]

    clf = svm.SVC(C=opt.C, kernel='poly', degree=opt.degree, verbose=False)
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    print 'Correct rate:', or_ml.correctRate(pred, y_test)

    joblib.dump(clf, opt.outfile)

    classnames = map(or_util.removeExtension, filenames)
    joblib.dump(classnames, or_util.appendToName(opt.outfile, '_classes'))",or_lib/scripts/shape_train.py,Beautiful-Flowers/object_recognizer,1
"	inv_made=0
	market_earn=0
	pred_invest=0

	X,y,Z=buildDataSet()
	y=np.array(y)
	print(len(X))

	###algortihm selection for testing and benchmarking

	#clf=svm.SVC(kernel=""poly"",degree=10,C=1)
	clf=RandomForestClassifier(max_features=None, oob_score=True)
	#clf=GradientBoostingClassifier()
	#kf_total = cross_validation.KFold(len(X), n_folds=2,  shuffle=True, random_state=4)
	#clf=NearestCentroid(metric='euclidean', shrink_threshold=None)
	#clf=LogisticRegression()
	#scores=[]
	#scores = cross_validation.cross_val_score(clf, X[:-test_size],y[:-test_size], cv=5)
	#print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
	clf.fit(X[:-test_size],y[:-test_size])",testing.py,aditya-pai/stockPrediction,1
"
  # Need to build 2D array for X, Y, Z
  for c in cs:
    powc = np.power(10,c)
    f1_scores.append([])
    cs_array.append([])
    gs_array.append([])
    for g in gammas:
      powg = np.power(10,g)

      clf = svm.SVC(kernel='rbf', C=powc, gamma=powg)
      clf.fit(xs[:TRAIN_SIZE], ys[:TRAIN_SIZE])

      ys_predicted = clf.predict(xs[TRAIN_SIZE:]) # Predict

      #accuracy = clf.score(xs[TRAIN_SIZE:], ys[TRAIN_SIZE:])
      f1_score = metrics.f1_score(ys[TRAIN_SIZE:], ys_predicted, average='micro')
      #accuracies.append(accuracy)
      cs_array[-1].append(c)
      gs_array[-1].append(g)",wholesale_customers/optimize_c_gamma.py,Josephu/svm,1
"from sklearn.svm import SVC
from sklearn.datasets import make_classification
from gat.classifiers import force_predict

# setup dataset
X, y = make_classification(n_informative=10, n_classes=2, n_samples=20)

svc = SVC(kernel='linear', probability=True)
svc.fit(X, y)
svc.predict(X)
svc.predict_proba(X)

svc = SVC()
svc_proba = force_predict(estimator=SVC(kernel='linear', probability=True))
svc_proba.fit(X, y)
svc_proba.predict(X)
",sandbox/decoding/force_predict_svc_example.py,kingjr/meg_perceptual_decision_symbols,1
"
  # Data splitting
  x = dataset.iloc[:,:-1]
  y = dataset.iloc[:,-1]

  from sklearn.cross_validation import train_test_split
  x_train, x_test, y_train, y_test = trian_test_split(x, y, test_size=0.2)
  
  # SVC
  from sklearn.svm import SVC
  svc = SVC()
  svc.fit(x_train, y_train)
  y_pred_svm = svc.predict(x_test)

  # Validation
  from sklearn.metrics import confusion_matrix
  confusion_matrix(y_test, y_pred_svm)
  # predicted  0      1    Total:
  # real 0 [7590, 1240]     8830
  #      1 [2715, 2187]     4902 ",hypo1.py,cgrohman/ponies,1
"features_train, features_test, labels_train, labels_test = preprocess()


#########################################################
### your code goes here ###

#########################################################
#features_train = features_train[:len(features_train)/100] 
#labels_train = labels_train[:len(labels_train)/100] 

clf = SVC(C=10000.0, kernel ='rbf')
t0 = time()
clf.fit(features_train, labels_train)
print ""training time:"", round(time()-t0, 3), ""s""

t0 = time()
pred = clf.predict(features_test)
print ""predicting time:"", round(time()-t0, 3), ""s""
acc = accuracy_score(pred, labels_test)
print acc",intro_to_machine_learning/lesson/lesson_2_svm/svm_author_id.py,tuanvu216/udacity-course,1
"start = time.clock()
clf = LogisticRegression('l2', C=1)
clf.fit(X_train,y_train)
accuracy = clf.score(X_test, y_test) * 100.0
end = time.clock()
print ""logistic regression accuracy on titanic dataset: %.2f%%"" % accuracy
print ""time to train logistic regression: %.2f seconds\n"" % (end - start)

#support vector machine w/ linear kernel on titanic
start = time.clock()
clf = LinearSVC(C=0.1)
clf.fit(X_train,y_train)
accuracy = clf.score(X_test, y_test) * 100.0
end = time.clock()
print ""linear support vector machine accuracy on titanic dataset: %.2f%%"" % accuracy
print ""time to train linear support vector machine: %.2f seconds\n"" % (end - start)

#support vector machine w/ rbf kernel on titanic
start = time.clock()
clf = SVC(C=1, kernel='rbf', gamma = 0.1)",Basic_ML/Classifier_Comparison/classifier_comparison.py,iamshang1/Projects,1
"                    check.description = 'Test l1_min_c loss=%r %s %s %s' % \
                                      (loss, X_label, Y_label, intercept_label)
                    yield check


def check_l1_min_c(X, y, loss, fit_intercept=True, intercept_scaling=None):
    min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)

    clf = {
        'log':  LogisticRegression(penalty='l1'),
        'l2':  LinearSVC(loss='l2', penalty='l1', dual=False),
    }[loss]

    clf.fit_intercept = fit_intercept
    clf.intercept_scaling = intercept_scaling

    clf.C = min_c
    clf.fit(X, y)
    assert_true((np.asarray(clf.coef_) == 0).all())
    assert_true((np.asarray(clf.intercept_) == 0).all())",venv/lib/python2.7/site-packages/sklearn/svm/tests/test_bounds.py,devs1991/test_edx_docmode,1
"#     ""Random Forest"",
#     ""Neural Net"",
#     # ""AdaBoost"",
#     # ""Naive Bayes"",
#     # ""QDA"",
#     ""LQA""
# ]

# CLASSIFIERS = [
#     KNeighborsClassifier(17), # ~49% acc
#     # LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, # fails to converge. takes long to run
#     #           multi_class='ovr', fit_intercept=True, intercept_scaling=1,
#     #           class_weight=None, verbose=VERBOSE, random_state=None, max_iter=1000),
#     # SVC(kernel=""linear"", C=0.025, verbose=VERBOSE), # ~47% acc, takes about 5 min to run on 20000 inputs
#     # SVC(gamma=2, C=1, verbose=VERBOSE), # takes too long on 20000 inputs
#     SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1, eta0=0.0,    # ~0.41 acc, 3 sec
#                   fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge',
#                   n_iter=5, n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
#                   shuffle=True, verbose=VERBOSE, warm_start=False),
#     # GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),     # takes too long",src/ml/voting_classifier.py,seokjunbing/cs75,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

print('Start training the model')
C_range = np.logspace(-2, 10, GRID_SIZE)
gamma_range = np.logspace(-9, 3, GRID_SIZE)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.1, random_state=2015)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, y)

print(""The best parameters are %s with a score of %0.2f""
      % (grid.best_params_, grid.best_score_))
print(""This took {} minutes to run"".format((time.time() - start)/60))

# plot the scores of the grid
# grid_scores_ contains parameter settings and scores
# We extract just the scores",svm.py,RamaneekGill/Emotion-Recognition-,1
"sys.path.extend([PROJECT_ROOT,EXPTS])
sys.dont_write_bytecode = True
from sklearn.svm import SVC
from george.lib import *
from expts.csvParser import parseCSV, randomPoints
import config


def builder(fname = config.TRAIN_FILE, kernel=""rbf""):
  points = parseCSV(fname, False)
  clf = SVC(kernel=kernel)
  X, y = [], []
  for point in points:
    X.append(point.x)
    y.append(point.y)
  clf.fit(X,y)
  return clf

def predictor(classifier, points):
  X,actuals = [], []",george/svm.py,ST-Data-Mining/crater,1
"        train[one_val] = [pos[one_val][i] for i in sorted(random.sample(xrange(len(pos[one_val])), int(len(pos[one_val])*2./3)))]
        test[one_val] = list(set(pos[one_val])-set(train[one_val]))
        labels_train.extend([label_id]*len(train[one_val]))
        labels_test.extend([label_id]*len(test[one_val]))
        label_id=label_id+1
    train_feats_id=[]
    for one_val in all_attr_data['attr_vals'][one_attr]:
        for sample in train[one_val]:
            train_feats_id.append(get_precompfeatid_fromhtid(all_attr_data['all_imgs'][sample]))
    train_feats=get_all_precomp_feats(train_feats_id)
    clf = svm.SVC()
    clf.fit(train_feats, labels_train)
    pickle.dump(clf,open('svmmodel_'+str(one_attr)+'.pkl','wb'))
    data={}
    data['train_ids']=train
    data['test_ids']=test
    data['labels_train']=labels_train
    data['labels_test']=labels_test
    pickle.dump(data,open('data_'+str(one_attr)+'.pkl','wb'))",ISIweakLabels/read_hdfs_attributes.py,ColumbiaDVMM/ColumbiaImageSearch,1
"           and platform.architecture()[0] == '64bit' \
           and self.oBuild.sKind == 'development' \
           and os.getenv('VERSIONER_PYTHON_PREFER_32_BIT') != 'yes':
            print ""WARNING: 64-bit python on darwin, 32-bit VBox development build => crash""
            print ""WARNING:   bash-3.2$ /usr/bin/python2.5 ./testdriver""
            print ""WARNING: or""
            print ""WARNING:   bash-3.2$ VERSIONER_PYTHON_PREFER_32_BIT=yes ./testdriver""
            return False;

        # Start VBoxSVC and load the vboxapi bits.
        if self._startVBoxSVC() is True:
            assert(self.oVBoxSvcProcess is not None);

            sSavedSysPath = sys.path;
            self._setupVBoxApi();
            sys.path = sSavedSysPath;

            # Adjust the default machine folder.
            if self.fImportedVBoxApi and not self.fUseDefaultSvc and self.fpApiVer >= 4.0:
                sNewFolder = os.path.join(self.sScratchPath, 'VBoxUserHome', 'Machines');",src/VBox/ValidationKit/testdriver/vbox.py,zhangpf/vbox,1
"	fig.set_tight_layout(True)
	plt.show()



X_train, X_test, y_train, y_test = load('Datasets/optdigits.tes', 'Datasets/optdigits.tra')
peekData()

print ""Training SVC Classifier...""

model = SVC(kernel ='rbf',C=1,gamma=0.001)
model.fit(X_train,y_train)

print ""Scoring SVC Classifier...""
score = model.score(X_test,y_test)
print ""Score:\n"", score

drawPredictions()

true_1000th_test_value = X_test.iloc[1000]",MachineLearning/SVC-optical.py,adrianjg/Tools,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.
#    from sklearn.cross_validation import StratifiedShuffleSplit
#    from sklearn.grid_search import GridSearchCV
#    C_range = np.logspace(-2, 4, 7)
#    gamma_range = np.logspace(-9, 3, 13)
#    param_grid = dict( C=C_range)
#    cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.2, random_state=42)
#    grid = GridSearchCV(LinearSVC(), param_grid=param_grid, cv=cv,verbose=10)
#    print ""starting grid search""
#    grid.fit(encoded_features, y_train)
#    
#    print(""The best parameters are %s with a score of %0.2f""",scripts/Keras_calculate_cv_allkernels.py,nickgentoo/scikit-learn-graph,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf, language='js').export()
print(output)

""""""
var Brain = function() {

    this.predict = function(atts) {",examples/classifier/LinearSVC/js/basics.py,nok/sklearn-porter,1
"            (f_inv, 'inv'),
            (f_lin, 'lin'),
            (f_neg, 'neg'),
            (f_exp, 'exp'),
            (f_rand, 'rand')]

    size_hist = 100
    pattern = lambda f: tuple(map(f, xrange(1, size_hist)))
    XY = [(pattern(f), lbl) for f, lbl in fs]
    X, Y = zip(*XY)
    learner = svm.LinearSVC()
    predictor = learner.fit(X, Y)

    N = 1000
    for _f, lbl in fs:
        f = lambda x: _f(x) + random.gauss(0, 2.0)
        X = [pattern(f) for _ in xrange(N)]
        Y = [lbl] * N
        print lbl, ':', predictor.score(X, Y)",predict_sqr.py,TurpIF/gestures-learning,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in getargspec(estimator.fit)[0]


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",sklearn/utils/validation.py,smartscheduling/scikit-learn-categorical-tree,1
"    K = np.empty((l,l))
    for i,j in itertools.product(range(l),repeat=2):
        K[i,j] = np.sum(rbf_kernel(data[i,:,:],
                                   data[j,:,:],
                                   gamma = .5/bw))/n**2
    for b,nu in enumerate(nus):
        train_id = np.random.choice(indices,size=int(.7*l),replace=False)
        test_id = indices[~np.in1d(indices,train_id)]
        k_train = K[:,train_id][train_id]
        k_test = K[:,train_id][test_id]
        svm = NuSVC(nu=nu,kernel='precomputed')
        svm.fit(k_train,y[train_id])
        error = np.sum(svm.predict(k_test)==y[test_id])/len(test_id)
        accuracy[a,b] = 1-error
        
plt.pcolormesh(accuracy)
opt_nu = nus[np.argmax(accuracy)%bandwidths.shape[0]]
opt_bw = bandwidths[np.argmax(accuracy)//bandwidths.shape[0]]
print('best accuracy: {0:4.3f}'.format(accuracy.max()))
print('optimal nu: {0:4.3f}'.format(opt_nu))",smm2.py,gregstarr/anomaly-detection,1
"        data.append([float(x) for j, x in enumerate(sline) if j != 0])
        storeDict[i] = [float(x) for j, x in enumerate(sline) if j != 0]

    data = np.array(data)
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.25, random_state=0)
    clf = ExtraTreesClassifier()
    clf = clf.fit(X_train, y_train)
    model = SelectFromModel(clf, prefit=True)
    X_new = model.transform(X_train)

    clfNew = svm.SVC(kernel='linear', C=1).fit(X_new, y_train)

    value_feature = list()
    countDict = dict()
    for key, val in storeDict.items():
        countDict[key] = 0
        for i, inval in enumerate(val):
            if inval in X_new[0]:
                countDict[key] = countDict[key] + 1
",feature_selection.py,Ambuj-UF/Machine-Learning-Theorems,1
"  tfidf_transformer = TfidfTransformer()
  print >>sys.stderr, ""Transforming word counts to tf-idf""
  X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
  # print >>sys.stderr, ""Converting to dense matrix""
  # X_train_tfidf_dense = X_train_tfidf.toarray()
  print >>sys.stderr, ""Fitting classifier to data""
  # clf = MultinomialNB(alpha=1.0).fit(X_train_tfidf, train_docs.target) # poor performance
  # clf = LogisticRegression().fit(X_train_tfidf, train_docs.target) # better performance but hoping for more
  # SVMs with kernel too slow
  # linear SVM like LogReg, but faster
  # clf = Pipeline([('feature-selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),
  #                 ('classification', GradientBoostingClassifier(n_estimators=100))]) # slow as number of estimators rises; but also 3% precision
  # clf = Pipeline([('anova', SelectKBest(f_regression, k=5)), ('clf', GradientBoostingClassifier(n_estimators=100))]) # too slow
  # clf = Pipeline([('feature-selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),
  #                 ('classification', SVC())]) # SVC still too slow
  # clf = Pipeline([('reduce-dim', PCA(n_components=100)),
  #                 ('classification', SVC())]) # SVC (? or PCA??) still too slow
  # PCA itself is far too slow! --- and apparently it doesn't help: linearsvc + pca 100 + gradient boosting 100 = worse than w/o pca
  # clf = Pipeline([('feature-selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),
  #                 ('classification', GradientBoostingClassifier(n_estimators=100))]) # this gives the old 3% and quite slow",document_classifier/classification/word_counter.py,HazyResearch/dd-genomics,1
"    testY = np.array(testY)
    print (trainY.shape , testY.shape)
    np.savetxt('yahoo_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+'_train_output_labels.txt', trainY, fmt = '%s')
    np.savetxt('yahoo_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+'_test_output_labels.txt', testY, fmt = '%s')

    return trainY, testY 


def evaluate_with_SVM(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting SVM"")
    clf = svm.SVC(kernel='linear')
    clf.fit(train_X, train_Y)
    predict_Y = clf.predict(test_X) 
    s=metrics.accuracy_score(test_Y, predict_Y) 
    print (""SVM Testing Acc: "", s)
    return s


def evaluate_with_KNN(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting KNN"")",Model/Yahoo/yahoo_LSTM-SVM_CV.py,irisliu0616/Short-text-Classification,1
"list_hog_fd = []
for feature in letter_image_features:
    print ""feature"", feature
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier
joblib.dump(clf, ""digits_cls_alpha1.pkl"", compress=3)


# LATER:",handwritingRecognition/generateClassifierAlpha.py,forrestgtran/TeamX,1
"
	data.append(row)
	count_row += 1

features = []
data = np.array(data)

features = data[:, [2, 4, 5, 6, 9]].astype(np.float)
output = data[:, 1]

clf = SVC()
clf.fit(features, output)

test_file = open(""test.csv"", ""rb"")
test_file_object = csv.reader(test_file)
header = test_file_object.next()

prediction_file = open(""svm.csv"", ""wb"")
prediction_file_object = csv.writer(prediction_file)
prediction_file_object.writerow([""PassengerId"", ""Survived""])",JT Inc./titanic/trial03/svm.py,narnars0/MyRepository,1
"	dataset = sklearn.datasets.load_files('/home/jason/Desktop/NYT/analysis/sources', encoding=""utf-8"", decode_error='ignore')

	count_vect = CountVectorizer()
	training_counts = count_vect.fit_transform(dataset.data)
	training_counts.shape

	tf_transformer = TfidfTransformer(use_idf=True).fit(training_counts)
	training_tf = tf_transformer.transform(training_counts)
	training_tf.shape

	support_vector_machine = svm.SVC(kernel='linear', probability=True)

	support_vector_machine.fit(training_tf, dataset.target)

	test_dataset = sklearn.datasets.load_files('/home/jason/Desktop/NYT/analysis/test_articles', encoding=""utf-8"", decode_error='ignore')
	new_counts = count_vect.transform(test_dataset.data)
	new_tfidf = tf_transformer.transform(new_counts)

	svm_prediction = support_vector_machine.predict(new_tfidf)
	svm_proba_prediction = support_vector_machine.predict_proba(new_tfidf)",classifier.py,Jasonmk47/OpenWPM,1
"
matP = np.zeros((n_kernels, n_kernels))
vecQ = np.zeros((n_kernels,1))
gamma = 0.01

names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"", ""Logistic Regression""]


classifiers = [
	SVC(kernel=""linear"", C=3.4,gamma=0.1),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4)]


def compute_J(N, theta):

	if N == 0:",UMKL/textData/add_kernels.py,akhilpm/Masters-Project,1
"from sklearn import svm
import sys
import os

""""""
Hacky script is needed because of version differences between sklearn on the sub and
other computers. This script retrains the SVC and saves it to a new pickle file
""""""

(clf, (X, y)) = pickle.load(open(sys.argv[1], 'r'))
clf2 = svm.SVC(probability=True)
clf2.fit(X, y)
o = (clf2, (X, y))
pickle.dump(o, open(sys.argv[1] + "".sub"", 'wb'))
",object-recognition/data/tosub.py,cuauv/software,1
"import numpy as np
import ClassificationUtils
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

svm = SVC()
bnb = BernoulliNB(alpha=0.2)
mnb = MultinomialNB(alpha=0.4)
gnb = GaussianNB()
rf = RandomForestClassifier(n_jobs=4,n_estimators=17)
knn = KNeighborsClassifier(n_neighbors=5)

trainDataFile = ""train.csv""
trainLabelFile = ""train_labels.csv""
",KaggleEmailSpam.py,rupakc/Kaggle---Email-Spam,1
"	y_2d = y[y < 2]
	# Test part
	T_2d = np.delete(T,range(25,43)+range(50,64)+range(75,93),axis=0)
	yy_2d = yy[yy < 2]
	#------------------------------ Standardize data ------------
	scaler = StandardScaler()
	X_2d = scaler.fit_transform(X_2d)
	T_2d_scaled = scaler.transform(T_2d)
	#------------------------------ Create Classifier ------------------
	manual_param = {'C':100,'gamma':0.1}
	clf = SVC(gamma=manual_param['gamma'], C=manual_param['C'])
	clf.fit(X_2d, y_2d)
	#------------------------------ Create Classifier ------------------
	C_range = np.logspace(1, 3, 3)
	gamma_range = np.logspace(-3, -1, 3)
	param_grid = dict(gamma=gamma_range, C=C_range)
	cv = StratifiedShuffleSplit(y_2d, n_iter=100, test_size=0.2, random_state=42)
	grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
	#------------------------------ Best parameter Aprox.---------------
	grid.fit(X_2d, y_2d)",EpocArmData/Features/feat_extraction_3_beta.py,maberyick/RPi-EPOC,1
"                                                                    num_samples_to_train_with,
                                                                    return_popped_songlist=False)
        svm_train_samples = svm_samples[train_sample_IDs]
        train_labels = svm_labels[train_sample_IDs]
        svm_train_samples_scaled = svm_scaler.fit_transform(svm_train_samples)
        #have to scale test samples each time by factors used to scale each distinct training set
        svm_test_samples_scaled = svm_scaler.transform(svm_test_samples)

        
        best_params, best_grid_score = grid_search(svm_train_samples_scaled,train_labels)
        svm_clf = SVC(C=best_params['C'],gamma=best_params['gamma'],decision_function_shape='ovr')
        svm_clf.fit(svm_train_samples_scaled,train_labels)
        svm_train_pred_labels = svm_clf.predict(svm_train_samples_scaled)
        svm_train_score = svm_clf.score(svm_train_samples_scaled,train_labels)
        svm_test_pred_labels = svm_clf.predict(svm_test_samples_scaled)
        svm_test_score = svm_clf.score(svm_test_samples_scaled,test_labels)
        svm_decision_func = svm_clf.decision_function(svm_test_samples_scaled)
        print("" svm score on train set: "",svm_train_score)
        print("" svm score on test set: "",svm_test_score)
",test_svmrbf_knn.py,NickleDave/hybrid-deep-finch,1
"        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",document_classification_20newsgroups.py,yaodi833/shorttext,1
"b = np.asarray(b)


data =  (a,b)
X_1 = data[0].todense().tolist()  # samples 72 features above 7129
y_1 = map(int,data[1])   # classes 2

#print(a.shape)
#print(b.shape)
#L1 SVM
l1svc = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

coef = l1svc.coef_.tolist()[0]
#print(coef)
#print(len(l1svc.coef_.tolist()))
",coeficientArcene.py,narendrameena/featuerSelectionAssignment,1
"import json

from sklearn import svm, cross_validation, datasets


def classify(data, cls):
    y = data[0, :]
    X = data[1:, :]
    
    if(cls == 'svm'):
        model = svm.SVC(kernel='linear');
        X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)
        
    
    # split the data 80/20

for path in sorted(glob.glob('motive.*.csv')):
    data = n.genfromtxt(path, delimiter=',')
    svmresult = classify(data, 'svm')
",src/main/resources/scripts/plot.classification.py,pbloem/motive,1
"models = []
#models.append(('IR', IsotonicRegression()))

#models.append(('LR', LogisticRegression()))

#models.append(('LDA', LinearDiscriminantAnalysis()))
#models.append(('KNN', KNeighborsClassifier()))
#models.append(('CART', DecisionTreeClassifier()))

#models.append(('NB', GaussianNB()))
#models.append(('SVM', SVC()))

models.append(('SVR', SVR()))

# evaluate each model in turn
results = []
names = []
for name, model in models:
	print (name)
	kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)",try-ml/try-local-v01.py,yvlasov/ConProbIN,1
"tfidf_train = transformer.fit_transform(countMatrix_train)
tfidf_test = transformer.transform(countMatrix_test)

tfidf_train2 = transformer.fit_transform(countMatrix_train2)
tfidf_test2 = transformer.transform(countMatrix_test2)

print tfidf_train.shape
print tfidf_test.shape
#X_train, X_test, y_train, y_test = inst[train], inst[test], classs[train], classs[test]

clf_svm = svm.SVC(kernel='linear')
clf_svm.fit(tfidf_train, y_train)

clf_mNB=MultinomialNB()
clf_mNB.fit(tfidf_train, y_train)

clf_knn = KNeighborsClassifier()
clf_knn.fit(tfidf_train, y_train)

clf_ada=RandomForestClassifier(n_estimators=25)",code_python27/tfidfANDclasification/simpleClassify.py,rcln/tag.suggestion,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",projects/scikit-learn-master/sklearn/metrics/tests/test_ranking.py,DailyActie/Surrogate-Model,1
"        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",lib/python2.7/site-packages/sklearn/grid_search.py,lancezlin/ml_template_py,1
"from sklearn.svm import SVC
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.manifold import Isomap

x = pd.read_csv('Datasets/parkinsons.data')
y = x['status'].copy()
x.drop(labels=['name', 'status'], axis=1, inplace=True)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=7)
svc = SVC()
svc.fit(x_train, y_train)
score = svc.score(x_test, y_test)
print('Score: %f' % score)

scaler = preprocessing.StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
",Module6/assignment3.py,Wittlich/DAT210x-Python,1
"    x = np.reshape(x, (m*n, 1))
    x = minmax_scale(x, feature_range=(0, 1), axis=0)
    data = np.reshape(x, (m, n))

    return data

def tuneTrainTest(f, X_train, y_train, X_test, y_test):
    print 'Evaluating ', f

    # tune
    clf = svm.SVC(kernel='precomputed')

    # train
    gram_train = util.computeGram(X_train, X_train, f)
    # gram_train = scale(gram_train)
    clf.fit(gram_train, y_train)

    # test
    gram_test = util.computeGram(X_test, X_train, f)
    # gram_test = scale(gram_test)",similarity/compound-kernel/genetic-programming/src/eval_svm.py,tttor/csipb-jamu-prj,1
"		self.seqY = seqYC1_40x1 + seqYC2_40x1 # 80x1

setTraining = DataSet()
setTest = DataSet()

# H8.2
print(""H8.2"")
C_range = np.logspace(-6, 10, num=""9"", base=2.0)
gamma_range = np.logspace(-5, 9, num=""8"", base=2.0)
param_grid = dict(gamma=gamma_range, C=C_range)
# grid = GridSearchCV(SVC(), param_grid=param_grid)
# grid.fit(setTraining.seqX, setTraining.seqY)

clf = SVC()
clf.fit(setTraining.seqX, setTraining.seqY)

countRight = 0
countTotal = 0

for rec in zip(setTraining.seqX, setTraining.seqY):",week08.py,allqoow/exerciseML,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    with use_log_level('error'):
        assert_raises(ValueError, gat.score, epochs2)",mne/decoding/tests/test_time_gen.py,Teekuningas/mne-python,1
"    clf = discriminant_analysis.QuadraticDiscriminantAnalysis()
    clf.fit(data[features], data[class_var])
    
    return clf

from sklearn import svm
def svc_eval(data, target):
    
    class_var = target
    features = data.columns.values[~data.columns.str.contains(class_var)]
    clf = svm.LinearSVC()
    clf.fit(data[features], data[class_var])
    
    return clf
    
#dt_eval(ds1_rv, target=""target"")
#ds1_rv.dropna(inplace=True)
#clf1=dt_eval(ds1_mf_rv, 'target', depth=4, class_weight=None)
#clf2=dt_eval(ds2_mf_rv, 'target', depth=4, class_weight=None)
#clf3=dt_eval(ds3_mf_rv, 'target', depth=4, class_weight=None)",notebooks/pawel_ueb2/evaluation.py,hhain/sdap17,1
"
#full dataset classification
X_data =images/255.0
Y = targets

#split data to train and test 
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data, Y, test_size=0.15, random_state=42)

# Create a classifier: a support vector classifier
kernel_svm = svm.SVC(gamma=.2)
linear_svm = svm.LinearSVC()

# create pipeline from kernel approximation
# and linear svm
feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
feature_map_nystroem = Nystroem(gamma=.2, random_state=1)

fourier_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_fourier),
                                        (""svm"", svm.LinearSVC())])",modeling/svm/svm_mnist_embedings.py,jwilliamn/handwritten,1
"not understand very clearly, but it works
""""""

def my_kernel(x, y):
    xx = pow(-1, x)
    yy = pow(-1, y)
    return xx @ yy.T

x = [[i] for i in range(10)]
y = [ i%2 for i in range(10)]
s = svm.SVC(kernel = my_kernel)
#s = svm.SVC()
s.fit(x, y)

yy = [ s.predict([[i]]) for i in range(2000)]
yy = [i[0] for i in yy]

yy_correct = [ i%2 for i in range(2000)]

nu_error = 0",python/sklearn/svm_odd_even.py,yuncliu/Learn,1
"y = np.random.randint(0, 2, 100)
X[y == 1, :] += snr
# unwhiten data
scale = 1e-6
X *= scale
shift = 500
X += shift

# 1. Classic pipeline --------------------------------------------------------
pipeline = Pipeline([('scaler', StandardScaler()),
                     ('svc', SVC(kernel='linear'))])
pipeline.fit(X, y)
y_pred = pipeline.decision_function(X)
score = roc_auc_score(y, y_pred)

# 2. A posteriori rescaling of SVC coef --------------------------------------
scaler = pipeline.steps[0][1]
svc = pipeline.steps[1][1]
SV = svc.__getattribute__('support_vectors_')
# combine the scaler and support vector steps",sandbox/decoding/rescale_svm.py,kingjr/meg_perceptual_decision_symbols,1
"        else:
            if len(set([adjMatrix[i][targetIndex] for i in range(nSource)])) == 1:
                gramTest = targetSim[targetIndex]
            else:
                gramTest = netSim[targetIndex]
            gramTrain = netSim
            for i in reversed(sorted(testIndex)):
                gramTest = np.delete(gramTest,i, 0)
                gramTrain = np.delete(gramTrain,i, 0)
                gramTrain = np.delete(gramTrain,i, 1)
            model = svm.SVC(kernel='precomputed', probability=True)
            model.fit(gramTrain, intProfile)

            if self._proba:
                prediction = model.predict_proba(gramTest.reshape(1,-1))
            else:
                prediction = model.predict(gramTest.reshape(1,-1))

        if self._proba:
            return (prediction[0][1],sourceIndex,targetIndex)",predictor/blmnii/blm_ajm.py,tttor/csipb-jamu-prj,1
"@param.float(""Gamma-Exp"", interval=[-6, 0], step=0.1)
def f(C_exp, gamma_exp):
    iris = datasets.load_iris()

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        iris.data, iris.target, test_size=0.4, random_state=0)

    C = 10 ** C_exp
    gamma = 10 ** gamma_exp

    clf = svm.SVC(C=C, gamma=gamma)

    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)


def main():
    from metaopt.core.optimize.optimize import optimize
    from metaopt.optimizer.gridsearch import GridSearchOptimizer",examples/showcase/svm_gridsearch_global_timeout.py,cigroup-ol/metaopt,1
"                X_valid,y_valid = valid_set
                X_test,y_test = test_set
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_05212015.py,magic2du/contact_matrix,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=verbose,
                probability=True)

            model.fit(x_train, y_train)",3part/nltk/parse/transitionparser.py,qianqians/Combed,1
"    return np.vstack(estimated_)


def squeeze_niimg(niimg):
    return nb.Nifti1Image(niimg.get_data().squeeze(),
                          affine=niimg.get_affine())


class Decoder(BaseEstimator):

    def __init__(self, estimator=LinearSVC(),
                 masker=NiftiMasker(),
                 labelizer=LabelEncoder(),
                 reporter=Reporter(),
                 estimated_name='coef_'):
        self.estimator = clone(estimator)
        self.masker = clone(masker)
        self.labelizer = clone(labelizer)
        self.reporter = reporter
        self.estimated_name = estimated_name",decoding.py,schwarty/nignore,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=0)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    pca = RandomizedPCA(n_components=2)
    X_reduced = pca.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


if __name__ == ""__main__"":
    import nose
    nose.runmodule()",python/sklearn/sklearn/ensemble/tests/test_forest.py,seckcoder/lang-learn,1
"                    # Number of trees.
                    'n_estimators': [3, 6, 12, 24],
                    # Number of leafs in the tree.
                    'max_depth': [3, 6, 12, None],
                    'min_samples_split': [2, 4, 8],
                    # TODO: this should rather be a user defined parameter
                    'class_weight': ['balanced', None]
                },
            },
            'svm': {
                'cls': SVC(cache_size=500, decision_function_shape='ovr'),
                # Scale to zero mean and unit variance
                'scaler': RobustScaler(quantile_range=(1.0, 99.0), copy=False),
                # Search optimal regularization parameters to control
                # model complexity.
                'search_space': {
                    'kernel': ['linear', 'rbf'],
                    'C': np.logspace(-5, 15, 10, base=2),
                    'gamma': np.logspace(-15, -3, 10, base=2)
                }",tmlib/tools/base.py,TissueMAPS/TmLibrary,1
"#print(rfe.support_)
#print(rfe.ranking_)
##
#print(Point.normaliseOverLongest([Point(1,1), Point(2,2), Point(3,2), Point(10,5), Point(4,3), Point(6,6)]))

import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
print(np.shape(X))
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC(probability=True, verbose=True)
clf.fit(X, y) 
print(clf.predict([[-0.8, -1]]))",FYP/testing.py,matt123miller/Learning-Python-ML,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_05_2014_server.py,magic2du/contact_matrix,1
"    print('Vectorization time:', vec_time)
    print('Data matrix size:', X_all.shape)

    y_train, X_train, ind_train, y_test, X_test, ind_test, X_unlab, ind_unlab =\
            dp.split_data(y_merged, X_all, split=0.7, seed=0)
    me = ModelEvaluator()

    # LinearSVC (liblinear SVM implementation, one-v-all)
    cross_validate = True
    if cross_validate:
        model = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001,\
                C=1, multi_class='ovr', fit_intercept=True, intercept_scaling=1,\
                class_weight='balanced', verbose=0, random_state=None, max_iter=1000)
        param_grid = {'C':np.logspace(-1,1,24).tolist()}
        grid_info, grid_best, grid_time = me.param_search(model, param_grid,\
                np.concatenate((y_train, y_test)), sp.vstack((X_train, X_test)), num_folds=3)
        C = grid_best['C']
    else:
        C = 1
    print(C)",trends.py,GautamShine/toxic-docs,1
"
    return np.concatenate(x, axis=0), np.concatenate(y, axis=0)


def get_svc_train_data(e):
    return _get_svc_data(e, range(CONFIG.svc_tr_usr_cnt))


def get_optimized_svc_evaluation(x_train, y_train, x_cv, y_cv):
    x, y = np.concatenate([x_train, x_cv]), np.concatenate([y_train, y_cv])
    estimator = NuSVC()
    param_grid = [{
        'kernel': ['rbf', 'sigmoid'],
        'nu': np.arange(start=0.600, stop=0.850, step=0.001, dtype=np.float64),
        'gamma': [
            0.1, 0.2, 0.3,
            0.01, 0.02, 0.03,
            0.001, 0.002, 0.003,
            0.0001, 0.0002, 0.0003,
            0.00001, 0.00002, 0.00003,",utils/evaluation/svc.py,kahrabian/signature_verification,1
"    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for logistic regression: ', auc)
    
    plot_curve(fpr, tpr, 'Logistic regression ' + str(auc))
    return clf


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_2.py,remigius42/code_camp_2017_machine_learning,1
"    #                     'C': [.001, .01, 10]}]
    
    tuned_parameters = [{'C': [.001, .01, 1, 10, 100, 1000]}]
    
    scores = [
        ('precision', precision_score),
        ('recall', recall_score),
    ]
   # 
    #for score_name, score_func in scores:
    clf = GridSearchCV(LinearSVC(C=1), tuned_parameters, n_jobs=4)
    clf.fit(X[train], y[train], cv=StratifiedKFold(y[train], 2))
    y_true, y_pred = y[test], clf.predict(X[test])

    print ""Classification report for the best estimator: ""
    print clf.best_estimator
#    print ""Tuned for '%s' with optimal value: %0.3f"" % (
#        score_name, score_func(y_true, y_pred))
    print classification_report(y_true, y_pred)
    print ""Grid scores:""",src/project6867/SVMTrain.py,melink14/6.867-Final-Project,1
"                    picks=picks, baseline=None, preload=True,
                    reject=dict(mag=1.5e-12), decim=decim, verbose=False)

# Build clfs
scaler = StandardScaler()
svc = force_predict(estimator=LinearSVC_Proba(C=1, probability=False,
                                              class_weight='auto'),
                    mode='decision_function')
linear_svr = LinearSVR(C=1)
svr = SVR(kernel='linear', C=1)
linear_svc = LinearSVC(C=1)
linear_svcp = force_predict(estimator=LinearSVC_Proba(C=1, probability=True))
svc = force_predict(SVC(kernel='linear', C=1, probability=True))
plr = force_predict(LogisticRegression())
pipeline = [Pipeline([('scaler', scaler), ('svc', linear_svcp)]),
            Pipeline([('scaler', scaler), ('svc', svc)]),
            Pipeline([('scaler', scaler), ('svr', plr)])]

gats = list()
for clf in pipeline:",sandbox/decoding/compare_memory_speed.py,kingjr/meg_perceptual_decision_symbols,1
"    ### return the fit classifier



    ### your code goes here!
    from sklearn.naive_bayes import GaussianNB
    clf = GaussianNB()
    
    # Test with SVM to see how the image behaves with RBF and large C
    #from sklearn.svm import SVC
    #clf = SVC(kernel=""rbf"", C=100000)
    
    clf.fit(features_train, labels_train)
    return clf",terrain-data/ClassifyNB.py,askldjd/udacity-machine-learning,1
"import numpy as np

# Get digits Dataset****
data=data.load_digits()
X=data.data #feature vector
Y=data.target #Label Vector
# **************************

if __name__=='__main__':
    # clsf = classifier
    clsf=SVC(kernel='rbf',gamma=0.001,C=0.1) #SVM Classier
    # There are other argument like
    # [[[C, cache_size, class_weight, coef0,
    # decision_function_shape, gamma, kernel,
    # max_iter, probability, random_state,shrinking,
    # tol,verbose]]]
    # you can pass in order to custmize your classifier

    # Trainig Classifier  ***
    clsf.fit(X, Y)",SVM Classifier/Svm Classifier on Digit data.py,sudhanshuptl/Machine-Learning,1
"
    def set_params(self, **parameters):
        self.__init__(**parameters)
        return(self)

class LinearSVCClassifier(BaseWeakClassifier):
    def __init__(self, n_jobs = 1,
                 excluded_features=None,
                 feature_confidence_estimator=PredictBasedFCE(),
                 regularizer_index = 10):
        learner = sklearn.svm.LinearSVC(penalty = 'l1',
                                        dual = False)
        self.regularizer_index = regularizer_index
        self.min_C = None
        super(LinearSVCClassifier, self).__init__(
            learner, n_jobs, excluded_features, feature_confidence_estimator)

    def get_params(self, deep=True):
        return( {
            'regularizer_index':self.regularizer_index,",rat.py,adrinjalali/Network-Classifier,1
"        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",exemplo/document_classification_20newsgroups.py,ebertti/nospam,1
"import app.analytics.filterSentences as fl
import networkx as nx
import matplotlib.pyplot as plt
G=nx.DiGraph()

np.seterr(divide='ignore',invalid='ignore')
trainData = eval(open('trainDoubleSet','r').readlines()[0])
testData = open('testDoubleSet','r').readlines()

listOfYears = []
clf = linear_model.LogisticRegression()#svm.SVC(probability=True)
probs = []
titles = []

def train(features):
    features = [item for item in features if len(item[0]) != 0]
    feats = [item[0] for item in features]
    A = len(features)
    B = min(map(len,feats))
    X = np.ones((A,B))",logistic-regression.py,JFriel/honours_project,1
"
sub_data = data[0][i.tolist()]
sub_sample = data[1][i.tolist()] # check for this step


X_1 = sub_data.todense().tolist()  # samples 72 features above 7129
y_1 = map(int,sub_sample)   # classes 2


#L1 SVM
l1svc = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

#print(len(l1svc.coef_[0]))





coef = l1svc.coef_.tolist()[0]
#print(coef[0])",subSamplingL1SvmArcene.py,narendrameena/featuerSelectionAssignment,1
"# ftype = np.bitwise_or.reduce(X[:,:4], axis=1)
# X = np.c_[ftype, X[:,-2]*200 + X[:,-1]]
# X = np.c_[ftype * X[:,-3], X[:,-2] * X[:,-1]]
y = np.load('target.np').astype(dtype=float)

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
x_min, x_max = -1, 1.25
y_min, y_max = -1, 1.5",scripts/svm.py,andyjost/slang,1
"Free Software Foundation, either version 3 of the License, or (at your option)
any later version.

RECIPE is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. See http://www.gnu.org/licenses/.

""""""

from sklearn.svm import NuSVC, SVC

def SVC(args):

	""""""Uses scikit-learn's SVC or NuSVC, classes capable of performing multi-class classification on a dataset.
    
    Parameters
    ----------
  
    nu or C : float
		Penalty parameter C of the error term.
",recipe/classifiers/SVC.py,RecipeML/Recipe,1
"	print ""training SVM...""
	
	# although one needs to choose these hyperparams
	C = 173
	gamma = 1.31e-5
	shrinking = True

	probability = True
	verbose = True

	svc = SVC( C = C, gamma = gamma, shrinking = shrinking, probability = probability, verbose = verbose )
	svc.fit( x_train, y_train )
	p = svc.predict_proba( x_test )	
	
	auc = AUC( y_test, p[:,1] )
	print ""SVM AUC"", auc	
	

	print ""training random forest...""
",vectorize_validation.py,zygmuntz/kaggle-happiness,1
"        #print i , ""++++++++++++""
        TempLabel = TrainLabel.copy()
        # Generate Label, once only a number's label will be +1 others will be -1
        for j in range(Num):
            if(TrainLabel[j] == i):
                TempLabel[j] = 1
            else:
                TempLabel[j] = -1
        #outputLabelList(TempLabel, ""TrainLabel"" + str(i), ""Train Label For Classifier"" + str(i))
        if(Kernel == 'linear'):
            clf = SVC(kernel='linear', C=c)
        elif(Kernel == 'poly'):
            clf = SVC(kernel='poly', C=c, gamma=Gamma)
        elif(Kernel == 'rbf'):
            clf = SVC(kernel='rbf', C=c, gamma=Gamma)
        #print '----------------------------------'
        #print ""Fit classifier "" , i
        clf.fit(TrainData, TempLabel)
        #print clf.support_vectors_.shape
        #print '----------------------------------'",src/Methods/TestMethods.py,dzh123xt/DigitRecognition,1
"

class Classifier:
    def __init__(self, input_data, classifier_type, train_data_percentage, input_features, target_feature):
        self.input_data = input_data
        self.train_data_percentage = train_data_percentage
        self.target_feature = target_feature
        self.input_features = input_features
        self.parallel_jobs = 1
        if classifier_type == 'Linear_SVM':
            self.clf = SVC(kernel='linear', C=0.025)
        elif classifier_type == 'Nearest_Neighbours':
            self.clf = KNeighborsClassifier(3)
        elif classifier_type == 'Decision_Tree':
            self.clf = DecisionTreeClassifier(max_depth=5)
        elif classifier_type == 'Random_Forest':
            self.clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, n_jobs=self.parallel_jobs)
        elif classifier_type == 'Naive_Bayes':
            self.clf = GaussianNB()
",cognitive/app/ml_models.py,CiscoSystems/cognitive,1
"    try:
        df_to_plot = hdf['res']
        plot_res_paper(df_to_plot)
        pylab.show()
    except Exception as e:
        print(""error {}"".format(e))

    df = hdf['df']
    clfs = []
    clfs.append(sklearn.ensemble.RandomForestClassifier(n_estimators=500, random_state=0))
    clfs.append(sklearn.svm.SVC(kernel='linear'))
    clfs.append(sklearn.svm.SVC(kernel='rbf'))
    clfs.append(sklearn.svm.SVC(kernel='poly', degree=3))
    clfs.append(sklearn.dummy.DummyClassifier())
    clfs.append(sklearn.neighbors.KNeighborsClassifier(n_neighbors=5))
    res_with_scaling = generate_res_as_in_paper(df, clfs, preprocess_scaling=True)
    hdf['res_with_scaling'] = pd.DataFrame(res_with_scaling)

    pylab.figure(""with_scaling"")
    plot_res_paper(hdf['res_with_scaling'])",evaluate_learning.py,laurent-george/protolab_sound_recognition,1
"
        warnings.filterwarnings(""ignore"", category=UndefinedMetricWarning)

    def __choose_algorithm(self):
        algorithms = []
        if self.task == 'classification':
            if self.data.X.shape[0] * self.data.X.shape[1] <= 1e+06:
                if self.data.X.shape[0] ** 2 * self.data.X.shape[1] <= 1e+09:
                    algorithms.append(
                        Algorithm(
                            SVC(random_state=self.random_state),
                            [{'kernel': ['rbf'],
                              'C': [1, 10, 100, 1000],
                              'gamma': [1e-3, 1e-2, 1e-1, 1.0]}],
                            'Support Vector Machine (RBF Kernel)',
                            ('http://scikit-learn.org/stable/modules/'
                             'generated/sklearn.svm.SVC.html')))
                    algorithms.append(
                        Algorithm(
                            RandomForestClassifier(",malss/malss.py,canard0328/malss,1
"    trn, trn_lbl, tst, feature_names= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/compete_alg.py,lioritan/Thesis,1
"    classifier_liblinear = svm.LinearSVC()
    t0 = time.time()
    classifier_liblinear.fit(train_vectors, train_labels)
    t1 = time.time()
    prediction_liblinear = classifier_liblinear.predict(test_vectors)
    t2 = time.time()
    time_liblinear_train = t1-t0
    time_liblinear_predict = t2-t1

    # Print results in a nice table
    print(""Results for SVC(kernel=rbf)"")
    print(""Training time: %fs; Prediction time: %fs"" % (time_rbf_train, time_rbf_predict))
    print(classification_report(test_labels, prediction_rbf))
    print(""Results for SVC(kernel=linear)"")
    print(""Training time: %fs; Prediction time: %fs"" % (time_linear_train, time_linear_predict))
    print(classification_report(test_labels, prediction_linear))
    print(""Results for LinearSVC()"")
    print(""Training time: %fs; Prediction time: %fs"" % (time_liblinear_train, time_liblinear_predict))
    print(classification_report(test_labels, prediction_linear))",train.py,ngrudzinski/sentiment_analysis_437,1
"        neginfo = self.get_samples(negpaths,count)
        if self.verbose == True:
            print 'neg ', neginfo[0].shape
        posnum = posinfo[0].shape[0]
        negnum = neginfo[0].shape[0]
        samples = np.vstack((posinfo[0], neginfo[0]))
        samples = self.normalization(samples)
        posinfo[1].extend(neginfo[1]) 
        paths = posinfo[1]
        labels = [1 for k in range(posnum)] + [0 for k in range(negnum)]
        #self.clf = SVC(C=0.0125,kernel='linear',verbose=False).fit(samples, labels)
        self.clf = SVC(C=2048,kernel='linear',verbose=False).fit(samples, labels)
        prds = self.clf.predict(samples)
        TP = 0
        TN = 0
        for k in range(prds.shape[0]):
            if prds[k] == 1 and labels[k] == 1:
                TP += 1
            if prds[k] == 0 and labels[k] == 0:
                TN += 1",pdd/clf_svm.py,z01nl1o02/tests,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_forward.svm_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeature/example/test_svm_forward.py,jundongl/scikit-feature,1
"        if classifier is not None:
            self.clf = classifier

            from sklearn.svm import LinearSVC
            import random
            if isinstance(self.clf, LinearSVC):
                self.clf.set_params().random_state = random.randint(0, 200)
        else:
            if clf_method == 'SVM':
                from sklearn import svm
                self.clf = svm.SVC()
            elif clf_method == 'ERF':
                from sklearn.ensemble import ExtraTreesClassifier
                self.clf = ExtraTreesClassifier(n_estimators=100,
                                                max_depth=None, min_samples_split=1,
                                                random_state=0)
            elif clf_method == 'GBC':
                from sklearn.ensemble import GradientBoostingClassifier
                self.clf = GradientBoostingClassifier(n_estimators=100,
                                                      max_depth=1)",neurosynth/analysis/classify.py,taylorhxu/neurosynth,1
"n_neighbors=50
n_components=15
scaler =preprocessing.StandardScaler().fit(grads);

grads = scaler.transform(grads)
grads2 = scaler.transform(grads2)
#Y = manifold.Isomap(n_neighbors, n_components).fit_transform(grads)
Y=grads #don't perform dimensionality reduction

#perform classification on the dataset, use support vector machine
test_svm = svm.SVC(kernel='rbf', C=20, gamma=0.15, probability=True)
test_svm=test_svm.fit(Y, scores[:, 1])

#get number of stable classified as unstable

pl.figure(0)
(Y, scores) = grads2, scores2
num_stab = sum(1 for s in scores if s[1] == 1)
missed_stab_arr = [s[0] for t, s, in zip(Y, scores) if s[1]==1 and test_svm.predict(t) != s[1]]
pl.hist(missed_stab_arr, bins=20)",src/scripts/python/learn-grad.py,UW-Kutz-Lab/lilac,1
"        'smote_tomek',
        'smoteenn'
    ]

    my_classifiers = [
        DummyClassifier(strategy='most_frequent', random_state=0),
        DummyClassifier(strategy='stratified', random_state=0),
        DummyClassifier(strategy='uniform', random_state=0),
        DummyClassifier(strategy='constant', random_state=0, constant=True),
        LogisticRegression(C=100),
        SVC(C=1.0, kernel='rbf', probability=True),
        SVC(C=1.0, kernel='linear', probability=True),
        KNeighborsClassifier(n_neighbors=10),
        tree.DecisionTreeClassifier(),
        NuSVC(probability=True),
        RandomForestClassifier(n_estimators=100)
    ]

    document_levels = ['review', 'sentence', 1]
",source/python/evaluation/classifier_evaluator.py,melqkiades/yelp,1
"                X.append(features)
                Y.append(clusterData.queryToCluster[qid][0])
                
                iter=iter+1
            
        #X = [[0, 0], [1, 1]]
        #y = [0, 1]
        X=np.array(X)
        Y=np.array(Y)
        print ""Training""
        clf = svm.SVC()
        clf.fit(X, Y) 
       
        if not os.path.exists(""Classifier""):
            os.makedirs(""Classifier"")

        paths=self.clusterDataPath.split('/')
        name=paths[len(paths)-1]
        parts=name.split('.')
        name=parts[0]",src/scripts/classifier.py,hubert667/AIR,1
"        }
        cv = RandomizedSearchCV(clf, params, n_iter=20, cv=best_cv_num(_n), n_jobs=2, verbose=1, scoring='f1')
        cv.fit(X, y)
        print('only: ' + d.columns[i])
        print(cv.best_score_)
        print(cv.best_params_)

    """"""
    # http://scikit-learn.org/stable/modules/feature_selection.html#selecting-non-zero-coefficients
    p = Pipeline([
        ('clf', LinearSVC(penalty='l1', dual=False)),
        ])
    params = {
        'clf__C': 10**numpy.linspace(-3,3,1000),
    }
    cv = RandomizedSearchCV(p, params, n_iter=20, cv=best_cv_num(_n), n_jobs=2, verbose=3)
    cv.fit(X, y)
    print(cv.best_score_)
    print(cv.best_params_)
    """"""",adult/ml.py,arosh/ml,1
"#tr_features = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54']

#tr_features = ['f1', 'f2', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54']

ts_features = 'class'

classifiers = [
    KNeighborsClassifier(1),                                                    #  0 - !
    KNeighborsClassifier(3),                                                    #  1
    KNeighborsClassifier(6),                                                    #  2
    SVC(kernel=""linear"", C=0.025),                                              #  3
    SVC(kernel=""linear"", C=1),                                                  #  4
    SVC(kernel=""linear"", C=100),                                                #  5
    SVC(gamma=0.5, C=0.1),                                                      #  6
    SVC(gamma=2, C=1),                                                          #  7
    SVC(gamma=50, C=100),                                                       #  8
    DecisionTreeClassifier(max_depth=5),                                        #  9
    DecisionTreeClassifier(max_depth=10),                                       # 10 - !
    SVC(gamma=2, C=1000),                                                       # 11
    SVC(gamma=2, C=100),                                                        # 12",KForest/KForest/KGump.py,Andrew414/KForest,1
"    # data_X_test = X_test[:int(DATA_SIZE*0.8)]
    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]

    print ""Train data size: "" + str(int(DATA_SIZE*0.8))
    print ""Test  data size: "" + str(len(X_target)-int(DATA_SIZE*0.8))


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))
    regr = OneVsRestClassifier(linear_model.LinearRegression())

    # train the model using the training data
    # regr.fit(X_train, X_target)
    regr.fit(data_X_train, data_Y_train)

    # predict to test
    print ""Predicting...""
    test_result = regr.predict(data_X_test)",DST_v1.03.py,totuta/deep-supertagging,1
"
# Loads the iris dataset from the scikit learn module
iris = datasets.load_iris()

# Splits up the data set training data and testing data
# THis is a random split
x_train, x_test, y_train, y_test = cross_validation.train_test_split(
    iris.data, iris.target, test_size=0.4, random_state=0)

# Trains data based on the training data partitioned previously
clf = svm.SVC(kernel='linear', C=1, probability=True).fit(x_train, y_train)

SCORE = clf.score(x_test, y_test)

# Computes accuracy of model after training based on cross-validation
CROSS_SCORES = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)
print('Cross-Validation Test Accuracy: %0.2f (+/- %0.2f)' % (CROSS_SCORES.mean() * 100, CROSS_SCORES.std() * 2,))
print ('Model trained!')

# Prompts for user input for prediction data",bayesian/classifier.py,aig787/Personal,1
"
        ysub[np.where(ysub <= 156)[0]] = -1
        ysub[np.where(ysub  > 156)[0]] =  1

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)
        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=1.0, kernel='rbf', gamma=0.0010)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]
        ytrue_cv[np.where(ytrue_cv <= 156)[0]] = -1
        ytrue_cv[np.where(ytrue_cv  > 156)[0]] =  1
        Xcv = (Xcv - x_mean) / x_std
        ypred_cv = clf.predict(Xcv)
        prec, recall, f1score = evalPerformance(ytrue_cv, ypred_cv)",codes/classify_half1.py,mirjalil/ml-visual-recognition,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_05_2015_server.py,magic2du/contact_matrix,1
"    def classify_vector(self, vector):
        return self.gnb.predict(vector)


class SVMClassifier(BaseClassifier):

    def __init__(self, *args, **kwargs):
        ktype = kwargs['ktype'].lower()
        del kwargs['ktype']
        if ktype == 'svc':
            self.clf = svm.SVC(*args, **kwargs)
        elif ktype == 'linearsvc':
            self.clf = svm.LinearSVC(*args, **kwargs)
        elif ktype == 'nusvc':
            self.clf = svm.NuSVC(*args, **kwargs)
        else:
            raise ValueError('Unknown SVM type: {0}'.format(ktype))

    def train(self, model, target):
        self.clf.fit(model, target)",dsl/representation/classifiers.py,juditacs/dsl,1
"train_data = pca.transform(train_data)
test_data = pca.transform(test_data)
valid_data = pca.transform(valid_data)

mms = preprocessing.MinMaxScaler()
mms.fit(train_data)
train_data = mms.transform(train_data)
test_data = mms.transform(test_data)
valid_data = mms.transform(valid_data)

clf = svm.SVC(C=10, verbose = 2)

clf.fit(train_data, labels)
test_preds = clf.predict(test_data)
valid_preds = clf.predict(valid_data)

np.savetxt('res/digits_test_001.predict', test_preds, '%1.5f')
np.savetxt('res/digits_valid_001.predict', valid_preds, '%1.5f')",Phase0/digits_main.py,abhishekkrthakur/AutoML,1
"    thresholds.append(float(i))
    i += float(dist)
  return thresholds

def train_and_predict(xs, ys):
  accuracies = []
  f1_scores = []
  cs = range_array(0.05, 100, 0.05)

  for c in cs:
    clf = svm.SVC(kernel='linear', C=c, probability=True)
    clf.fit(xs[:TRAIN_SIZE], ys[:TRAIN_SIZE])

    logging.info(""current time %s"" % time.strftime(""%c"") )
    # logging.info(""support vectors:"")
    # logging.info(clf.support_vectors_)
    # logging.info(""number of support vectors for each class: "" + str(clf.n_support_))
    # logging.info(""threshold: ""+ str(clf._intercept_))

    ys_predicted = clf.predict(xs[TRAIN_SIZE:]) # Predict",forestfires/optimize_c.py,Josephu/svm,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features on training set
        idx = CFS.cfs(X[train], y[train])

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",skfeature/example/test_CFS.py,jundongl/scikit-feature,1
"    #parse input and determin its type
    try:
        featureValue= float(input_dict[""featureIn""]) if '.' in input_dict[""featureIn""] else int(input_dict[""featureIn""]) #return int or float
    except ValueError:
        featureValue= input_dict[""featureIn""] #return string
    clf = tree.DecisionTreeClassifier(max_features=featureValue, max_depth=int(input_dict[""depthIn""]))
    output_dict={}
    output_dict['treeOut'] = clf
    return output_dict

def scikitAlgorithms_linearSVC(input_dict):
    from sklearn.svm import LinearSVC
    clf = LinearSVC(C=float(input_dict[""penaltyIn""]),loss=input_dict[""lossIn""],penalty=input_dict[""normIn""], multi_class=input_dict[""classIn""])
    output_dict={}
    output_dict['SVCout'] = clf
    return output_dict

def scikitAlgorithms_SVC(input_dict):
    from sklearn.svm import SVC
    clf = SVC(C=float(input_dict[""penaltyIn""]), kernel=str(input_dict[""kernelIn""]), degree=int(input_dict[""degIn""]))",workflows/scikitAlgorithms/library.py,xflows/clowdflows,1
"        default = array([])
    data_test : numpy.ndarray
        default = array([])

    Returns
    -------
    targets : numpy.ndarray

    """"""

    clf = svm.SVC(kernel).fit(data_train, target_train)
    targets = clf.predict(data_test)
    return targets


def lreg_learn(data_train=np.array([]), target_train=np.array([]), data_test=np.array([])):
    """"""
    estimate position using logistic regression

    Parameters",pylayers/location/learning/loclearn.py,pylayers/pylayers,1
"# Train classifier
#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

C_range = 10.0 ** np.arange(-2, 9)
gamma_range = 10.0 ** np.arange(-5, 4)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedKFold(y=Y, n_folds=3)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, Y)

print(""The best classifier is: "", grid.best_estimator_)

# Now we need to fit a classifier for all parameters in the 2d version
# (we use a smaller set of parameters here because it takes a while to train)
C_2d_range = [1, 1e2, 1e4]
gamma_2d_range = [1e-1, 1, 1e1]
classifiers = []",python/sklearn/examples/svm/plot_rbf_parameters.py,seckcoder/lang-learn,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MRMR.mrmr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_MRMR.py,jundongl/scikit-feature,1
"            #msg = ""Best_params:{}, Best Score:{}"".format(best_params, clf.best_score_)
            #print >> sys.stderr, msg
        #except AttributeError:
            #pass
        return best_params, best_estimator, clf.best_score_


class SVCWrapper(ClassifierWrapper):
    
    def __init__(self, name, **kwargs):
        super(SVCWrapper, self).__init__(name, SVC(**kwargs))
        gamma = 10.0 ** np.arange(-5, 4)
        C = 10.0 ** np.arange(-2, 6)
        #C = [0.003, 0.03, 0.3, 1, 9, 27, 81, 243, 729, 2000, 6000]
        if self.classifier.kernel == ""rbf"":
            self.parameters = {'C': C, 'gamma': gamma}
        elif self.classifier.kernel == ""linear"":
            self.parameters = {'C': C}
 
class MultinomialNBWrapper(ClassifierWrapper):",run/classifier.py,osmanbaskaya/mapping-impact,1
"    sys.path.append(path.abspath(path.join(path.dirname(__file__), '..')))

from sklearn import svm
from sklearn.neural_network import MLPClassifier
import edsudoku
import copy

def test_classifier_1():
    X = [[0, 0], [1, 1]]
    y = [0, 1]
    clf = svm.SVC()
    clf.fit(X, y)  
    assert (clf.predict([[2., 2.]])[0] == 1)

def test_classifier_2():
    X = [[1, 2, 3], [1, 3, 2], [3, 2, 1], [1, 1, 1], [1, 2, 2], [2, 3, 3]]
    y = [1, 1, 1, 0, 0, 0]
    clf = svm.SVC()
    clf.fit(X, y)  
    assert (clf.predict([[2, 3, 1]])[0] == 1)",riddle/classisifer_example.py,liuyonggg/learning_python,1
"
    def test_process(self):
        train_mat = [\
                     [0.12, 0.25],\
                     [3.24, 4.33],\
                     [0.14, 0.45],\
                     [7.30, 4.23],\
                     ]
        train_label = [[0,1], [1,0], [0,1], [1,0]] # out bit is 2

        svc = SVC(train_mat, train_label)
        svc.fit(C = 5.0, toler = 0.001, epoch = 50)
        
        r1 = autotest.eval_predict_one(svc,[0.10,0.33], [0., 1.], self.logging)
        r2 = autotest.eval_predict_one(svc,[4.40,4.37], [1., 0.], self.logging)

        assert r1
        assert r2
               
class test_SVC_iris(test_Suite):",test_pytrain/test_SVM/test_SVC.py,becxer/pytrain,1
"        self.assert_numpy_array_almost_equal(result[0], expected[0])
        self.assert_numpy_array_almost_equal(result[1], expected[1])
        self.assert_numpy_array_almost_equal(result[2], expected[2])

    def test_validation_curve(self):
        digits = datasets.load_digits()
        df = expd.ModelFrame(digits)

        param_range = np.logspace(-2, -1, 2)

        svc = df.svm.SVC(random_state=self.random_state)
        result = df.learning_curve.validation_curve(svc, 'gamma',
                                                    param_range)
        expected = lc.validation_curve(svm.SVC(random_state=self.random_state),
                                       digits.data, digits.target,
                                       'gamma', param_range)

        self.assertEqual(len(result), 2)
        self.assert_numpy_array_almost_equal(result[0], expected[0])
        self.assert_numpy_array_almost_equal(result[1], expected[1])",expandas/skaccessors/test/test_learning_curve.py,sinhrks/expandas,1
"# The following code will train a linear SVM on the dataset and plot the
# decision boundary learned
#

print('Training Linear SVM')

# You should try to change the C value below and see how the decision
# boundary varies (e.g., try C = 1000)

c = 1000
clf = svm.SVC(c, kernel='linear', tol=1e-3)
clf.fit(X, y)

pd.plot_data(X, y)
vb.visualize_boundary(clf, X, 0, 4.5, 1.5, 5)

input('Program paused. Press ENTER to continue')

# ===================== Part 3: Implementing Gaussian Kernel =====================
# You will now implement the Gaussian kernel to use",machine-learning-ex6/ex6/ex6.py,nsoojin/coursera-ml-py,1
"# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20PERCENT data randomly selected as a validation set.
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)

estimator = GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

title = ""Learning Curves (SVM, RBF kernel, $\gamma=0.001$)""
# SVC is more expensive so we do a lower number of CV iterations:
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
estimator = SVC(gamma=0.001)
plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

plt.show()",plot_learning_curve.py,JVP3122/Python-Machine-Learning-NFL-Game-Predictor,1
"
#C_linear = [0.1, 1, 10, 100]
C_linear = [3]

result_linear = []

print ""C value chosen from: "", C_linear
print ""Calculating accuracy with K-fold...""

for C in C_linear:
    svc_linear = svm.SVC(kernel='linear', C=C)
    scores = cross_validation.cross_val_score(svc_linear, X_train, y_train, scoring='accuracy', cv=6)
    result_linear.append(scores.mean())

print ""result:"", result_linear
#Result with different C are equal, so here choose C=1 directly as the best parameter.
best_param_linear = {""C"": 3}


#linear_test_score = svm.SVC(kernel='linear', C=best_param_linear.get(""C"")).fit(X_test, y_test).score(X_test, y_test)",analysis.py,Sapphirine/Human-Activity-Monitoring-and-Prediction,1
"
	# calculate projection
	transformed = matrix.dot(matrix_w)
	tf = transformed.T

	#recData = transformed.dot(matrix_w.T) + matrix.mean(axis=1)[:, None]
	#plot(recData[0].reshape((32,32)))

	#Start to train SVM
	#gamma = 1/sigma^2
	OVRC = OneVsRestClassifier(SVC(kernel = ""rbf"", gamma = 0.000001)).fit(transformed, labels)

	# Print Scatter plot
	# color = [str((item+1) * 24./255.) for item in labels]
	# pl.scatter(tf[0],tf[1],c = color )
	# pl.show()

	# Print the shape of classifier
	# h = 10
	# x_min, x_max = transformed[:, 0].min() - 1, transformed[:, 0].max() + 1",Src/pca_unpar.py,motian12ps/Bigdata_proj_yanif,1
"        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",sklearn_doc_classification_eg.py,linucks/textclass,1
"from sklearn.svm.classes import LinearSVC

from ..Classifier import Classifier
from ...language.JavaScript import JavaScript as JS


class LinearSVCJSTest(JS, Classifier, TestCase):

    def setUp(self):
        super(LinearSVCJSTest, self).setUp()
        self.mdl = LinearSVC(C=1., random_state=0)

    def tearDown(self):
        super(LinearSVCJSTest, self).tearDown()",tests/classifier/LinearSVC/LinearSVCJSTest.py,nok/sklearn-porter,1
"                 map(os.path.split,
                     map(os.path.dirname, labels)))  # Get the directory.
    fname = ""{}/reps.csv"".format(args.workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)
    labelsNum = le.transform(labels)
    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))

    if args.classifier == 'LinearSvm':
        clf = SVC(C=1, kernel='linear', probability=True)
    elif args.classifier == 'GridSearchSvm':
        print(""""""
        Warning: In our experiences, using a grid search over SVM hyper-parameters only
        gives marginally better performance than a linear SVM with C=1 and
        is not worth the extra computations of performing a grid search.
        """""")
        param_grid = [
            {'C': [1, 10, 100, 1000],
             'kernel': ['linear']},",openface1/demos/classifier.py,EliHar/Pattern_recognition,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,jaeilepp/mne-python,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ls_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeature/example/test_ls_l21.py,jundongl/scikit-feature,1
"	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#'''
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


	clf = svm.SVC(kernel=arc_cosine, cache_size=4096)
	#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/stl10/kernel_GRAY.py,akhilpm/Masters-Project,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also
    --------",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/svm/classes.py,RPGOne/Skynet,1
"    X_test = data_test[:, 0:-1]
    Y_test = data_test[:, -1]
    
    X_train_sub = get_subfeature(X_train, feature_idx)
    X_test_sub = get_subfeature(X_test, feature_idx)

    C_best = 0
    accuracy_cv_best = 0
    
    for c in C_LST:
        model = svm.LinearSVC(C=c, penalty='l2', dual=False, tol=TOL)
        accuracy_cv = cross_validation.cross_val_score(model, X_train_sub, Y_train, cv=K, n_jobs=N).mean()
        
        if accuracy_cv > accuracy_cv_best:
            C_best = c
            accuracy_cv_best = accuracy_cv
        
        print 'C = %s\tcross_validation_accuracy = %.1f%%' % (c, accuracy_cv*100)
        sys.stdout.flush()
        ",lsvm.py,uci-cbcl/polyAcode,1
"from sklearn.svm import SVC

for option in ['xor', 'circle', 'random']:
    if option == 'xor':
        X, y = xor_data(num_points)
    elif option == 'circle':
        X, y = circle_data(num_points, 1)
    else:
        X, y = random_data(num_points)
        
    svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)
    svm.fit(X, y)
    plot_decision_regions(X, y, classifier=svm, title=option)",code/ch03/kernel_svm.py,1iyiwei/pyml,1
"        self.done = 0
        # total count of calculation
        self.total = len(np.arange(min_g, max_g, step))

    def run(self):
        for gamma in np.arange(self.min_g, self.max_g, self.step):
            if self.running == 0:
                break
            if gamma == 0.0:
                gamma = ""auto""
            clf = svm.SVC(kernel=""rbf"", gamma=gamma, C=self.C)
            score = round(cross_validation.cross_val_score(
                clf, self.x, self.y, cv=5).mean() * 100, 2)
            # record result
            self.result.append(score)
            self.done = self.done + 1

    def getprogress(self):
        # how many have we done
        return float(self.done) / self.total",Task.py,soyking/MewCoo,1
"    return arg

def train(X, y,outpath=None, verbose=True):
    def build(X, y=None):
        """"""
        Inner build function that builds a single model.
        """"""
        model = Pipeline([
            ('preprocessor',NLTKPreprocessor()),
            ('vectorizer', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)),
            ('clf', OneVsRestClassifier(LinearSVC()))])

        model.fit(X, y)
        return model

    # Label encode the targets
    labels = preprocessing.MultiLabelBinarizer()
    y = labels.fit_transform(y)

    model= build(X, y)",app/core/sentenceClassifer.py,alfredfrancis/ai-chatbot-framework,1
"    rands = np.random.random_sample(m)
   
    # select cases where random no from above is <= threshold
    trn_data = data[rands <= (trn_perc/100), :]
    cv_data = data[rands > (trn_perc/100), :]
 
    return trn_data, cv_data
 
def build_classifier(X, y, reg):
    # rbf is guassian kernal
    clf = svm.SVC(kernel='rbf', C=reg, cache_size=1000)
    return clf.fit(X, y)
 
def main():
 
    global adj_rate
    reg, reg_loop, reg_dir = 1.0, 0, 'up'
    reg_rec, trn_rec, cv_rec = [], [], []
   
    # import training data",classifier.py,Prosserc/Blackbox-Classification,1
"                                      stop_words=""english"",
                                      strip_accents=""unicode"",
                                      dtype=np.float32,
                                      decode_error=""replace"")),
            (""scaling"", Normalizer())
        ])

    X = transformer.fit_transform(records)
    y = np.array([r[0][""decision""] for r in records])

    grid = GridSearchCV(LinearSVC(),
                        param_grid={""C"": np.linspace(start=0.2, stop=0.5,
                                                     num=20)},
                        scoring=""accuracy"", cv=3, verbose=3)
    grid.fit(X, y)

    return Pipeline([(""transformer"", transformer),
                     (""classifier"", grid.best_estimator_)])

",beard_server/modules/predictor/arxiv.py,jacenkow/beard-server,1
"    # N elements
    Y = np.array([x[1] for x in trainset])
    # Remove the leading 1 for X points!
    X = np.array([x[0][1:] for x in trainset])
    
    """"""
    support, support_vectors, n_class_SV, \
    sv_coef, intercept, probA, probB, x = libsvm.fit(X, Y, 0, 'linear')
    """"""
    
    ssvm = svm.SVC(kernel='linear', C=10000000)
    ssvm.fit(X, Y)
    """"""This parameters can be accessed through the members dual_coef_ which holds the product y_i \alpha_i,
    support_vectors_ which holds the support vectors, and intercept_ which holds the independent term \rho
    """"""

    w = np.zeros(2)
    for i in range(len(ssvm.support_vectors_)):
        orig_i = ssvm.support_[i]
        w += ssvm.dual_coef_[0][i]  * X[orig_i]",python/hw_7.py,nicolov/learning-from-data-homework,1
"		
		lr = sklearn.linear_model.LogisticRegression()
		lr.fit(x_train, y_train)
		pred = lr.predict(x_test)

		rf = sklearn.ensemble.RandomForestClassifier(n_estimators = 150)
		rf.fit(x_train, y_train)
		rf_pred = rf.predict(x_test)
		rf_prob = rf.predict_proba(x_test)

		svm1 = sklearn.svm.SVC(probability = True, kernel = 'linear', C = 1)
		svm1.fit(x_train, y_train)
		svm_pred = svm1.predict(x_test)

		svm2 = sklearn.svm.SVC(probability = True, kernel = 'linear', C = 10)
		svm2.fit(x_train, y_train)

		svm3 = sklearn.svm.SVC(probability = True, kernel = 'linear', C = 0.1)
		svm3.fit(x_train, y_train)
",models.py,iskandr/brainmets,1
"	X, y = load_svmlight_file(""../data/"" + str(sys.argv[i]))
	for curC in [0.0001, 0.001, 0.01, 0.1, 1., 10.]:
		print(""=================================================================="")
		#print(""# File = %s"" % (str(sys.argv[i])))
		#print(""loss = SqrHingeLoss,"")
		#print(""reg = SqrL2Reg,"")
		#print(""solver = DualCoordinateDescent,"")
		#print(""tol = %f,"" % 1e-10)
		print(""C = %f,"" % curC)

		svc = LinearSVC(C = curC, tol=1e-10, max_iter=50000, verbose=1, loss=""l1"")

		svc.fit(X, y)
		results = svc.predict(X)
		accuracy = accuracy_score(y, results)
		print(""Accuracy = {}"".format(accuracy))
		print("""")",test/reference/compute_reference.py,Evizero/KSVM.jl,1
"            return True
    else:
        if (last_acc + 1.5) > global_acc: ##expand while we are close from the best
            return True
  
    return False
        

def classification_svm (train_set, test_set): ##train a classifier and returns its error rate
    #st = time.time()
    clf = svm.SVC(C=100)  
    clf.fit(train_set.fts_values, train_set.fts_pred)
    predictions = clf.predict(test_set.fts_values)
    preds = []
    for i in predictions:
        #print i
        preds.append(int(i))
    accuracy = check_accuracy_rate(preds, test_set.fts_pred)
    accuracy = round(accuracy, 2)
 ",MITWS_distributed_memory/wrapper_part.py,JSilva90/MITWS,1
"        else:
            model_select_ini = ensemble.RandomForestClassifier()
            model_select_best = ensemble.RandomForestClassifier()
            
    # SupportVectorMachines --------------------------------------------------------------------------
    if model[""algorithm""] == ""SupportVectorMachines"":    
        if model[""type""] == ""Regressor"":
            model_select_ini = SVR()
            model_select_best = SVR()
        else:
            model_select_ini = SVC()
            model_select_best = SVC()            
   
    if method == ""cluster"":
        print(""Clustering Model Loading done"")
    else:
        print(model[""type""] + "" Model Loading done"")
    
# Clean-up -------------------------------------------------------------------------------------------
for variables in [""data_set"", ""grid_search_fixed"", ""grid_search_opt"", ""predict_best_params""]:",framework/workflow/model_loader.py,kvistrup/mlfp,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,Teekuningas/mne-python,1
"def Model(Trx, Try, Tsx, Tsy, Verbose=True, Normalize=False):
  # Average fit
  if Normalize:
    Trx = 2.*Trx - 1.
    Try = 2.*Try - 1.
  

  # SVM fitting
  Models = [
 
  [svm.SVC(), ""SVM""],
 
#  [ensemble.RandomForestClassifier(n_estimators=256,),""RF"",],
 
  [linear_model.LogisticRegression(),""LR"",], 
 
#  [linear_model.Perceptron(), ""PCP""] 
    
#  [svm.SVC(C=3.16, cache_size=200, class_weight=None, coef0=0.0, degree=7, gamma=.1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=0),""SVM2"",],
",svmtest.py,rodrigosurita/freepuf,1
"class SVCMetaModel:
    """""" SVC meta model which classfies feasible and infeasible points """"""

    def train(self, feasible, infeasible, parameter_C = 1.0,\
        parameter_gamma = 0.0):

        """""" Train a meta model classification with new points """"""

        points_svm = [i.value for i in infeasible] + [f.value for f in feasible]
        labels = [-1] * len(infeasible) + [1] * len(feasible) 
        self._clf = svm.SVC(C = parameter_C, gamma = parameter_gamma)
        self._clf.fit(points_svm, labels)

    def check_feasibility(self, individual):
        """""" Check the feasibility with meta model """"""

        prediction = self._clf.predict(individual.value) 
        if(prediction < 0):
            return False
        else:",evopy/metamodel/svc_meta_model.py,jpzk/evopy,1
"

    # In[ ]:

    scores = cross_val_score(rf,X,y,n_jobs=-1,cv=StratifiedShuffleSplit(y,n_iter=7,test_size=0.3))
    print(""X RF Accuracy: %0.3f (+- %0.2f)"" % (scores.mean(), scores.std() * 2))
#    scores_f1 = cross_val_score(rf,X,y,n_jobs=-1,cv=StratifiedShuffleSplit(y,n_iter=10,test_size=0.22),scoring='f1')
#    print(""X RF f1: %0.3f (+- %0.2f)"" % (scores_f1.mean(), scores_f1.std() * 2))

     # In[ ]:
    svc = LinearSVC(C=20, penalty='l1', dual=False)
    svc.fit(X, y)
    selected_feature_names = feature_cols[[list(set(np.where(svc.coef_ != 0)[-1]))]]
    X_svm = svc.transform(X)
    print(""X_svm L1 transformed:"", X_svm.shape)
    X=X_svm


     # In[ ]:
",ProFET/feat_extract/VisualizebestFeatHist.py,ddofer/ProFET,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",kbe/res/scripts/common/Lib/email/test/test_email.py,theheros/kbengine,1
"
	def split_test_train(self):
		self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.33)

	# Cheating to test hypothesis
	def gridsearchSVM(self):


		cv = KFold(len(self.y), n_folds=5, shuffle=True)

		estimator = SVC(kernel=""linear"", probability=True)
		# linear_clf = grid_search.GridSearchCV(estimator, param_grid={'C': [2**-3, 2**2]}, n_jobs=5)
		linear_clf = grid_search.GridSearchCV(SVC(kernel=""linear"", probability=True),param_grid={'C': [2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]},cv=5,n_jobs=5)
		rbf_clf = grid_search.GridSearchCV(SVC(probability=True),param_grid={'gamma': [2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1], 'C': [2**-1, 2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]},cv=cv,n_jobs=1)

		linear_clf.fit(self.X, self.y)
		rbf_clf.fit(self.X, self.y)

		# print linear_clf.grid_scores_
		l_params = linear_clf.best_params_",sklearn-server/automate.py,daviddao/luminosity,1
"#!/usr/bin/env python

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import validate_dataset as validate

# Make predictions using Support Vector Machines
svm = SVC()
# load train data from validate
svm.fit(validate.X_train, validate.Y_train)
# get predictions and registers from validation
predictions = svm.predict(validate.X_validation)
registers = validate.Y_validation


if __name__ == '__main__':
    # accuracy score 0.9333 is equal 93,33%",app/svm_prediction.py,lucasb/iris-machine-learning,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_30_2014_server3.py,magic2du/contact_matrix,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_08_2015_03.py,magic2du/contact_matrix,1
"from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def doSVM(X_train, X_test, y_train, y_test, outputFile):
    # Linear
    print('------ Linear SVM -------')
    tuned_parameters_linear = {'kernel': ['linear'],
                               'C': [0.1, 0.5, 1, 5, 10, 50, 100]}
    clf1 = GridSearchCV(SVC(), tuned_parameters_linear, cv = 5)   
    clf1.fit(X_train, y_train)
    print(clf1.best_params_)
    bestScore = clf1.best_score_
    testScore = clf1.score(X_test, y_test)
    writeData(outputFile, 'svm_linear', bestScore, testScore)
    # Polynomial
    print('------ Polynomial SVM -------')
    tuned_parameters_poly = {'kernel': ['poly'],
                             'C': [0.1, 1, 3], ",Exercises/Exercise_3/part_3/problem3_3.py,Stocastico/AI_MOOC,1
"    y_test=np.array(classs)[test]
    
    print ""<Results for Fold[""+str(f)+""]>""
    features=getVocabulary(x_train)
    X_train=vectorizePostSetToTags(x_train,features)
    print ""Fold train Matrix[""+str(f)+""]:""+str(X_train.shape)
    X_test=vectorizePostSetToTags(x_test,features)
    print ""Fold test Matrix[""+str(f)+""]:""+str(X_test.shape)
    
    #Create and train classifiers
    clf_svm = svm.SVC(kernel='linear')
    clf_svm.fit(X_train, y_train)
    
    clf_mNB=MultinomialNB()
    clf_mNB.fit(X_train, y_train)
    
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(X_train, y_train)
    
    clf_ada=RandomForestClassifier(n_estimators=25)",code_python27/tfidfANDclasification/classByTags.py,rcln/tag.suggestion,1
"
        """"""
        self.randParameter('C', args, sample=numpy.random.random()*2.)
        self.randParameter('epsilon', args, sample=numpy.random.random()*.1)
        self.randParameter('tolerance', args, sample=10.**-(numpy.random.randint(10)))
        return super(SVMBatchModel, self).randomize_parameters(**args)

    def _genregressor(self, isReward=False, isTermination=False):
        return SVR(C=self.params['C'], epsilon=self.params['epsilon'], tol=self.params['tolerance'])
    def _genclassifier(self):
        return SVC()


class GaussianProcessBatchModel(BatchModel):
    def __init__(self, **kwargs):
        BatchModel.__init__(self, **kwargs)
        self.params.setdefault('theta0', 1e-2)
        self.params.setdefault('thetaL', 1e-4)
        self.params.setdefault('thetaU', 1.)
        self.params.setdefault('random_start', 100)",pyrl/agents/models/batch_model.py,kastnerkyle/python-rl,1
"def classify(clf):
    clf.fit(X, y)
    return clf


if clfier == 'ridge':
    clf = sklm.RidgeClassifier(tol=1e-2, solver=""lsqr"")
elif clfier == 'perceptron':
    clf = sklm.Perceptron(n_iter=100)
elif clfier == 'svm':
    clf = sksvm.LinearSVC(loss='l2', penalty='l2', tol=1e-3)
elif clfier == 'graddesc':
    clf = sklm.SGDClassifier(alpha=0.0001, n_iter=100, penalty='l2')

clsff = classify(clf)
prediction(clf)
",classification.py,f00barin/sol,1
"    print ""{failed classifications, correct classifications}, accuracy of the classifier""
    print counts, ""%.4f"" % accuracy, ""\n""


def test_svm_classes():
    """"""
    Testing all the possible classes of the sklearn.svm library.
    """"""
    # SVM, Linear Support Vector Classification
    classifiers = [
        LinearSVC(),
        NuSVC(),
        NuSVR(),
        OneClassSVM(),
        SVC(),
        SVR()
    ]
    for c in classifiers:
        print c
        classifier = SklearnClassifier(c)",code/classification/svm_bayes_classification.py,magnuskiro/master,1
"	""""""
	Run the MNIST experiment. Each CV split is executed sequentially.
	
	@param base_dir: The full path to the base directory. This directory should
	contain the config as well as the pickled data.
	""""""
	
	# Get the keyword arguments for the SP
	with open(os.path.join(base_dir, 'config.json'), 'rb') as f:
		kargs = json.load(f)
	kargs['clf'] = LinearSVC(random_state=kargs['seed'])
	
	# Get the data
	(tr_x, tr_y), (te_x, te_y) = load_mnist()
	x, y = np.vstack((tr_x, te_x)), np.hstack((tr_y, te_y))
	
	# Get the CV splits
	with open(os.path.join(base_dir, 'cv.pkl'), 'rb') as f:
		cv = cPickle.load(f)
	",src/examples/mnist_runner.py,tehtechguy/mHTM,1
"            with open(svmTrainedData):
                print(' Opening SVM training model...\n')
                clf = joblib.load(svmTrainedData)
        else:
            raise ValueError('Force retraining SVM model')
    except:
        #**********************************************
        ''' Retrain training model if not available'''
        #**********************************************
        print(' Retraining SVM data...')
        clf = svm.SVC(C = svmDef.Cfactor, decision_function_shape = 'ovr', probability=True)
        
        if svmDef.subsetCrossValid == True:
            print("" Iterating training using: "",str(nnDef.percentCrossValid*100), ""% as test subset, iterating"",str(nnDef.iterCrossValid),"" time(s) ...\n"")
            for i in range(svmDef.iterCrossValid):
                As, Cls, As_cv, Cls_cv = formatSubset(A, Cl, svmDef.percentCrossValid)
                clf.fit(As, Cls)
                print('  Mean accuracy: ',100*clf.score(As_cv,Cls_cv),'%')
        else:
            print("" Training on the full training dataset\n"")",Other/Archive/20170609c/SpectraLearnPredict.py,feranick/SpectralMachine,1
"        #stacking_create_training_set(path_to_file+'ensemble_duke_output_raw_n%d.txt' %N,path_to_file+'training_set_n%d.csv' %N, gold_standard_name, N)

        #read it and make machine learning on it

        data = pd.read_csv(path_to_file+'training_set_n%d.csv' %N)

        X = data.values[:,2:(N+2)] #x variables
        y = np.array(data['y']) #class variables

        #fit an SVM with rbf kernel
        clf = SVC( kernel = 'rbf',cache_size = 1000)
        #parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
        parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

        gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
        gs_rbf.fit(X,y)

        clf = gs_rbf.best_estimator_

        project_name = path_to_config_list[-1]",src/Serializer.py,enricopal/STEM,1
"
# For decoding, standardizing is often very important
masker = NiftiMasker(mask_img=data_files['mask_vt'][0], standardize=True)
masked_timecourses = masker.fit_transform(
    data_files.func[0])[resting_state == False]

### Classifiers definition

# A support vector classifier
from sklearn.svm import SVC
svm = SVC(C=1., kernel=""linear"")

from sklearn.grid_search import GridSearchCV
# GridSearchCV is slow, but note that it takes an 'n_jobs' parameter that
# can significantly speed up the fitting process on computers with
# multiple cores
svm_cv = GridSearchCV(SVC(C=1., kernel=""linear""),
                      param_grid={'C': [.1, .5, 1., 5., 10., 50., 100.]},
                      scoring='f1')
",plot_haxby_different_estimators.py,abenicho/isvr,1
"        return self._selectedFeatureIndex is not None

    @property
    def classifier(self):
        return True

    def train(self, X_tuple, Y, OriginalX = None):
        X, numFeatures = X_tuple
        assert self._numOfFeatures <= numFeatures
        Xflat = X.reshape((X.shape[0], -1))
        clf = LinearSVC(C=self._penalty, random_state=self._settings.get('seed', 0))
        print(""start fitting"")
        clf.fit(Xflat, Y)
        allCoef = clf.coef_.reshape((10,) + X.shape[1:])
        allCoef = abs(allCoef)
        allCoef = np.sum(allCoef,axis = 0)
        allCoef = np.sum(allCoef,axis = 0)
        allCoef = np.sum(allCoef,axis = 0)
        print(""finish fitting"")
        self._selectedFeatureIndex = np.argsort(allCoef)[::-1][:self._numOfFeatures]",pnet/intermediateSupervisionLayer.py,jiajunshen/partsNet,1
"
    def svm(self, data, params):
        training_ids, testing_ids         = self.unpack_ids(data)
        training_samples, testing_samples = self.unpack_samples(data)
        training_labels, testing_labels   = self.unpack_labels(data)

        kernel       = 'rbf' if not 'kernel' in params else params['kernel']
        C            = 1.0 if not 'penalty-parameter' in params else params['penalty-parameter']
        degree       = 3 if not 'degree' in params else params['degree']
        gamma        = 'auto' if not 'gamma' in params else params['gamma']
        svm_clf      = svm.SVC(C=C, kernel = kernel, degree = degree, gamma = gamma)
        svm_clf.fit(training_samples, training_labels)
        if testing_samples.any():
            testing_predicted_labels = svm_clf.predict(testing_samples)
        return self.pack_data(training_ids = training_ids, testing_ids = testing_ids, training_samples = training_samples, testing_samples = testing_samples, \
                training_labels = training_labels, testing_labels = testing_labels, testing_predicted_labels = testing_predicted_labels)

    def random_forest(self, data, params):
        training_ids, testing_ids         = self.unpack_ids(data)
        training_samples, testing_samples = self.unpack_samples(data)",utils/Algorithms.py,hzxie/Sharp-V,1
"except:
    pass
scientist = whetlab.Experiment(access_token=access_token,
                               name=""Web page classifier"",
                               description=""Training a polynomial kernel SVM to classify web pages."",
                               parameters=parameters,
                               outcome=outcome)

job = scientist.suggest()
from sklearn import svm
learner = svm.SVC(kernel='poly',**job)
learner.fit(*train_set)
accuracy = learner.score(*validation_set)
scientist.update(job,accuracy)
scientist.report()

n_iterations = 19
for i in range(n_iterations):
    job = scientist.suggest()
    learner = svm.SVC(kernel='poly',**job)",examples/tutorial_example.py,schevalier/Whetlab-Python-Client,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = FCBF.fcbf(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_FCBF.py,jundongl/scikit-feature,1
"    :return: An array of predicted classes 
    """"""
    try:
        predicted = []
        # Retrieve Gram Matrix from string kernel
        if verboseON():
            prettyPrint(""Generating Gram Matrix from documents"", ""debug"")
        X_gram = string_kernel(X, X)
        y = numpy.array(y)
        # Define classifier
        clf = svm.SVC(kernel=""precomputed"")
        if selectKBest > 0:
            X_gram_new = SelectKBest(chi2, k=selectKBest).fit_transform(X_gram, y)
            prettyPrint(""Performing %s-fold CV on the %s best features"" % (kfold, selectKBest))
            predicted = cross_val_predict(clf, X_gram_new, y, cv=kfold).tolist()
        else:
            prettyPrint(""Performing %s-fold CV"" % kfold)
            predicted = cross_val_predict(clf, X_gram, y, cv=kfold).tolist()
    except Exception as e:
        prettyPrintError(e)",data_inference/learning/ScikitLearners.py,aleisalem/Aion,1
"
    estimator = EstimatorWithFit()
    scorer = check_scoring(estimator, allow_none=True)
    assert_true(scorer is None)


def test_check_scoring_gridsearchcv():
    # test that check_scoring works on GridSearchCV and pipeline.
    # slightly redundant non-regression test.

    grid = GridSearchCV(LinearSVC(), param_grid={'C': [.1, 1]})
    scorer = check_scoring(grid, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    pipe = make_pipeline(LinearSVC())
    scorer = check_scoring(pipe, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    # check that cross_val_score definitely calls the scorer
    # and doesn't make any assumptions about the estimator apart from having a",434-MachineLearning/final_project/linearClassifier/sklearn/metrics/tests/test_score_objects.py,neale/CS-program,1
"# - k: number of folds
# ------------------------------------------------------------------
# Example:
# SVM1 = tandem_classification.svm_train(X1_train,y1_train,k)

def svm_train(X,y,k):
	C_range = 10.0 ** np.arange(-2, 9)
	gamma_range = 10.0 ** np.arange(-5, 4)
	param_grid = dict(gamma=gamma_range, C=C_range)
	cv = StratifiedKFold(y=y,n_folds=k)
	svm = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
	svm.fit(X,y)
	return svm

# ------------------------------------------------------------------
# Tandem mSVM 
# ------------------------------------------------------------------
# Executes the final classification using the tandem structure
# defined in the classification layer.
#       - If SVM1 classifies as 0 it goes to SVM2, otherwise it goes",tandem_classification.py,anaherey/tandem-mSVM,1
"    def __init__(self, models: list=None, n_folds: int=3):
        if models is None:
            self.models = [
                #GradientBoostingClassifier(),
                XGBClassifier(),
                RandomForestClassifier(),
                ExtraTreeClassifier(),
                LogisticRegression(),
                #FMClassification(),
                KNeighborsClassifier(),
                LinearSVC(),
                RidgeClassifier(),
                MLPClassifier()]
        else:
            self.models = models
        self.skf = StratifiedKFold(n_folds)

    def fit_transform(self, X_tr, Y_tr, X_ts, method: str='hard'):
        if isinstance(Y_tr, pd.Series):
            Y_tr = np.array(Y_tr)",bards/ensembling.py,dbftdiyoeywga/bards,1
"	labels.append(0)

#Convert to numpy
np.asarray(data)
np.asarray(labels)

#Split training set for Cross-Validation
data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 0.4, random_state = 0)

#Create a Linear SVM and train it
clf = LinearSVC()
print ""Training a Linear SVM Classifier""
clf.fit(data_train, labels_train)
print ""Model trained""

#Perform cross validation
scores = cross_val_score(clf, data_train, labels_train, cv=5, scoring = 'f1_macro')
print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

#Dump model and weights",ucf_sub/src/sub_utils/src/svm.py,RoboticsClubatUCF/RoboSub,1
"
def knn_model(X, y):
    model = KNeighborsClassifier(10)

    model.fit(X, y)

    return model


def svm_model(X, y):
    model = SVC()

    model.fit(X, y)

    return model


def init(data_file):
    X, y = load_data(data_file)
",test/ml.py,SquirrelMajik/GRec,1
"# #         
# #     print y_sampled.Target
# # 
# #     
# #     print X.shape, y.shape, X_sampled.shape, y_sampled.shape
# #     
# #     
# #     
# #     
# #     
# #     #svc = SVC(kernel=""linear"")
# #     
# #     
# #     cval(X_sampled, y_sampled, clf, n_folds = 1, test_size = 0.90)
# #==============================================================================
#     clf = CausalClassifier()
#    
#     rfecv = RFECV(estimator=clf, step=1,loss_func = lambda y_true, y_pred:  -r2_score(y_true, y_pred))
#     rfecv.fit(X_sampled, y_sampled.Target)
#     ",feature_selection.py,ssamot/causality,1
"from sklearn import svm

# we create 40 separable points
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

t0 = time.time()

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)
clf.predict(X[0])

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]
",semester2/task12/example2.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"test_features2 = myzscore(test_features,1)



# Run SVM
from sklearn import svm, datasets

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(train_features2, ytr)

out = svc.predict(train_features2)
out = svc.predict(test_features2)


# Stats
tps = sum((out == 1).__and__(yts == 1))
fps = sum((out == 1) .__and__ (yts == 0))
fns = sum((out == 0) .__and__ (yts == 1))",run_compneuro_svm.py,davestanley/compnet-email-classifier,1
"import numpy as np
import os
from nltk.tokenize import RegexpTokenizer
from collections import Counter
import sys


class EvaluateSentimentVec(object):
    def __init__(self):
        # self.lin_clf = linear_model.LogisticRegression()
        self.lin_clf = svm.LinearSVC()
        self.tokenizer = RegexpTokenizer(r""[\w'-]+"")
        self.word2vec = {}
        self.dim = 0
        self.documentPos = []
        self.documentNeg = []

    def loadVector(self, path):
        print 'loading vector...'
        try:",evaluate_sentiment_vec.py,sidenver/ConstituentRetrofit,1
"
    `intercept_` : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
            gamma=0.0, kernel='rbf', probability=False, shrinking=True,
            tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [ 1.]

    See also
    --------",venv/lib/python2.7/site-packages/sklearn/svm/classes.py,devs1991/test_edx_docmode,1
"    model.fit(X_train, y_train_decoded)
    predict = model.predict(X_test)
    return metrics.f1_score(y_test_decoded, predict, average=average)

models_to_compare = [
                     ('log_reg_sgd', linear_model.SGDClassifier(loss='log')),
                     ('log_reg_liblinear', linear_model.LogisticRegression()),
                     ('log_reg_cv', linear_model.LogisticRegressionCV()),
#                    ('bernoulli nb',  BernoulliNB()),
                     ('gaussian nb',  GaussianNB()),
#                    ('svm_linear', svm.LinearSVC()),
#                    ('svm_rbf', svm.SVC(kernel='rbf')),
#                    ('K=5nn', KNeighborsClassifier(n_neighbors=5)),
                    ]
ntm_models_to_compare = [
                     ('srntm 32*32', ntm),
                    ]                        

for name, model in ntm_models_to_compare:
    f1_scores = [f1_score(s[0], s[1], 'binary', model) for s in sequences]",evaluate.py,tombosc/ntm_experiments,1
"#         else:
#             num_consarticles += 1

#     except:
#         print ""TRAINING SET APPEND OP ERROR: "" + title


#clf = Pipeline([('clf', GradientBoostingClassifier(max_depth = 7))])
print ""***TRAINING CLASSIFIER***""

#clf = svm.SVC()
clf = GradientBoostingClassifier(n_estimators = 1000, max_depth = 10)#, warm_start = True)
clf.fit(Xtrain, Ytrain)

print "">>>DONE TRAINING CLASSIFIER<<<\n""

# dump the classifier to be used elsewhere

print ""***DUMPING CLASSIFIER***""
",clean_articles.py,jombooth/slantparse,1
"          metrics.adjusted_mutual_info_score(labels,  estimator.labels_)))

#  print 'KNN classifier:'
#  knn = neighbors.KNeighborsClassifier(n_neighbors = 10)
#  knn.fit(dataNP,labels)
#  print knn.predict(predNP)
#  print knn.score(predNP, labels)

#  print
#  print 'Linear SVC'
#  clf = svm.LinearSVC()
#  clf.fit(dataNP, labels)  
#  print clf.predict(predNP)
#  print clf.score(predNP, labels)

#  print
#  print 'Gaussian NB'
#  gnb = GaussianNB()
#  gnb.fit(dataNP,labels)
#  print gnb.predict(predNP)",code/src/cluster.py,dkdfirefly/speaker_project,1
"from sklearn import svm
from sklearn.model_selection import cross_val_score
from random import shuffle
from sklearn import preprocessing

#Definicion de la clase.
class Emotrix():

    #Definicion del metodo init
    def __init__(self):
        self.clf = svm.SVC()

    #Metodo utilizado para obtener un bloque/segmento especifico, segun la etiqueta enviada como parametro.
    def get_block(self, blocks, tag):
        for block in blocks:
            if block.tag == tag:
                return block
    #Metodo utilizado para obtener las graficas de la data, para un usuario especifico.
    def graph(self, file_data=""user1.csv""):
        raw = RawData(file_data)",emotrix/emotrix.py,henzer/EMOTRIX,1
"class SVM:
    def __init__(self, problem='binary', C=0, gamma=0, kernel='rbf'):
        self.problem = problem
        self.C = 10**C
        self.gamma = 10**gamma
        self.kernel = kernel
        self.name = 'SVM'

    def eval(self):
        if self.problem == 'binary':
            mod = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma, probability=True, random_state=20)
        else:
            mod = SVR(kernel=self.kernel, C=self.C, gamma=self.gamma)
        return mod


class RF:
    def __init__(self, problem='binary', n_estimators=10, max_features=0.5,
                 min_samples_split=0.3, min_samples_leaf=0.2):
        self.problem = problem",testing/modaux.py,hawk31/pyGPGO,1
"    
    
    #('PCA', PCA()),
    
    pipelines = []
    pipelines.append(('ScaledLR', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('LR', LogisticRegression())])))
    pipelines.append(('ScaledLDA', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))
    pipelines.append(('ScaledKNN', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))
    pipelines.append(('ScaledCART', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))
    pipelines.append(('ScaledNB', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('NB', GaussianNB())])))
    pipelines.append(('ScaledSVM', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('SVM', SVC())])))
    
    results = []
    names = []
    
    for name, model in pipelines:
        kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=SCORING)
        results.append(cv_results)
        names.append(name)",lib/eda2.py,FabricioMatos/ifes-dropout-machine-learning,1
"import optunity
import optunity.metrics
import sklearn.svm

# score function: twice iterated 10-fold cross-validated accuracy
@optunity.cross_validated(x=data, y=labels, num_folds=10, num_iter=2)
def svm_auc(x_train, y_train, x_test, y_test, logC, logGamma):
    model = sklearn.svm.SVC(C=10 ** logC, gamma=10 ** logGamma).fit(x_train, y_train)
    decision_values = model.decision_function(x_test)
    return optunity.metrics.roc_auc(y_test, decision_values)

# perform tuning
hps, _, _ = optunity.maximize(svm_auc, num_evals=200, logC=[-5, 2], logGamma=[-5, 1])

# train model on the full training set with tuned hyperparameters
optimal_model = sklearn.svm.SVC(C=10 ** hps['logC'], gamma=10 ** hps['logGamma']).fit(data, labels)",docs/examples/python/sklearn/svc.py,MarkAWard/optunity,1
"
    @staticmethod
    def SklearnLogisticRegression():
        return NltkClassifierWrapper(SklearnClassifier((LogisticRegression())))

    @staticmethod
    def SklearnSGDClassifier():
        return NltkClassifierWrapper(SklearnClassifier(SGDClassifier(loss='log')))

    @staticmethod
    def SklearnSVC():
        return NltkClassifierWrapper(SklearnClassifier(SVC(probability=True)))

    @staticmethod
    def SklearnLinearSVC():
        return NltkClassifierWrapper(SklearnClassifier(LinearSVC()))

    @staticmethod
    def SklearnNuSVC():
        return NltkClassifierWrapper(SklearnClassifier(NuSVC(probability=True)))",analysis/textclassification/NltkClassifierFactory.py,dhermyt/WONS,1
"			#print c, len(x), x[:10], y[:10]
		plt.savefig('svm_classifier.pdf', bbox_inches='tight')
		#plt.close()
		print 'svm_classify plotting done.'
	u = points.mean(axis=0)
	s = points.std(axis=0)
	def transform(p):
		p = numpy.asarray(p)
		return (p - u) / s
	points = transform(points)
	clf = sklearn.svm.NuSVC(nu=0.05, probability=True, kernel='rbf')
	clf.fit(points, classes)
	#print clf.get_params()

	if plot:
		x = numpy.linspace(0, 1, 100)
		y = numpy.linspace(0, 1, 100)
		grid = numpy.array([[[xi, yi] for xi in x] for yi in y])
		dists = numpy.array([[clf.predict_proba(transform([[xi, yi]]))[0][0] for xi in x] for yi in y])
		print 'levels:', dists.max(), dists.min()",nested_sampling/samplers/svm/__init__.py,JohannesBuchner/UltraNest,1
"        from sklearn.svm import SVC
        from sklearn.cross_validation import StratifiedKFold

        paramgrid = {""kernel"": [""rbf""],
                     ""C""     : np.logspace(-9, 9, num=25, base=10),
                     ""gamma"" : np.logspace(-9, 9, num=25, base=10)}

        random.seed(1)

        from evolutionary_search import EvolutionaryAlgorithmSearchCV
        cv = EvolutionaryAlgorithmSearchCV(estimator=SVC(),
                                           params=paramgrid,
                                           scoring=""accuracy"",
                                           cv=StratifiedKFold(y, n_folds=10),
                                           verbose=1,
                                           population_size=50,
                                           gene_mutation_prob=0.10,
                                           gene_crossover_prob=0.5,
                                           tournament_size=3,
                                           generations_number=10)",xtoy/evolutionary_search.py,kootenpv/xtoy,1
"            ""surprise""]

# cache
CACHE = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
# detector
DETECTOR = dlib.get_frontal_face_detector()
# predictor
PREDICTOR = dlib.shape_predictor(""shape_predictor_68_face_landmarks.dat"")

# Set the classifier as a support vector machines with polynomial kernel
CLASSIFIER = SVC(C=10, kernel='linear')


def get_files(emotion):
    """"""Define function to get file list, shuffle it and split 80/20.""""""
    files = glob.glob(""dataset/sorted_set/%s/*"" % emotion)
    random.shuffle(files)
    training = files
    return training
",Video/train.py,mencattini/ReMIx,1
"        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., n_jobs=1,",summary/sumy/sklearn/grid_search.py,WangWenjun559/Weiss,1
"    ""Nearest Neighbors"", ""Decision Tree"",
    ""Random Forest"", ""AdaBoost"",
    ""Linear SVM"", ""RBF SVM"",
    ""Naive Bayes""
]
classifiers = [
    KNeighborsClassifier(3),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianNB()
]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)",semester2/task13/exercise2.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"                for fea in ['acc avg', 'acc std', 'peak avg', 'peak std',
                            'max psd freq', 'max psd value',
                            'fourier peak index', 'fourier peak value',
                            'peak to acc avg ratio', 'peak to acc std ratio',
                            'single ax to total avg ratio',
                            'single ax to total std ratio']]

with open('live_classification_experiment.txt','w') as f:
    for X, Y, labels in zip(Xs, Ys, Ls):
        dt = tree.DecisionTreeClassifier()
        sv1 = svm.SVC(kernel='linear', C=1)
        sv2 = svm.SVC(kernel='linear', C=1)
        nb1 = naive_bayes.GaussianNB()
        nb2 = naive_bayes.GaussianNB()
        dt.fit(X,Y.transpose())
        sv1.fit(X,Y[0])
        sv2.fit(X,Y[1])
        nb1.fit(X,Y[0])
        nb2.fit(X,Y[1])
        print >>f, 'Decision Tree'",classifyrunner/experiments/live.py,toonn/mlii,1
"    # The channels to be used while decoding
    picks = mne.fiff.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                                stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    concatenator = ConcatenateChannels()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,christianbrodbeck/mne-python,1
"    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline

    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)

    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])

    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svn
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
",sklean-hmm/pipeline.py,Sklearn-HMM/scikit-learn-HMM,1
"		'''C is parameter varying'''
		nsamples= self.X.shape[0]
		ind= kfold_indices(kfolds,nsamples)
		err=np.zeros(kfolds)-1
		keep_clf=dict(err=1.)
		for i in range(kfolds):
			ival= ind[i,:]
			itrain=np.array(list(set(ind.flatten())-set(ival)))
			assert(len(list(set(ival) | set(itrain))) == len(ind.flatten()))
			# Train
			clf = SVC(C=C,kernel='rbf',degree=3)
			clf.fit(self.X[itrain,:],self.y[itrain])
			#get error for this kth sample
			pred= clf.predict(self.X[ival,:])
			err[i],wrong= benchmark(pred, self.y[ival])
			if err[i] <= keep_clf['err']: 
				keep_clf['lsvc']=clf
				keep_clf['err']=err[i]
		return err,keep_clf
",py/obiwan/decals_sim_priors.py,legacysurvey/legacypipe,1
"        return seed
    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)

def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in getargspec(estimator.fit)[0]


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",sklearn/utils/validation.py,loli/semisupervisedforests,1
"
    # reguralization
    scaler = preprocessing.StandardScaler()
    _X = scaler.fit_transform(_X)

    x_train, x_test, y_train, y_test = train_test_split(_X, y, test_size=0.25, random_state=42)

    candidates = [{'kernel': [""rbf""], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]},
                  {'kernel': ['linear'], 'C': [1, 10, 100]}]

    clf = GridSearchCV(svm.SVC(C=1), candidates, cv=5, scoring=""f1"")
    clf.fit(x_train, y_train)

    for params, mean_score, scores in sorted(clf.grid_scores_, key=lambda s: s[1], reverse=True):
        print(""%0.3f (+/-%0.03f) for %r"" % (mean_score, scores.std() / 2, params))

    model = clf.best_estimator_
    y_predict = model.predict(x_test)

    print(classification_report(y_test, y_predict, target_names=[""good"", ""bad""]))",sayuri/machine/evaluator/make_model.py,icoxfog417/sayuri-server,1
"        k_values = [1, 2]
        C_values = [1, 2]
        n_samples = 500
        n_features = 10000
        n_cores = 2
        X, y = datasets.make_classification(n_samples=n_samples,
                                            n_features=n_features,
                                            n_informative=5)
        # epac workflow for paralle computing
        pipelines = Methods(*[Pipe(SelectKBest(k=k),
                              Methods(*[SVC(kernel=""linear"", C=C)
                              for C in C_values]))
                              for k in k_values])
        pipeline = CVBestSearchRefitParallel(pipelines,
                                             n_folds=n_folds_nested)
        wf = CV(pipeline, n_folds=n_folds)

        sfw_engine = SomaWorkflowEngine(tree_root=wf,
                                        num_processes=n_cores,
                                        remove_finished_wf=False,",epac/tests/test_cv_best_search_refit_parallel_reduce.py,neurospin/pylearn-epac,1
"# Train classifiers
#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

# C_range = np.logspace(-2, 10, 13)
# gamma_range = np.logspace(-9, 3, 13)
# param_grid = dict(gamma=gamma_range, C=C_range)
# cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)
# grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
# grid.fit(X, y)
#
# print(""The best parameters are %s with a score of %0.2f""
#       % (grid.best_params_, grid.best_score_))

# Now we need to fit a classifier for all parameters in the 2d version
# (we use a smaller set of parameters here because it takes a while to train)

C_2d_range = [1]",ryu/app/reduce_t/classifier_demo.py,Zouyiran/ryu,1
"        if classifier:
            self.classifier = classifier
        else:
            self.classifier = self._build_classifier()
            print(""Using {} classifier"".format(self.classifier))

        self.classes = classes

    def _build_classifier(self):
        classifiers = {
            ""SVM"": SVC(),
            ""RF"": RandomForestClassifier(),
            ""LDA"": LinearDiscriminantAnalysis(),
        }
        clf = classifiers[self.config[""classifier_name""]]
        clf.set_params(**self.config[""classifier_params""])
        return clf

    def classification_probas(self, X_train, y_train, X_test):
        self.classifier.fit(X_train, y_train)",ml/classification.py,pbrusco/ml-eeg,1
"
        # To count for RuntimeWarning: divide by zero encountered in log
        if (not self.x_train or 0 not in self.y_train or
                1 not in self.y_train):
            self.l.error(""Not enough data yet to feed the classifier"")
            return None

        self.classifier = Pipeline([
            ('vectorizer', CountVectorizer(stop_words=self.stop_words)),
            ('tfidf', TfidfTransformer()),
            ('clf', LinearSVC())])

        try:
            self.classifier.fit(self.x_train, self.y_train)
        except ValueError:
            self.l.error(""Not enough data yet to train the classifier"")
            return None

        elapsed_time = datetime.datetime.now() - start_time
        self.l.debug(""Initializing classifier in {0}"".format(elapsed_time))",predictor.py,chembrows/ChemBrows,1
"  # print(y)

  le = preprocessing.LabelEncoder()
  sorted_y = sorted(set(y))
  le.fit(sorted_y)
  y_train = le.transform(y)


  # le.inverse_transform(0)
  pipeline = Pipeline([
      ('clf', SVC(kernel='rbf', gamma=0.01, C=100, probability=True))
  ])
  parameters = {
      'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
      'clf__C': (0.1, 0.3, 1, 2, 10, 30)
  }
  grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1,
                            scoring='accuracy')
  grid_search.fit(X_train, y_train)
  ",create_toc_mapper.py,rupendrab/py_unstr_parse,1
"ssmodel = CPLELearningModel(basemodel)
ssmodel.fit(X, ys)
print ""CPLE semi-supervised log.reg. score"", ssmodel.score(X, ytrue)

# semi-supervised score, WQDA model
ssmodel = CPLELearningModel(WQDA(), predict_from_probabilities=True) # weighted Quadratic Discriminant Analysis
ssmodel.fit(X, ys)
print ""CPLE semi-supervised WQDA score"", ssmodel.score(X, ytrue)

# semi-supervised score, RBF SVM model
ssmodel = CPLELearningModel(sklearn.svm.SVC(kernel=""rbf"", probability=True), predict_from_probabilities=True) # RBF SVM
ssmodel.fit(X, ys)
print ""CPLE semi-supervised RBF SVM score"", ssmodel.score(X, ytrue)",examples/example.py,tmadl/semisup-learn,1
"    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]

    print ""Train data size: "" + str(int(DATA_SIZE*0.8))
    print ""Test  data size: "" + str(len(X_target)-int(DATA_SIZE*0.8))


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(linear_model.LinearRegression())
    # regr = OneVsRestClassifier(svm.SVC(kernel='linear'))
    # regr = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))
    # regr = OneVsRestClassifier(svm.SVC(kernel='rbf'))
    regr = OneVsRestClassifier(svm.SVC(kernel='poly', `probability=True, degree = 5))
    # regr = cluster.KMeans(n_clusters=100)    
    # regr = neighbors.KNeighborsClassifier()


    # train the model using the training data
    # regr.fit(X_train, X_target)",DST_v1.05.py,totuta/deep-supertagging,1
"print(""default score without scaling: %f"" % SVC().fit(X_train, y_train).score(X_test, y_test))

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(""default score with scaling: %f"" % SVC().fit(X_train_scaled, y_train).score(X_test_scaled, y_test))

grid_search.fit(X_train_scaled, y_train)
",day3-machine-learning/solutions/svms.py,AstroHackWeek/AstroHackWeek2015,1
"from sklearn import svm
from sklearn.model_selection import train_test_split

dataset = pd.read_csv('dataset.csv', sep=',')

images = dataset.iloc[:, 1:]
labels = dataset.iloc[:, :1]

train_images, test_images, train_labels, test_labels = train_test_split(images, labels, train_size=0.9, random_state=0)

clf = svm.SVC()
clf.fit(train_images, train_labels.values.ravel())

print(clf.score(test_images, test_labels))

joblib.dump(clf, 'clf_svm_def.pkl', 3)",ocr/classifier/train.py,BumagniyPacket/ocr,1
"imputer = Imputer(missing_values='NaN', strategy='most_frequent', axis=1)
X=imputer.fit_transform(X, y)

print 'NaN in X: ', np.any(np.isnan(X))
print 'NaN in Y: ',np.any(np.isnan(y))

# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# SVM
estimator = svm.SVC(decision_function_shape='ovo')
svm_model=estimator.fit(X,y)
#joblib.dump(estimator, 'data/models/dewiki/dewiki.pkl') 
print 'SVM = ',svm_model.score(X, y)
# For testing
#pred_decision=estimator.predict(X_test)
#print 'SVM classes true',y_test
#print 'SVM classes predicted',pred_decision

# Cross validation",wikiwhere/machine_learning/simple_logreg.py,mkrnr/wikiwhere,1
"# y is filled with integers coding for the class to predict
# We must have X.shape[0] equal to y.shape[0]
X = [e.get_data()[:, data_picks, :] for e in epochs_list]
y = [k * np.ones(len(this_X)) for k, this_X in enumerate(X)]
X = np.concatenate(X)
y = np.concatenate(y)

from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score, ShuffleSplit

clf = SVC(C=1, kernel='linear')
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(X), 10, test_size=0.2)

scores = np.empty(n_times)
std_scores = np.empty(n_times)

for t in xrange(n_times):
    Xt = X[:, :, t]
    # Standardize features",examples/decoding/plot_decoding_sensors.py,effigies/mne-python,1
"# The number of repetitions of 'regular' experiments
n_splits_regular = 100

# The number of repetitions of 'permutation' experiments
n_splits_permutation = 100

#######################
#  LEARNER OPTIONS  ###
#######################

vs = RFE(LinearSVC(loss='hinge'), step=0.3)

param_grid = {
    'n_features_to_select': [10, 20, 50],
    'estimator__C': np.logspace(-4, 0, 5),
}

estimator = GridSearchCV(vs, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=1)

# Set options for ModelAssessment",palladio/config_templates/config_binclass.py,slipguru/palladio,1
"    labels = list(hidden_representations_pickle['shape'])
    values = df_col_to_matrix(hidden_representations_pickle['hidden_repr'])
    Y_data = np.asarray(labels)

    # prepare shuffle split cross-validation, set random_state to a arbitrary constant for recomputability
    X_train, X_test, y_train, y_test = train_test_split(values, Y_data, test_size=0.2, random_state=0)
    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)

    if type is ""linear"":
        # create linear SVM and find best C with shuffle split cv
        estimator = SVC(kernel='linear')
        # add more C values if necesssary (one is used here due to recomputability)
        C_values = [1, 10, 100, 1000]
        classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=dict(C=C_values))
        classifier.fit(X_train, y_train)
        estimator = estimator.set_params(C=classifier.best_estimator_.C)


        title = 'Learning Curves (SVM (kernel=linear), C=%.1f, %i shuffle splits, %i train samples)' % (
            classifier.best_estimator_.C, cv.get_n_splits(), X_train.shape[0] * (1 - cv.test_size))",data_postp/similarity_computations.py,jonasrothfuss/DeepEpisodicMemory,1
"    labels_train_pos = np.ones((data_train_pos.shape[0], 1))
    data_train_neg = load_data(new_train_data_path, type=""neg/"")
    labels_train_neg = np.zeros((data_train_neg.shape[0], 1))

    # Merging the pos and neg examples and labels in a single structure
    x_train = np.concatenate((data_train_pos, data_train_neg), axis=0)
    y_train = np.concatenate((labels_train_pos, labels_train_neg), axis=0)

    # Extracting the features
    x_train_features = hog_extraction(x_train)
    # error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.01))
    # error, clf = cross_validation(x_train_features, y_train, AdaBoostClassifier(n_estimators=50))
    
    print(""Premier apprentissage."")
    # error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.05), N=0) # TODO: change N
    error, clf = cross_validation_svm(x_train_features, y_train, N=3)

    window_w = SIZE_TRAIN_IMAGE[0]
    window_h = window_w
",extract_examples.py,jjerphan/SY32FacialRecognition,1
"from sklearn import datasets
# each item in X is the averaged data for strength.
# training data
X = [[1,4,4,4],[4,1,4,4],[5,4,1,4],[4,5,5,1]]
Y = ['regime1','regime2','regime3','regime4']
from sklearn import svm
clf = svm.SVC()
clf.fit(X,Y)
# predict outcome(Y) for new data point
print clf.predict([6,6,6,7])",machine_learning_trial.py,robin-lee/urban-hackathon,1
"    y = data[:,3:]
    if y.size > 0:
        y= np.reshape(y, len(y))
    return x,y

def saveData(path, data):
    
    np.savetxt(path, data, delimiter=',',fmt='%i')

def getClassifier(X,y):
    clf = svm.LinearSVC()
    clf.fit(X,y);
    return clf

def getDummyClassifier(X,y):
    clf = DummyClassifier('most_frequent')
    clf.fit(X,y);
    return clf

",Recomender System/Recomender System/RecomenderSystem.py,TechnicHail/COMP188,1
"        D_tr = (np.vstack(D_tr))

        y_tr = np.concatenate(y_tr)
        n = len(class_labels[tr_inds,cl])

        if str.lower(opt_criterion) == 'map':
            grid_scorer = make_scorer(average_precision_score, greater_is_better=True)
        else:
            grid_scorer = make_scorer(average_binary_accuracy, greater_is_better=True)
        LOOCV = cross_validation.StratifiedKFold(class_labels[tr_inds,cl], n_folds=2, shuffle=False, random_state=74)
        clfs[cl] = grid_search.GridSearchCV(svm.SVC(), svm_parameters, \
                                       n_jobs=20, cv=LOOCV, scoring=grid_scorer, verbose=False)
        clfs[cl].fit(D_tr,y_tr)
        clfs[cl].best_params_

    val_scores = [clf.best_score_ for clf in clfs]
    print val_scores
    print np.mean(val_scores), np.std(val_scores)

    # quit()",classification.py,aclapes/darwintree,1
"
        print(""Training on {0} rows"".format(len(week_train_df)))
        #filter NaN vaule rows
        available_train = pd.notnull(week_train_category)
        available_test = pd.notnull(week_test_df['X'])
        week_train_df = week_train_df[available_train]
        week_train_category = week_train_category[available_train]
        week_test_df = week_test_df[available_test]
        
        #train model
        clf = svm.SVC(probability=True)
        clf.fit(week_train_df, week_train_category)
        print(""Predicting on {0} rows"".format(week_test_df))
        predictions = [predictions, pd.DataFrame(clf.predict_proba(week_test_df))]
        predictions = pd.concat(predictions)


print(""Length of predictions: {0}"".format(len(predictions)))
print(""Length of raw test: {0}"".format(len(raw_test)))
print(""Saving to csv"")",src/week_windows.py,gnu-user/sf-crime-classification,1
"    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=""g"")
    plt.plot(train_sizes, train_scores_mean, label=""Training score"", color=""r"")
    plt.plot(train_sizes, test_scores_mean, label=""Cross-validation score"", color=""g"")
    plt.legend(loc=""best"")
    plt.show()

#### main ####
trainData, trainLabel = loadTrainData()
cv = cross_validation.ShuffleSplit(len(trainData), n_iter=3, test_size=0.3, random_state=0)
train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes, train_scores, test_scores = learning_curve(SVC(C=10, gamma=0.01), trainData, ravel(trainLabel), train_sizes=train_sizes, cv=cv, n_jobs=-1)
plot(train_scores, test_scores, train_sizes)

#testData = loadTestData()
#testLabel = SVM(trainData, trainLabel, testData)
#saveTestLabel(testLabel)
print ""%ss used."" % str(time.clock())",ml/MNIST/scikit-learn/svm.py,huangshenno1/algo,1
"    dt = test_df[col].dtype
    #fill in nan
    if dt== float:
        test_df[col].fillna(test_df[col].mean(), inplace=True)
    #normalize data
    if (dt==float or dt==int) and col != 'PassengerId':    
        test_df[col]-=test_df[col].max()/2
        test_df[col] /=max( test_df[col].max(),math.fabs(test_df[col].max()))


clf = svm.SVC(kernel='rbf', degree=3,gamma=9)
model = clf.fit(train_df.ix[:,columns],train_df.ix[:,'Survived'])

y_pred=clf.predict(test_df.ix[:,columns])
test_df[""Survived""] = pd.Series(y_pred)
test_df.to_csv(""result.csv"", cols=['PassengerId', 'Survived'], index=False)

print train_df.head()

",Titanic: Machine learning from Disaster/titanic.py,Radzell/KagglePractice,1
"test = rawtest[:,feat_inds]
norm_test = (test - test.mean(axis=0)) / np.sqrt(test.var(axis=0,ddof=1))
N = test.shape[0]
D = data.shape[1]
#sys.exit()

trn_labels = np.hstack(( np.zeros(Ntrn/2), np.ones(Ntrn/2) ))
tst_labels = np.hstack(( np.zeros(N/2), np.ones(N/2) ))
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_data, trn_labels)
skknn.fit(norm_data, trn_labels)
sksvm.fit(norm_data, trn_labels)
print(""skLDA error: %f"" % (1-sklda.score(norm_test, tst_labels)))
print(""skKNN error: %f"" % (1-skknn.score(norm_test, tst_labels)))
print(""skSVM error: %f"" % (1-sksvm.score(norm_test, tst_labels)))

labels = np.hstack((np.zeros(N/2), np.ones(N/2)))
n,gext,grid = get_grid_data(np.vstack(( norm_data0, norm_data1 )))",tests/poisson.py,binarybana/samcnet,1
"        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",src/code.py,yr3var14/ata,1
"from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.utils.multiclass import type_of_target

from helpers.log_config import log_to_info


class RbfClassifier(object):
    def classify(self, mp, x_train, y_train, x_test):
        feature_map_nystroem = Nystroem(random_state=1, gamma=1.1, n_components=1000)  # gamma=0.00005,
        clf = pipeline.Pipeline([(""feature_map"", feature_map_nystroem), (""svm"", LinearSVC())])
        log_to_info('Fitting a RBF SVM to labeled training data...')
        clf = clf.fit(x_train, y_train)
        log_to_info('Predicting test value')
        y_test = clf.predict(x_test)
        log_to_info('Done!')

        return y_test

        # y_train = list(y_train.values)",code/classifiers/rbf_classifier.py,lukaselmer/hierarchical-paragraph-vectors,1
"# -*- encoding: utf8 -*-
'''Train a svm model using iris model.
'''

from sklearn import datasets, svm
import cPickle as pkl

__author__ = 'noahsark'

digits = datasets.load_digits()
clf = svm.LinearSVC()
size = int(digits.data.shape[0] * 0.8)
clf.fit(digits.data[:size], digits.target[:size])
with open('svm_model.pkl', 'wb') as fp_:
    pkl.dump(clf, fp_)
",thrift_demo/gen-py/train_model.py,jimmylai/slideshare,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_forward.svm_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",PyFeaST/example/test_svm_forward.py,jundongl/PyFeaST,1
"def get_data():
    train_vecs=np.load(modeldir + '/train_vecs.npy')
    y_train=np.load(modeldir + '/y_train.npy')
    test_vecs=np.load(modeldir + '/test_vecs.npy')
    y_test=np.load(modeldir + '/y_test.npy') 
    return train_vecs,y_train,test_vecs,y_test
    

##训练svm模型
def svm_train(train_vecs,y_train,test_vecs,y_test):
    clf=SVC(kernel='rbf',verbose=True)
    clf.fit(train_vecs,y_train)
    if not os.path.exists(modeldir + '/svm_model'):
        os.mkdir(modeldir + '/svm_model')
    joblib.dump(clf, modeldir + '/svm_model/model.pkl')
    print clf.score(test_vecs,y_test)
    
    
##得到待预测单个句子的词向量    
def get_predict_vecs(words):",static/code/sentiment_svm.py,wdxtub/Patriots,1
"	#print x.shape
	#print y.shape

	# Make the dev set and test set
	x_train, x_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.2)

	# Train the model 
	#clf = LogisticRegression(fit_intercept=True, n_jobs=4)
	#clf.fit(x_train, y_train)

	clf = svm.SVC()
	clf.fit(x_train, y_train)

	# Save the classifier
	with open('test.pickle', 'wb') as f:
		pickle.dump(clf, f)

	accuracy = clf.score(x_test, y_test)",Machine Learning Algorithms/Classifier using Pixel Classifier Version 0 - Logistic Regression - LPF/LogisticRegression.py,lozuwa/Automatic-Digital-Microscope,1
"    # train model
    d = math.floor(len(X) * 0.1)  # defines split
    train_X = X[:-d]
    test_X = X[-d:]
    train_y = y[:-d]
    test_y = y[-d:]

    X = np.array(train_X)
    y = np.array(train_y)

    clf = SVC(kernel='linear', C=1, probability=True)
    model = clf.fit(X, y)

    # lets define a function that outputs the probability of each label

    def predict_proba(xs):
        return clf.predict_proba(xs)  # here xs is  a list of feature vectors

    stops = set(stopwords.words('english'))
",TwitterSentimentAnalysis.py,DayneSorvisto/Twitter-Sentiment-Analyzer,1
"

validation_size = 0.20
seed = 7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state = seed)

scoring = 'accuracy'

kfold = model_selection.KFold(n_splits = 10, random_state = seed)
cv_log_result = model_selection.cross_val_score(LogisticRegression(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_svc_result = model_selection.cross_val_score(LinearSVC(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_rfc_result = model_selection.cross_val_score(RandomForestClassifier(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_knn_result = model_selection.cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_gnb_result = model_selection.cross_val_score(GaussianNB(), X_train, Y_train, cv = kfold, scoring = scoring)

print('Logistic Regression: %f (%f)' % (cv_log_result.mean(), cv_log_result.std()))
print('Linear SVC: %f (%f)' % (cv_svc_result.mean(), cv_svc_result.std()))
print('Random Forest Classifier: %f (%f)' % (cv_rfc_result.mean(), cv_rfc_result.std()))
print('KNN Classifier: %f (%f)' % (cv_knn_result.mean(), cv_knn_result.std()))
print('Gaussian Naive Bayes: %f (%f)' % (cv_gnb_result.mean(), cv_gnb_result.std()))",factors_model_comparison.py,czhroailsky/csv_table_ngram,1
"        test_labels = high_confidence_predicted_labels[test_indices]

        L_prime = scipy.sparse.vstack([L, augment])

        y_l_prime = np.concatenate([y_l, augment_labels])
        L_prime_ids = np.concatenate([L_ids, augment_ids])

        saved_L_primes.append(L_prime)
        saved_L_prime_ids.append(L_prime_ids)    

        svm_prime = sklearn.svm.LinearSVC(penalty='l2', C=10, dual=False)
        accuracy = sklearn.cross_validation.cross_val_score(svm_prime, L_prime, y_l_prime, cv=5, n_jobs=7).mean()

        saved_cv_accuracies.append(accuracy)
            
    best_index = np.argmax(saved_cv_accuracies)
    best_L_prime_ids = saved_L_prime_ids[best_index]
    best_accuracy = saved_cv_accuracies[best_index]
    
    return best_L_prime_ids, best_accuracy",incremental_tsvm_news.py,CalculatedContent/tsvm,1
"from sklearn import datasets
from sklearn import svm
import matplotlib.pyplot as plt

iris = datasets.load_iris()
digits = datasets.load_digits()

clf = svm.SVC(gamma=.001, C=100)
clf.fit(digits.data[:-1], digits.target[:-1])

# print clf.predict(digits.data[-1])
",Tests/mlPractice.py,hubwayPredict/main,1
"        fs, labels, test_size=0.4, random_state=0
    )

    clf = None
    pred = None
    grid_search = False
    if config.CLASSIFIER == 'NaiveBayes':
        clf = BernoulliNB()
    elif config.CLASSIFIER == 'LinearSVC':
        if config.SELF_TRAINING:
            clf = LinearSVC(C=1)
        else:
            clf = GridSearchCV(LinearSVC(), tuned_params, cv=5, scoring='accuracy')
            grid_search = True
    elif config.CLASSIFIER == 'SVC':
        clf = GridSearchCV(SVC(), svc_tuned_params, cv=5, scoring='accuracy')
        grid_search = True
    elif config.CLASSIFIER == 'DecisionTree':
        clf = DecisionTreeClassifier()
        fs_train = fs_train.toarray()",magpie/sklearn-solution.py,s1na/magpie,1
"from epac.configuration import conf


class TestPipeline(unittest.TestCase):

    def test_pipeline(self):
        X, y = datasets.make_classification(n_samples=20,
                                            n_features=5,
                                            n_informative=2)
        # = With EPAC
        wf = Pipe(SelectKBest(k=2), SVC(kernel=""linear""))
        r_epac = wf.top_down(X=X, y=y)

        # = With SKLEARN
        pipe = sklearn.pipeline.Pipeline([('anova', SelectKBest(k=2)),
                                          ('svm', SVC(kernel=""linear""))])
        r_sklearn = pipe.fit(X, y).predict(X)

        key2cmp = 'y' + conf.SEP + conf.PREDICTION
        # = Comparison",epac/tests/test_primitives.py,neurospin/pylearn-epac,1
"from sklearn.svm import SVC

train = pandas.read_csv('svm-data.csv',
                        header=0,
                        index_col=None,
                        names=['Y', 'A', 'B'])

X = train[['A', 'B']]
Y = train['Y']

svc = SVC(C=100000, kernel='linear', random_state=241)

svc.fit(X, Y)

print svc.support_vectors_",data science/Assessment 6/assessment 6.py,sergiy-evision/math-algorithms,1
"    GNB.fit(train_features, train_target)
    pred = GNB.predict(test_features)
    GNBaccuracy = metrics.accuracy_score(test_target, pred)
    accuracy_dict[""GNB""] = GNBaccuracy
    print GNBaccuracy

    """"""
       SVM classifier
    """"""
    print("" Accuracy by SVM"")
    svm =  SVC(gamma=2, C=1)
    svm.fit(train_features, train_target)
    pred = svm.predict(test_features)
    svmaccuracy =  metrics.accuracy_score(test_target, pred)
    accuracy_dict[""SVM""] = svmaccuracy
    print svmaccuracy

    """"""
       AdaBoostClassifier classifier
    """"""",QuoraAnswersClr/quora_answerclassifier.py,anilcs13m/Projects,1
"
import numpy as np
import pylab as pl
import pickle
import codecs
from sklearn import svm 
from sklearn import preprocessing

class SVM():
    def __init__(self):
        self.clf = svm.SVC(kernel='rbf', C=1)
        return

    def load_tsv_file(self, filepath):
        return self.load_delimited_file(filepath , delimiter=""\t"")

    def load_csv_file(self, filepath):
        return self.load_delimited_file(filepath , delimiter="","")

    def load_delimited_file(self, filepath, delimiter):",webapp/webapp/recommend/svm.py,yu-iskw/toy-recommend-webpage,1
"def crossdomain(documents_stack, documents_wiki):
    print ""Cross Domain""
    # documents_stack=stack_data.values() 
    # documents_wiki=wiki_data.values()
    PolitenessFeatureVectorizer.generate_bow_features(documents_stack, bow)

    X_stack, y_stack = documents2feature_vectors(documents_stack)
    X_wiki, y_wiki = documents2feature_vectors(documents_wiki)

    print ""Fitting""
    clf = svm.SVC(C=0.02, kernel='linear', probability=True)
    # clf = RandomForestClassifier(n_estimators=50)
    clf.fit(X_stack, y_stack)
    y_pred = clf.predict(X_wiki)
    print ""Trained on Stack and results predicted for wiki"" 
    # Test
    #print(classification_report(y_wiki, y_pred))
    print(clf.score(X_wiki, y_wiki))

    print ""------------------------------------------------------""",train_and_test.py,nidishrajendran/computational-politeness,1
"for i in xrange(K):

	testDataset = orangeData.select(folds, i)
	trainDataset = orangeData.select(folds, i, negate = 1)
	
	# Convert Train Dataset
	converted_train_data = np.array([[ d[f].value for f in orangeData.domain if f != orangeData.domain.class_var] for d in trainDataset])
	converted_train_targets = np.array([ 0 if d[orangeData.domain.class_var].value == 'ALL' else 1 for d in trainDataset ])

	# Train SVM
	clf = svm.SVC(kernel='linear')
	clf.fit(converted_train_data, converted_train_targets)

	# Testing

	# Convert Test Dataset
	converted_test_data = np.array([[ d[f].value for f in orangeData.domain if f !=	orangeData.domain.class_var] for d in testDataset])
	converted_test_targets = np.array([0 if d[orangeData.domain.class_var].value == 'ALL' else 1 for d in testDataset ])

	if oneShot:",baseline.py,Sh1n/AML-ALL-classifier,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0705_2015.py,magic2du/contact_matrix,1
"sys.path.append('..')
from sklearn import ensemble
from sklearn import neighbors
import embedding_forest
def get_classifier(name, vectorizer):
  if name == 'logreg':
    return linear_model.LogisticRegression(fit_intercept=True)
  if name == 'random_forest':
    return ensemble.RandomForestClassifier(n_estimators=1000, random_state=1, max_depth=5, n_jobs=10)
  if name == 'svm':
    return svm.SVC(probability=True, kernel='rbf', C=10,gamma=0.001)
  if name == 'tree':
    return tree.DecisionTreeClassifier(random_state=1)
  if name == 'neighbors':
    return neighbors.KNeighborsClassifier()
  if name == 'embforest':
    return embedding_forest.EmbeddingForest(vectorizer)
class ParzenWindowClassifier:
    def __init__(self):
        #self.kernel = lambda x, sigma : np.exp(-.5 * x.dot(x.T)[0,0] / sigma ** 2) / (np.sqrt(2 * np.pi * sigma **2))",parzen_windows.py,marcotcr/lime-experiments,1
"rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    """"""
    Test parameters on classes that make use of libsvm.
    """"""
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[0.25, -.25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    """"""",sklean-hmm/svm/tests/test_svm.py,Sklearn-HMM/scikit-learn-HMM,1
"features_train, features_test, labels_train, labels_test = preprocess()



#########################################################
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

if __name__ == '__main__':

    clt = SVC(kernel=""linear"")
    time0 = time()
    clt.fit(features_train, labels_train)
    print ""Training time: "", round(time()-t0, 3), ""seconds.""
    
    t0 = time()
    pred = clt.predict(features_test)
    print ""Prediction Time: "", round(time()-t0, 3), ""seconds.""
    
    acc = accuracy_score(pred, labels_test)",Intro to Machine Learning/ud120-projects/svm/svm_author_id.py,zhongyuanzhou/FCH808.github.io,1
"from sklearn.decomposition import PCA
import load_data

print(__doc__)

data = load_data.load_shelve('test')

X = preprocessing.scale(np.array(data['data']))
y = np.array(data['state'])

estimators = [('pca', PCA(whiten=True)), ('svm', SVC(probability=True))]

# Set the parameters by cross-validation
tuned_parameters = {'svm__kernel': ['rbf',], 'svm__gamma': [1e-3],
                     'svm__C': [100], 'pca__n_components': [100,]}
tuned_clf = Pipeline(estimators) 

scores = ['roc_auc']

for score in scores:",tests/test_grid_search.py,schae234/gingivere,1
"
posFile.close()
negFile.close()

precisionSum = 0.0
recallSum = 0.0
accuracySum = 0.0
for i in range(5):
    feature_train, feature_test, label_train, label_test = cross_validation.train_test_split(meta, labels, test_size=0.2, random_state=42)

    svmModel = svm.SVC().fit(feature_train, label_train)
    predictions = svmModel.predict(feature_test)
    print predictions
    print label_test
    precisionCount = 0.0
    totalCount = 0.0
    recallCount = 0.0
    if len(predictions) != len(label_test):
        print 'inference error!'
    for index, label in enumerate(predictions):",metaModel.py,renhaocui/tweetInfluencer,1
"# l1 data (only 5 informative features)
X_1, y_1 = datasets.make_classification(n_samples=n_samples,
										n_features=n_features, n_informative=5,
										random_state=1)

# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features / 5)

clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
											 tol=1e-3),
						 np.logspace(-2.3, -1.3, 10), X_1, y_1),
						(LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
											 tol=1e-4),
						 np.logspace(-4.5, -2, 10), X_2, y_2)]

colors = ['b', 'g', 'r', 'c']

for fignum, (clf, cs, X, y) in enumerate(clf_sets):",svc.py,canchengliu/RandomForest,1
"def compare(imputed, log_path=""results.txt""):
    """"""
    PARAMETERS
    ----------
    imputed: [(str, np.ndarray), (str, np.ndarray)...]
       List of tuples containing (imputation algorithm name, imputed data)
    RETURNS
    ------
    .txt file of classification results on imputed data
    """"""
    clfs = [[""SVC"", SVC()],
            [""KNeighbours"", KNeighborsClassifier(2)],
            [""GaussianNB"", GaussianNB()],
            [""RandomForestClassifier"", RandomForestClassifier()]]

    results = {imputation_name: [] for imputation_name, _ in imputed}

    for imputation_name, data in imputed:
        X, y = data
        X_train, X_test, y_train, y_test = train_test_split(X, y,",impyute/utils/compare.py,eltonlaw/impyute,1
"        components += 1
    
    pca = PCA(n_components = components + 1)
    pca.fit(features)

    return pca

# Train a Support Vector Machine classifier with Radial Basis Function kernel with
# chosen C and gamma arguments
def svmTrain(C, gamma, X, y):
    svm = SVC(C = C, gamma = gamma, kernel = 'rbf')
    svm.fit(X, y)
    return svm

# Train a Random Forest classifier with chosen n_estimators and max_features arguments
def randomForestTrain(n_estimators, max_features, X, y):
    if (max_features is None):
        rf = RandomForestClassifier(n_estimators = n_estimators, max_features = None)
    else:
        rf = RandomForestClassifier(n_estimators = n_estimators, max_features = max_features)",processing/classification.py,marcusangeloni/smc2016,1
"        4:  single arm -> less weight depending on choices
    """"""
    dataSet = np.array(dataSet)
    dataLabels = np.array(dataLabels)
    
    print ""Fitting classifier to data""
    
    flag = 0
    if dataSet.ndim==1:  # Single arm data
        flag = 1
        clf = SVC(C=0.75)
        dataSet = dataSet.reshape(-1, 1)
        testSet = testSet.reshape(-1, 1)
        if mode==4:
            weight = np.zeros((len(label_names)))
            for idx,name in enumerate(label_names):
                weight[idx] = 1.0/len(name)
        else:
            weight = 1
            ",recognition/dataProcessing.py,shllybkwrm/swarm_gesture_control,1
"import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import validation_curve
from sklearn.svm import SVC

digits = load_digits()
X, y = digits.data, digits.target

param_range = np.logspace(-6, -1, 5)
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name=""gamma"", param_range=param_range,
    cv=10, scoring=""accuracy"", n_jobs=1)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title(""Validation Curve with SVM"")
plt.xlabel(""$\gamma$"")
plt.ylabel(""Score"")",projects/scikit-learn-master/examples/model_selection/plot_validation_curve.py,DailyActie/Surrogate-Model,1
"        r['precision'] = precision 
        r['recall']    = recall     
        r['f1']        = f1        
        r['accuracy']  = accuracy  
        return r

class LSVM(LClassifier):
    def __init__(self):
        super(LSVM,self).__init__()
        self.name = 'svm'
        self.learner  = svm.SVC(probability=True)
        self.param_dist = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),'kernel': ['rbf'] }
        self.n_iter = 20

class LKNN(LClassifier):
    def __init__(self):
        super(LKNN,self).__init__()
        self.name = 'knn'
        self.learner = neighbors.KNeighborsClassifier()
        self.param_dist = {'n_neighbors': range(1,50)}",learner.py,edsontm/flexrank,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>'

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',
         penalty='l2', random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",sklearn/metrics/classification.py,loli/semisupervisedforests,1
"		return

	from sklearn.decomposition import PCA
	pca = PCA( n_components = n_components )
	pca.fit( X )
	print ""PCA Explained Variance Ratio""
	print pca.explained_variance_ratio_ 
	X_pca = pca.fit_transform( X )

	from sklearn import svm
	clf_pc = svm.SVC( kernel = kernel, C = C, gamma = gamma, degree = degree )
	clf_pc.fit( X_pca, y )

	svm_fig = pl.figure( ""SVM"" )

	# Draw the projection of the decision function
	x_min, x_max = X_pca[ :, 0 ].min() - 1, X_pca[ :, 0 ].max() + 1
	y_min, y_max = X_pca[ :, 1 ].min() - 1, X_pca[ :, 1 ].max() + 1
	h = max( x_max - x_min, y_max - y_min ) / 1000
	xx, yy = np.meshgrid( np.arange( x_min, x_max, h ), np.arange( y_min, y_max, h ) )",final_project/final/code/learning.py,griffinmilsap/cs475,1
"


def runSVM( dataSet, dataLabels, label_names, testSet, testLabels, title = ""Learning Curves"" ):
    dataSet = np.array(dataSet)
    dataLabels = np.array(dataLabels)
    
    print ""Fitting classifier to data (with cross-validation)""
    
    if dataSet.ndim==1:
        clf = SVC(C=0.75)
        dataSet = dataSet.reshape(-1, 1)
        testSet = testSet.reshape(-1, 1)
        
    else:
        clf = SVC(C=1.0)
    
#        X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataSet, dataLabels, test_size=0.1, random_state=0)
#        clf.fit(X_train, y_train)
#        print ""Accuracy:"", clf.score(X_test, y_test)",recognition/dataProcessing_structuredTestPoints.py,shllybkwrm/swarm_gesture_control,1
"
lNbEstimatorsInEnsembles = 12

models=[DecisionTreeClassifier(max_depth=5, random_state = 1960) ,
        AdaBoostClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        GradientBoostingClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        SGDClassifier( random_state = 1960),
        LogisticRegression( random_state = 1960),
        RandomForestClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        GaussianNB(),
        SVC(max_iter=200, probability=True, kernel='linear'),
        SVC(max_iter=400, probability=True, kernel='poly'),
        SVC(max_iter=200, probability=True, kernel='rbf'),
        MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 5), random_state=1960),
        RidgeClassifier(random_state = 1960)]


dialects = [""db2"", ""hive"", ""mssql"", ""mysql"", ""oracle"", ""postgresql"", ""sqlite""];

for clf in models:",tests/classification/test_client_iris_many_models.py,antoinecarme/sklearn2sql_heroku,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in getargspec(estimator.fit)[0]


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",gplearn/skutils/validation.py,alexandrudaia/gplearn,1
"def combineFeatures(pca, selection):
    combined_features = FeatureUnion([(""pca"", pca), \
                                      (""univ_select"", selection)])  # Univariate selection
    return combined_features

def transformData(combined_features, X, y):
    X_features = combined_features.fit(X, y).transform(X)
    return X_features

def createSVM():
    svm = SVC(kernel='linear')
    return svm

def classify(svm, X_features, y):
    return svm.fit(X_features, y)

def searchGrid(combined_features, svm, X, y):
    pipeline = Pipeline([('features', combined_features), \
                         ('svm', svm)])
",python/sklearn/examples/general/feature_extraction.py,kwailamchan/programming-languages,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value",shinken/external_command.py,kaji-project/shinken,1
"        }
    ]
    return pipe, grid


def svm_pipe():
    pipe = make_pipeline(
        PadSubsequence(),
        FlattenShape(),
        StandardScaler(),
        LinearSVC(),
    )
    grid = [
        {
            ""paddedsubsequence__length"":[1,2,4,8,16],
            ""linearsvc__C"":10 ** np.linspace(-10, 10, 51)
        }
    ]
    return pipe, grid
",noxer/sequences.py,iaroslav-ai/noxer,1
"        kernel: (optional) Kernel to be used in the svm classifier can be 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'
    
    Returns
    -------
        prediction: predicted labels of the test data
        accuracy: percent of test data labels accurately predicted
    """"""

    time_1 = time.time()

    estimator = svm.SVC()
    
    #set up parameters that will be used by all kernels
    if(passed_parameters is None):
        parameters = {'C': [1e0, 5e0, 1e1, 5e1]}
    else:
        parameters = passed_parameters 

    #create cross validation iterator
    cv = ShuffleSplit(training_features.shape[0], n_iter=5, test_size=0.2, random_state=0)",python/classifiers.py,njpayne/euclid,1
"import numpy as np #for calculation
import pylab as pl #for plot
from sklearn import svm

# create 40 separable points
np.random.seed(0) #generate the same random num
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",Support_Vector_Machine(SVM)/SVM_Example.py,LeoZ123/Machine-Learning-Practice,1
"
    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM':
    make_pipeline(Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM':
    make_pipeline(RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4)
}


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('--classifiers', nargs=""+"",",scikit-learn-0.17.1-1/benchmarks/bench_mnist.py,RPGOne/Skynet,1
"    #x = np.hstack((x,extra_train))
    #t = np.hstack((t,extra_test))

    print x.shape

    label = np.array(label)

    clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=True,C=20,tol=1e-9,class_weight=None, random_state=None, intercept_scaling=1.0)
    #clf = AdaBoostClassifier(n_estimators=100)
    #clf = SGDClassifier(loss=""log"",n_iter=300, penalty=""l2"",alpha=0.00005,fit_intercept=Tr1ue)#sgd 的训练结果也不错
    #clf = svm.SVC(C=1,degree=9,gamma=10,probability=True)
    #clf = RandomForestClassifier(n_estimators=2000, criterion =""gini"",max_depth=20,min_samples_split=1, random_state=0,n_jobs=3)
    #clf = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0,max_depth=1, random_state=0)
    print ""cross validation""
    print np.mean(cross_validation.cross_val_score(clf,x,label,cv=20,scoring='roc_auc'))

    clf.fit(x,label)
    #验一下自己的结果
    print ""训练自己"",clf.score(x,label)
    answer =  clf.predict_proba(t)[:,1]",lda.py,ezhouyang/class,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0629_2015.py,magic2du/contact_matrix,1
"    For best results, this accepts a matrix in csr format
    (scipy.sparse.csr), but should be able to convert from any array-like
    object (including other sparse representations).

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm.sparse import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
            gamma=0.0, kernel='rbf', probability=False, shrinking=True,
            tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [ 1.]
    """"""

    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma=0.0,",venv/lib/python2.7/site-packages/sklearn/svm/sparse/classes.py,devs1991/test_edx_docmode,1
"    default_params = {}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # We don't want instances overwritting the class default params.
        self.default_params = self.default_params.copy()
        self.name = self.__class__.__name__

    def train(self, cv, X, y, **kwargs):
        X = X['categorical'] if self.data == 'categorical' else X['default']
        clf = svm.SVC(self.svc, max_iter=2**20)
        search = self.searcher(estimator=clf, cv=cv, **self.default_params)
        search.fit(X=X, y=y, **kwargs)
        return search

    def test(self, search, X, y, **kwargs):
        X = X['categorical'] if self.data == 'categorical' else X['default']
        prediction = search.predict(X=X)
        results = {'test_score': np.mean(prediction == y)}
        results.update(search.details)",kcat/kernels/helpers.py,Alkxzv/categorical-kernels,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_estimator_)
    print()
    print(""Grid scores on development set:"")
    print()
    for params, mean_score, scores in clf.grid_scores_:",example_classification_report.py,dropofwill/author-attr-experiments,1
"    
    #compute class weighted (inverse of proportion)
    weights = {}
    for cl in class_prop.keys():
        weights[cl] = 1. / class_prop[cl]
    
    #svm
    if svm == True:
        #learn       
        start = time.time()    
        clf = SVC(C=C)        
        clf.fit(train_X, train_y)   
        svm_time = time.time() - start
        
        #train error
        pred_y_train = clf.predict(train_X)
        perf_svm_train = computePerfForConf(conf_mat(train_y, pred_y_train))
        
        #test error
        pred_y_test = clf.predict(test_X)",Run_Data/preprocess.py,onefishy/ComplexPerfMeasure,1
"
    plt.show()

#data = dl.nor(dl.loadData('datafew/'))
#
#test1 = dl.nor(dl.loadData('datalarge/training/'))
#
#test2 = dl.nor(dl.loadData('datalarge/test/'))
#
#
#dl.trainingSVC(test2, data, 0.00001, 2)

#dl.plot2DataSet(test2, test1, 1)

def acc_image(training_data, tarining_label, test_data, test_label):
    n_train = training_data.shape[0]  # samples for training
    n_test = test_data.shape[0]       # samples for testing
    n_averages = 50                   # how often to repeat classification
    n_features_max = 5  # maximum number of features
    step = 1  # step size for the calculation",src/clf/main.py,changkun/MotionTouch,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_08_2015_01.py,magic2du/contact_matrix,1
"    accuracy, recall, precision, f1, tr_err = classifier_statistics(y_train, y_test, y_tr_pred, y_pred)
    
    return C, accuracy, recall, precision, f1, tr_err


def fit_svm_sklearn_liblinear_with_cv(C, X_train, X_test, y_train, y_test):
        
    X_train, y_train, X_test, y_test = dtype_ensure(X_train, y_train, X_test, y_test)
            
    #clf = LinearSVC(C=C, dual=False,penalty='l1',loss='l2', class_weight='auto')
    clf = LinearSVC(C=C, dual=False,penalty='l1',loss='l2')
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    
    y_tr_pred = clf.predict(X_train)
    
    accuracy, recall, precision, f1, tr_err = classifier_statistics(y_train, y_test, y_tr_pred, y_pred)
    
    return C, accuracy, recall, precision, f1, tr_err",lib/BING-Objectness/source/svm_container.py,bubae/gazeAssistRecognize,1
"
tic = time.clock()

df = pd.read_csv('dataset/winequality-red.csv', header=0, sep=';')

X = df[list(df.columns)[:-1]]
y = df['quality']
print ""1""
X_train, X_test, y_train, y_test = train_test_split(X, y)
print ""splitted""
model_lin = svm.LinearSVC()
model_rbf = svm.SVC()
print ""made model""
model_lin.fit(X_train, y_train)
model_rbf.fit(X_train, y_train)
print ""fitting""
y_predict_lin = model_lin.predict(X_test)
y_predict_rbf = model_rbf.predict(X_test)
print ""predicted""
mse_lin = mean_squared_error(y_predict_lin, y_test)",svm-red.py,behrtam/wine-quality-prediction,1
"def evaluate_accuracy_on_each_risk_level():
    """"""
    Run the experiment to evluate the acutal prediction accuracy on each risk level of users
    """"""
    train_book_y, train_book_X = load_training_data(""training_data/feature_avg_filtered.txt"")
    learners = [
        linear_model.LogisticRegression(),
        ensemble.RandomForestClassifier(n_estimators=10),
        ensemble.GradientBoostingClassifier(n_estimators=10),
        ensemble.AdaBoostClassifier(n_estimators=10),
        svm.SVC(probability=True)]
    learner_names = ['lr', 'rf', 'gbc', 'ada', 'svm']

    y_prob_all = np.zeros((len(learner_names), len(train_book_y)))
    for ln_idx, lname in enumerate(learner_names):
        bx_risk_X, bx_risk_y = load_predict_data(lname)
        y_prob_all[ln_idx] = risk_estimation(bx_risk_X, bx_risk_y)
    y_prob_mean = np.mean(y_prob_all, axis=0)

    for each_learner in learners:",risk_estimation.py,wangleye/age_risk_estimation_book,1
"	return render_template('error.html', template_folder=tmpl_dir, error=404, error_msg=""Page Not Found"",
		return_home=""We can't find what you're looking for.""
	)

def get_predictor():
	try:
		svm = joblib.load(""data/svm.pkl"")
	except:
		crime_data = scale(np.loadtxt(open(""data/crimeData.csv"", ""rb""), delimiter="","", skiprows=1))
		crime_target = np.loadtxt(open(""data/crimeLabels.csv"", ""rb""), delimiter="","", dtype=str)
		svm = SVC(kernel=""rbf"", probability=True).fit(crime_data, crime_target)
		joblib.dump(svm, ""data/svm.pkl"")
	finally:
		return svm

if __name__ == ""__main__"":
	port = int(os.environ.get(""PORT"", 5000))
	app.run(host='0.0.0.0', port=port)",app.py,alanplotko/Cryme,1
"plt.legend(loc='upper left')
plt.show()

lr.predict_proba(X_test_std[0, :])

# Support Vector Machines (SVM) - maximizing the margin or the distance between the separating hyperplane (decision
# boundary) and the training samples that are closest to this hyperplane, the so called support vectors

from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()

# Using kernel functions to project inseparable data into higher dimensionality for separation
# Radial Basis Function is one of the most widely used kernel functions",ClassifiersTour.py,petritn/MachineLearning,1
"q = Queue()

# Performs recursive feature elimination until 'attribute count' has been reached
def _eliminate_features(X_test, X_train, attribute_count, y_train):
    print ""Eliminating features until %d has been reached"" % attribute_count

    pca = RandomizedPCA(n_components=attribute_count+10).fit(X_train)
    X_train = pca.transform(to_float(X_train))
    print ""Finished pca""

    clf = SVC(**SVC_parameters)
    rfe = RFE(clf, n_features_to_select=attribute_count, step=0.1)
    fit = rfe.fit(X_train, y_train)
    print ""Finished rfe""

    # Reduce the feature matrices to contain just the selected features
    X_train = [fit.transform(X) for X in X_train]
    X_test = [fit.transform(X) for X in pca.transform(to_float(X_test))]
    return X_test, X_train
",source/classification/cv_subjects_multithreaded_pca.py,jschavem/facial-expression-classification,1
"

@cli.command()
def visualize_pathways_for_desease():
    X, y = DataReader().read_fva_solutions('fva_without.transports.txt')
    X = PathwayFvaDiffScaler().fit_transform(X, y)
    vect = DictVectorizer(sparse=False)
    X = vect.fit_transform(X, y)
    # X = X[:, None]
    y = np.array([1 if i == 'bc' else 0 for i in y], dtype=int)
    # clf = LinearSVC(C=0.01, random_state=43).fit(X, y)
    if len(X) == 1:
        X = X + np.reshape(np.random.normal(1, 100, size=len(X)), X.shape)
        clf = DecisionTreeClassifier(max_depth=2).fit(X, y)
        plot_decision_regions(X, y, clf=clf, res=0.5, legend=2)
        plt.xlabel(vect.feature_names_[0])
    else:
        for fn in set(map(lambda x: x[:-4], vect.feature_names_)):
            try:
                x = X[:, (vect.feature_names_.index('%s_min' % fn),",src/scripts/visualizations.py,MuhammedHasan/metabolitics,1
"                ('heart_ischemic',410,415),
                ('heart_failure',428,429),
                ('pulmonary',460,520),
                ('digestive',520,580),
                ('renal_insufficiency',580,630)]
    B = {d[0]: LogisticRegression(class_weight='auto', random_state=0) for d in diseases}
    #B = {d[0]: RandomForestClassifier(n_estimators=100, max_features=None, n_jobs=-1, random_state=0) for d in diseases}

    #classifiers = {d[0]:
        #[#KNeighborsClassifier(3),
         #SVC(kernel=""linear"", C=0.025),
         #SVC(gamma=2, C=1),
         #DecisionTreeClassifier(criterion='entropy',max_depth=4,min_samples_split=10,random_state=0)] for d in diseases}#,
         #RandomForestClassifier(max_depth=10, n_estimators=100, max_features=1)] for d in diseases}#,
         #AdaBoostClassifier()] for d in diseases}#,
         #GaussianNB(),
         #LDA(),
         #QDA()]for d in diseases}
         #RandomForestClassifier(n_estimators=200, max_features=None, n_jobs=-1, random_state=0)] for d in diseases}
         #GradientBoostingClassifier(n_estimators=1000, max_depth=None, max_features=None)] for d in diseases}",examples/mimic2/logreg.py,twareproj/tware,1
"        train_name.append(name)


test = np.array(test)
train = np.array(train)
test_label = np.array(test_label)
train_label = np.array(train_label)

clf = LogisticRegression()
#clf = MultinomialNB()
#svm = sklearn.svm.SVC()
#svm = sklearn.svm.LinearSVC()
clf.fit(train, train_label)
pred_train = clf.predict(train)
pred_test = clf.predict(test)

def test_error(pred, true_class):
    n_correct = np.sum(pred == true_class)
    return 1 - n_correct / pred.shape[0]
",learning/old/test1.py,fcchou/CS229-project,1
"from sklearn import linear_model, svm, tree, neighbors, kernel_ridge, discriminant_analysis, ensemble
from sklearn.base import TransformerMixin, RegressorMixin, ClassifierMixin, clone, is_classifier
from sklearn.utils.multiclass import type_of_target, unique_labels
from collections import namedtuple


estimator_tuple = namedtuple(""estimator_tuple"", [""name"", ""model""])

CLASSIFIERS = [(""lr"", linear_model.LogisticRegression()),
               (""lda"", discriminant_analysis.LinearDiscriminantAnalysis()),
               (""svc"", svm.SVC(probability=True)), 
               (""gbc"", ensemble.GradientBoostingClassifier())]

REGRESSORS = [(""rr"", linear_model.Ridge()), 
              (""lars"", linear_model.Lars()), 
              (""svr"", svm.SVR()), 
              (""gbr"", ensemble.GradientBoostingRegressor())]

META_CLASSIFIER = (""lr"", linear_model.LogisticRegression())
",stacklearn/validation.py,gdouzas/stack-learn,1
"

def _gs_SVC_r0(xM, yVc, params):
    """"""
    Since classification is considered, we use yVc which includes digital values 
    whereas yV can include float point values.
    """"""

    print(xM.shape, yVc.shape)

    clf = svm.SVC()
    #parmas = {'alpha': np.logspace(1, -1, 9)}
    kf5_c = model_selection.KFold(n_splits=5, shuffle=True)
    kf5 = kf5_c.split(xM)
    gs = model_selection.GridSearchCV(clf, params, cv=kf5, n_jobs=-1)

    gs.fit(xM, yVc)

    return gs
",jgrid.py,jskDr/jamespy_py3,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",monitor/api/python/Lib/test/test_email/test_email.py,FFMG/myoddweb.piger,1
"from sklearn.svm import SVC

from collate import collate_all_from_breath_meta_to_data_frame
from learn import preprocess_x_y, perform_initial_scaling, perform_subsequent_scaling
from sms import send_text

TEST_FRACTION = 0.02

app = Flask(__name__)
# Declare global svm with optimal params
clf = SVC(cache_size=1024, C=10, gamma=0.02)


def get_data():
    to_stack = 20
    samples = None
    df = collate_all_from_breath_meta_to_data_frame(to_stack, samples)
    x, y, vents_and_files = preprocess_x_y(df)
    if samples:
        x = x.sample(n=samples)",flask_demo.py,hahnicity/ecs251-final-project,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_estimator_)
    print()
    print(""Grid scores on development set:"")
    print()
    for params, mean_score, scores in clf.grid_scores_:",DigitRecognizer/digitssample.py,n7jti/kaggle,1
"import multiprocessing
import time
import numpy as np
from sklearn.svm import SVC
from sklearn import datasets

def mp_worker(q):
    print(""Entered worker"")
    clf = SVC()
    iris = datasets.load_iris()
    clf.fit(iris.data, iris.target_names[iris.target])
    while 1:
        str = q.get()
        l = [float(i) for i in str.split(',')]
        print(""Got list:"", l )
        print(clf.predict(l))
    print(""Exited worker"")
        ",nanoservice/example/test.py,nave91/nanoservice,1
"@click.option('--segmented', type=click.BOOL, default=False)
def main(sname, svm_c, segmented):
    if segmented:
        cub = CUB_200_2011_Segmented(settings.CUB_ROOT)
    else:
        cub = CUB_200_2011(settings.CUB_ROOT)
    ft_storage = datastore(settings.storage(sname))
    ft_extractor = CNN_Features_CAFFE_REFERENCE(ft_storage, make_net=False)

    Xtrain, ytrain, Xtest, ytest = cub.get_train_test(ft_extractor.extract_one)
    model = svm.LinearSVC(C=svm_c)
    model.fit(Xtrain, ytrain)
    predictions = model.predict(Xtest)

    print 'accuracy', accuracy_score(ytest, predictions)
    print 'mean accuracy', utils.mean_accuracy(ytest, predictions)

    pred_storage = datastore(settings.PREDICTIONS_BASE, global_key='preds')
    storage_path = pred_storage.get_instance_path('preds', sname, '%s.mat' % sname)
    pred_storage.ensure_dir(os.path.dirname(storage_path))",src/scripts/save_prediction.py,yassersouri/omgh,1
"  with open(clf_fn, ""wb"") as f:
    pickle.dump(clf2, f)
    print ""classifier saved to file.""
  with open(clf_fn, ""rb"") as f:
    clf2 = pickle.load(f)
    print ""classifier loaded from file.""
    pred_labels2 = clf2.predict(test_feats)
    score2 = metrics.accuracy_score(test_labels, pred_labels2)
    print ""restored logistic classifier score: {}"".format(score2)

  clf3 = svm.SVC()
  clf3.fit(train_feats, train_labels)
  pred_labels3 = clf3.predict(test_feats)
  score3 = metrics.accuracy_score(test_labels, pred_labels3)
  print ""svm score: {}"".format(score3)

  return

  # train tf linear classifier; similar as finetuning the last layer.
  model_params = commons.ModelParams(",deepmodels/tf/samples/img_classifier.py,learningsociety/deepmodels,1
"
		if _S.shape[0]>1:

			_ST = np.ones((_S.shape[-1]))
			_BT = -1*np.ones((_B.shape[-1]))
			_X = np.concatenate((_S,_B),axis=1)
			_Y = np.concatenate((_ST,_BT),axis=0)
			_X=_X.transpose()

			print 'USING:',self.chosenkernel
			svm = SVC(C = float(self.chosencvalue), kernel = self.chosenkernel)
			svm.fit(_X,_Y)

			_S_TrainHist = svm.decision_function(_S.transpose()).transpose()[0]
			_B_TrainHist = svm.decision_function(_B.transpose()).transpose()[0]
			self.mainaxis2.hist(_S_TrainHist, 50,alpha=0.75,label=(self.infile.split('/')[-1].split('.')[0]),histtype='step')
			self.mainaxis2.hist(_B_TrainHist, 50,edgecolor='red',alpha=0.75,label=(self.infile2.split('/')[-1].split('.')[0]),histtype='step')

			self.mainaxis2.axvline(x=0,color='black',linestyle='--',alpha=0.2)
",dGlimpse.py,darinbaumgartel/dGlimpse,1
"def sigma_to_gamma(sigma):
    return 1.0/(sigma**2)

if __name__ == '__main__':
    data = sio.loadmat('ex6data1.mat')
    y = data['y'].astype(np.float64).ravel()
    X = data['X']
    visualize_boundary(X, y, None)

    C = 1
    lsvc = LinearSVC(C=C, tol=0.001)
    lsvc.fit(X, y)
    svc = SVC(C=C, tol=0.001, kernel='linear')
    svc.fit(X, y)
    visualize_boundary(X, y, {'SVM(linear kernel) C = {}'.format(C): svc,
                              'LinearSVC C = {}'.format(C): lsvc})

    C = 100
    lsvc = LinearSVC(C=C, tol=0.001)
    lsvc.fit(X, y)",ex6/ex6_sklearn.py,mlyundin/Machine-Learning,1
"    roi_score: float,
               p-value obtained of the association between y and x,
               obtained by permutation test

    """"""
    if k == 0:
        return 0
    x = X[:, atlas_labels == k]
    if do_classif:
        cv_scores = cross_val_score(
            SVC(kernel='linear'), x, y, cv=5, n_jobs=2,
            verbose=1)
        print k, x.shape, cv_scores.mean()
        roi_score = cv_scores.mean()

    elif do_rsa:
        stim_similarity = (y[np.newaxis, :] == y[:, np.newaxis]).astype(
            np.float)
        voxels_similarity = np.corrcoef(x)
        # extract lower triangular part of symmetric",rsa/script_haxby.py,bthirion/representational_similarity_analysis,1
"  v = DictVectorizer()
  input_vector = v.fit_transform(input_list)
  return (input_vector, target_list)

def classifyUsingDecisionTree(input_train, input_test, result_train, result_test):
  return None  


def classifyAfterFeatureSelectionUsingRandomForest(input_train, input_test, result_train, result_test):
  classifier = Pipeline([
  ('feature_selection', LinearSVC(penalty=""l1"",dual=False)),
  ('classification', RandomForestClassifier())
  ])
  classifier.fit(input_train.toarray(), result_train)
  score(classifier, input_test.toarray(), result_test, 'Pipeline: LinearSVC feature_selection -> RandomForestClassifier')

def classifyUsingNaiveBayes(input_train, input_test, result_train, result_test):
  classifier = MultinomialNB()
  classifier.fit(input_train, result_train)
  score(classifier, input_test, result_test, 'MultinomialNB')",src/classification.py,deepankgupta/cs229_project,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the t-score of each feature
        score = t_score.t_score(X, y)

        # rank features in descending order according to score
        idx = t_score.feature_ranking(score)
",PyFeaST/example/test_t_score.py,jundongl/PyFeaST,1
"            if drop_cols:
                ncols = training.shape[1]
                training.drop(drop_cols, axis=1, inplace=True)
                assert training.columns.shape[0] == ncols - len(drop_cols)

                ncols = testing.shape[1]
                testing.drop(drop_cols, axis=1, inplace=True)
                assert testing.columns.shape[0] == ncols - len(drop_cols)

            # fit SVM on interactome_train
            model = svm.SVC(kernel=kernel,
                            probability=True,
                            class_weight=class_weight)

            train_on = training.ix[
                :, ~training.columns.isin(['name', 'class'])]

            test_on = testing.ix[
                :, ~testing.columns.isin(['name', 'class'])]
",src/05-fit_predict.py,chendaniely/spring_2016_cs_5854-PathLinker,1
"# tfidf_transformer = TfidfTransformer(smooth_idf=False) #pake tf
# tfidf_transformer = TfidfTransformer(smooth_idf=True) #pake idf
tfidf_transformer = TfidfTransformer() #pake tfidf
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
# print X_train_tfidf
print X_train_counts.shape
print count_vect.vocabulary_.get(u'algorithm')

#text classification algorithm
clf = SGDClassifier().fit(X_train_tfidf, train_labels)
# clf = svm.SVC().fit(X_train_tfidf, train_labels)
# clf = svm.SVC(kernel='linear', probability=True, class_weight='auto').fit(X_train_tfidf, train_labels)


#ubah data test ke bentuk vector tfidf
X_new_counts = count_vect.transform(test_gotg)
X_new_tfidf = tfidf_transformer.transform(X_new_counts)

#prediksi data test
predicted = clf.predict(X_new_tfidf)",Hasil/experiment-supervised.py,gemmaan/moviesenal,1
"#
# Copyright (c) 2015 by Antonio Molina García-Retamero. All Rights Reserved.
#

from DataManager import DataManager
from sklearn import svm

clf = svm.SVC()
X = DataManager.getQuotesYahoo(""MSF"")[1]
Y = DataManager.byWiningDayTagging(X)

testX = X[int(len(X)*0.75):]
X = X[:int(len(X)*0.75)]

testY = Y[int(len(Y)*0.75):]
Y = Y[:int(len(Y)*0.75)]
",test1.py,aydevosotros/tradingMachine,1
"        self.training_val = []
        self.training_names = []
        self.training_dict = {}
        
        self.ml_methods_inter = {}
        self.training_inter = []
        self.training_val_inter = []
        self.training_names_inter = []
        self.training_dict_inter = {}
        
        self.ml_methods[""svm""] = svm.SVC(gamma=0.001)
        self.ml_methods[""knn""] = KNeighborsClassifier(fault_threshold+1)
        self.ml_methods[""dtc""] = DecisionTreeClassifier(max_depth=fault_threshold+2)
        
        self.ml_methods_inter[""svm""] = svm.SVC(gamma=0.001)
        self.ml_methods_inter[""knn""] = KNeighborsClassifier(intermittent_threshold+1)
        self.ml_methods_inter[""dtc""] = DecisionTreeClassifier(max_depth=intermittent_threshold+2)
        
        self.stat_names = 0
        self.stat_names_inter = 0",src/main/python/SystemHealthMonitoring/FaultClassifier/MachineLearning.py,siavooshpayandehazad/ScheduleAndDepend,1
"	with open(INFILE, ""r"") as inf:
		songs = json.load(inf)

	english_indices = np.where(np.array([song[""language""] for song in songs]) == ""en"")[0]
	english_songs = np.array(songs)[english_indices]

	labels = get_labels(english_songs)
	features = get_features(english_songs)
	train_data, test_data, train_labels, test_labels = split_dataset(features, labels, topic=0, test_size=0.2)

	svm = SVC()
	svm.fit(train_data, train_labels)

	predictions = svm.predict(test_data)
	precision, recall, _ = precision_recall_curve(test_labels, predictions)
	print(""Precision: {}"".format(precision))",classification/classify_topic.py,DeastinY/text-mining,1
"	for i in xrange(no_of_layers):
	
		trainX_kpca, testX_kpca = multi_KPCA(trainX, trainY, testX, param[i], k_type[i])
	
		#trainX, testX = select_features(trainX_kpca, testX_kpca, trainY)
		selector.fit(trainX_kpca, trainY)
		trainX = selector.transform(trainX_kpca)
		testX = selector.transform(testX_kpca)
		#parameters = {'n_neighbors' : list(np.arange(20)+1)}
		#clf = GridSearchCV(KNeighborsClassifier(weights='distance', n_jobs=-1), parameters)
		clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
		clf.fit(trainX, trainY)
		print('no. of SV = %d' %len(clf.support_))
		print('no of support vectors per class')
		print(clf.n_support_)

		pred = clf.predict(testX)
		print(accuracy_score(testY, pred))	

		print(trainX_kpca.shape)",UMKL/add_kernels.py,akhilpm/Masters-Project,1
"    X_submit, label_submit = read_data.read_feature(smpath, label=True)
    # Y_submit = model.predict(X_submit)
    Y_submit = label_error.transform_label(model.predict(X_submit), pmpath)
    f = open(outpath, 'w')
    f.write('Id,Prediction\n')
    for i in range(len(label_submit)):
        f.write(label_submit[i] + ',' + Y_submit[i] + '\n')
    f.close()

def train_experiment(X_train, Y_train, X_test, Y_test, epoch=20):
    # model = svm.SVC(kernel='poly', C=1E-2, gamma=1E-2, degree=2)
    # model = svm.LinearSVC(C=1E0)
    # model = linear_model.LogisticRegression()
    dims = [X_train.shape[1], 2000, 2000, 2000, np.max(Y_train)+1]
    param_grid = [
          {
              'Dims': [dims],
              'Eta': [3E-3, 1E-2, 3E-2, 1E-1], 
              'Drate': [0.9999, 0.99999, 0.99999],
              'Minrate': [0.2], ",codes/learn_test.py,AlphaLambdaMuPi/DLSpeech,1
"
    epochs_per_subj = int(sys.argv[5])
    num_subjs = int(sys.argv[6])
    # the following line is an example to leaving a subject out
    #vs = VoxelSelector(labels[0:204], epochs_per_subj, num_subjs-1, raw_data[0:204])
    # if using all subjects
    vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data)
    # if providing two masks, just append raw_data2 as the last input argument
    #vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data, raw_data2=raw_data2)
    # for cross validation, use SVM with precomputed kernel
    clf = svm.SVC(kernel='precomputed', shrinking=False, C=10)
    results = vs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        logger.info(
            'correlation-based voxel selection is done'
        )
        #print(results[0:100])
        mask_img = nib.load(mask_file)
        mask = mask_img.get_data().astype(np.bool)",examples/fcma/voxel_selection.py,IntelPNI/brainiak,1
"
# We learn the digits on the first half of the digits
data_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]


# Now predict the value of the digit on the second half:
data_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:]
#data_test = scaler.transform(data_test)

# Create a classifier: a support vector classifier
kernel_svm = svm.SVC(gamma=.2)
linear_svm = svm.LinearSVC()

# create pipeline from kernel approximation
# and linear svm
feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
feature_map_nystroem = Nystroem(gamma=.2, random_state=1)
fourier_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_fourier),
                                        (""svm"", svm.LinearSVC())])
",python/sklearn/examples/plot_kernel_approximation.py,seckcoder/lang-learn,1
"import numpy as np
from sklearn import svm

# Read the data
train = np.loadtxt(open(""train.csv"",""rb""), delimiter="","", skiprows=0)
trainLabels = np.loadtxt(open(""trainLabels.csv"",""rb""), delimiter="","", skiprows=0)
test = np.loadtxt(open(""test.csv"",""rb""), delimiter="","", skiprows=0)


X, y = train, trainLabels
s = svm.SVC()
s.fit(X, y)

predictions = s.predict(test)
np.savetxt(""fancySVMSubmission.csv"", predictions.astype(int), fmt='%d', delimiter="","")",svm_sklearn.py,ujjwalkarn/DataSciencePython,1
"    parameter settings to try as values, or a list of such
    dictionaries, in which case the grids spanned by each dictionary
    in the list are explored. This enables searching over any sequence
    of parameter settings.\
""""""
_grid_example = """"""\
>>> import dask_searchcv as dcv
>>> from sklearn import svm, datasets
>>> iris = datasets.load_iris()
>>> parameters = {'kernel': ['linear', 'rbf'], 'C': [1, 10]}
>>> svc = svm.SVC()
>>> clf = dcv.GridSearchCV(svc, parameters)
>>> clf.fit(iris.data, iris.target)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
GridSearchCV(cache_cv=..., cv=..., error_score=...,
        estimator=SVC(C=..., cache_size=..., class_weight=..., coef0=...,
                      decision_function_shape=..., degree=..., gamma=...,
                      kernel=..., max_iter=-1, probability=False,
                      random_state=..., shrinking=..., tol=...,
                      verbose=...),
        iid=..., n_jobs=..., param_grid=..., refit=..., return_train_score=...,",dask_searchcv/model_selection.py,jcrist/dask-searchcv,1
"          (0.0, -2.7),
          (1.3, 2.1)].T
Y = [0] * 8 + [1] * 8

fignum = 1

pl.figure(figsize=(18, 5))

for kernel in ('linear', 'poly', 'rbf'):
    # 分類器を訓練
    clf = svm.SVC(kernel=kernel, gamma=2)
    clf.fit(X, Y)

    pl.subplot(1, 3, fignum)

    cmap = ListedColormap(['red', 'blue'])

    # 訓練データをプロット
    pl.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=cmap)
",sklearn/plot_svm_kernels.py,TenninYan/Perceptron,1
"#pca, todo grid search parameters for PCA
n_comp = 50
print(""INFO: PCA, n_components=""+str(n_comp))
pca = PCA(n_components=n_comp,whiten=True,svd_solver='auto')
pca.fit(training_data)
traindata_pca = pca.transform(training_data)
test_pca = pca.transform(test_data)

#train model via grid search
print(""INFO: Grid search"")
gsc = GridSearchCV(estimator=svm.SVC(),
                param_grid={ 'C': [1, 2, 3, 4, 5],
                'kernel': [ 'linear', 'poly', 'rbf', 'sigmoid' ] },
                cv=5 )
gsc.fit(traindata_pca,training_target)

#print results
res = gsc.score(test_pca,test_target)
print(""%3f""%(res))
#result 0.714",scripts/detect_eggs.py,chickenpi/ChookEggCount,1
"    ## ================
    X, y = datasets.make_classification(n_samples=options.n_samples,
                                        n_features=options.n_features,
                                        n_informative=options.n_informative)

    ## 2) Build Workflow
    ## =================
    time_start = time.time()
    ## CV + Grid search of a pipeline with a nested grid search
    cls = Methods(*[Pipe(SelectKBest(k=k),
                         SVC(kernel=""linear"", C=C))
                    for C in C_values
                    for k in k_values])
    pipeline = CVBestSearchRefit(cls,
                                 n_folds=options.n_folds_nested,
                                 random_state=random_state)
    wf = Perms(CV(pipeline, n_folds=options.n_folds),
               n_perms=options.n_perms,
               permute=""y"",
               random_state=random_state)",examples/run_multi_processes.py,neurospin/pylearn-epac,1
"    ax.set_xlabel('gamma', fontsize = 'medium')
    ax.set_ylabel('AUC', fontsize = 'medium')
    
    return fig    

if __name__ == '__main__':    
    ## get data
    X, y, attribute_names, target_attribute_names = get_dataset(59)
    
    ## define classifier - rbf svm
    rbf_svm = SVC(kernel = 'rbf')
    
    start_time = time.time()
    ## grid search - gamma and C, grid_den = 20, time needed = 13.36s
    grid_den = 400
    param_grid = {'C': np.logspace(-15, 15, num = grid_den, base = 2.0),
                  'gamma': np.logspace(-15, 15, num = grid_den, base = 2.0)
                  }
    grid_search = GridSearchCV(rbf_svm, param_grid = param_grid, scoring = 'roc_auc',
                               cv = 10, pre_dispatch = '2*n_jobs', n_jobs = 4)",landscape_analysis.py,lidalei/DataMining,1
"def LDA(train_x, train_y, test_x, test_y):
    clf = LinearDiscriminantAnalysis()
    clf.fit(train_x,train_y)
    test_error = clf.score(test_x, test_y)
    test_auc = clf.predict_proba(test_x)
    return test_error, test_auc

def Lsvm_patatune(train_x,train_y):
    tuned_parameters = [
        {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000, 10000]}]
    clf = GridSearchCV(SVC(C=1, probability=True), tuned_parameters, cv=5, n_jobs=-1
                       )  # SVC(probability=True)#SVC(kernel=""linear"", probability=True)
    clf.fit(train_x, train_y)
    return clf.best_params_['C']


def Lsvm(c,train_x, train_y, test_x, test_y):
    clf = SVC(kernel=""linear"", C=c, probability=True)
    clf.fit(train_x,train_y)
    test_error = clf.score(test_x, test_y)",final20/WeightedMVdis20.py,hongliuuuu/Results_Dis,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",ProgramData/SystemFiles/Python/Lib/encodings/email/test/test_email.py,kenshay/ImageScripter,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",lib/rubyfox/server/data/lib/Lib/email/test/test_email.py,neopoly/rubyfox-server,1
"    model.fit(trndata,trnlabs)
    Atrndata = model.transform(trndata)
    Atstdata = model.transform(tstdata)
    costs = model.get_costs()
    
    model = pca.PCA(n_components=10)
    model.fit(trndata)
    Ptrndata = model.transform(trndata)
    Ptstdata = model.transform(tstdata)
            
    clf = svm.LinearSVC()
    clf.fit(Ptrndata,etrnlabs)
    pscore += clf.score(Ptstdata,etstlabs)
    
    clf = svm.LinearSVC()
    clf.fit(Atrndata,etrnlabs)
    nscore += clf.score(Atstdata,etstlabs)

print pscore/float(R)
print nscore/float(R)",src/emo_sd.py,shahmohit/pynca,1
"    from scipy.io import loadmat, savemat
    import cProfile
    X = np.random.multivariate_normal([0, 0], [[0.5, 0], [0, 0.5]], 50)
    y = -np.ones(X.shape[0])
    Xa = np.random.rand(5, 2)
    ya = np.ones(Xa.shape[0])
    Xtr, ytr = np.r_[X, Xa], np.r_[y, ya]
    # data = savemat(""syndata.mat"", mdict={'Xtr': Xtr, 'ytr': ytr})
    # data = loadmat('../datasets/syndata.mat')
    # Xtr, ytr = data['X'], data['Y']
    clf1 = H3iSVC(C=0.2, gamma=2, kernel='rbf', display=True, labeling=True)
    cProfile.run(""clf1.fit(Xtr)"")
    # from H3iSVC_QP import H3iSVC_QP as svcqp
    # clf2 = svcqp(C=0.05, gamma=1.5, kernel='rbf')
    # clf2.fit(Xtr)
    print '--- done! ---'
    plt.subplot(1, 1, 1)
    clf1.draw_model(Xtr)
    # # plt.subplot(1,2,2)
    # # clf2.draw_model(Xtr)",mlcore/models/H3iSVC.py,feuerchop/h3lib,1
"        lam = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        #if len(np.unique(y)) == 1:
        #    clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
        #              gamma=gamma, coef0=coef0, degree=degree)
        #    clf.fit(X)
        #else:
        #mysvm = svm.SVC(kernel=kernel_map[self.kernel.get()], C=1000,
        #                  gamma=gamma, coef0=coef0, degree=degree)
        #mysvm.fit(X, y)
        #l = 0.1;
        clf = komd.KOMD(lam=lam, Kf=kernel_map[self.kernel.get()], rbf_gamma=gamma, poly_deg=degree, poly_coeff=coef0)

        clf.fit(X,y)
        #print clf.gamma
        #global gamma, bias
        #gamma = clf.gamma",Python/komd_gui.py,jmikko/EasyMKL,1
"
    def get_data(self):
        return self.X_train, self.y_train, self.X_test, self.y_test


@experiment_runner.experiment(files_to_backup='..')
class SVMExperiment(experiment.Experiment):
    @experiment_runner.timeit
    def setup(self, config):
        self.X_train, self.y_train, self.X_test, self.y_test = config.get_data()
        self.clf = svm.SVC(C=config.svm_C)

    @experiment_runner.timeit
    def run(self):
        self.clf.fit(self.X_train, self.y_train)
        self.score = self.clf.score(self.X_test, self.y_test)
        warnings.warn('An example warning')

    def get_output(self):
        return {'accuracy': self.score}",examples/svm_experiment_example.py,erensezener/sicco,1
"from sklearn.grid_search import GridSearchCV
from sklearn import datasets, svm 
import numpy as np
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
svc = svm.SVC(C=1, kernel='linear')

Cs = np.logspace(-6, -1, 10)
clf = GridSearchCV(estimator=svc, param_grid = dict(C=Cs), n_jobs=-1)
clf.fit(X_digits[:1000], y_digits[:1000])
print(clf.best_score_)
print(clf.best_estimator_.C)",sklearnLearning/modelSelection/Grid-search.py,zhuango/python,1
"            self.assertTrue(os.path.isfile(file_name))

    def test_inspect_instance_returns_json_dict(self):
        metrics = self.clipper_inst.inspect_instance()
        self.assertEqual(type(metrics), dict)
        self.assertGreaterEqual(len(metrics), 1)

    def test_model_deploys_successfully(self):
        # Initialize a support vector classifier 
        # that will be deployed to a no-op container
        model_data = svm.SVC()
        container_name = ""clipper/noop-container""
        input_type = ""doubles""
        result = self.clipper_inst.deploy_model(
            self.deploy_model_name, self.deploy_model_version, model_data,
            container_name, input_type)
        self.assertTrue(result)
        model_info = self.clipper_inst.get_model_info(
            self.deploy_model_name, self.deploy_model_version)
        self.assertIsNotNone(model_info)",integration-tests/clipper_manager_tests.py,rmdort/clipper,1
"X = np.array(X, dtype = 'float32')
print(X.dtype)

transformer = random_projection.GaussianRandomProjection()
X_new = transformer.fit_transform(X)
print(X_new.dtype)

from sklearn import datasets
from sklearn.svm import SVC
iris = datasets.load_iris()
clf = SVC()
models = clf.fit(iris.data, iris.target)
print(models)
result = list(clf.predict(iris.data[:3]))
print(result)

models = clf.fit(iris.data, iris.target_names[iris.target])
print(models)
result = list(clf.predict(iris.data[:3]))",sklearnLearning/quickStart/typeCasting.py,zhuango/python,1
"##########################################################################
##  Test for Classification Report
##########################################################################

class ClassificationReportTests(VisualTestCase):

    def test_class_report(self):
        """"""
        Assert no errors occur during classification report integration
        """"""
        model = LinearSVC()
        model.fit(X,y)
        visualizer = ClassificationReport(model, classes=[""A"", ""B""])
        visualizer.score(X,y)
        self.assert_images_similar(visualizer)",tests/test_classifier/test_classification_report.py,pdamodaran/yellowbrick,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf, language='c').export()
print(output)

""""""
#include <stdlib.h>
#include <stdio.h>
#include <math.h>",examples/classifier/LinearSVC/c/basics.py,nok/sklearn-porter,1
"
        with open('ocr.gnb', 'wb') as pickle_file:
            pickle.dump(clf, pickle_file)

        statinfo = os.stat('ocr.gnb')
        print('size', statinfo.st_size)

    def test_svc(self):
        self.all_digits, self.all_numbers = self.removeInf()
        X_trn, X_tst, y_trn, y_tst = train_test_split(self.all_digits, self.all_numbers, test_size=0.2)
        clf = SVC()
        clf.fit(X_trn, y_trn)
        pred = clf.predict(X_tst)
        acc = accuracy_score(pred, y_tst)
        print('accuracy', '{:.2f}'.format(acc * 100.0), '%')

        with open('ocr.svm', 'wb') as pickle_file:
            pickle.dump(clf, pickle_file)

        statinfo = os.stat('ocr.svm')",python/Recognize/Train.py,spidgorny/energy-monitor,1
"step      = 5;
nsample   = 1000000;
interval  = total / step;
feats     = ['tfidf', 'meta', 'cnnfv', 'cnnvlad', 'idt', 'mfcc', 'osift', 'mosift'];
upath  = os.path.join(model_root, '/%s%d/final-U.dat');
vpath  = os.path.join(model_root, '/%s%d/final-V.dat');
featset1  = {'meta':0, 'tfidf':1, 'cnnfv':2, 'cnnvlad':3, 'idt':4, 'mfcc':5, 'osift':6, 'mosift':7};
featset2  = {'cnnfv':0, 'cnnvlad':1, 'idt':2, 'mfcc':3, 'osift':4, 'mosift':5};

def get_weights(trscores, uids, trvids, i):
    clf = svm.LinearSVC(C=0.01);
    usm = dict();
    x   = list();
    y   = list();
    for line in open(trpath%i):
        terms = line.strip().split(',');
        uid   = int(terms[0]);
        likes = list();
        for j in range(1, len(terms)):
            vid  = int(terms[j].split(':')[0]);",methods/sfusion.py,domainxz/top-k-rec,1
"from sklearn.feature_selection import RFE
import feature_selector
import raw_data_matrix_generator as rdmg
import stats_matrix_generator as smg
import path
import pandas as pd
import numpy as np
import os


clf = svm.SVC(kernel='linear', C=1)
# iris = load_iris()
# scores = cross_val_score(clf, iris.data, iris.target, cv=KFold(5))
# print ""Iris data shape: "", iris.data.shape
# print ""Iris target shape: "", iris.target.shape
#
# print scores
# print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

year_list = [2012, 2013]",MoneyMaker/cross_validation.py,yiskylee/MoneyMaker,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    svd = TruncatedSVD(n_components=2)
    X_reduced = svd.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_random_hasher_sparse_data():
    X, y = datasets.make_multilabel_classification(random_state=0)
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    X_transformed = hasher.fit_transform(X)
    X_transformed_sparse = hasher.fit_transform(csc_matrix(X))",venv/lib/python3.4/site-packages/sklearn/ensemble/tests/test_forest.py,valexandersaulys/airbnb_kaggle_contest,1
"
dataset_regr = ['concrete.tab', 'friedman.tab', 'housing.tab', 'laser.tab', 'mortgage.tab']  # 'abalone.tab'
dataset_regr = ['benchmark/regression/'+dataset for dataset in dataset_regr]

classifiers = ['NaiveBayesLearner()', 'LogisticRegressionLearner()', 'SVMLearner(probability=True)',
               'KNNLearner(n_neighbors=10)', 'KNNLearner(n_neighbors=30)',
               'RandomForestLearner(n_estimators=10)', 'RandomForestLearner(n_estimators=30)']
nc_class_str = [
    ""InverseProbability(%s)"" % m for m in classifiers] + [
    ""ProbabilityMargin(%s)"" % m for m in classifiers] + [
    ""SVMDistance(SVC())""] + [
    ""KNNDistance(Euclidean, %d)"" % k for k in [1, 10, 30]] + [
    ""KNNFraction(Euclidean, %d, weighted=%s)"" % (k, w) for k in [10, 30] for w in [False, True]] + [
    ""LOOClassNC(NaiveBayesLearner(), Euclidean, %d, relative=%s, include=%s, neighbourhood='%s')"" % (k, r, i, n)
    for k in [10, 30] for r in [False, True] for i in [False, True] for n in ['fixed', 'variable']] + [
    ""LOOClassNC(LogisticRegressionLearner(), Euclidean, %d, relative=%s, include=%s, neighbourhood='%s')"" % (k, r, i, n)
    for k in [10, 30] for r in [False, True] for i in [False, True] for n in ['fixed', 'variable']] + [
    ""LOOClassNC(RandomForestLearner(n_estimators=10), Euclidean, %d, relative=%s, include=%s, neighbourhood='%s')"" % (k, r, i, n)
    for k in [10, 30] for r in [False, True] for i in [False, True] for n in ['fixed', 'variable']]
",ConfPred/conformal-master/evaluate_nc.py,JonnaStalring/AZOrange,1
"def fit_ipw_svm(x, y, ts, weights, C):
	#convert to numpy arrays
	nX = np.array(x,ndmin=2)
	ntsX = np.array(ts,ndmin=2)
	nY = np.array(y,ndmin=1)/1.0
	#for tuning
	paramGrid = [
	{'C': C, 'kernel': ['linear']}
	]
	#train
	trnMdl = svm.SVC()
	trnMdl_grid = grid_search.GridSearchCV(trnMdl, paramGrid, fit_params={'sample_weight': weights})
	trnMdl_grid.fit(X=nX, y=nY)
	trnMdl_best = trnMdl_grid.best_estimator_
	#return SVM weights and predictions
	trnMdl_rho = trnMdl_best.intercept_[0:1]
	trnMdl_rho = list(trnMdl_rho)
	trnMdl_w = trnMdl_best.coef_[0,:]
	trnMdl_w = list(trnMdl_w)
	yhats = trnMdl_best.predict(ntsX)",fit_svm.py,kalinn/IPW-SVM,1
"    featuresets = []  # list of tuples of the form (post, features)
    for post in posts:  # applying the feature extractor to each post
        # post.get('class') is the label of the current post
        featuresets.append((dialogue_act_features(post.text), cls_set.index(post.get('class'))))

    shuffle(featuresets)
    size = int(len(featuresets) * .1)  # 10% is used for the test set
    train = featuresets[size:]
    test = featuresets[:size]
    # SVM with a Linear Kernel and default parameters
    clf = SklearnClassifier(LinearSVC())
    clf.train(train)
    test_skl = []
    t_test_skl = []
    for d in test:
        test_skl.append(d[0])
        t_test_skl.append(d[1])
    # run the classifier on the train test
    print t_test_skl
",nps_chat/nps_chat.py,ourren/scikit_note,1
"            X_tr = X_tr[subsam, :]
            y_tr = y_tr[subsam]
        if binarize:
            X_tr.data = numpy.ones_like(X_tr.data)
        X_tr = X_tr.toarray()

        # Train classifier
        if classifier == 'RF':
            clf = RFC(n_estimators=200, n_jobs=1 if subsample else -1)
        elif classifier == 'SVM':
            clf = SVC(kernel='rbf', gamma=0.0025, C=12)
        sample_weight = None
        print('Training set size: {}'.format(X_tr.shape))
        clf.fit(X_tr, y_tr, sample_weight=sample_weight)
        tr_n_feats = X_tr.shape[1]
        del X_tr

        # Load and classify test data
        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")",src/experiment.py,srndic/hidost-reproduction,1
"    stacking : classifier, optional (default=None).
        Feed output of weighted individual predictions into another classifier. 
        Suggested model: LogisticRegression. 
        
    
    verbose : bool, optional (default = False).
        Print diagnostic output. 
    """"""

    def __init__(self, 
            base_model = LinearSVC(), 
            num_models = 50, 
            bagging_percent=0.5, 
            bagging_replacement=True, 
            feature_subset_percent = 1.0, 
            weighting = None, 
            stacking_model = None,
            randomize_params = {}, 
            verbose=False):
                ",treelearn/classifier_ensemble.py,capitalk/treelearn,1
"    Lidx = idx[:90]
    Tidx = idx[90:]

    # Define the learn and the test sets
    X_learn = X[Lidx]
    y_learn = y[Lidx]
    X_test = X[Tidx]
    y_test = y[Tidx]

    # Fit data.
    clf = svm.SVC()
    clf.fit(X_learn, y_learn)

    # Test all data.
    predictions = clf.predict(X_test)

    # Create confusion matrix.
    cm = np.zeros((3, 3))
    for prediction, truth in zip(predictions, y_test):
        cm[truth, prediction] += 1",wk6/iris.py,Timvanz/uva_statistisch_redeneren,1
"        predictions = clf.predict(xapp[mask, :])
        errors[i] = np.mean(predictions != yapp[mask])
    # Learning with all the data.
    clf.fit(xapp, yapp)
    return np.mean(errors), clf


def cross_validation_svm(xapp, yapp, N=5, C_values=[0.01, 0.05, 0.1, 0.5, 1]):
    mean_errors = 1
    for C in C_values:
        clf = svm.SVC(kernel='linear', C=C)
        errors = np.zeros(N)
        for i in range(N):
            mask = np.zeros(xapp.shape[0], dtype=bool)
            mask[np.arange(i, mask.size, N)] = True
            clf.fit(xapp[~mask, :], yapp[~mask])
            predictions = clf.predict(xapp[mask, :])
            errors[i] = np.mean(predictions != yapp[mask])
        e = np.mean(errors)
        if e < mean_errors:",learning_phase.py,jjerphan/SY32FacialRecognition,1
"
    return mat, y, pairs




    C = 1
    m = p.n_levels
    matr,rv, pairs = gen_ranking_matrix(train_k,R,m)
    print()
    svm = SVC(C=C, kernel = 'precomputed',verbose=True)
    svm.fit(matr,rv)
    print()
    train_g = np.empty(l)
    for i in range(l):
        train_g[i] = np.sum(svm.dual_coef_*(train_k[pairs[svm.support_,0],i] -
                                            train_k[pairs[svm.support_,1],i]))
    R_hat = np.empty(l)
    for i in range(l):
        R_hat[i] = np.sum(train_g[i] > train_g)/l",extracode.py,gregstarr/anomaly-detection,1
"                                lambAccuracy = []
                                for lamb, i in enumerate(regFactors):
                                    LDAOutput = test_std_aug.dot(weights_all[:, :, i])
                                    pred_std = ((LDAOutput[:, 1] - LDAOutput[:, 0]) > 0) * 1
                                    lambAccuracy.append(np.mean(pred_std == y_test_std))
                                trialBlockAccuracy.append(lambAccuracy)
                                '''

                            elif(clfScheme == clfSchemes[2]):
                                # Train and test Support Vector Machine algorithm
                                clf_std = SVC(cache_size=2048)
                                '''
                                ###############################################
                                # Grid Search
                                param_grid = [{'kernel': ['linear'], 'C': C_range}]
                                #cv = StratifiedKFold(y=y_train_std, n_folds=3)
                                gridSearch = GridSearchCV(clf_std, param_grid=param_grid,
                                                    pre_dispatch=n_jobs)
                                gridSearch.fit(train_std, y_train_std)
                                #gridSearch.fit(train_std, y_train_std)",Wronkiewicz_JNE_2015/pooledBCI_Sens.py,drammock/LABSN-pubs,1
"

#print('Creating mesh to visual predictions...')
# reduce the number of data we actually plot
#features, targets = features[:500], targets[:500]
features = scale(features) # standardize the data
""""""
v_pca = PCA(n_components = 2) #transform array from 784 dim -> 2 dim
v_features = v_pca.fit_transform(features)
# create a classifier on only 2 dim for Visualization purposes
v_clf = SVC(kernel = 'rbf')
v_clf.fit(v_features, targets)

#import pdb; pdb.set_trace()

#creating mesh to visual predictions
h = .1 # step size on the graph
x_min, x_max = v_features[:, 0].min() - 1, v_features[:,0].max()+1 #creation of boundary points
y_min, y_max = v_features[:,1].min()-1, v_features[:,1].max()+1
xx, yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))",main.py,RakshakTalwar/mnist_htine,1
"from sklearn import datasets
from sklearn import svm

iris = datasets.load_iris()

X = iris.data[:, :2]
y = iris.target

# Training the model
clf = svm.SVC(kernel='rbf')
clf.fit(X, y)

# Doing predictions
new_data = [[4.85, 3.1], [5.61, 3.02]]",11_svm.py,halflings/python-data-workshop,1
"	kpca_train = arc_cosine(trainX[0:1000], trainX[0:1000])
	kpca.fit(kpca_train)

	kernel_train = arc_cosine(trainX, trainX[0:1000])
	kernel_test = arc_cosine(testX, trainX[0:1000])

	trainX_kpca = kpca.transform(kernel_train)
	testX_kpca = kpca.transform(kernel_test)
	print testX_kpca.shape

	#clf = svm.SVC(kernel=kernel.arc_cosine)
	clf = KNeighborsClassifier(19)
	clf.fit(trainX_kpca, trainY)

	pred = clf.predict(testX_kpca)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",kpcaWithTreeFS/mnistBackRandom/mnistRAND_withoutFS.py,akhilpm/Masters-Project,1
"		X = train_data[:, :2]
		print(X)
		y = train_data[:, 2:]
		C = 0.8
		# fit LinearSVC

		# multi label binarizer to convert iterable of iterables into processing format
		mlb = MultiLabelBinarizer()
		y_enc = mlb.fit_transform(y)

		# train_vector = svm.LinearSVC(C=C)
		train_vector = OneVsRestClassifier(svm.SVC(probability=True))
		# train_vector = OneVsRestClassifier(MultinomialNB())
		# train_vector = svm.SVC(kernel='linear')
		classifier_rbf = train_vector.fit(X, y_enc)
		# todo use pickle to persist
		test_vector_reshaped = np.array(test_vector.ravel()).reshape((1, -1))
		prediction = classifier_rbf.predict(test_vector_reshaped)

		print(""Predicted usernames: \n"")",src/backup-predict.py,rajikaimal/emma,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also",sklearn/svm/classes.py,ngoix/OCRF,1
"
xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
                     np.linspace(-3, 3, 500))
np.random.seed(0)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)
print(X)
print(Y)

# fit the model
clf = svm.NuSVC()
clf.fit(X, Y)

# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

pl.imshow(Z, interpolation='nearest',
            extent=(xx.min(), xx.max(), yy.min(), yy.max()),
            aspect='auto', origin='lower', cmap=pl.cm.PuOr_r)",kinect/pySVM/test/nonLinearSVC.py,hackliff/domobot,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ls_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeast/example/test_ls_l21.py,jundongl/scikit-feast,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/fitness_dses_dsessvcal/setup.py,jpzk/evopy,1
"
#Setting up scaler for standardisation
from sklearn import preprocessing
scaler = preprocessing.StandardScaler();


# Training SVM
from sklearn import svm
from sklearn import linear_model
print ""Declaring SVM""
#clf = svm.LinearSVC(); # linearsvc1
clf = svm.LinearSVC(C=100.0, class_weight='auto', penalty='l1', dual=0); # linearsvc2
#clf = svm.SVC(cache_size = 1000, class_weight='auto', kernel = 'poly'); # Predicts all as POSITIVE :((
#clf = linear_model.SGDClassifier();  # not tried yet
print ""standardising training data""
Xfeatures = scaler.fit_transform(Xfeatures, YLabels);
print ""Fitting Data To SVM""
clf.fit(Xfeatures, YLabels);
print ""SVM trained""
",Preliminary experimentation/code/BigramSVMIgnoreUNK.py,prernaa/NLPCourseProj,1
"        bnb = BernoulliNB(**kwargs)
        return bnb

    elif modelType == 'randomForest':
        from sklearn.ensemble import RandomForestClassifier
        rfc = RandomForestClassifier(random_state=234, **kwargs)
        return rfc

    elif modelType == 'svm':
        from sklearn.svm import SVC
        svc = SVC(random_state=0, probability=True, **kwargs)
        return svc

    elif modelType == 'LinearRegression':
        #assert column, ""Column name required for building a linear model""
        #assert dataframe[column].shape == target.shape
        from sklearn import linear_model
        l_reg = linear_model.LinearRegression(**kwargs)
        return l_reg
",datascienceutils/utils.py,greytip/data-science-utils,1
"            
    clf = MultinomialNB()    
    

#    estimators = [('normalizer', preprocessing.Normalizer(norm='l2')),     
#                  ('pca', sklearn.decomposition.TruncatedSVD(n_components=30)),
#                  ('svm', sklearn.svm.LinearSVC(C=130, class_weight='auto'))]
#    clf = Pipeline(estimators)    
    
#    estimators = [('normalizer', TfidfTransformer(norm='l2', use_idf=True)),
#                  ('svm', sklearn.svm.SVC(kernel='poly', degree=1, C=27825, 
#                                          class_weight='auto'))]
#    
#    clf = Pipeline(estimators)
#    
#    estimators = [('normalizer', TfidfTransformer(norm='l1', use_idf=True)),
#                  ('svm', sklearn.svm.LinearSVC(C=130, class_weight='auto'))]
#    clf = Pipeline(estimators)
    
#    tuned_parameters = {'svm__C': np.logspace(0,3,10)}",learning/old/test1_cv.py,fcchou/CS229-project,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",jyhton/Lib/email/test/test_email.py,phalax4/CarnotKE,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_12_2014_server.py,magic2du/contact_matrix,1
"        [102..110]
        [148..169]
        [211..end]
    """"""
    p1 = list(range(5,102))
    p2 = list(range(111,148))
    p3 = list(range(170,211))
    Mp = M[:,:,p1+p2+p3]
    return Mp

def test_SVC(data, result_path):
    r = util.ROIs(data.shape[0], data.shape[1])
    r.add('Alfalfa', {'poly': ((67,98),(73,98),(75,101),(70,101))})
    r.add('Corn-notill', {'rec': (33,31,41,56)})
    r.add('Corn-min', {'rec': (63,6,71,21)}, {'rec': (128,20,134,46)})
    r.add('Corn', {'poly': ((35,7),(35,5),(48,10),(48,23),(45,22),(44,16),(35,10),(35,5))})
    r.add('Grass/Pasture', {'rec': (75,4,85,21)})
    r.add('Grass/Trees', {'rec': (48,28,70,35)})
    r.add('Grass/pasture-mowed', {'rec': (73,109,78,112)})
    r.add('Hay-windrowed', {'rec': (39,124,59,138)})",pysptools/tests/test_SVC.py,ctherien/pysptools,1
"        label.set_fontname('Arial')
        label.set_fontsize(fontsize)
    
    cm.savefig('../figures/clustermap.pdf')

def loo(X, labels):
    label_encoder = LabelEncoder()
    int_labels = label_encoder.fit_transform(labels)
    print(int_labels)

    clf = SVC(kernel='linear')#, probability=True)
    nb = X.shape[0]
    loo = LeaveOneOut(nb)

    silver, gold = [], []
    for train, test in loo:
        print('.')
        X_train, X_test = X[train], X[test]
        y_test = [int_labels[i] for i in test]
        y_train = [int_labels[i] for i in train]",src/letter_analysis.py,mikekestemont/grimm,1
"
 



def plotting(X, Y, Xt, Yt, labelx, labely, outputfile):
    h = .02  # step size in the mesh
    classifiers = dict(
    knn=neighbors.KNeighborsClassifier(4),
    logistic=linear_model.LogisticRegression(C=1e4, penalty=l1),
    svm=svm.SVC(C=1e4),
    svmlinear=svm.LinearSVC(C=1e4),
    adaboost=ensemble.AdaBoostClassifier(),
    naivebay=naive_bayes.GaussianNB())

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    fignum = 1
    # we create an instance of Neighbours Classifier and fit the data.",MLNet-2.0/plotting/lr-knn-svm-comparison/plot_no_out_no_zero.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"
class NBClassifier(Classifier, ProbClassifier):

    def __init__(self):
        self.cl = MultinomialNB()


class SVMClassifier(Classifier):

    def __init__(self):
        self.cl = svm.SVC()


class OneClassClassifier(Classifier):

    def __init__(self, nu=0.5):
        self.cl = svm.OneClassSVM(nu=nu)

    def retrain(self, vectorFeature, vectorTarget):
        assert(vectorFeature.shape[0] == len(vectorTarget))",src/CipCipPy/classification/__init__.py,giacbrd/CipCipPy,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIFS.mifs(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_MIFS.py,jundongl/PyFeaST,1
"        y_arr = []
        for l in range(len(with_attr[n])):
            X_arr.append(with_attr[n][l])
            y_arr.append(True)
        for l in range(len(without_attr[n])):
            X_arr.append(without_attr[n][l])
            y_arr.append(False)
        X = np.array(X_arr)
        y = np.array(y_arr)
        # svc = svm.LinearSVC(C=C, class_weight=""balanced"").fit(X, y)
        svc = svm.LinearSVC(C=C).fit(X, y)
        # get the separating hyperplane
        w = svc.coef_[0]
        # print(w)

        #FIXME: this is a scaling hack.
        m1 = np.mean(with_attr[n],axis=0)
        m2 = np.mean(without_attr[n],axis=0)
        mean_vector = m1 - m2
        mean_length = np.linalg.norm(mean_vector)",plat/bin/atvec.py,dribnet/plat,1
"        mus,_ = v.encode(traindata[j,:].reshape(1,DIM))
        z_train[k,:]=mus
        k=k+1
    k=0
    for j in testidx:
        mus,_ = v.encode(traindata[j,:].reshape(1,DIM))
        z_test[k,:]=mus
        k=k+1
        
    # Train multiclass SVC
    clf = svm.SVC(decision_function_shape='ovr')
    clf.fit(z_train, trainlabels[trainidx]) 
    p=clf.predict(z_test[testidx])
        




",challenge/challenge3_unsup.py,juanka1331/VAN-applied-to-Nifti-images,1
"np.random.seed(1)

# Combine Earth with LogisticRegression in a pipeline to do classification
earth_classifier = Pipeline([('earth', Earth(max_degree=3, penalty=1.5)),
                             ('logistic', LogisticRegression())])

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA"", ""Earth""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA(),
    earth_classifier]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",examples/plot_classifier_comp.py,DucQuang1/py-earth,1
"    if classifier == ""NN"":
       clf = MLPClassifier(hidden_layer_sizes=(50), activation='relu', solver='sgd', alpha=1e-2, random_state=None)   
    elif classifier == ""LR"":
        clf = linear_model.LogisticRegression(C=1e3)
        #clf = tree.DecisionTreeClassifier()
    if classifier == ""RF"":
        clf = RandomForestClassifier()
    elif classifier == ""NB"":
        clf = GaussianNB()
    elif classifier == ""SVM"":
        clf = LinearSVC()
    elif classifier == ""KNN"":
        clf = NearestCentroid()
    
    skf = StratifiedKFold(n_splits=nfold, shuffle=True)
    y_test_total = []
    y_pred_total = []

    for train_index, test_index in skf.split(document_term_matrix, labels):
        X_train, X_test = document_term_matrix[train_index], document_term_matrix[test_index]",classification.py,bahmanh/DocumentClassification,1
"		return data, repins

	idx = random.sample(range(len(data)), size)
	return (data[idx], repins[idx])


def grid_search(data, repins) :

	x, y = to_classification(data, repins, 0.8)

	_svm_c = SVC()  
	_svm_p = [{'kernel':['linear'], 'C':[0.1, 1, 10]},
			 			{'kernel':['rbf'], 'C':[0.1, 1, 10], 'gamma':[0.1, 0.001, 0.0001]}]

	# Random forest
	_forest_c = ExtraTreesClassifier(criterion='gini')
	_forest_p = [{
							'n_estimators' : range(200, 600, 20),
#							'n_estimators' : [200,400],
							'bootstrap' : [True, False],",analysis/prediction.py,luamct/WebSci14,1
"            labels.append(1)
        for k in range(noNeg):
            labels.append(0)

        #Create numpy Array for word frequencies : Feature Vector
        trainFreqArr = np.array(featList)
        trainLabels = np.array(labels)


        #Fit SVM
        # docClassifier = svm.SVC( C=1000)
        self.docClassifier = svm.LinearSVC()
        self.docClassifier.fit(trainFreqArr, trainLabels) 


    def getFeat(self, line):
        listItem = [0]*self.noFeat
        fileFreqDist = nltk.FreqDist(SVM.tokenize(line))

        i = 0",data/stackoverflow/csharp/title_filtering/SVM.py,sriniiyer/codenn,1
"    assert_equal(pipe.get_params(deep=True),
                 dict(svc__a=None, svc__b=None, svc=clf))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)
    # Smoke test the repr:
    repr(pipe)
",sklearn/tests/test_pipeline.py,B3AU/waveTree,1
"# function to be mapped over all paired folds
def trainAndValidate(folds):
    X_train = getFoldFeatures(folds, 0);
    y_train = getFoldTargets(folds, 0);
    assert(len((X_train)==len(y_train)))
    
    X_test = getFoldFeatures(folds, 1);
    y_test = getFoldTargets(folds, 1);
    assert(len((X_test)==len(y_test)))

    svc = SVC(kernel='linear').fit(X_train[:,featureCols], y_train);
    decision = svc.decision_function(X_test[:,featureCols])
    
    # X_test[:,0] corresponds to CADD scores alone
    return ({'y': y_test, 'decision': list(decision)}, roc_auc_score(y_test, decision), roc_auc_score(y_test, X_test[:,0]))
    

if __name__ == '__main__':
    p = Pool(n_cores)
",CLASSIC.py,QuantCare/CLASSIC,1
"				trainX = np.vstack((trainX,failDataArray))
				trainY = np.hstack((trainY,newGenArrayY))	
			print trainX.shape,trainY.shape

	#0.4 0.0001 0.01 0.50 0.78 0.61
	#1 0.0001 0.01 0.57037 0.78 0.66
	#10 0.0001 0.01 0.66851 0.82313 0.7378
	#100 0.0001 0.01 0.69 0.23 0.35
	#10 0.001 0.01 0.48 0.86 0.62
	# clf=svm.SVC(C=10, cache_size=2000, decision_function_shape='ovr', gamma=0.0001, kernel='rbf', class_weight='balanced' ,tol=0.01)   
	clf=svm.LinearSVC(penalty='l1', tol=0.0001, C=1, dual=False, fit_intercept=True,
	intercept_scaling=1, class_weight='balanced',  max_iter=1000)
	clf=clf.fit(trainX,trainY)
	testingX=temLogNumPerTW[traingSetSize:]
	testingY=labelTWL[traingSetSize:]
	# testingX = trainX
	# testingY = trainY
	prediction=list(clf.predict(testingX))
	
	if len(prediction)!=len(testingY):",SVM/slidingWindow_SVM.py,cuhk-cse/loglizer,1
"        trainFilepaths, trainClassLabels, trainDistLabels = utils.readlistfile(trainListFile,labelling);
        trainFeatures =  utils.readfeatures(curSupervecPath, trainFilepaths);
        
        testListFile = os.path.join(curListPath, ""testset_"" + str(fold) + "".lst"");
        testFilepaths, testClassLabels, testDistLabels = utils.readlistfile(testListFile,labelling);
        testFeatures =  utils.readfeatures(curSupervecPath, testFilepaths);
        
        scaler = preprocessing.MinMaxScaler(feature_range=(-1,1));
        scaler.fit(trainFeatures);

        svm = SVC(C=C, kernel='rbf', gamma=gamma);
        svm.fit(scaler.transform(trainFeatures), trainClassLabels);
        predLabels = svm.predict(scaler.transform(testFeatures)); #in ogni testlist ci sono 72 elementi quindi fiacvcio un totale di 72 x 4 = 288 test 
        scores[fold-1] = f1_score(testClassLabels, predLabels);
        for index in range(len(testFeatures)):
            if label[testClassLabels[index]] != label[predLabels[index]]:
                print(str(testFilepaths[index])+""-->""+str(label[predLabels[index]]));
           
                
        allPredLabels.extend(predLabels);",Supervectors/test.py,vespero89/Snoring_Challenge,1
"     LogisticRegression()),
    ('Random Forest',
     GridSearchCV(RandomForestClassifier(n_jobs=options.n_jobs),
                  {n_estimators: [10, 50, 100, 500]}, scoring=options.scoring,
                  n_jobs=options.n_jobs, cv=options.cv)),
    ('Extremely Randomized Trees',
     GridSearchCV(ExtraTreesClassifier(n_jobs=options.n_jobs),
                  {n_estimators: [10, 50, 100, 500]}, scoring=options.scoring,
                  n_jobs=options.n_jobs, cv=options.cv)),
    ('Linear SVC',
     SVC(kernel='linear', C=1., probability=True, random_state=0))]
  for name, classifier in classifiers:
    for i in [0, 3, 7, 10]:
      if i > 0:
        pca = PCA(n_components=i).fit(stars.data).transform(stars.data)
        print(name, ""using"", i, ""Principal Components"")
        test(pca, stars.target, stars.target_names, classifier)
      else:
        print(name, ""using raw lightcurves"")
        test(stars.data, stars.target, stars.target_names, classifier)",src/learn.py,astroswego/supervised-classification,1
"
#---------------------------------------------------------------
#very ugly way to bring vectors to the right shape for SVC fit()
a = []
for x in resize_set:
    a.append(x.tolist())
#----------------------------------------------------------------
X = a               #reshaped images (training)
y = resize_labels   #labels

clf = svm.SVC(gamma=1.0)  #load SVC
clf.fit(X, y)               #fit SVC

#-------------------------------------------------------------------
#very ugly way to bring vectors to the right shape for SVC predict()
a = []
for x in resize_test_set:
    a.append(x.tolist())
#-------------------------------------------------------------------    
",Meeting 6/SVM.py,jdnz/qml-rg,1
"    # parameters = {'kernel': ['rbf'],
    #               'C': [2 ** x for x in np.arange(-12, 12, 0.5)],
    #               'gamma': [2 ** x for x in np.arange(-12, 12, 0.5)],
    #               'class_weight': [{0: 0.1, 1: 0.9}]}

    parameters = {'kernel': ['rbf'],
                  'C': [2 ** x for x in np.arange(-12, 12, 0.5)],
                  'gamma': [2 ** x for x in np.arange(-12, 12, 0.5)],
                  'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)]}

    svc = svm.SVC(probability=True, verbose=False, cache_size=2000)

    # if args.scorer == 'f1':
    #     scorer = f1Bias_scorer_CV
    # else:
    scorer = Twobias_scorer_CV

    if args.whichsearch == 'grid':
        clf = ModifiedGridSearchCV(svc, parameters, cv=lkf, n_jobs=-1, scoring=scorer, verbose=1, iid=False)
    else:",puffMarker.py,MD2Korg/cStress-model,1
"            prediction = np.vstack((prediction, pred))
        else:
            pred1, pred2 = srm.submit()
            prediction = np.vstack((prediction, pred1, pred2))
    p_train = prediction.T[train_index]
    p_test = prediction.T[test_index]
    print p_train.shape, p_test.shape
##    prediction = prediction.astype(float) / 7
##    prediction = np.array(map(lambda x: round(x), prediction)).astype(int)
##    prediction = np.array(map(lambda x: max(min(x, 4), 1), prediction)).astype(int)
##    clf = SVC()
##    clf.fit(p_train, y_train)
##    output = clf.predict(p_test)
    output = map(lambda x: x.mean(), p_test)
    output = np.array(map(lambda x: round(x), output)).astype(int)
    output = np.array(map(lambda x: max(min(x, 4), 1), output)).astype(int)
    loss = quadratic_weighted_kappa(y_test, output)
    print loss
    return loss
",src/CrossValidation.py,JimingAndYuqi/secret,1
"print(X[1])
min_max_scaler = preprocessing.MinMaxScaler()
X_Scaled_Feature_Vecs = min_max_scaler.fit_transform(preprocessing.normalize(X))

print(X_Scaled_Feature_Vecs[5])

C_range = 10.0 ** np.arange(-3, 3)
gamma_range = [0.0]
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedKFold(y=Y_Train, n_folds=10)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X_Scaled_Feature_Vecs, Y_Train)
store = open(""Result.txt"",""w"")
store.write(str(grid.best_estimator_))
store.write(""\n""+str(grid.best_score_))
store.close()
save_obj(grid,""TestGrid"")
print(str(grid.best_estimator_)+""\n""+str(grid.best_score_))",OLD/SVM OLD/test.py,tpsatish95/Youtube-Comedy-Comparison,1
"        model_FreesoundDb_CBOW = Word2Vec(X_FreesoundDb, size=size_w2v, window = 1000, min_count = 5, workers = 4, sg = 0)
#        model_UrbanSound8K_SG = Word2Vec(X, size=size_w2v, window = 1000, min_count = 5, workers = 4, sg = 1)
#        model_FreesoundDb_SG = Word2Vec(X_FreesoundDb, size=size_w2v, window = 1000, min_count = 5, workers = 4, sg = 1)
        
        w2v_UrbanSound8K_CBOW = {w: vec for w, vec in zip(model_UrbanSound8K_CBOW.index2word, model_UrbanSound8K_CBOW.syn0)}
        w2v_FreesoundDb_CBOW = {w: vec for w, vec in zip(model_FreesoundDb_CBOW.index2word, model_FreesoundDb_CBOW.syn0)}
#        w2v_UrbanSound8K_SG = {w: vec for w, vec in zip(model_UrbanSound8K_SG.index2word, model_UrbanSound8K_SG.syn0)}
#        w2v_FreesoundDb_SG = {w: vec for w, vec in zip(model_FreesoundDb_SG.index2word, model_FreesoundDb_SG.syn0)}        

        # Model definition
        svc = Pipeline([(""count_vectorizer"", CountVectorizer(analyzer=lambda x: x)), (""linear svc"", SVC(kernel=""linear""))])
        svc_tfidf = Pipeline([(""tfidf_vectorizer"", TfidfVectorizer(analyzer=lambda x: x)), (""linear svc"", SVC(kernel=""linear""))])
        svc_w2v_UrbanSound8K_CBOW = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v_UrbanSound8K_CBOW)), 
                                (""linear svc"", SVC(kernel=""linear""))])
        svc_w2v_tfidf_UrbanSound8K_CBOW = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v_UrbanSound8K_CBOW)), 
                                (""linear svc"", SVC(kernel=""linear""))])
        svc_w2v_FreesoundDb_CBOW = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v_FreesoundDb_CBOW)), 
                                (""linear svc"", SVC(kernel=""linear""))])
        svc_w2v_tfidf_FreesoundDb_CBOW = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v_FreesoundDb_CBOW)), 
                                (""linear svc"", SVC(kernel=""linear""))])",script_sound_classification_w2v.py,xavierfav/freesound-python,1
"        if classifier is not None:
            self.clf = classifier

            from sklearn.svm import LinearSVC
            import random
            if isinstance(self.clf, LinearSVC):
                self.clf.set_params().random_state = random.randint(0, 200)
        else:
            if clf_method == 'SVM':
                from sklearn import svm
                self.clf = svm.SVC()
            elif clf_method == 'ERF':
                from sklearn.ensemble import ExtraTreesClassifier
                self.clf = ExtraTreesClassifier(n_estimators=100,
                                                max_depth=None, min_samples_split=1,
                                                random_state=0)
            elif clf_method == 'GBC':
                from sklearn.ensemble import GradientBoostingClassifier
                self.clf = GradientBoostingClassifier(n_estimators=100,
                                                      max_depth=1)",neurosynth/analysis/classify.py,chrisfilo/Neurosynth,1
"# Code source: Gaël Varoquaux
# Adaptations by Joaquin Vanschoren

def plot_svm_linear():
    # we create 40 separable points
    np.random.seed(0)
    X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
    Y = [0] * 20 + [1] * 20

    # fit the model
    clf = svm.SVC(kernel='linear')
    clf.fit(X, Y)

    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = a * xx - (clf.intercept_[0]) / w[1]

    # plot the parallels to the separating hyperplane that pass through the",mglearn/plot_svm.py,JoostVisser/ml-assignment2,1
"
survival = survival.astype(bool)

# Split data into test and train datasets
#exp_train,exp_test,surv_train,surv_test = train_test_split(expression.values[1:,:],
#                                                           survival,
#                                                           train_size=0.8)

exp_vals = expression.values[1:,:]
modelPipeline = Pipeline(steps=[
    ('feature_selection',LinearSVC(penalty=""l1"",dual=False)),
    ('classification', RandomForestClassifier())        
])
#modelPipeline.fit(exp_vals,survival.ravel())


scores = cross_validation.cross_val_score(modelPipeline,exp_vals,survival.ravel(),cv=5,scoring='roc_auc')


print ""RAW ROC AUC VALUES: "",scores",python/scikitlearn/survivaltest/scripts/kaggleinspired_nowmitAttributeSel.py,jdurbin/sandbox,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_forward.svm_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeast/example/test_svm_forward.py,jundongl/scikit-feast,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 15, 2)],
        cv_method = KFold(20, 5))

    meta_model = RSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[5.0, 5.0]]),
        sigma = 1.0,
        beta = 0.8,
        meta_model = meta_model)

    return method
",evopy/examples/problems/TR/CMAESRSVC.py,jpzk/evopy,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MRMR.mrmr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_MRMR.py,jundongl/scikit-feast,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_09_2015_01.py,magic2du/contact_matrix,1
"       print ""Exceptions:"",seq_record.id    
    j=-1
    for nuc in tetranucleotides:
        j+=1
        referencedata[j,i]=dnaseq.count(nuc) # Calculating tetranucleotide frequency
referencedata=np.transpose(referencedata)
print ""Reference data is:"",referencedata
# Machine learning starts

# Training
clf = svm.SVC(gamma=0.0003, C=1.2)
#clf = svm.SVC(gamma=0.0010, C=2)
clf.fit(querydata, querynames)
referencepredict2=clf.predict(referencedata)
print len(referencenames),len(referencepredict2)
print ""Confusion matrix is:"",confusion_matrix(referencenames, referencepredict2)
print ""Accuracy:"", accuracy_score(referencenames, referencepredict2)
#print referencenames,referencepredict
#print ""Check\n""
#for i in range(len((seq_records4))):",tetranucleotide4.py,rop14009/Repeat-analysis-pipeline,1
"    
    print '\n=== Evaluate algorithms ==='
    
    # Spot Check Algorithms
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC()))
    
    results = []
    names = []


    for name, model in models:
        kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=SCORING)
        results.append(cv_results)",lib/eda1.py,FabricioMatos/ifes-dropout-machine-learning,1
"        #test_set = Sel.transform(feature[test])
        clf.fit(train_set, labels[train])
        pred = clf.predict(test_set)
        score.append(accuracy_score(labels[test], pred))
        score.append(precision_score(labels[test], pred))
        score.append(recall_score(labels[test], pred))
        score.append(f1_score(labels[test], pred))
        scores.append(score)
    avg = np.average(scores, axis=0)
    return avg
print validate(SVC(C=10000, gamma=0.75), feature, 500)
print validate(LinearSVC(C=100), feature, 500)
print validate(LogisticRegression(C=100), feature, 500)

#
# Grid search for parameters
#sfk = cv.StratifiedShuffleSplit(labels, 40)
#params = {'C':[1, 100, 10000, 100000]}
#gs = GridSearchCV(
#        SVC(), params, scoring='f1',",learning/final1.py,fcchou/CS229-project,1
"            self.predictions_.append(classifier.predict_proba(X))
        return np.mean(self.predictions_, axis=0)

def train_custom(categories, comments, badwords):
    from sklearn.pipeline import Pipeline
    from sklearn.svm import LinearSVC
    from sklearn.ensemble import GradientBoostingRegressor


    text_clf = Pipeline([('vect', CustomTransformer(badwords)),
                      ('clf', LinearSVC(random_state=42))])
    text_clf = text_clf.fit(comments, categories)
    return text_clf


def train_assembling(categories, comments, badwords):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import LogisticRegression
    from sklearn.pipeline import Pipeline, FeatureUnion
    from sklearn.feature_selection import SelectPercentile, chi2",src/trolling_detection/train.py,rafaharo/trolling_detection,1
"        pred = np.zeros_like(test_label)
        pred[np.logical_and(pred1 == 1, pred2 == 1)] = 1

        score.append(accuracy_score(test_label, pred))
        score.append(precision_score(test_label, pred))
        score.append(recall_score(test_label, pred))
        score.append(f1_score(test_label, pred))
        scores.append(score)
    avg = np.average(scores, axis=0)
    return avg
print validate(SVC(C=10000, gamma=0.75), feature, 500)
print validate(LinearSVC(C=100), feature, 500)
print validate(LogisticRegression(C=100), feature, 500)",learning/final3.py,fcchou/CS229-project,1
"
  C=[]
  gamma=[]

  for i in range(21): C.append(10.0**(i-5))
  for i in range(17): gamma.append(10**(i-14))

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}, {'kernel': ['linear'], 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/tune-rain.py,n7jti/machine_learning,1
"# in the outer cross-validation procedure
# we make the decorator explicitly so we can reuse the same folds
# in both tuned and untuned approaches
folds = optunity.cross_validation.generate_folds(data.shape[0], num_folds=3)
outer_cv = optunity.cross_validated(x=data, y=labels, num_folds=3, folds=[folds],
                                    aggregator=optunity.cross_validation.identity)
outer_cv = optunity.cross_validated(x=data, y=labels, num_folds=3)

# compute area under ROC curve of default parameters
def compute_roc_standard(x_train, y_train, x_test, y_test):
    model = sklearn.svm.SVC().fit(x_train, y_train)
    decision_values = model.decision_function(x_test)
    auc = optunity.metrics.roc_auc(y_test, decision_values)
    return auc

# decorate with cross-validation
compute_roc_standard = outer_cv(compute_roc_standard)
roc_standard = compute_roc_standard()
print('Nested cv area under ROC curve of non-tuned model: ' + str(roc_standard))
",bin/examples/python/sklearn/svc_structured.py,MarkAWard/optunity,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)",mne/decoding/tests/test_csp.py,ARudiuk/mne-python,1
"
    if return_times:
        ret['times'] = [fit_time, score_time]

    return ret, estimator

def _clf_build(clf_type):
    from sklearn import svm
    from sklearn.ensemble import RandomForestClassifier as RFC
    if clf_type == 'svc_linear':
        return svm.LinearSVC(C=1)
    elif clf_type == 'svc_rbf':
        return svm.SVC(C=1)
    elif clf_type == 'rfc':
        return RFC()",mriqc/classifier/sklearn/cv_nested.py,poldracklab/mriqc,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_12_01_2014.py,magic2du/contact_matrix,1
"#########################################################
### your code goes here ###

#########################################################

from sklearn.svm import SVC

features_train = features_train[:len(features_train)]
labels_train = labels_train[:len(labels_train)]

#clf = SVC(C=1, kernel='linear')
clf = SVC(C=10000, kernel='rbf')

t0 = time()
clf.fit(features_train, labels_train)
print ""Training Time "",round(time()-t0, 3),""s""
t1 = time()
prediction = clf.predict(features_test)
print ""Predicting Time "",round(time()-t1, 3 ),""s""
print ""Accuracy "", clf.score(features_test,labels_test)",Machine_Learning/Supervised_Learning/svm/svm_author_id.py,abhipr1/DATA_SCIENCE_INTENSIVE,1
"    parser = argparse.ArgumentParser(description='Trains the classifier models')
    parser.add_argument(""--feature-map-file"", dest=""feature_map_file"", required=True,
            help=""Input pickle file containing the feature map"")
    parser.add_argument(""--svm-file"", dest=""svm_file"", required=False,
            help=""Output file where the pickled SVM model will be stored"")
    return parser

class ClassifierTrainer(object):
    def __init__(self, X, label_words):
        self.le = preprocessing.LabelEncoder()  
        self.clf = OneVsOneClassifier(LinearSVC(random_state=0))

        y = self._encodeLabels(label_words)
        X = np.asarray(X)
        self.clf.fit(X, y)

    def _fit(self, X):
        X = np.asarray(X)
        return self.clf.predict(X)
        ",Module 2/9/training.py,PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python,1
"
# ----------------------------------------------------------------------------------------------------

annotator, X, y, groups = get_model_and_data(sentence_distance, predict_entities)
# X = X.toarray()
print(""SVC after preprocessing, #features: {} && max value: {}"".format(X.shape[1], max(sklearn.utils.sparsefuncs.min_max_axis(X, axis=0)[1])))

print(""Shape X, before: "", X.shape)

feature_selections = [
    # (""LinearSVC_C=4.0"", SelectFromModel(LinearSVC(C=4.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    (""LinearSVC_C=2.0"", SelectFromModel(LinearSVC(C=2.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    (""LinearSVC_C=1.0"", SelectFromModel(LinearSVC(C=1.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    # (""LinearSVC_C=0.5"", SelectFromModel(LinearSVC(C=0.5, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    # (""LinearSVC_C=0.25"", SelectFromModel(LinearSVC(C=0.25, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),

    # (""RandomizedLogisticRegression_C=1"", SelectFromModel(RandomizedLogisticRegression(C=1))),
    # (""RandomizedLogisticRegression_C=0.5"", SelectFromModel(RandomizedLogisticRegression(C=0.5))),

    # (""PCA_2"", PCA(2)),",scripts/quick_feature_selection_and_pipeline_evaluation.py,Rostlab/LocText,1
"        See SVC.__init__ for details.
    coef0 : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    degree : int
        Degree of kernel, if kernel is polynomial.
        See SVC.__init__ for details.
    """"""

    def __init__(self, C, kernel='rbf', gamma=1.0, coef0=1.0, degree=3):
        estimator = SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0,
                        degree=degree)
        super(DenseMulticlassSVM, self).__init__(estimator)

    def fit(self, X, y):
        """"""
        Fit underlying estimators.

        Parameters
        ----------",pylearn2/models/svm.py,se4u/pylearn2,1
"#svc_params['probability'] = True

svc_params['kernel']     = 'poly'
svc_params['C']          = 1.0
svc_params['gamma']      = 0.0
svc_params['degree']     = 3
svc_params['coef0']      = 1

# the classifier
# ==============
svc_clf = SVC(**svc_params)


# ##RANDOMIZED grid search

# In[ ]:

t0 = time.time()

# search grid",svm.scikit/svm_poly.scikit_random_gridsearch.py,sanpi0205/MNIST,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = DISR.disr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_DISR.py,jundongl/scikit-feast,1
"    
# ex11()


# In[ ]:

def ex12():
    X = [[1.,0.], [0.,1.], [0.,-1.], [-1.,0.], [0.,2.],[0.,-2.],[-2.,0.]]
    Y = [-1.,-1.,-1.,1.,1.,1.,1.]

    ssvm = svm.SVC(kernel='poly',
                   C=1e10,
                   gamma=1,
                   degree=2,
                   coef0=1)
    ssvm.fit(X, Y)

    return len(ssvm.support_vectors_)

ex12()",python/final.py,nicolov/learning-from-data-homework,1
"    # noise-free simple 2d-data
    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,
                      cluster_std=0.1, shuffle=False, n_samples=80)
    # split dataset into two folds that are not iid
    # first one contains data of all 4 blobs, second only from two.
    mask = np.ones(X.shape[0], dtype=np.bool)
    mask[np.where(y == 1)[0][::2]] = 0
    mask[np.where(y == 2)[0][::2]] = 0
    # this leads to perfect classification on one fold and a score of 1/3 on
    # the other
    svm = SVC(kernel='linear')
    # create ""cv"" for splits
    cv = [[mask, ~mask], [~mask, mask]]
    # once with iid=True (default)
    grid_search = GridSearchCV(svm, param_grid={'C': [1, 10]}, cv=cv)
    grid_search.fit(X, y)
    first = grid_search.grid_scores_[0]
    tm.assert_equal(first.parameters['C'], 1)
    tm.assert_array_almost_equal(first.cv_validation_scores, [1, 1. / 3.])
    # for first split, 1/4 of dataset is in test, for second 3/4.",dklearn/tests/test_grid_search.py,jcrist/dask-learn,1
"

class SVMClassifier(BaseClassifier):
    """"""
    SVM Classifier that uses the sklearn SVM implementation
    to train/test with TestSample objects.
    """"""

    def __init__(self):
        BaseClassifier.__init__(self)
        self.clf = svm.SVC()

    def train(self,X,y):
        """"""
        Train the classifier with X, a set of instances,
        and y, a set of class labels.
        """"""
        self.clf.fit(np.asarray(X),np.asarray(y))

",src/py/cssigps/offlineclassifier.py,j-rock/cs598ps,1
"                         ""my_data_processed/acceleration_train_x.csv"",
                         ""my_data_processed/acceleration_train_y.csv"",
                         ""my_data_processed/acceleration_train_z.csv"",
                         ""my_data_processed/gyroscope_train_x.csv"",
                         ""my_data_processed/gyroscope_train_y.csv"",
                         ""my_data_processed/gyroscope_train_z.csv"")

data = ndarray(shape=(len(data), 21), dtype=float, buffer=np.asanyarray(data))
target = ndarray(shape=(len(target),), dtype=int, buffer=np.asanyarray(target))

clf = svm.SVC()
clf.fit(data, target)

target, data = load_data(""my_data_processed/activity_test.csv"",
                         ""my_data_processed/acceleration_test_x.csv"",
                         ""my_data_processed/acceleration_test_y.csv"",
                         ""my_data_processed/acceleration_test_z.csv"",
                         ""my_data_processed/gyroscope_test_x.csv"",
                         ""my_data_processed/gyroscope_test_y.csv"",
                         ""my_data_processed/gyroscope_test_z.csv"")",Activity_and_Context_Recognition/Classifier/test_dataset.py,VizLoreLabs/LCI-FIC2-SE,1
"""""""
start = 1
stop = 9
features = list(train_df.columns[start:stop])
train_y = train_df[""place_id""]
train_X = train_df[features]
# Decision Tree
# clf = DecisionTreeClassifier(min_samples_split=50, random_state=99)

# Linear Support Vector Machine
# clf = svm.SVC(kernel='linear', C=1)

# K Nearest Neighbors
# clf = KNeighborsClassifier(3)

# Naive Bayes
# clf = GaussianNB()

## Ensemble Methods
# Random Forest ('Generalized' DT)",learn.02.py,knmnyn/orbital16-scikit,1
"    digits_cv_split_filenames = mmap_utils.persist_cv_splits('digits_10', X, y, 10)

    mmap_utils.warm_mmap_on_cv_splits(client, digits_cv_split_filenames)
    from sklearn.svm import LinearSVC
    from collections import OrderedDict
    import numpy as np

    linear_svc_params = OrderedDict((
        ('C', np.logspace(-2, 2, 5)),
    ))
    linear_svc = LinearSVC()

    linear_svc_search = model_selection.RandomizedGridSeach(lb_view)

    linear_svc_search.launch_for_splits(linear_svc, linear_svc_params, digits_cv_split_filenames)",datascienceutils/parutils.py,greytip/data-science-utils,1
"train_samples = 100  # Samples used for training the models

X_train = X[:train_samples]
X_test = X[train_samples:]
y_train = y[:train_samples]
y_test = y[train_samples:]

# Create classifiers
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier(n_estimators=100)

###############################################################################
# Plot calibration plots

plt.figure(figsize=(10, 10))
ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
ax2 = plt.subplot2grid((3, 1), (2, 0))
",projects/scikit-learn-master/examples/calibration/plot_compare_calibration.py,DailyActie/Surrogate-Model,1
"X = X[order]
y = y[order].astype(np.float)

X_train = X[:.9 * n_sample]
y_train = y[:.9 * n_sample]
X_test = X[.9 * n_sample:]
y_test = y[.9 * n_sample:]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    pl.figure(fig_num)
    pl.clf()
    pl.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=pl.cm.Paired)

    # Circle out the test data
    pl.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
",exemplo/plot_iris_exercise.py,ebertti/nospam,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,antiface/mne-python,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",org.modelsphere.sms/lib/jython-2.2.1/Lib/email/test/test_email.py,DarioGT/OMS-PluginXML,1
"
class TestClassificationMetrics(tm.TestCase):

    def setUp(self):
        import sklearn.svm as svm
        digits = datasets.load_digits()
        self.data = digits.data
        self.target = digits.target
        self.df = pdml.ModelFrame(digits)

        estimator1 = self.df.svm.LinearSVC(C=1.0, random_state=self.random_state)
        self.df.fit(estimator1)

        estimator2 = svm.LinearSVC(C=1.0, random_state=self.random_state)
        estimator2.fit(self.data, self.target)
        self.pred = estimator2.predict(self.data)
        self.decision = estimator2.decision_function(self.data)

        # argument for classification reports
        self.labels = np.array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])",pandas_ml/skaccessors/test/test_metrics.py,sinhrks/pandas-ml,1
"  [
    [1,2],
    [5,8],
    [1.5,1.8],
    [8,8],
    [1,0.6],
    [9,11]
  ]
)
y = [0,1,0,1,0,1]
clf = svm.SVC(kernel='linear', C = 1.0)
clf.fit(X,y)
print(clf.predict([0.58,0.76]))
print(clf.predict([10.58,10.76]))
w = clf.coef_[0]
print(w)
a = -w[0] / w[1]
xx = np.linspace(0,12)
yy = a * xx - clf.intercept_[0] / w[1]
h0 = plt.plot(xx, yy, 'k-', label=""non weighted div"")",p11.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"from sklearn.cross_validation import train_test_split


def return_best_svm(X, Y, N, C, penalties):
    """"""
    Returns the best model for X data and Y targets
    """"""
    skf = StratifiedKFold(Y, N)

    # We define the logistic regression
    lg = SVC(tol=0.0001)

    param_grid = [{'C': C, 'kernel': penalties}]

    rsearch = GridSearchCV(estimator=lg, param_grid=param_grid, cv=skf)
    rsearch.fit(X, Y)
    
    return rsearch.best_estimator_, rsearch.best_score_, rsearch

",4_day/svm.py,h-mayorquin/g_node_data_analysis_205,1
"
    def get_classifier(self, train=True, test=True):

        all_output = """"
        h = .02  # step size in the mesh
        self.names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
                      ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
                      ""Quadratic Discriminant Analysis""]
        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LinearDiscriminantAnalysis(),
            QuadraticDiscriminantAnalysis()]

        for i in range(0, len(self.names)):",history/models.py,igorpejic/pytrader,1
"def generate_value(name, known_list):
    if name not in known_list:
        return 0
    else:
        return known_list.index(name) + 1


if __name__ == ""__main__"":
    # clf = tree.DecisionTreeClassifier()
    # clf = RandomForestClassifier(n_estimators=10)
    clf = svm.SVC()

    train_set = read_from_file(""../../data/hw1/adult.data.txt"")
    clf = clf.fit(train_set[0], train_set[1])
    test_set = read_from_file(""../../data/hw1/adult.test.txt"")

    num = 0
    correct = 0
    for i in clf.predict(test_set[0]):
        if i == test_set[1][num]:",src/hw1_Classification/Classification.py,MyXOF/Data-Mining-Algorithm,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",libs/nltk/parse/transitionparser.py,adazey/Muzez,1
"
### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier
from sklearn.pipeline import Pipeline

svc_ovo = OneVsOneClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

svc_ova = OneVsRestClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

### Cross-validation scores ###################################################
from sklearn.cross_validation import cross_val_score",plot_haxby_multiclass.py,abenicho/isvr,1
"
all_train = read_file(ftrain)
all_test = read_file(ftest)


# In[ ]:

def run_svm(X, Y, C, K):
    N = len(Y)
    
    ssvm = svm.SVC(kernel='poly', C=10000000, gamma=1, degree=K, coef0=1)
    ssvm.fit(X, Y)

    return ssvm


# In[ ]:

def build_vs_all_set(data, which):
    """"""Prepare a which-vs-all dataset.""""""",python/hw_8.py,nicolov/learning-from-data-homework,1
"from ..scorer_model import ScorerModel
from ..svc import RBFSVC, SVC, LinearSVC
from .util import (FEATURES, get_and_format_info, pickle_and_unpickle,
                   train_score)


def test_svc():
    model = SVC(FEATURES)
    get_and_format_info(model)
    train_score(model)
    pickle_and_unpickle(model)
    get_and_format_info(model)

    model = SVC(FEATURES, scale=True, center=True, balanced_sample=True)
    get_and_format_info(model)
    train_score(model)
    pickle_and_unpickle(model)",revscoring/scorer_models/tests/test_svc.py,wiki-ai/revscoring,1
"            ##############
        try:
            past = np.argsort(self.body[""time""])[::-1][:self.interval * self.step]
            self.lastprob = np.mean(clf.predict_proba(self.csr_mat[past])[:,pos_at])
            # self.lastprob = np.mean(np.array(prob)[order][:self.step])
        except:
            pass

    ## Train model ##
    def train(self,pne=False):
        clf = svm.SVC(kernel='linear', probability=True)
        poses = np.where(np.array(self.body['code']) == ""yes"")[0]
        negs = np.where(np.array(self.body['code']) == ""no"")[0]
        left = poses
        decayed = list(left) + list(negs)
        unlabeled = np.where(np.array(self.body['code']) == ""undetermined"")[0]
        try:
            unlabeled = np.random.choice(unlabeled,size=np.max((len(decayed),self.atleast)),replace=False)
        except:
            pass",src/util/mar.py,ai-se/MAR,1
"

if __name__ == ""__main__"":
    from sklearn.pipeline import Pipeline
    from sklearn.svm import SVC
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import SGDClassifier
    from pprint import pprint

    print(""Fast default grid for SVC"")
    pprint(get_grid(SVC()))

    print(""Typical grid for text classification"")
    p = Pipeline([
        ('vectorizer', TfidfVectorizer()),
        ('classifier', SGDClassifier()),
    ])
    pprint(get_grid(p, all_grids='slow'))",oglearn/param_search.py,ogrisel/oglearn,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                      gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print (""Accuracy:"" % (clf.score(X, y) * 100))
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",SVM/svm_gui.py,diego0020/va_course_2015,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_09_2015_02.py,magic2du/contact_matrix,1
"	return gs

def _gs_SVC_r0( xM, yVc, params):
	""""""
	Since classification is considered, we use yVc which includes digital values 
	whereas yV can include float point values.
	""""""

	print(xM.shape, yVc.shape)

	clf = svm.SVC()
	#parmas = {'alpha': np.logspace(1, -1, 9)}
	kf5 = cross_validation.KFold( xM.shape[0], n_folds=5, shuffle=True)
	gs = grid_search.GridSearchCV( clf, params, cv = kf5, n_jobs = -1)

	gs.fit( xM, yVc)

	return gs

def gs_SVC( xM, yVc, params, n_folds = 5):",repository/jgrid (james-90X3A's conflicted copy 2016-04-21).py,jskDr/jamespy_py3,1
"
    searcher.fit(X, y, clf__spam=np.ones(10), clf__eggs=np.zeros(10))
    # Test with dask objects as parameters
    searcher.fit(X, y, clf__spam=da.ones(10, chunks=2),
                 clf__eggs=dask.delayed(np.zeros(10)))


@ignore_warnings
def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)

    # XXX: It seems there's some global shared state in LinearSVC - fitting
    # multiple `SVC` instances in parallel using threads sometimes results in
    # wrong results. This only happens with threads, not processes/sync.
    # For now, we'll fit using the sync scheduler.
    grid_search = dcv.GridSearchCV(clf, {'C': Cs}, scoring='accuracy',",dask_searchcv/tests/test_model_selection_sklearn.py,jcrist/dask-searchcv,1
"#        test_set = feature[test]
#        clf.fit(train_set, labels[train])
#        pred = clf.predict(test_set)
#        score.append(accuracy_score(labels[test], pred))
#        score.append(precision_score(labels[test], pred))
#        score.append(recall_score(labels[test], pred))
#        score.append(f1_score(labels[test], pred))
#        scores.append(score)
#    avg = np.average(scores, axis=0)
#    return avg
##print validate(SVC(kernel='poly', degree=2, C=1000000), feature, 500)
#print validate(SVC(C=10000, gamma=0.75), feature, 500)
#print validate(LinearSVC(C=100), feature, 500)
#print validate(LogisticRegression(C=100), feature, 500)

#clf = SVC(C=10000, gamma=0.75, probability=True)
#clf.fit(feature, labels)
#prob = clf.predict_proba(feature)

clf = SVC(C=10000, gamma=0.75, probability=True)",learning/final.py,fcchou/CS229-project,1
"#scale features for optimal performance
#standardize the features using the StandardScaler class from scikit-learn's perprocessing module
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

#train a SVM model to classify the different flowers in our Iris dataset:
from sklearn.svm import SVC
svm  = SVC(kernel='linear',C=1.0,random_state=0)
svm.fit(X_train_std,y_train)

#draw decision surface
X_combined_std = np.vstack((X_train_std,X_test_std))
y_combined = np.hstack((y_train,y_test))
import DecisionBoundary
DecisionBoundary.plot_decision_regions(X_combined_std,y_combined,classifier=svm,test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')",1_supervised_classification/15-SVM/svm/svm.py,PhenixI/machine-learning,1
"from sklearn import svm
from sklearn.datasets import load_svmlight_files

X_train, y_train, X_test, y_test = load_svmlight_files((
    'data/ml14fall_train.dat', 'data/ml14fall_test1_no_answer.dat'))

print ""read data finished""

poly_clf = svm.SVC(kernel='poly', degree=5)
poly_clf = poly_clf.fit(X_train[:50], y_train[:50])
print ""fit model finished""
prediction = poly_clf.predict(X_test[:50])
print prediction

def write_result(pred, result_path):
    result_content = '\n'.join([str(int(p)) for p in pred])
    with open(result_path, 'w') as result:
        result.write(result_content)",svm_approach.py,ryaninhust/Press-Start-Button,1
"
    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM': make_pipeline(
        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM': make_pipeline(
        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4),
    'MultilayerPerceptron': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
        algorithm='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
        tol=1e-4, random_state=1),
    'MLP-adam': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,",projects/scikit-learn-master/benchmarks/bench_mnist.py,DailyActie/Surrogate-Model,1
"
# initiate PCA and fit to the training data
pca = PCA(n_components=40)
pca.fit(X_train)

# transform
X_transformed = pca.transform(X_train)
newdata_transformed = pca.transform(X_test)

#initiate a classifier and then fit eigen faces and labels
clf = SVC()
clf.fit(X_transformed,y_train)

# predict new labels using the trained classifier
pred_labels = clf.predict(newdata_transformed)

#output the accuracy_score
score = accuracy_score(y_test,pred_labels,True)
print(score)
",Image_Recognition/PCABasedImageReco.py,spurihwr/ImageProcessingProjects,1
"import pandas
import numpy as np
from sklearn.svm import SVC


def learn(X,y):
    clf = SVC(C = 100000, random_state=241, kernel='linear')
    clf.fit(X, y)
   
    return clf

def test(X, y, clf):
    y_actual = clf.predict(X)
    result = accuracy_score(y, y_actual)
    
    return result",week 3/part 1/task 1/task 1.py,GrimRanger/Coursera,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit
from mne.decoding import CSP

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,effigies/mne-python,1
"
  clf = RandomForestClassifier(n_estimators = 30)
  clf = clf.fit(X, y)
  return clf

def build_linear_svm():
  X, y = build_feature_vector()
  X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.1,
                                     random_state = 0)

  clf = svm.SVC(kernel = 'linear')
  clf.fit(X_train, y_train)

  scores = cross_validation.cross_val_score(clf, X, y, cv = 5)
  print scores.mean()

def build_poly_svm():
  X, y = build_feature_vector()
  X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.1,
                                     random_state = 0)",build_model.py,agadiraju/SeniorDesign,1
"
class PrototypeScreen(BoxLayout):
    """"""PrototypeScreen is a pumped box layout used for the first iteration.""""""
    result = StringProperty()
    train_file = StringProperty()
    predict_file = StringProperty()

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.current_app = App.get_running_app()
        self.current_app.estimator = SVC()
        self.current_app.cv = LeaveOneOut()

    def load_popup(self, string_property):
        popup = FileDialog(dir='~', filters=['*.csv'], size_hint=(0.8, 0.8))
        popup.bind(selected_file=partial(self.bind_strings,
                                         string_property=string_property))
        popup.open()

    def bind_strings(self, instance, value, string_property):",persimmon/view/test.py,AlvarBer/Persimmon,1
"    plt.plot(sorted_data_2, yvals_2, 'g')
    xmin, xmax, ymin, ymax = plt.axis()
    plt.axis([xmin-xmax*0.05, xmax+xmax*0.05, -0.05, 1.05])
    plt.xlabel(""CDF - Ratio of tweets posted on "" + weekdays[d] + ""s"")
    plt.show()  


#Try Support Vector Classification with different kernels
#This only works when kernel='rbf', can't figure out why
X_train, X_test, y_train, y_test = train_test_split(dataset_X, dataset_Y, test_size = 0.8, random_state = 0)
clf = svm.SVC(kernel='rbf')
clf.fit(X_train, y_train)
prediction = clf.predict(X_test)
i = 0
e = 0
while i < len(prediction):
    if prediction[i] != y_test[i]:
        e += 1
    i += 1
print(""# errors (total: %d): "" % len(X_test))",src/analyze.py,tapilab/is-xhuang1994,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_30_2014_server.py,magic2du/contact_matrix,1
"            f.append([float(val) for val in a['feature']])

        # create label vector from database
        l=[];
        for a in self.db.labeledinstances.find({""dsid"":dsid}):
            l.append(a['label'])

        # fit the model to the data
        # c1 = KNeighborsClassifier(n_neighbors=10);
        c1 = GaussianNB()
        # c1 = SVC(gamma=0.001)
        acc = -1;
        if l:
            c1.fit(f,l) # training
            lstar = c1.predict(f)
            self.clf[dsid] = c1
            acc = sum(lstar==l)/float(len(l))
            bytes = pickle.dumps(c1)
            self.db.models.update({""dsid"":dsid},
                {  ""$set"": {""model"":Binary(bytes)}  },",tornado_bare/sklearnhandlers.py,Tsiems/mobile-sensing-apps,1
"class _NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,ivano666/tensorflow,1
"if __name__ == '__main__':
    UNDERSAMPLING_RATIO = 1.1
    FRATE = 100
    for monkey in ['Titi_monkeys', 'colobus', 'Blue_Fuller', 'Blue_Murphy',
                   'Blue_merged', 'all']:
        X, y, labels = _load_files_labels(monkey,
                                          under_sampling_ratio=UNDERSAMPLING_RATIO)
        X_train, X_test, y_train, y_test = train_test_split(X, y)
        pipeline = Pipeline([('data', FeatureUnion([('audio', AudioLoader()),
                                                    ('vad', VADLoader())])),
                             ('svm', SVC(kernel='rbf', gamma=1e-5, C=20))
                             ])
        paramdist = {'svm__C': np.logspace(0, 2, 50),
                     'data__vad__stacksize': scipy.stats.randint(11, 51),
                     'data__audio__stacksize': scipy.stats.randint(11, 51)}
        clf = RandomizedSearchCV(pipeline, paramdist, n_iter=500, verbose=1,
                                 cv=1,
                                 n_jobs=35)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)",transcriber.py,bootphon/monkey_business,1
"
def pre_process(data, target):
    count_vectorized = CountVectorizer(binary='false', ngram_range=(0, 1))
    data = count_vectorized.fit_transform(data)
    tfidf_data = TfidfTransformer(use_idf=True, smooth_idf=True).fit_transform(data)
    print ""Calculating term frequency.""
    return tfidf_data


def learn_model(reviews, stars):
    svm = OneVsRestClassifier(SVC(C=1, kernel='linear', gamma=1, verbose=False, probability=False))

    print ""-"" * 60, ""\n""
    print ""Results with 10-fold cross validation:\n""
    print ""-"" * 60, ""\n""

    predicted = cross_validation.cross_val_predict(svm, reviews, stars, cv=10, n_jobs=1)
    print ""*"" * 20
    print ""\t Accuracy Score\t"", metrics.accuracy_score(stars, predicted)
    print ""*"" * 20",SVM_KFold_CrossValidation.py,akshaykamath/ReviewPredictionYelp,1
"from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(C=0.5,penalty='l2',tol=0.0001)#0.8137

#from sklearn.naive_bayes import GaussianNB      #nb for 高斯分布的数据
#clf=GaussianNB() #0.7744  

#from sklearn.neighbors import KNeighborsClassifier  
#clf=KNeighborsClassifier(n_neighbors=8)#0.8081

#from sklearn import svm   
#clf = svm.SVC(C=10,gamma=0.1)#0.8238
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))

#from sklearn.tree import DecisionTreeClassifier
#clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1,random_state=1)#0.778

#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(min_samples_split = 16, n_estimators = 300)#0.8350",my_2_featrue_pred.py,zlykan/my_test,1
"# Stack the features together
feat = FeatureUnion([('words', words),
	                 ('char', char)
])

# Construct transformation pipeline
text_clf = Pipeline([('feat', feat),
	                 # ('select', select),
                     # ('clf', MultinomialNB()),
                     #('clf', SGDClassifier(penalty='l2'))
                     #('clf', LinearSVC(penalty='l2'))
                     ('clf',SVC(C=0.5, kernel='linear'))
])

# Set the parameters to be optimized in the Grid Search
parameters = {'feat__words__ngram_range': [(1,5), (1,6)],
			  # 'feat__words__stop_words': (""english"", None),
              'feat__words__min_df': (2,3),
              'feat__words__use_idf': (True, False),
              'feat__char__use_idf': (True, False)",SAM/model.py,tanayz/Kaggle,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",fail/334_test_email.py,mancoast/CPythonPyc_test,1
"                 window_size = 10, surf_window_size = 20):

        self.surf_window = surf_window_size
        self.window_size = window_size

        self.levels = int(np.floor(np.sqrt(ncolors)))
        self.ncolors = ncolors
        self.ntrain = ntrain

        # declare classifiers
        self.svm = [SVC(probability=probability, gamma=svmgamma, C=svmC) for i in range(self.ncolors)]
        #self.svm = [LinearSVC() for i in range(self.ncolors)]

        self.scaler = preprocessing.MinMaxScaler()                          # Scaling object -- Normalizes feature array
        self.pca = PCA(npca)

        self.centroids = []
        self.probability = probability
        self.colors_present = []
        self.surf = cv2.DescriptorExtractor_create('SURF')",SvmColorizer.py,gokulvk99/ColorPrism,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_backward.svm_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",PyFeaST/example/test_svm_backward.py,jundongl/PyFeaST,1
"X_norm = normalizer.transform(X)


def optimizeAdaBoostSVM(X_norm, y, kFolds=10):
    # grid search 多参数优化
    parameters = {
        'base_estimator__gamma': np.logspace(0, 3, 3),
        'base_estimator__C': np.logspace(0, 3, 3),
        'n_estimators': np.linspace(1, 100, 3, dtype=np.dtype(np.int16)),
    }
    svm = SVC(probability=True, kernel='rbf')
    clf = AdaBoostClassifier(base_estimator=svm)

    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['base_estimator__gamma'], gs.best_params_['base_estimator__C'], gs.best_params_['n_estimators'], gs.best_score_


alpha, C, n_estimators, score = optimizeAdaBoostSVM(X_norm, y, kFolds=10)
print 'alpha',alpha, 'C',C, 'n_estimators=',n_estimators, 'score=',score",finance/WeekTest/AdaboostSVMTest.py,Ernestyj/PyStudy,1
"

def dropout_channels_monte_carlo(input_matrix: np.array, output_labels: np.array) -> np.array:
    """"""
    Perform 10 fold shuffle split on the data and
    do cross validation to find channels with
    highest correlation to the output variable.
    """"""
    from sklearn.svm import SVC

    clf = SVC(C=1, kernel='linear')

    trials, channels, samples = np.shape(input_matrix)

    def monte_carlo_channel(channel):
        from sklearn.cross_validation import ShuffleSplit, cross_val_score
        from .features import pool

        cross_validation = ShuffleSplit(trials, n_iter=5, test_size=0.2)
        input_pooled = pool(input_matrix[:, [channel]])",atone/preprocessing.py,wohlert/atone,1
"    '''
    An example of usage
    '''
    parameters = [{'C': [1, 10, 100, 1000], 'tol': [0.001, 0.0001],
                   'class_weight': [None, 'balanced']},
                  {'C': [1, 10, 100, 1000], 'multi_class': ['crammer_singer'],
                   'tol': [0.001, 0.0001]}]
    xtrain = np.random.random((100, 20))
    xtrain[xtrain < 0] = 0
    ytrain = (np.random.random(100) > 0.5).astype(int)
    lsvc = LinearSVC()
    optGridSearchCV(lsvc, xtrain, ytrain, parameters, reduction_ratio=2,
                    iter_num=3, scoring='f1_macro', fold_num=5, first_rand=False,
                    n_jobs=4)

if __name__ == '__main__':
    example()",OptGridSearchCV.py,VasLem/KinectPainting,1
"	# Gaussian samples operation
	####################################################################

	for i in range(100):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.1_{0}.txt"".format(i,dim),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.075_{0}.txt"".format(i,dim)))

	#Earlier for SVM we had C=496.6 and gamma=0.00767

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        clf = SVC(C=290.4,gamma=0.0961,probability=True, cache_size=7000)
        args=[str(dim)+ ""Dgaussian_same_projection_redefined__0_1__0_075_optimised_svm"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),200,6]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/svm_gaussian_same_projection/svm_Gaussian_same_projection_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"
for Model in [LinearSVC, GaussianNB, KNeighborsClassifier]:
    clf = Model().fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print Model.__name__, metrics.f1_score(y_test, y_pred)
    
print '------------------'

# test SVC loss
for loss in ['l1', 'l2']:
    clf = LinearSVC(loss=loss).fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print ""LinearSVC(loss='{0}')"".format(loss), metrics.f1_score(y_test, y_pred)
    
print '-------------------'
    
# test K-neighbors
for n_neighbors in range(1, 11):
    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)
    y_pred = clf.predict(X_test)",notebooks/solutions/04C_validation_exercise.py,samstav/scipy_2015_sklearn_tutorial,1
"        print('full run')
        self.build_initial_dataset()        
        final_scores = np.zeros(N)
        test_fold = 9
        hat_policies = []
        X_train = sum([self.dataset[i] for i in self.dataset.keys() if i != test_fold], [])
        y_train = sum([self.labels[i] for i in self.labels.keys() if i != test_fold], [])
        agg_data = [X_train, y_train]
        print(""Dataset size at the beginning"")
        print(len(agg_data[0]))
        #hat_policy = svm.LinearSVC()
        hat_policy = svm.SVC(C=10, kernel='linear')
#        hat_policy = linear_model.SGDClassifier(loss=""squared_hinge"", penalty=""l2"", n_iter=10)
        hat_policy.fit(sparse.vstack(agg_data[0]),np.vstack(agg_data[1]))
        hat_policies.append(hat_policy)
        
        hamming_scores = []
        y_pred = hat_policy.predict(sparse.vstack(self.dataset[test_fold]))
        print(1-hamming_loss(y_pred,self.labels[test_fold]))
        hamming_scores.append(hamming_loss(y_pred,self.labels[test_fold]))",dagger.py,Rachine/Imitiation-Learning,1
"
X = scaled_scores



X = [[0, 0], [1, 1]]
Y = [0, 1]

print len(X), len(Y)
print 'here0'
clf = svm.SVC(X, Y)
print 'here1'
new = [3.763761404912697,3.5271142624328102,3.5254192377240696,2.7094604955856845,2.593200204503558,2.4213357369519297,2.404425610557944,2.2427608092026974,1.825039408110762,1.263463687150838,1.0679353254330592,1.012474755217698,0.6900170856724432,0.631731843575419,0.45773963730569944,0.0,0.0,0.0,0.0,0.0,0.0]
#new = np.ndarray(new, np.float)
print type(new[0])
normalized_new = preprocessing.scale(new)

prediction = clf.predict(normalized_new)",harnesses/normalize.py,gavinmh/helloTablet-api,1
"    test = pd.read_csv('./datasets/test_noduplicate.txt', header = 0, index_col = 0, sep = '\t')
    if 'Rank' in test.columns:
        del test['Rank']
        print('Deleted Ranks')
    test = test.T
    test = test[probes]
    labels_test = pd.read_csv('./datasets/labels_test.txt', header = None, sep = '\t')
    labels_test = labels_test.unstack().tolist()

    pipe_svc = Pipeline([('scl', StandardScaler()),
                ('clf', SVC(random_state=1))])

    param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
    gamma_range = [0.00006, 0.0006, 0.006, 0.06, 0.6]

    param_grid = [{'clf__C': param_range, 
                   'clf__kernel': ['linear']},
                  {'clf__C': param_range, 
                   'clf__gamma': gamma_range, 
                   'clf__kernel': ['rbf', 'poly', 'sigmoid']}]",scleroderma-prediction/SVM_grid_search.py,Karl-Marka/data-mining,1
"    model.fit(trndata,trnlabs)
    Atrndata = model.transform(trndata)
    Atstdata = model.transform(tstdata)
    costs = model.get_costs()
    
    model = pca.PCA(n_components=20)
    model.fit(trndata)
    Ptrndata = model.transform(trndata)
    Ptstdata = model.transform(tstdata)
            
    clf = svm.LinearSVC()
    clf.fit(Ptrndata,etrnlabs)
    pscore += clf.score(Ptstdata,etstlabs)
    
    clf = svm.LinearSVC()
    clf.fit(Atrndata,etrnlabs)
    nscore += clf.score(Atstdata,etstlabs)

print pscore/float(R)
print nscore/float(R)",src/example_emotions.py,shahmohit/pynca,1
"features_train, features_test, labels_train, labels_test = preprocess()


#########################################################
# import SVM algorithm
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# create classifier
clf = SVC(kernel=""rbf"", C=10000)

# reduce training set
#features_train = features_train[:len(features_train)/100]
#labels_train = labels_train[:len(labels_train)/100]

# fit classifier
t0 = time()
clf.fit(features_train, labels_train)
print(""training time: "", round(time()-t0, 3), ""s"")",p5/svm/svm_author_id.py,stefanbuenten/nanodegree,1
"from sklearn import svm
from sklearn import datasets

clf = svm.SVC()
iris = datasets.load_iris()
X, y = iris.data, iris.target
clf.fit(X, y)

import pickle
s = pickle.dumps(clf)
",python/svm_model_persist.py,suresh/notes,1
"                feat_generator=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 1))),
                dict(name=""tfidf_ng2"",
                feat_generator=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,2))),
                dict(name=""tfidf_ng3"",
                feat_generator=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,3))),

           ]

# classifiers
classifiers = [
#                 dict(name=""svm_rbf"",parameter_tunning=False, tune_clf=GridSearchCV(svm.SVC(tol=1e-3,kernel='rbf',cache_size=500),[{'C': [1,10,100] , 'gamma':[0.1,1,10]}] , cv=3 ) ,clf=svm.SVC(tol=1e-3,kernel='rbf',C=20,cache_size=500,gamma=0.8)), 
#                 dict(name=""svm"", parameter_tunning=False, clf=LinearSVC(loss='l2', penalty=""l2"", dual=False, tol=1e-3)),
                dict(name=""Logistic Regression"", parameter_tunning=False, tune_clf=GridSearchCV(LogisticRegression(), [{'penalty': ['l2'], 'C': [1, 10, 100]}], cv=3) , clf=LogisticRegression(penalty='l2', C=1)),
                dict(name=""Passive Aggresive"",parameter_tunning=False, clf = PassiveAggressiveClassifier(n_iter=100)),
                dict(name=""SVM"", parameter_tunning=False, clf=LinearSVC(loss='l2', penalty=""l2"", dual=False, tol=1e-3)),
                dict(name=""Perceptron"",parameter_tunning=False, clf = Perceptron(n_iter=100)),
#                
                dict(name=""bnb"",parameter_tunning=False,clf=BernoulliNB(binarize=0.5)),
                dict(name=""sgd"",parameter_tunning=False,clf=SGDClassifier(loss=""hinge"", penalty=""l2"")),
                dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=3,metric='euclidean')),",python/Definations.py,mahmoudnabil/labr,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",website/js/pypy.js-0.3.0/lib/modules/email/test/test_email.py,jedie/pypyjs-standalone,1
"    randomize_split_params : dict, optional (default={})
        Maps names of split classifier parameters to functions which generate
        random values.
        
    randomize_leaf_params : dict, optional (default={})
        Maps names of leaf model params to functions which randomly generate
        their values. 
    """"""
    
    def __init__(self, 
            leaf_model = LinearSVC(), 
            split_classifier = LinearSVC(), 
            num_features_per_node = None, 
            max_depth=3, 
            min_leaf_size=50, 
            randomize_split_params={}, 
            randomize_leaf_params={}, 
            verbose = False):
                
        # check everyone's types -- I can't give up the OCaml instincts ",treelearn/oblique_tree.py,capitalk/treelearn,1
"            elif transform_type == 't-sne':
                pipeline_steps.append(('transform', pipeline_TSNE(n_components=2, init='pca')))

        # Add estimator
        if estimator in estimator_options:
            if estimator == 'knn':
                pipeline_steps.append(('estimator', KNeighborsClassifier()))
            elif estimator == 'logistic_regression':
                pipeline_steps.append(('estimator', LogisticRegression()))
            elif estimator == 'svm':
                pipeline_steps.append(('estimator', SVC()))
            elif estimator == 'polynomial_regression':
                pipeline_steps.append(('pre_estimator', PolynomialFeatures()))
                pipeline_steps.append(('estimator', LinearRegression()))
            elif estimator == 'multilayer_perceptron':
                pipeline_steps.append(('estimator', MLPClassifier(solver='lbfgs',alpha=1e-5)))
            elif estimator == 'random_forest':
                pipeline_steps.append(('estimator', RandomForestClassifier()))
            elif estimator == 'adaboost':
                pipeline_steps.append(('estimator', AdaBoostClassifier())) #AdaBoostClassifier(n_estimators=100)",pyplearnr/old.py,JaggedParadigm/pyplearnr,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    svd = TruncatedSVD(n_components=2)
    X_reduced = svd.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_random_hasher_sparse_data():
    X, y = datasets.make_multilabel_classification(random_state=0)
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    X_transformed = hasher.fit_transform(X)
    X_transformed_sparse = hasher.fit_transform(csc_matrix(X))",projects/scikit-learn-master/sklearn/ensemble/tests/test_forest.py,DailyActie/Surrogate-Model,1
"    Uses grid search and cross-validation to discover the best meta-parameters for a Support Vector
    Classifier. Prints out the best parameters and best score (using the specified scoring method).
    Returns the best parameters.
    '''
    # Scale variables:
    #scaler = RobustScaler()
    scaler = StandardScaler()
    scaled_X = scaler.fit_transform(X)

    # SVC:
    svc = SVC()

    # Cross-validation and grid search:
    cv = StratifiedShuffleSplit(n_splits = 100, test_size = 0.3, random_state = seed * 2)
    gscv = GridSearchCV(svc, param_grid = param_grid, scoring = scoring, cv = cv, verbose = 1, 
        n_jobs = -1)

    # Fit model:
    gscv.fit(scaled_X, y)
",project/ml_models.py,LucFrachon/enron_fraud_detection,1
"    with warnings.catch_warnings(record=True):
        gat.fit(epochs[0:6])
    gat.predict(epochs[7:])
    gat.score(epochs[7:])

    # Test wrong testing time
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,fraimondo/mne-python,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",PythonD/lib/python2.4/email/test/test_email.py,Eureka22/ASM_xf,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value",shinken/external_command.py,KerkhoffTechnologies/shinken,1
"            ('message', pipeline_message),
            ('numeric', pipeline_numeric),
            ('contains_sha', regex_pipeline(1, r'[0-9a-eA-E]{6,}')),
            # ('contains_http', regex_pipeline(1, r'https?://')),
            # ('contains_bugzilla', regex_pipeline(1, r'bugzilla\.kernel\.org')),
            # ('contains_lkml', regex_pipeline(1, r'lkml\.kernel\.org')),
        ])),
        # ('densifier', Densifier()),
        # ('scaler', StandardScaler(with_mean=False)),
        # ('scaler', StandardScaler()),
        ('clf', LinearSVC()),
        # ('clf', LogisticRegression()),
    ])

    parameters = {
        'features__summary__vect__max_df': (0.25, 0.5),
        # 'features__summary__vect__max_df': (0.5, 0.75, 1.0),
        'features__summary__vect__max_features': (None, 10, 100, 1000),
        # 'features__summary__vect__max_features': (None, 5000, 10000, 50000),
        # 'features__summary__vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams",models.py,aplanas/hackweek11,1
"                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            print ""*"" * 60
            for x in X:
                print x[:4]
            print y
            self.svm = GridSearchCV(SVC(C=0.5, probability=True), param_grid, cv=5).fit(X, y)

        return training_result

    def remove_face(self, identity):
        X = []
        y = []

        remove_faces = []
        identities_set = set()",project/face_api_server/face_service.py,kanak87/oldboy_rep,1
"                                                             * RandomForest

    Output:  - model: A trained scikit-learn One-vs-All multi-label scheme of linear SVC models.
    """"""
    if classifier_type == ""LinearSVC"":
        if X_train.shape[0] > X_train.shape[1]:
            dual = False
        else:
            dual = True

        model = OneVsRestClassifier(LinearSVC(C=svm_hardness, random_state=0, dual=dual,
                                              fit_intercept=fit_intercept),
                                    n_jobs=number_of_threads)
        model.fit(X_train, y_train)
    elif classifier_type == ""LogisticRegression"":
        if X_train.shape[0] > X_train.shape[1]:
            dual = False
        else:
            dual = True
",reveal_graph_embedding/learning/classification.py,MKLab-ITI/reveal-graph-embedding,1
"from analyze.results_utils import CLASSIFIER_COL_NAME, DATASET_COL_NAME
from analyze.compare_algos import extractBestResults, avgRanks
from analyze.plot_results import plotErrs, plotZvals, plotPvals
# from analyze.viz_clusters import showClusters
from viz.clusters import *
# from viz.clusters import makeKMeans, makeMeanShift, makeSparseClusterer

def classify(name, Xtrain, Ytrain, Xtest, Ytest):
	rbfParams = [{'C': [.01, .1, 1, 10, 100]}]
	linearParams = [{'C': [.01, .1, 1, 10, 100]}]
	rbf = (SVC(), rbfParams)
	linearSVM = (LinearSVC(), linearParams)
	# classifiers = [rbf, linearSVM, LDA()] # lda is slow / hanging on medImgs
	classifiers = [rbf, linearSVM]
	# classifiers = [linearSVM]

	d = [(CLASSIFIER_COL_NAME, classifiers)]

	df = tryParams(d, Xtrain, Ytrain, Xtest, Ytest, cacheBlocks=False)
	df[DATASET_COL_NAME] = pd.Series([name] * df.shape[0])",python/main/main.py,dblalock/flock,1
"        grid_values = [c_values, kernel_values, gamma_values, degree_values]
        if not False in [len(x) == 1 for x in grid_values]: # only sinle parameter settings
            settings = {}
            for i, parameter in enumerate(parameters):
                settings[parameter] = grid_values[i][0]
        else:
            iterations=int(iterations)
            param_grid = {}
            for i, parameter in enumerate(parameters):
                param_grid[parameter] = grid_values[i]
            model = svm.SVC(probability=True)
            if multi:
                model = OutputCodeClassifier(model)
            paramsearch = RandomizedSearchCV(model, param_grid, cv = 5, verbose = 2, n_iter = iterations, n_jobs = 10, pre_dispatch = 4)
            paramsearch.fit(trainvectors, self.label_encoder.transform(labels))
            settings = paramsearch.best_params_
        # train an SVC classifier with the settings that led to the best performance
        self.model = svm.SVC(
           probability = True,
           C = settings[parameters[0]],",quoll/classification_pipeline/functions/classifier.py,LanguageMachines/quoll,1
"
			opf_results[i,3] = time()-t
			t = time()
			predicted = O.predict(data_test)
			opf_results[i,0] = precision_score(label_test_32, predicted)
			opf_results[i,1] = recall_score(label_test_32, predicted)
			opf_results[i,2] = f1_score(label_test_32, predicted)
			gc.collect()

		def _svm():
			clf = svm.SVC(C=1000)
			t = time()
			clf.fit(data_train, label_train)
			svm_results[i,3] = time()-t
			predicted = clf.predict(data_test)
			svm_results[i,0] = precision_score(label_test, predicted)
			svm_results[i,1] = recall_score(label_test, predicted)
			svm_results[i,2] = f1_score(label_test, predicted)
			gc.collect()
",examples/benchmark.py,LibOPF/LibOPF,1
"groups = [['AD', 'Normal'], ['AD', 'EMCI'], ['AD', 'LMCI'],
          ['EMCI', 'LMCI'], ['EMCI', 'Normal'], ['LMCI', 'Normal']]
score = np.zeros((nb_iter, len(groups)))
for gr in groups:
    g1_feat = cov_feat[idx[gr[0]][0]]
    g2_feat = cov_feat[idx[gr[1]][0]]
    x = np.concatenate((g1_feat, g2_feat), axis=0)
    y = np.ones(len(x))
    y[len(x) - len(g2_feat):] = 0
    
    estim = SVC(kernel='linear')
    sss = StratifiedShuffleSplit(y, n_iter=nb_iter, test_size=0.2)
    # 1000 runs with randoms 80% / 20% : StratifiedShuffleSplit
    counter = 0
    for train, test in sss:
        Xtrain, Xtest = x[train], x[test]
        Ytrain, Ytest = y[train], y[test]
        Yscore = estim.fit(Xtrain,Ytrain)
        score[counter, pg_counter] = estim.score(Xtest, Ytest)
        counter += 1",covariance_baseline_rs_fmri_adni.py,mrahim/adni_rs_fmri_analysis,1
"        logger.info(""Restoring model from {}"".format(store_path))
        return joblib.load(store_path)

    def store(self, model, store_path):
        logger.info(""Storing model at {}"".format(store_path))
        return joblib.dump(model, store_path)

    def train(self, feature_tensor, correct):
        logger.info(""Training model..."")
        squeezed = feature_tensor.squeeze(axis=1)
        clf = svm.SVC(kernel=self.kernel)
        model = clf.fit(squeezed, correct)
        if self.store_path:
            self.store(model, self.store_path)
        self.model = model
        logger.info(""Training session done"")

    def test(self, feature_tensor):
        logger.info(""Loading model..."")
        self.model = self.restore(self.store_path)",utils/models.py,jimmycallin/master-thesis,1
"		for n_neighbors in [ 2, 4, 8, 16, 32, 64, 128, 256 ] ]

nn_features = pd.DataFrame( { name : knn.predict( X_train ) for name, knn in knn_clf } )

X_train[nn_features.columns] = nn_features


pipeline = Pipeline( [
	( ""features"", combined_features ),
## Use rbf-kernel svm
	# ( ""svm"", SVC( kernel = ""linear"" ) ),
	( ""svm"", SVC( kernel = ""rbf"" ) ),
	# ( ""forest"", RandomForestClassifier( ) ),
] )



le = LabelEncoder( ).fit( df[df.columns[-1]].values )

i_train, i_hat = le.transform( y_train ), le.transform( nn_features.values[:,8] )",data_study/otto_group/main.py,ivannz/study_notes,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",imblearn/metrics/tests/test_classification.py,chkoar/imbalanced-learn,1
"    transform = True

    svc_params = {'penalty':'l2',
                  'loss':'l2', 
                  'dual':False,
                  'C':33.0, 
                  'intercept_scaling':1e4, 
                  'class_weight':'auto',
                  'random_state':42}

    bc_params = {'base_estimator':LinearSVC(**svc_params),
                 'n_estimators':96, 
                 'max_samples':0.1, 
                 'max_features':0.8,  
                 'oob_score':False,
                 
                 # if you have tons of memory (i.e. 32gb ram + 32gb swap)
                 #  incresaing this parameter may help performance.  else,
                 #  increasing it may cause ""out of memory"" errors.
                 'n_jobs':1,",phil/bagged_svm.py,drewabbot/kaggle-seizure-prediction,1
"# Test options and evaluation metric
seed = 7
scoring = 'accuracy'
# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())",Machine Learning iris.py,Gokultalele/Iris,1
"data -= data.mean(axis=0)

# We learn the digits on the first half of the digits
data_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]

# Now predict the value of the digit on the second half:
data_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:]
# data_test = scaler.transform(data_test)

# Create a classifier: a support vector classifier
kernel_svm = svm.SVC(gamma=.2)
linear_svm = svm.LinearSVC()

# create pipeline from kernel approximation
# and linear svm
feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
feature_map_nystroem = Nystroem(gamma=.2, random_state=1)
fourier_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_fourier),
                                        (""svm"", svm.LinearSVC())])
",projects/scikit-learn-master/examples/plot_kernel_approximation.py,DailyActie/Surrogate-Model,1
"#         (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
#         (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
#     print('=' * 80)
#     print(name)
#     results.append(benchmark(clf))

# for penalty in [""l2"", ""l1""]:
#     print('=' * 80)
#     print(""%s penalty"" % penalty.upper())
#     # Train Liblinear model
#     results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
#                                             dual=False, tol=1e-3)))

#     # Train SGD model
#     results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
#                                            penalty=penalty)))

# # Train SGD with Elastic Net penalty
# print('=' * 80)
# print(""Elastic-Net penalty"")",data/test_algo.py,ajribeiro/rtapp,1
"vals,bins = np.histogram(dist,20,density=True)

Y = true_Y.copy()
for i in range(n_samples):
    j = np.searchsorted(bins,dist[i])
    if rs.rand() < (bins[j]-bins[j-1])*vals[j-1]:
        Y[i] = not Y[i]


from sklearn.svm import LinearSVC
svc = LinearSVC()
svc.fit(X,Y)
estm_w = svc.coef_[0].copy()
estm_t = -svc.intercept_[0] / np.linalg.norm(estm_w)
estm_w /= np.linalg.norm(estm_w)

Z = X - estm_t*estm_w
estm_Q = np.c_[estm_w, np.zeros((n_dim,n_dim-1))]
estm_Q,R = np.linalg.qr(estm_Q)
estm_Q = -estm_Q if R[0,0] < 0 else estm_Q",code/py/boundary-preserving-pca.py,notmatthancock/notmatthancock.github.io,1
"		'mysource.cpp'
	]

	if WengoOSWindows():
		libs += [
			'libwindows1',
			'libwindows2'
		]
		defines['WIN32'] = 1

	if WengoCCMSVC():
		cc_flags += ['-myflag']

	if WengoOSLinux():
		libs += ['liblinux1']
		defines['LINUX'] = 1
		sources += ['mylinuxsource.cpp']

	env.WengoAddLibPath(lib_path)
	env.WengoAddCCFlags(cc_flags)",shell/build_webrtc_audio/Env/environment.py,erikge/iOS_trick,1
"def test_classifler(classifler, X, y):
    return cross_validation.cross_val_score(classifler, X, y, cv=5)


def test_knn(X, y):
    model = KNeighborsClassifier(10)
    return test_classifler(model, X, y).mean()


def test_svm(X, y):
    model = SVC()
    return test_classifler(model, X, y).mean()


def test_knn_and_snm(data_file):
    X, y = ml.load_data(data_file)
    return test_knn(X, y), test_svm(X, y)


def test_data():",test/run.py,SquirrelMajik/GRec,1
"	testDataset = Orange.preprocess.selectPRandom(orangeData, P=25)

	# Extract Train Dataset
	trainDataset = [d for d in orangeData if d not in testDataset]

	# Convert Train Dataset
	converted_train_data = np.array([[ d[f].value for f in orangeData.domain if f != orangeData.domain.class_var] for d in trainDataset])
	converted_train_targets = np.array([d[orangeData.domain.class_var] for d in trainDataset ])

	# Train SVM
	clf = svm.SVC(kernel='linear')
	clf.fit(converted_train_data, converted_train_targets)

	# Testing

	# Convert Test Dataset
	converted_test_data = [[ d[f].value for f in orangeData.domain if f !=	orangeData.domain.class_var] for d in testDataset]
	converted_test_targets = [d[orangeData.domain.class_var] for d in testDataset ]

	# Confusion Matrix",train.py,Sh1n/AML-ALL-classifier,1
"    if output:
        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',
            Instances=n_instances, Results=metrics_results)

if __name__ == '__main__':
    from nltk.classify import NaiveBayesClassifier, MaxentClassifier
    from nltk.classify.scikitlearn import SklearnClassifier
    from sklearn.svm import LinearSVC

    naive_bayes = NaiveBayesClassifier.train
    svm = SklearnClassifier(LinearSVC()).train
    maxent = MaxentClassifier.train

    demo_tweets(naive_bayes)
    # demo_movie_reviews(svm)
    # demo_subjectivity(svm)
    # demo_sent_subjectivity(""she's an artist , but hasn't picked up a brush in a year . "")
    # demo_liu_hu_lexicon(""This movie was actually neither that funny, nor super witty."", plot=True)
    # demo_vader_instance(""This movie was actually neither that funny, nor super witty."")
    # demo_vader_tweets()",logisland-plugins/logisland-scripting-processors-plugin/src/main/resources/nltk/sentiment/util.py,MiniPlayer/log-island,1
"    fname = ""{}/reps.csv"".format(args.workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)

    labelsNum = le.transform(labels)

    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))

    if args.classifier == 'LinearSvm':
        clf = SVC(C=1, kernel='linear', probability=True)
    elif args.classifier == 'GridSearchSvm':
        print(""""""
        Warning: In our experiences, using a grid search over SVM hyper-parameters only
        gives marginally better performance than a linear SVM with C=1 and
        is not worth the extra computations of performing a grid search.
        """""")
        param_grid = [
            {'C': [1, 10, 100, 1000],
             'kernel': ['linear']},",face_register/classifier.py,SeonghoBaek/RealtimeCamera,1
"        learning_rate = 0.1,
        n_estimators = 100,
        max_depth = 3,
        subsample = 0.9,
        colsample_bytree = 0.9,
        silent = False
        )

#clf = RandomForestClassifier(n_estimators=100,max_features=17)  

#clf = svm.SVC()        
        
clf.fit(X_train,Y_train)

print('good')

importance = clf.feature_importances_

dfi = pd.DataFrame(importance, index=df.columns, columns=[""Importance""])
dfi = dfi.sort_values(['Importance'],ascending=False)",Code/Spotify-XGBClassifier.py,JerryGuangXu/Spotify-Music-Data-Analysis,1
"    pickle.dump(data[""scaler""], open(os.path.join(MODEL_DIR, prefix + ""scaler.p""), 'wb'))
    
    
    
    #### Support vector machines and leafy classifiers
    
    
    classifier_list = [
        (""libsvm"", lambda: svm.SVC(kernel='linear', class_weight='auto', C=0.001)),
        (""logreg"", lambda: svm.LinearSVC(C=0.01, penalty=""l1"", dual=False, class_weight='auto')),
        (""rbf_svm"", lambda: svm.SVC(kernel='rbf')),
        (""rbf_nu_svm"", lambda: svm.NuSVC(kernel='rbf')),
        (""random_forest"", lambda: ensemble.RandomForestClassifier(criterion=""entropy"", n_estimators=50, min_samples_split=3, max_depth=6)),
        (""decision_tree"", lambda: tree.DecisionTreeClassifier(criterion=""entropy"", min_samples_split=3, max_depth=6)),
        (""naive_bayes"", lambda: naive_bayes.GaussianNB())
    ]

    for name, clf_gen in classifier_list:
        try:
            clf = clf_gen()",scripts/train.py,sebschu/cds-detector,1
"         label='petal width')
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.legend(loc='upper left')
plt.xscale('log')
plt.show()

# svm
from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=test_idx, xlabel='petal length [standardized]', ylabel='petal width [standardized]')

# kernel svm
for gamma in [0.2, 100]:
    svm = SVC(kernel='rbf', random_state=0, gamma=gamma, C=1.0)
    svm.fit(X_train_std, y_train)
",code/ch03/iris.py,xdnian/pyml,1
"  X = array[:,1:10]
  Y = array[:,0]
  X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)
  num_instances = len(X_train)
  results, names, models = [], [], []
  models.append(('Logistic Regression', LogisticRegression()))
  models.append(('Linear Discrimination Analysis', LinearDiscriminantAnalysis()))
  models.append(('K-Neighbors Classifier', KNeighborsClassifier()))
  models.append(('Decision Tree Classifier', DecisionTreeClassifier()))
  models.append(('Gaussian NB', GaussianNB()))
  models.append(('SVC', SVC()))
  for name, model in models:
    model.fit(X_train, Y_train)
    predictions = model.predict(X_validation)
    with open(file, 'a') as fo:
      fo.write(str(""#""*70))
      fo.write(str(""\n""))
      fo.write(str(name))
      fo.write(str(""\n""))
      fo.write(str(accuracy_score(Y_validation, predictions)))",mood/select_model.py,YeasterEgg/spotify_classify,1
"#    print_debug(train_labels, ""train_labels"")

    for j in range(TEST_SIZE):
        test_labels[j] = np.sum(np.multiply(converter, mnist.test.labels[j, :]))

#    print_debug(test_features, ""test_features"")
#    print_debug(test_labels, ""test_labels"")

    initial_time = time.time()

    clf = svm.SVC(kernel=krnl)
    clf.fit(train_features, train_labels)
    training_time = time.time()-initial_time
    print(""\nTraining Time = "", training_time)

    accuracy = clf.score(test_features, test_labels)
#    test_time = time.time() - (training_time + initial_time)
#    print(""\nTest Time = "", test_time)

    print(""\n"", krnl, ""kernel SVM accuracy ="", accuracy)",main.py,dlmacedo/SVM-CNN,1
"    clf.fit(X_train, y_train)
    end = time.time()
    print('Training Time: '+str((end - start))+'s')

    y_pred = clf.predict(X_test)

    print np.sum(y_pred == y_test)/len(y_pred)
    return y_pred

def getSVMClassifier():
    return svm.SVC(kernel='rbf', C=10, gamma=10, probability=True)

def getSVMLinearClassifier():
    return svm.SVC(kernel='linear', C=10, probability=True)

def getSimpleRDFClassifier():
    rdf = RandomForestClassifier(max_features = 'auto', max_depth=10)
    return rdf

def getCustomRDFClassifier():",src/classifiers.py,javierfdr/credit-scoring-analysis,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 15, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/parameter_c_dsessvc/setup.py,jpzk/evopy,1
"

#
# TODO: Create an SVC classifier. Leave C=1, but set gamma to 0.001
# and set the kernel to linear. Then train the model on the training
# data / labels:
print (""Training SVC Classifier..."")
#
# .. your code here ..
from sklearn.svm import SVC
model = SVC(C=1, gamma=0.001, kernel='linear')
model.fit(X_train, y_train)


# TODO: Calculate the score of your SVC against the testing data
print (""Scoring SVC Classifier..."")
#
# .. your code here ..
score = model.score(X_test, y_test)
print (""Score:\n"", score)",Module6/assignment2.py,jeffmkw/DAT210x-Lab,1
"        for i in range(5):
            print 'case ' + str(i)
            feature_train, feature_test, label_train, label_test = cross_validation.train_test_split(features, classes, test_size=0.2, random_state=0)

            if trainMode == 'MLP':
                #this requires scikit-learn 0.18
                model = MLPClassifier(algorithm='sgd', activation='logistic', learning_rate_init=0.02, learning_rate='constant', batch_size=10)
            elif trainMode == 'RF':
                model = ExtraTreesClassifier(n_estimators=50, random_state=0)
            else:
                model = svm.SVC()

            model.fit(feature_train, label_train)
            predictions = model.predict(feature_test)

            correctCount = 0.0
            totalCount = 0.0
            if len(predictions) != len(label_test):
                print 'inference error!'
                resultFile.write('inferece error!\n')",basicModel.py,renhaocui/adPlatform,1
"def depth_based_selection(root=default_root, max_depth=default_max_depth):
    relation_cache = CategoryRelationCache(
        subcat_index_file=util.resource('wikipedia/uri-to-subcats'),
        supercat_index_file=util.resource('wikipedia/uri-to-supercats'))
    full_selection = CategorySelection(root, max_depth=max_depth, relation_cache=relation_cache)
    full_selection.run()
    return full_selection


def default_classifier(**params):
    return SVC(**params)


default_features = TopicFeatures({
 'unity' : lambda selection, node: 1,
 'normalized_depth': normalized_depth_feature,
 'frac_parents_in_graph': frac_parents_in_graph_feature,
#  'frac_coparents_in_graph': frac_coparents_in_graph_feature,
#  'frac_children_with_parents_in_graph': frac_children_with_parents_in_graph_feature,
 'parent_max_jaccard_sim': parent_max_jaccard_similarity,",dswont/topics.py,anonymous-ijcai/dsw-ont-ijcai,1
"# License: BSD (3-clause)

from sklearn.svm import SVC, LinearSVC
from sklearn.datasets import make_classification
from gat.classifiers import SVC_Light, LinearSVC_Proba

# setup dataset --------------------------------------------------------------
X, y = make_classification(n_informative=10, n_classes=2)

# 1. Classic pipeline --------------------------------------------------------
svc = SVC(kernel='linear')
svc.fit(X, y)
y_pred = svc.decision_function(X)
score = svc.score(X, y)

# 2. Linear SVC ---------------------------------------------------------------
linearsvc = LinearSVC()
linearsvc.fit(X, y)
y_pred = linearsvc.decision_function(X)
score_linear = linearsvc.score(X, y)",sandbox/decoding/svm_light.py,kingjr/meg_perceptual_decision_symbols,1
"
def sgrna_from_doench_on_fold(feature_sets, train, test, y, y_all, X, dim, dimsum, learn_options):
    assert len(feature_sets.keys()) == 1, ""should only use sgRNA Score here""
    assert feature_sets.keys()[0] == ""sgRNA Score""
    y_pred = X[test][:, 0]
    return y_pred, None


def SVC_on_fold(feature_sets, train, test, y, y_all, X, dim, dimsum, learn_options):
    y_bin = y_all[learn_options['binary target name']].values[:, None]
    clf = LinearSVC(penalty='l2', dual=False)
    clf.fit(X[train], y_bin[train].flatten())
    #y_pred = clf.predict(X[test])[:, None] # this returns 0/1
    y_pred = clf.decision_function(X[test])[:, None]
    return y_pred, clf


    ",azimuth/models/baselines.py,mayavanand/RMMAFinalProject,1
"        support_multipliers = lagrange_multipliers[support_vector_indices]
        support_vectors = X[support_vector_indices]
        support_vector_labels = y[support_vector_indices]

        # http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf
        # bias = y_k - \sum z_i y_i  K(x_k, x_i)
        # Find error from SVC for each sample
        if bias is None:  # bias is None for Lagrangian()
            # use support vectors to compute difference from truth set
            bias = np.mean(
                [y_k - SVC(SMO, self.kernel, 0.0).predict(x_k,
                    weights=support_multipliers,
                    sv=support_vectors,
                    svl=support_vector_labels)
                 for (y_k, x_k) in zip(support_vector_labels, support_vectors)])

        print 'bias {}'.format(bias)
        return bias, support_multipliers, support_vectors, support_vector_labels,

def eval_f(x, X, y, kernel, alphas, bias):",Homeworks/HW6/mysvm.py,alliemacleay/MachineLearning_CS6140,1
"    #labels = np.zeroes(shape=(250,1))
    h = 0.02
    C =1.0
    #with open(filename, 'r') as procFile:
    #    for line in procFile:
    #            features.appen

    features = np.genfromtxt(filename, dtype=int, delimiter=' ', usecols=(0, 1))
    labels = np.genfromtxt(filename, dtype=int, delimiter=' ', usecols=(-1))
    start = clock()
    svc = svm.SVC(kernel='linear', C=C).fit(features, labels)
    end = clock()
    print ""SVC Runtime: "" + str(end - start)

    start = clock()
    rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(features, labels)
    end = clock()
    print ""RBFSVC Runtime: "" + str(end - start)

    ''''SVC with polynomial (degree 3) kernel',",wip-scripts_data/linearSvcProc.py,gbugaisky/bimm_185_conotoxin,1
"ind = [i for i in range(150)]
random.shuffle(ind)
X=X[ind]
y=y[ind]
train_x = X[0:100]
test_x = X[100:]

train_y = y[0:100]
test_y = y[100:]

clf = svm.SVC()
clf.fit(train_x, train_y)
p = list(clf.predict(test_y))
",3A/UV403/svm/classifier_svm2.py,jianfeipan/TB,1
"    if verbose:
        print('vectorizing...')
    t0 = time()
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')
    X_train = vectorizer.fit_transform(train_data)
    duration = time() - t0
    print('vectorized in {:.2f} seconds.'.format(duration))

    penalty = 'l2'
    clf = LinearSVC(loss='l2', penalty=penalty, dual=False, tol=1e-3)

    if verbose:
        print('training model...')
    clf.fit(X_train, y_train)

    if classdb == 'yelp':
        print('categorizing Yelp data...')
        classdf = get_yelp_reviews(engine, remove_shorts=False)
    if classdb == 'ta':",code/classify_review_type.py,mattgiguere/doglodge,1
"        #for int or float: fill NaN
        tmp_len = len(train[train_series.isnull()])
        if tmp_len>0:
            train.loc[train_series.isnull(), train_name] = -999
        tmp_len = len(test[test_series.isnull()])
        if tmp_len>0:
            test.loc[test_series.isnull(), test_name] = -999


kf = KFold(train.shape[0], n_folds=3, random_state=1)
#alg = svm.SVC(max_iter=300)
alg=Pipeline([
  #('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=100,criterion= 'entropy'))),
  ('classification',svm.SVC(max_iter=500) )
])
 
#RandomForestClassifier(n_estimators=100,criterion= 'entropy')
for c in train.columns:
    predictions = []
    for trainkf, test in kf:",svm.py,souravsarangi/BNPParibasKaggle,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",windows/python/Lib/email/test/test_email.py,gusai-francelabs/datafari,1
"    trn, trn_lbl, tst, new_features= blor.get_new_table(test, tst_ents, 1)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    ",problems/alg10_ohsumed_flatten_depth.py,lioritan/Thesis,1
"    data = readData(annotationPath)

    # Split the data into features (x) and labels (y)
    df = data[['neutral', 'happiness', 'sadness', 'anger', 'fear', 'surprise',
               'disgust']]

    x = np.array(df.values.tolist())
    y = np.array(data['game'].tolist())

    # Create the SVM classifier
    clf = svm.SVC(kernel='rbf', gamma=0.001, C=10, decision_function_shape='ovr')

    # Perform the cross validation
    scores = cross_val_score(clf, x, y, cv=5, n_jobs=-1)

    print(scores)

    return 0

#---------------------------------------------",fsdk/detectors/test-emotions.py,luigivieira/fsdk,1
"        solver = ['lbfgs']
        max_iter = [1000]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(mlp, dict(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter, early_stopping=[False]), X, y)
        f = open('output/age.mlp.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates SVM
        svm = SVC()
        kernel = ['linear', 'rbf', 'poly', 'sigmoid']
        Cs = np.logspace(-3, 4, 8) # C = [0.001, 0.01, .., 1000, 10000]
        gamma = np.logspace(-3, 4, 8) # gamma = [0.001, 0.01, .., 1000, 10000]
        degree = [2, 3]
        coef0 = [0.0]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(svm, dict(kernel=kernel, C=Cs, gamma=gamma, degree=degree, coef0=coef0), X, y)
        f = open('output/age.svm.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):",tests/test_age.py,fberanizo/author-profiling,1
"    print('generic percentage: %f%%' % (float(count_generic)/len(my_records)))

    my_labels = numpy.array([record['specific'] == 'yes' for record in my_records])

    classifiers = [
        DummyClassifier(strategy='most_frequent', random_state=0),
        DummyClassifier(strategy='stratified', random_state=0),
        DummyClassifier(strategy='uniform', random_state=0),
        # DummyClassifier(strategy='constant', random_state=0, constant=True),
        LogisticRegression(C=100),
        SVC(C=1.0, kernel='rbf'),
        SVC(C=1.0, kernel='linear'),
        KNeighborsClassifier(n_neighbors=10),
        tree.DecisionTreeClassifier(),
        NuSVC(),
        LinearSVC()
    ]
    scores = [[] for _ in range(len(classifiers))]

    Xtrans = my_metrics",source/python/topicmodeling/hiddenfactortopics/main.py,melqkiades/yelp,1
"        available_test = pd.notnull(month_test_df['X'])
        month_train_df = month_train_df[available_train]
        month_train_category = month_train_category[available_train]
        month_test_df = month_test_df[available_test]

        #standlize the data
        month_train_df,scaler = preprocess_data(month_train_df)
        month_test_df,_ = preprocess_data(month_test_df, scaler)
        
        #train model
        clf = svm.SVC(probability = True)
        clf.fit(month_train_df, month_train_category)
        print(""Predicting on {0} rows"".format(month_test_df))
        temp_predictions = clf.predict_proba(month_test_df)
        
        #concatenates results
        temp_predictions = pd.DataFrame(temp_predictions)
        temp_predictions.columns = clf.classes_
        #print temp_predictions
        predictions = predictions.append(temp_predictions, ignore_index=True)",src/month_windows.py,gnu-user/sf-crime-classification,1
"        self.setBestPipelines()

        #[m() for m in evaluators]
        return self

    def defineAlgorithms(self):

        models = []
        #LDA : Warning(Variables are collinear)
        models.append(('LDA', LinearDiscriminantAnalysis()))
        models.append(('SVC', SVC()))
        models.append(('GaussianNB', GaussianNB()))
        models.append(('KNeighborsClassifier', KNeighborsClassifier()))
        models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))
        models.append(('LogisticRegression', LogisticRegression()))

        #Bagging and Boosting
        #models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=150)))
        models.append(('ExtraTreesClassifier', ExtraTreesClassifier()))
        models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))",pymach/evaluate.py,gusseppe/pymach,1
"    
    max_learner_count = 25
    rat_scores = dict()
    all_scores = defaultdict(list)
    score_dump_file = working_dir + '/results/%s-%d-%s.dmp' % \
                (method, cv_index, str(uuid.uuid1()))

    if method == 'all' or method == 'others':

        log('svms')
        model = sklearn.svm.SVC()
        params = {'C': pow(2.0, np.arange(-10, 11)), 'gamma': pow(2.0, np.arange(-10, 11)),
                      'kernel': ['linear', 'rbf']}
        machine = sklearn.grid_search.RandomizedSearchCV(model, param_distributions=params,
                                                               n_iter=100, n_jobs=cpu_count, cv=10,
                                                               verbose=0)
        machine.fit(Xtrain, ytrain)
        scores = sklearn.metrics.average_precision_score(ytest, machine.decision_function(Xtest))
        log('svm\t%s' % scores)
        this_method = 'SVM'",run.py,adrinjalali/Network-Classifier,1
"y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1  
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),  
                     np.arange(y_min, y_max, h))  
  
''''' SVM '''  
# title for the plots  
titles = ['LinearSVC (linear kernel)',  
          'SVC with polynomial (degree 3) kernel',  
          'SVC with RBF kernel',  
          'SVC with Sigmoid kernel']  
clf_linear  = svm.SVC(kernel='linear').fit(x, y)  
#clf_linear  = svm.LinearSVC().fit(x, y)  
clf_poly    = svm.SVC(kernel='poly', degree=3).fit(x, y)  
clf_rbf     = svm.SVC().fit(x, y)  
clf_sigmoid = svm.SVC(kernel='sigmoid').fit(x, y)  
  
for i, clf in enumerate((clf_linear, clf_poly, clf_rbf, clf_sigmoid)):  
    answer = clf.predict(np.c_[xx.ravel(), yy.ravel()])  
    print(clf)  
    print(np.mean( answer == y_train))  ",SVM/SVMs.py,imyeego/MLinPy,1
"		train_data = []
		y = []
		for train_data_index in train_data_indices:
			(data, y_temp) = data_sets[train_data_index]
			train_data.extend(data)
			y.extend(y_temp)
		
		X_train = count_vect.transform(train_data)
		y_train = np.asarray(y)

		clf =svm.SVC()

		# clf = Classifier(
		# 	layers = [
		# 		Layer(""Sigmoid"", units = 10),
		# 		Layer(""Sigmoid"", units = 7),
		# 		Layer(""Softmax"")
		# 	],
		# 	learning_rate = 0.01,
		# 	n_iter = 100)",DeepLearningWithWordNet.py,J-A-S-A/PSA,1
"    idx = 10 
    cols = list(train_df.columns.values)[:idx]
    train_df[cols] = StandardScaler().fit_transform(train_df[cols])
    test_df[cols] = StandardScaler().fit_transform(test_df[cols])

    X_data = train_df.values            
    X_train,X_test,y_train,y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=123)
    
    #grid search for SVM hyperparameters
    svm_parameters = [{'kernel': ['rbf'], 'C': [1,10,100,1000]}]                
    clf = GridSearchCV(SVC(), svm_parameters, cv=3, verbose=2)
    clf.fit(X_train, y_train)    
    clf.best_params_
    clf.grid_scores_
    
    #SVM training
    C_opt = 10
    clf = SVC(C=C_opt, kernel='rbf')
    clf.fit(X_data, y_data)    
    clf.n_support_",forest_cover/forest_kernel.py,vsmolyakov/kaggle,1
"                           clf.predict_proba(self.X_train))
        assert_array_equal(adapter.predict_real(self.X_train),
                           clf.predict_proba(self.X_train))

    def test_adapt_logistic_regression(self):
        adapter = SklearnProbaAdapter(LogisticRegression(random_state=1126))
        clf = LogisticRegression(random_state=1126)
        self.check_functions(adapter, clf)

    def test_adapt_linear_svc(self):
        adapter = SklearnAdapter(LinearSVC(random_state=1126))
        clf = LinearSVC(random_state=1126)
        self.check_functions(adapter, clf)

    def test_adapt_knn(self):
        adapter = SklearnAdapter(KNeighborsClassifier())
        clf = KNeighborsClassifier()
        self.check_functions(adapter, clf)

",libact/models/tests/test_sklearn_adapter.py,ntucllab/libact,1
"
    ret = []

    for k in sorted( f.keys() ):
        ret.append( f[k] )

    return ret

def learn( data, labels ):

    estimator = svm.SVC()
    grid = [
        {'C': numpy.arange( 0.5 , 10, .5 ), 'gamma': numpy.arange( .0001, .1, .0005) , 'kernel': ['rbf', 'sigmoid'] },
    ]

    model = grid_search.GridSearchCV( estimator , grid, cv = 10, verbose = 5 )

    data = numpy.array( data )
    labels = numpy.array( labels )
",app/alpha/dqi.py,HIIT/deliberation-classifier,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import PassiveAggressiveClassifier

iris = datasets.load_iris()
data, y = iris.data, iris.target
rng = np.random.RandomState(0)


def test_transform_linear_model():
    for clf in (LogisticRegression(C=0.1),
                LinearSVC(C=0.01, dual=False),
                SGDClassifier(alpha=0.001, n_iter=50, shuffle=True,
                              random_state=0)):
        for thresh in (None, "".09*mean"", ""1e-5 * median""):
            for func in (np.array, sp.csr_matrix):
                X = func(data)
                clf.set_params(penalty=""l1"")
                clf.fit(X, y)
                X_new = assert_warns(
                    DeprecationWarning, clf.transform, X, thresh)",phy/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_from_model.py,marcsans/cnn-physics-perception,1
"    trn, trn_lbl, tst, feature_names= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=100)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/compete_alg_new.py,lioritan/Thesis,1
"    C = model_dict['penconst']
    penalty = model_dict['penalty']
    if adv is None:
        adv_mag = None

    # Create model based on parameters
    if svm_model == 'linear':
        dual = True
        if penalty == 'l1':
            dual = False
        clf = svm.LinearSVC(C=C, penalty=penalty, dual=dual)
        # clf = linear_model.SGDClassifier(alpha=C,l1_ratio=0)
    elif svm_model != 'linear':
        clf = svm.SVC(C=C, kernel=svm_model)

    # Train model
    clf.fit(X_train, y_train)
    print('Finish training in {:d}s'.format(int(time.time() - start_time)))

    # Save model",lib/utils/svm_utils.py,arjunbhagoji/ml_defense,1
"        'train_results': os.sep.join(['res', 'results', ALGORITHM + '_train.csv']),
        'test_results': os.sep.join(['res', 'results', ALGORITHM + '_test.csv']),
        'train_data': os.sep.join(['res', 'features', '']) + 'TrainData_' + KEYWORD + '.pkl',
        'test_data': os.sep.join(['res', 'features', '']) + 'TestData_' + KEYWORD + '.pkl'
    }
    environment.print_lines(5)
    print(""Testing file counts {}"".format(TEST_FILE_COUNTS))

    # learner = mixture.GMM(n_components=len(TRAINING_DIRS))
    # learner = GaussianNB()
    learner = SVC()
    evaluate_classifier(paths, learner)",learn.py,aagnone3/audio_analysis,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0531_2015.py,magic2du/contact_matrix,1
"from sklearn import svm
X = [[0.23, 0.13, 0.17], 
     [0.89, 0.92, 0.95]]
Y = [0, 1]
clasificador = svm.SVC()
clasificador.fit(X,Y)",plantilla/presentacion/sourceCode/entrenamientoSVM.py,antoniosv/homomorphic-counter,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load.py,magic2du/contact_matrix,1
"# -*- coding: utf-8 -*-

import subprocess as subp
from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target
clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

data = Porter(clf, language='c').export(details=True)

# Save model:
with open(data.get('filename'), 'w') as f:
    f.write(data.get('model'))

# Compile model:",examples/classifier/LinearSVC/c/compilation.py,nok/sklearn-porter,1
"
    # a carefully hand-designed dataset lol
    y[7] = 0
    y[27] = 0
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

    for ax, C in zip(axes, [1e-2, 1, 1e2]):
        discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)

        svm = SVC(kernel='linear', C=C, tol=0.00001).fit(X, y)
        w = svm.coef_[0]
        a = -w[0] / w[1]
        xx = np.linspace(6, 13)
        yy = a * xx - (svm.intercept_[0]) / w[1]
        ax.plot(xx, yy, c='k')
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())",mglearn/plot_linear_svc_regularization.py,JoostVisser/ml-assignment2,1
"		j += 1

precision, recall, f1, accuracy, support, fn = 0, 0, 0, 0, 0, 0

loo = LeaveOneOut()

start = timer()
for train, test in loo.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf = LinearSVC().fit(X_train, y_train)
	y_pred = clf.predict(X_test)
	precision += precision_score(y_test, y_pred, average = 'micro')
	recall += recall_score(y_test, y_pred, average = 'micro')
	f1 += f1_score(y_test, y_pred, average = 'micro')
	accuracy += accuracy_score(y_test, y_pred)
	y = y_test - y_pred
	fn += sum(y[y > 0]) / len(y_test)
end = timer()
",LeaveOneOut/SVM.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"stacking_create_training_set(path_to_file+'ensemble_duke_output_raw_n%d.txt' %N,path_to_file+'training_set_n%d.csv' %N, gold_standard_name, N)

#read it and make machine learning on it

data = pd.read_csv(path_to_file+'training_set_n%d.csv' %N)

X = data.values[:,2:(N+2)] #x variables
y = np.array(data['y']) #class variables

#fit an SVM with rbf kernel
clf = SVC( kernel = 'rbf',cache_size = 1000)
#parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
gs_rbf.fit(X,y)
#save the output
output = np.reshape(gs_rbf.predict(X),(len(data),1))

#dump it to file",src/old_core/ensemble_duke_stacking.py,enricopal/STEM,1
"#               }


random_state = 42
n_folds = 2

C_range = np.logspace(-2, 10, 13)
gamma_range = np.logspace(-9, 3, 13)
param_dist = dict(gamma=gamma_range, C=C_range)

clf = SVC(probability=True, cache_size=2048)
# searhc = GridSearchCV(clf, param_dist, random_state=42, cv=2, scoring='log_loss', verbose=3, n_jobs=-1)
skf = cross_validation.StratifiedKFold(target,
                                       n_folds=n_folds,
                                       random_state=random_state)
# cv = StratifiedShuffleSplit(target, n_iter=5, test_size=0.5, random_state=42)
random_search = RandomizedSearchCV(clf,
                                   param_dist,
                                   random_state=random_state,
                                   cv=skf,",src/SVM_parameter_search.py,ternaus/kaggle_otto,1
"    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    
    sc=[]
    for train_index, test_index in kf:
        #print(""TRAIN:"", train_index, ""TEST:"", test_index)
    
        #generated train and test lists, incuding indices of the examples in training/test
        #for the specific fold. Indices starts from 0 now
        
        clf = svm.SVC(C=c, kernel='precomputed')
        train_gram = [] #[[] for x in xrange(0,len(train))]
        test_gram = []# [[] for x in xrange(0,len(test))]
          
        #generate train matrix and test matrix
        index=-1    
        for row in gram:
            index+=1
            if index in train_index:
                train_gram.append([gram[index,i] for i in train_index])",scripts/cross_validation_from_matrix.py,nickgentoo/scikit-learn-graph,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_08_2015_02.py,magic2du/contact_matrix,1
"                train_set[:,index] /= len(hyp_nums)
            else:
                numer, denom = band_name.split("":"")   # split ratios up
                if (numer in avail_bands) and (denom in avail_bands):
                    numerArr = np.reshape(train_set[:,avail_bands.index(numer)],
                                          train_set.shape[0])
                    denomArr = np.reshape(train_set[:,avail_bands.index(denom)],
                                          train_set.shape[0])
                    train_set[:,index] = numerArr/denomArr

        clf = svm.SVC(kernel=svmKern, gamma=svmGamma, C=svmC)
        clf.fit(train_set, training_bands[""label""])
        return clf

    def process_bands(self, band_arr):
        """"""Classify data and output to file.

        Parameters
        ----------
        band_arr: np.NDArray",geonode/wheel/analytics/svmclassifier.py,LabAdvComp/geonode,1
"    # 0-1 scaling
    X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                                        test_size=test_size,
                                                        random_state=0)
    
    #models    
    rbm = BernoulliRBM(random_state=0, verbose=True)
    
    multilabel=  OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, 
                                                        random_state=0))
    


    classifier = Pipeline(steps=[('rbm', rbm), ('multilabel', multilabel)])
       
    ###############################################################################
    # Training
    rbm.learning_rate = 0.06",rbm.py,abarch/expmodel,1
"from sklearn import svm, grid_search, datasets
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}
svr = svm.SVC()
clf = grid_search.GridSearchCV(svr, parameters)
clf.fit(iris.data, iris.target)
print clf.grid_scores_
print 'Best best_estimator_ @@@@@@@@@@@@@@@@@@@'
print clf.best_estimator_",finalproject/gridsearch.py,Technipire/Statistical-Pattern-Recognition,1
"            target.append(int(sline[16]))
            data.append([x for i, x in enumerate(sline) if i not in [0, 16]])


    X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.25, random_state=0)


    # Running SVM

    print ""SVM results""
    clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    scores = cross_validation.cross_val_score(clf, np.asarray(data), np.asarray(target), cv=5) # 5 crossvalidation
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))


    prediction_result = clf.predict(X_test)

    precesion = precision_score(y_test, prediction_result, average='macro')
    recall = recall_score(y_test, prediction_result, average='macro')
",cross_validation.py,Ambuj-UF/Machine-Learning-Theorems,1
"Xtest = np.array(TestVal)
ytest = np.array(TestFam)

print ""FIN DE LA CREATION, LANCEMENT DE SVC""

h = 10000

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)

print ""FIN DE SVC-LINEAR, PREPARATION A L'AFFICHAGE""

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
",Old/9-class-SVM-SVC.py,oubould/TestsML,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.multiclass import OutputCodeClassifier
from sklearn.svm import LinearSVC
import sklearn.svm

def MakeClassification(index,instancesData,classesData,instancesTest,type=""proba"",classifiersType=""normal""):
	classifiers = [
	OneVsRestClassifier(sklearn.svm.SVC(probability=1),4),
	DecisionTreeClassifier(random_state=0),
	KNeighborsClassifier(n_jobs=4),
	MLPClassifier(),
	GaussianNB(),
	OutputCodeClassifier(LinearSVC(random_state=0),code_size=2, random_state=0)
	]
	if (classifiersType == ""ova""):
		classifiers = [
			OneVsRestClassifier(sklearn.svm.SVC(probability=1),4),",src/classifier.py,rsboos/DistributedClassifier,1
"    n_samples = 100
    X, y = make_classification(n_samples=2 * n_samples, n_features=6,
                               random_state=42)

    sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))
    X_train, y_train, sw_train = \
        X[:n_samples], y[:n_samples], sample_weight[:n_samples]
    X_test = X[n_samples:]

    for method in ['sigmoid', 'isotonic']:
        base_estimator = LinearSVC(random_state=42)
        calibrated_clf = CalibratedClassifierCV(base_estimator, method=method)
        calibrated_clf.fit(X_train, y_train, sample_weight=sw_train)
        probs_with_sw = calibrated_clf.predict_proba(X_test)

        # As the weights are used for the calibration, they should still yield
        # a different predictions
        calibrated_clf.fit(X_train, y_train)
        probs_without_sw = calibrated_clf.predict_proba(X_test)
",scikit-learn-0.18.1/sklearn/tests/test_calibration.py,RPGOne/Skynet,1
"from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.utils.multiclass import type_of_target

from helpers.log_config import log_to_info


class RbfSVCClassifier(object):
    def classify(self, mp, x_train, y_train, x_test):
        clf = SVC(cache_size=6000, verbose=True)
        log_to_info('Fitting a RBF SVC to labeled training data...')
        clf = clf.fit(x_train, y_train)
        log_to_info('Predicting test value')
        y_test = clf.predict(x_test)
        log_to_info('Done!')

        return y_test

        # feature_map_nystroem = Nystroem(random_state=1, gamma=1.1, n_components=1000)  # gamma=0.00005,",code/classifiers/rbf_svc_classifier.py,lukaselmer/hierarchical-paragraph-vectors,1
"
def getError(a,p):
	return np.absolute(np.sum(a-p))

def learn_curve(train,tarin_label,valid,valid_label,rangeOf=1,degree=3,kernel=""rbf"",plot=False):
	""""""
		this algorithm print the learning curve
	""""""
	trainError=[]
	validError=[]
	clf=svm.SVC(degree=degree,kernel=kernel)
	for i  in xrange(2,train.shape[0],rangeOf):
		clf.fit(train[:i],tarin_label[:i])
		predictTrain=clf.predict(train[:i])
		predictValid=clf.predict(valid)
		trainError.append(getError(tarin_label[:i],predictTrain))
		validError.append(getError(valid_label,predictValid))
	trainError=np.array(trainError)
	vaildError=np.array(validError)
	if plot==True:",performance_metric.py,madan-ram/TagMe,1
"
# import some data to play with
#iris = datasets.load_iris()
#X = iris.data[:, :2]  # we only take the first two features.
#Y = iris.target
XTmp, Y = datasets.load_svmlight_file(""../SVMData.txt"")
X = XTmp.toarray()

h = .02  # step size in the mesh

clf = svm.SVC(C=1.0, kernel='linear')

# we create an instance of SVM Classifier and fit the data.
clf.fit(X, Y)

# Plot the decision boundary. For that, we will asign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))",kinect/pySVM/test/plotLinearSVC.py,hackliff/domobot,1
"
# ..
# .. dimension reduction ..
pca = decomposition.RandomizedPCA(n_components=150, whiten=True)
pca.fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

# ..
# .. classification ..
clf = svm.SVC(C=5., gamma=0.001)
clf.fit(X_train_pca, y_train)

print 'Score on unseen data: '
print clf.score(X_test_pca, y_test)

",scripts/faces.py,briansudo/Atlas,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_04_22_2015.py,magic2du/contact_matrix,1
"seed = 7
scoring = 'accuracy'

# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)",Python/ml.py,gedman4b/MachineLearning,1
"
        self.partition = partition
        self.train_indices = self.partition.train
        self.test_indices = self.partition.test
        self.test_kernel =  [[self.full_kernel[i][j] for i in self.train_indices] for j in self.test_indices]
        self.train_kernel = [[self.full_kernel[i][j] for i in self.train_indices] for j in self.train_indices]

        self.training_labels = [self.full_labels[i] for i in self.train_indices]
        self.label_set = list(set(self.training_labels))
        self.testing_labels = [self.full_labels[i] for i in self.test_indices] 
        self.svm = svm.SVC(kernel='precomputed', C=self.C)


    def train(self) :
        self.svm.fit(self.train_kernel, self.training_labels)
        return self.training_labels
    
    def test(self) :
        testing = self.svm.predict(self.test_kernel)
        return LearningResult(self.partition.state, self.training_labels, self.testing_labels, testing.tolist())",python/persistence/KernelLearning.py,gpersistence/tstop,1
"             ""Decision Tree"", \
             ""Random Forest"", \
             ""AdaBoost"", \
             ""Naive Bayes"", \
             ""LDA"", \
             ""QDA"" \
            ]

    classifiers = [\
                   KNeighborsClassifier(3), \
                   SVC(kernel='linear', C=0.025), \
                   SVC(gamma=2, C=1), \
                   DecisionTreeClassifier(max_depth=5), \
                   RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), \
                   AdaBoostClassifier(), \
                   GaussianNB(), \
                   LDA(), \
                   QDA()
                  ]
",examples/scikit-learn/examples/general/classifier_comparison.py,KellyChan/Python,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    with use_log_level('error'):
        assert_raises(ValueError, gat.score, epochs2)",mne/decoding/tests/test_time_gen.py,jniediek/mne-python,1
"from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import skflow
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
class Model:
    """"""
    Machine learning models for fitting and predicting data
    """"""
    def __init__(self):
        self.clf = SVC(C = 10,  kernel = ""rbf"", decision_function_shape = ""ovr"")
        self.scaler = StandardScaler()

    def fit(self, X, Y, sample_weight = None):
        X = self.scaler.fit_transform(X)
        if sample_weight is None:
            self.clf.fit(X, Y)
        else:
            self.clf.fit(X, Y, sample_weight = sample_weight)
    ",src/models.py,b29308188/MMAI_final,1
"    ### Training
    training_data = data[training_idx, :]
    training_label = label[training_idx]
    ### Testing 
    testing_data = data[testing_idx, :]
    testing_label = label[testing_idx]

    # Declare the random forest
    #crf = RandomForestClassifier(n_estimators=100, n_jobs=n_jobs)
    #crf = AdaBoostClassifier(n_estimators=100)
    #crf = LinearSVC()
    crf = LDA()

    # Train the classifier
    crf.fit(training_data, training_label)

    # Test the classifier
    pred_labels = crf.predict(testing_data)
    pred_probs = crf.predict_proba(testing_data)
    #pred_probs = crf.decision_function(testing_data)",pipeline/feature-classification/test_classification.py,I2Cvb/data_balancing,1
"    X_scaler = StandardScaler().fit(X)
    scaled_X = X_scaler.transform(X)

    # Define the labels vector
    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))

    # Split up data randomly for training and testing
    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2)

    # Train with SVC
    svc = LinearSVC()
    t = time.time()
    svc.fit(X_train, y_train)
    t_training = round(time.time() - t, 2)

    # computes the accuracy
    accuracy = round(svc.score(X_test, y_test), 4)

    # Print back the training info
    print('color:{}'.format(color_space), 'spatial_size:', spatial_size,",utils.py,tnedev/Vehicle-Detection-and-Tracking,1
"        showSaveReport(report);
    
    return peaks
    
#########################################################################################
##################################################################################
###################################################################################
####Test for runEvaluation###
###Initialize the classifier
##clf = KNeighborsClassifier(1);
#clf = svm.SVC(kernel=""linear"")
##clf = LogisticRegression
#clfName = 'svc_linear'
##clfName = 'RandomForest'
####classifier =DecisionTreeClassifier();
##classifier = RandomForestClassifier();
###################################to run
#dirPath = 'C:\\Users\\LLP-admin\\workspace\\weka\\token-experiment-dataset\\';
#fname = 'user_' + str(0)
#fmt = '.csv'",simpleBatch_v1.py,cocoaaa/ml_gesture,1
"	#for i in range(1):

		#comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_5__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_0__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i)))

		#comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

	comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_sin_100_periods_1D_sample_0.txt"",os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_sin_99_periods_1D_sample_0.txt"")]

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        clf = SVC(C=496.6,gamma=0.00767,probability=True, cache_size=7000)
        args=[str(dim)+ ""Dlegendre_100vs99_svm"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/svm_legendre/svm_Legendre_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"        self.assertIs(df.multiclass.OneVsRestClassifier, multiclass.OneVsRestClassifier)
        self.assertIs(df.multiclass.OneVsOneClassifier, multiclass.OneVsOneClassifier)
        self.assertIs(df.multiclass.OutputCodeClassifier, multiclass.OutputCodeClassifier)

    def test_Classifications(self):
        iris = datasets.load_iris()
        df = pdml.ModelFrame(iris)

        models = ['OneVsOneClassifier', 'OneVsOneClassifier']
        for model in models:
            svm1 = df.svm.LinearSVC(random_state=self.random_state)
            svm2 = svm.LinearSVC(random_state=self.random_state)
            mod1 = getattr(df.multiclass, model)(svm1)
            mod2 = getattr(multiclass, model)(svm2)

            df.fit(mod1)
            mod2.fit(iris.data, iris.target)

            result = df.predict(mod1)
            expected = mod2.predict(iris.data)",pandas_ml/skaccessors/test/test_multiclass.py,sinhrks/pandas-ml,1
"                                                                       binary_data[""income""], train_size=0.80)
  scaler = preprocessing.StandardScaler()
  X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
  X_test = scaler.transform(X_test)


  # LOGISTIC REGRESSION
  # cls = linear_model.LogisticRegression()

  # LINEAR SVC
  # cls = svm.LinearSVC()

  # SVC
  # Too bad results
  # cls = svm.SVC(kernel=""rbf"", verbose=2)


  # ENSEMBLE SVM
  # n_estimators = 10
  # cls = OneVsRestClassifier(",src/binary_class/different_classifiers.py,cassinius/right-to-forget-data,1
"import urllib
import tarfile

h = .2  # step size in the mesh

names = [""NearNb"", ""LinSVM"", ""RBFSVM"", ""NaiveBayes"", ""LDA"",
         ""QDA"", ""MLP"", ""Lasso"", ""Lin.Regr"", ""Log.Regr"", ""Dec.Tree"",
         ""RandFor"", ""AdaBoost"", ""EnsembleMLP""]
classifiers = [
    KNeighborsClassifier(5),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,2),random_state=1,verbose=True),
    LassoCV(),
    LinearRegression(),
    LogisticRegression(),
    DecisionTreeClassifier(max_depth=5),",MLslippagesrc/MLslippage/mltraining.py,jagrio/MachineLearningSlippage,1
"
# Split up data into randomized training and test sets
rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(
    scaled_X, y, test_size=0.2, random_state=rand_state)

print('Using:',orient,'orientations',pix_per_cell,
      'pixels per cell and', cell_per_block,'cells per block')
print('Feature vector length:', len(X_train[0]))
# Use a linear SVC
svc = LinearSVC()
# Check the training time for the SVC
t=time.time()
svc.fit(X_train, y_train)
t2 = time.time()
print(round(t2-t, 2), 'Seconds to train SVC...')
# Check the score of the SVC
print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
# Check the prediction time for a single sample
t=time.time()",projects/MOOCs/udacity/drive/project-5-vehicle-detection/src/explore/hog_model.py,seansu4you87/kupo,1
"
r = np.exp(-(X[:, 0] **2 + X[:, 1] **2))
# r = np.array(-(X[:, 0] **2 + X[:, 1] **2))
plt.figure(2)
ax = plt.gca(projection='3d')
ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=80, cmap='coolwarm')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('r')

clf = SVC(kernel='rbf')
clf.fit(X, y)

plt.figure(3)
plt.scatter(X[:, 0], X[:, 1], c=y, s=80, cmap='coolwarm')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=200, facecolors='none')
",codes/Support Vector Machines/MappingDemo.py,GaryLv/GaryLv.github.io,1
"        print im_features.shape
        np.savetxt(""/home/iglu/Desktop/features.txt"", im_features)
        nbr_occurences = np.sum((im_features > 0) * 1, axis=0)
        idf = np.array(np.log((1.0 * n + 1) / (1.0 * nbr_occurences + 1)), 'float64')

        # Scaling the words
        self.stdSlr = StandardScaler().fit(im_features)
        im_features = self.stdSlr.transform(im_features)

        # Train the Linear SVM
        self.clf = SVC(probability=True)
        self.clf.fit(im_features, names)
        print self.clf.classes_
        joblib.dump((self.clf, self.clf.classes_, self.stdSlr, self.k, self.voc), self.path, compress=3)
        return self.clf.classes_

    def trainwoBoW(self, descriptors,des_list,names, n):
        self.voc, variance = kmeans(descriptors, self.k, 1)
        im_features = np.zeros((n, self.k), ""float64"")
        for i in xrange(n-1):",src/RGBDHand/src/BoW.py,pazagra/catkin_ws,1
"        this GridSearch instance after fitting.

    verbose: integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
        estimator=SVC(C=1.0, cache_size=..., coef0=..., degree=...,
            gamma=..., kernel='rbf', max_iter=-1, probability=False,
            shrinking=True, tol=...),
        fit_params={}, iid=True, loss_func=None, n_jobs=1,
            param_grid=...,",python/sklearn/sklearn/grid_search.py,seckcoder/lang-learn,1
"  ### you'll need to use Pipelines. For more info:
  ### http://scikit-learn.org/stable/modules/pipeline.html

  # Provided to give you a starting point. Try a variety of classifiers.


  estimators= { 'nc': [('pca', PCA()),  ('nearest_neighbours', NearestCentroid( ))] ,
                'ada': [('pca', PCA()),  ('rf', AdaBoostClassifier(random_state=False))],
                'gnb': [('pca', PCA()),  ('gnb', GaussianNB())],
                'sgd' : [('pca', PCA()),  ('sgd', SGDClassifier(random_state=False))],
            #    'svc' : [ ('pca', PCA()),  ('svc', LinearSVC(random_state=False))],
              }

  pca_params =  range(1, len(features_list))

  parameters = {'nc' : {  'pca__n_components':pca_params,
                      'nearest_neighbours__metric': ('manhattan', 'euclidean',  'hamming', 'jaccard' )},

                'ada' : { 'pca__n_components':pca_params,
                      'rf__n_estimators' : (10, 15, 20)},",MachineLearning/final_project/poi_id.py,sinanh/udacity,1
"class _NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,sachinpro/sachinpro.github.io,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the chi-square score of each feature
        score = chi_square.chi_square(X, y)

        # rank features in descending order according to score
        idx = chi_square.feature_ranking(score)
",PyFeaST/example/test_chi_square.py,jundongl/PyFeaST,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",OpenSource/Python-3.6.1/Lib/test/test_email/test_email.py,wanliming11/FPPAlgorithms,1
"
	print testX.shape, '\n'

	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)
	
	#fit the svm model and compute accuaracy measure
	#clf = svm.SVC(kernel=kernel.arc_cosine, cache_size=2048)
	#regr = SVR(kernel='rbf', C=1e3, gamma=0.1)
	regr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5, n_jobs=-1,
		param_grid={""C"": np.logspace(-2, 2, 20),""gamma"": np.logspace(-2, 2, 20)})
	#[1e0, 1e1, 1e2, 1e3]
	#regr = SVR(kernel='linear', C=1e3)
	#regr = SVR(kernel='poly', C=1e3, degree=2)
	regr.fit(trainX, trainY)

	pred = regr.predict(testX)",regression/regression.py,akhilpm/Masters-Project,1
"        self.labels = labels

    def eval_params(self, nparr):
        cutoff = int(self.X.shape[0]*.7)
        C = nparr[0]
        gamma = nparr[1]
        if gamma < 0:
            gamma = 1e-3
        if C < 0:
            C = 1e-3
        svm = SVC(C=C, gamma=gamma)
        X_train, y_train = self.X[:cutoff], self.labels[:cutoff]
        X_test, y_test = self.X[cutoff:], np.array(self.labels[cutoff:])
        svm.fit(X_train, y_train)
        y_pred = svm.predict(X_test)
        # fitness = (y_pred == y_test).sum() / (y_test.shape[0]+.0)
        fitness = prfs(y_test, y_pred, average='macro')[2]
        print ""F-score macro: %.6f achieved with C=%.6f and gamma=%.6f"" % (fitness, C, gamma)
        return fitness",classifier.py,larissapassos/TAIA-finalproject,1
"  Space = [10000,100000, 20]
  Split = .91
  Mode = ""log""


  PUFs = [
#  [""crpuf"",5,5,
#    sim.DelayGenerator(""crpuf"",5,5), 
#    sim.ChallengeGenerator(""crpuf"",5,10000),
#    [ 
#      #[svm.SVC(),  ""SVM""],
#      [ensemble.RandomForestClassifier(n_estimators=128,),""RF"",],
#      #[linear_model.LogisticRegression(),""LR"",], 
#    ],
#    [""arrxor""],
#  ],
#  [""crpuf"",5,5,
#    sim.DelayGenerator(""crpuf"",5,5), 
#    sim.ChallengeGenerator(""crpuf"",5,10000),
#    [ ",PUFMLAttack.py,rodrigosurita/freepuf,1
"  mnb_clf = MultinomialNB (fit_prior=False)
  mnb_clf.fit(train_data, train_labels)
  mnb_labels = mnb_clf.predict(test_data)
  print ""Number of mislabeled objects (MNB) : %d"" % (test_labels != mnb_labels).sum()
  save_results(test_labels, mnb_labels, ""bayes"", classification_dir)

  gamma_range = [10. ** -1, 1, 10. ** 1]
  C_range = [10. ** -2, 1, 10. ** 2]
  for C in C_range:
      for gamma in gamma_range:
        svm_clf = OneVsRestClassifier(svm.SVC(kernel='rbf', gamma=gamma, C=C))
        svm_clf.fit(train_data_normalized, train_labels)
        svm_labels = svm_clf.predict(test_data_normalized)
        print ""Number of mislabeled objects (SVM) : %d"" % (test_labels != svm_labels).sum()
        save_results(test_labels, svm_labels, ""svm_ova_gamma_%.1f_C%.2f"" % (gamma, C), classification_dir)

  mark2 = time.time();
  fos = open(classification_dir + ""/classification_time.txt"", 'w')
  fos.write(str(mark2-mark1));
  fos.close()",bmvc12/bof/classify_no_object.py,mirestrepo/voxels-at-lems,1
"        cpu._bitwise_instruction(lambda x, y: x ^ y, None, *operands)
        cpu.commitFlags()

    @instruction
    def TST(cpu, Rn, Rm):
        shifted, carry = Rm.read(withCarry=True)
        result = Rn.read() & shifted
        cpu.setFlags(N=HighBit(result), Z=(result==0), C=carry)

    @instruction
    def SVC(cpu, op):
        if op.read() != 0:
            logger.warning(""Bad SVC number: {:08}"".format(op.read()))
        raise Interruption(0)

    @instruction
    def CMN(cpu, src, add):
        result, carry, overflow = cpu._ADD(src.read(), add.read())
        return result, carry, overflow
",manticore/core/cpu/arm.py,montyly/manticore,1
"X, Y = util.load_data()

Xd = X - np.mean(X,axis=0)
Xs = (Xd/np.std(Xd,axis=0))
Xw = np.dot(Xd, util.whitening_matrix(Xd))

# >>> kf = KFold(4, n_folds=2, shuffle=True)
# >>> for train, test in kf:

# fit the model
clf = svm.NuSVC()
clf.fit(Xw, Y)

query = util.plot_svm(Xw, Y, clf, 0, 1, (-3,-3), (3,3), (500,500))

",classification/svm.py,NICTA/MLSS,1
"def tag_feat(autotags, vocab):
    feat = np.zeros((len(autotags), len(vocab)))
    for idx, (_, tags) in enumerate(autotags):
        for tag, v in tags.items():
            if tag in vocab:
                feat[idx,vocab[tag]] = v

    return feat

def stump(X, y):
    score = cross_val_score(LinearSVC(), X, y, cv = 5, n_jobs=5, scoring = 'average_precision')
    clf = LinearSVC()
    clf.fit(X, y)
    coef = clf.coef_[0,0]
    inter = clf.intercept_[0]
    return np.mean(score), np.sign(coef), inter / np.abs(coef)

def main():

    import sys",code/gifs-filter/c3d-models/rank_tags.py,raingo/TGIF-Release,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also",projects/scikit-learn-master/sklearn/svm/classes.py,DailyActie/Surrogate-Model,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",kbe/src/lib/python/Lib/email/test/test_email.py,cnsoft/kbengine-cocos2dx,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the f-score of each feature
        score = f_score.f_score(X, y)

        # rank features in descending order according to score
        idx = f_score.feature_ranking(score)
",skfeast/example/test_f_score.py,jundongl/scikit-feast,1
"        features = [hog(im) for im in img_data]
    elif detector_name == ""pixel"":
        features = [pixel_vec(im) for im in img_data]
    else:
        print(""unknown detector."")
        raise Exception

    train_data = np.array(np.float32(features))
    labels = np.array(labels)

    clf = SVC(kernel='linear', C=2.67, decision_function_shape= ""ovo"", max_iter=5000000, probability=True, verbose=True)
    print train_data.shape
    print labels.shape
    clf.fit(train_data, labels)",src/train.py,liang-chen/Vintager,1
"#clf = LinearDiscriminantAnalysis() olmadi
#clf.fit(trainingData, trainingScores)
#print(""LinearDiscriminantAnalysis"")
#print(clf.predict(predictionData))

#clf = GaussianNB() olmadi float
#clf.fit(trainingData, trainingScores)
#print(""GaussianNB"")
#print(clf.predict(predictionData))

#clf = SVC()
#clf.fit(trainingData, trainingScores)
#print(""SVC"")",Python/example.py,OO-E/MoviePrediction,1
"## Run a machine learning model
# @param data The data array object
# @param modelType The type of model to run (Linear SVM, Logistic Regression)
def runModel(data, modelType):
    #store the means for our ROC curve
    meanTPRate = 0.0
    meanFPRate = np.linspace(0, 1, 100)

    #initialize our classifier
    classifier = Pipeline([
        ('feature_selection', LinearSVC()),
        ('classification', svm.SVC(kernel='linear', probability=True, random_state=0))
    ])

    #see if we wanted logistic regression
    if(modelType == 'Logistic Regression'):
        classifier = Pipeline([
            ('feature_selection', LinearSVC()),
            ('classification', linear_model.LogisticRegression(C=1e5, random_state=0))
        ])",machine-learning/scikit_test.py,dyermd/legos,1
"print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

Xtrain_aug = numpy.concatenate((Xtrain, Xtrain_f))
Xtrain_c_aug = numpy.concatenate((Xtrain_c, Xtrain_fc))
ytrain_aug = numpy.concatenate((ytrain, ytrain_f))

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(numpy.concatenate((Xtrain_aug, Xtrain_c_aug), 1), ytrain_aug)
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(numpy.concatenate((Xtest, Xtest_c), 1))
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_concat_augment.py,yassersouri/omgh,1
"            
    blor= FeatureGenerationFromRDF(msg_objs,msg_entities,  message_labels, relations)
    before=time.time()
    blor.generate_features()    
    
    print time.time()-before
    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test, tst_ents)
    #print len(floo)
    print floo[-5:]
    from sklearn.svm import SVC
    bf = SVC(C=1.0)
    bf.fit(trn, trn_lbl)
    print mean(bf.predict(tst)==test_lbl)
    print trn[:,-5:]

    ",problems/compete_alg_isa.py,lioritan/Thesis,1
"
        if(Z[1][i] == 'nosing' and R>0 ):
            R = R - 1
            FINAL[0].append(Z[0][i])
            FINAL[1].append(Z[1][i])



        

    clf = svm.SVC(cache_size=2000)
    print ""######### "" + str(len(Z[0]))
    clf.fit(FINAL[0], FINAL[1])
    loader = Loader(path+""/test/"", 32,16)
    

    print ""Loading test""
    for p in ds.validationTracks():
        X = []
        y=[]",vd.py,m87/vocal-detection-svm,1
"is too high, the classifier will overfit, which means that the training score
is good but the validation score is poor.
""""""


def test_plot_validation_curve():
    from sklearn.datasets import load_digits
    from sklearn.svm import SVC
    digits = load_digits()
    X, y = digits.data, digits.target
    estimator = SVC()
    param_range = np.logspace(-6, -1, 5)
    plot_validation_curve(
            estimator, X, y, param_name=""gamma"",
            param_range=param_range,
            title=""Validation Curve for SVC"", ylim=None, cv=None, n_jobs=-1,
            scoring=None, ax=None)
    plt.show()

",kgml/learning_curve.py,orazaro/kgml,1
"    'you\'ll','you\'re','you\'ve','your','yours','yourself','yourselves']


    f = codecs.open(os.path.expanduser(""~/Data/cqa/uiuc/train_5500.utf8.txt""),encoding='utf-8',errors='ignore')
    X,miniX,Y = extract(f)
    f.close()
    train_set,d_train_set = X[:len(Y[0])], miniX[:len(Y[5])]
    test_set,d_test_set = X[len(Y[0]):], miniX[len(Y[5]):]
    svms = []
    for i in xrange(6):
        svms.append(svm.LinearSVC())
    print ""training""
    for sv,i in zip(svms,xrange(6)):
        print str(i)+"" /5""
        if i == 5:
            sv.fit(d_train_set,Y[i])
        else:
            sv.fit(train_set,Y[i])
        #     sv.fit(d_train_set,Y[i])
        # else:",fex.py,dsc381/yahoo_cqa,1
"sys.path.append('../')
from liblinearutil import *
from data_processing.read_data import read_preprocessed_data

INPUT_FILE = '../../data/plants/label_scores.txt'
FEATURES_FILE = '../../data/aaindex/aaindex_used.txt'



def set_svm(labels, features):
    clf = svm.SVC(kernel='linear', cache_size=500)
    clf.fit(features, labels)
    return clf
    pass

# -s svm_type : set type of SVM (default 0)
#     0 -- C-SVC
#     1 -- nu-SVC
#     2 -- one-class SVM
#     3 -- epsilon-SVR",src/ml/svm.py,seokjunbing/cs75,1
"from sklearn.svm import SVC
# First instantiate the ""Support Vector Classifier"" (SVC) model
clf = SVC()

# Next split the data (X and y) into a training and test set
from sklearn.cross_validation import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)

# fit the model to the training data
clf.fit(Xtrain, ytrain)

# compute y_pred, the predicted labels of the test data",notebooks/solutions/06-1_svm_class.py,jakevdp/sklearn_pycon2014,1
"    plt.ylim(all_scores[min_index]*90, best_score*110)
    plt.xticks(range(len(names)+1), names)
    plt.ylabel('Accuracy (%)')
    plt.savefig('img_classifying_graph_log.png')
    
    # return the set of features with the best performance
    return best_features
    

def svm_classify(std_features, surf_features, labels):
    score_std = cross_validation.cross_val_score(svm.SVC(), std_features, labels, cv=5)
    print('Accuracy (5 fold x-val) with svm [std features]: %s%%' % (0.1* round(1000*score_std.mean())))
    
    # do logistic regression with SURF features
    print('predicting...')
    scoreSURFlr = cross_validation.cross_val_score(
            svm.SVC(), surf_features, 
            labels, cv=5).mean()
    print('Accuracy (5 fold x-val) with svm [SURF features]: %s%%' % (0.1* round(1000*scoreSURFlr.mean())))
    ",main.py,shawnohare/image-classifier,1
"num_features = X.shape[1]

layers0 = [('input', InputLayer),
           ('dense0', DenseLayer),
           ('dropout', DropoutLayer),
           ('dense1', DenseLayer),
           ('output', DenseLayer)]

clf1 = RandomForestClassifier(n_estimators=800, n_jobs=-1)
clf2 = GradientBoostingClassifier(n_estimators=1000)
clf3 = OneVsRestClassifier(SVC(C=5, cache_size=2048), n_jobs=-1)
clf4 = NeuralNet(layers=layers0,

                 input_shape=(None, num_features),
                 dense0_num_units=512,
                 dropout_p=0.5,
                 dense1_num_units=512,
                 output_num_units=num_classes,
                 output_nonlinearity=softmax,
",src/ensemble_merging.py,ternaus/kaggle_otto,1
"from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df = 0, stop_words = 'english')
tfidf_matrix = tf.fit_transform(corpus)
print(tfidf_matrix.shape)
training, test = tfidf_matrix[:8988], tfidf_matrix[8988:]
l_train = labels[:8988]

from sklearn import svm
from sklearn.metrics import accuracy_score

model = svm.SVC(kernel='rbf', C=2, gamma=1) 
# # there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.Train the model using the training sets and check score
model.fit(training, l_train)
# #model.score(tfidf_matrix, labels)
# #Predict Output
predicted = model.predict(test)
print(predicted)


",Codes/tokenizer.py,saumiko/MovieSuccessPrediction,1
"        
        img = Image.open(""data/zero.bmp"")
        img = img.resize(STANDARD_SIZE)
        img = list(img.getdata())
        img = map(list, img)
        x = array(img)
        X.append(x.flatten().tolist())
        y.append(0)
        
        
        self.__model = svm.SVC()
        
        # Regress y on X
        self.__model.fit(X,y)
        
        # Save out model 
        fp = open(self.__dataFile, ""w"")
        pickle.dump(self.__model, fp)
        fp.close()
        ",src/tmachine/DigitClassification.py,timothydgreer/turing_machine,1
"    clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=1, random_state=0, n_jobs=-1)
    scores = cross_val_score(clf, inputs, targets, n_jobs=-1, cv=lkf)
    print scores
    print scores.mean()

def run_Bagging(num_estimator=10, num_iter=5, include_mirror=False, do_cv=False, reload=False):
    if not reload:
        train_inputs, train_targets, valid_inputs, valid_targets = load_data(include_mirror)
    else:
        train_inputs, train_targets, valid_inputs, valid_targets, test_inputs, test_targets = reload_data_with_test_normalized()
    # myClassifier = LinearSVC()
    # myClassifier = RidgeClassifier()
    myClassifier = Perceptron(n_iter=num_iter)
    # myClassifier = SGDClassifier(loss='perceptron',n_iter=num_iter)
    # myClassifier = OneVsRestClassifier(LinearSVC(random_state=0))
    # clf = BaggingClassifier(base_estimator=myClassifier, n_estimators=num_estimator, n_jobs=-1)

    if do_cv:
        # Do cross validation
        scores = cross_val_score(clf, train_inputs, train_targets)",basicBoosting.py,Takonan/csc411_a3,1
"    clf = MockClassifier(allow_nd=False)
    assert_raises(ValueError, cross_val_score, clf, X_3d, y2)


def test_cross_val_score_predict_labels():
    # Check if ValueError (when labels is None) propagates to cross_val_score
    # and cross_val_predict
    # And also check if labels is correctly passed to the cv object
    X, y = make_classification(n_samples=20, n_classes=2, random_state=0)

    clf = SVC(kernel=""linear"")

    label_cvs = [LeaveOneLabelOut(), LeavePLabelOut(2), LabelKFold(),
                 LabelShuffleSplit()]
    for cv in label_cvs:
        assert_raise_message(ValueError,
                             ""The labels parameter should not be None"",
                             cross_val_score, estimator=clf, X=X, y=y, cv=cv)
        assert_raise_message(ValueError,
                             ""The labels parameter should not be None"",",434-MachineLearning/final_project/linearClassifier/sklearn/model_selection/tests/test_validation.py,neale/CS-program,1
"    
    def testSimpleUVC(self):
        puzzle = PuzzleFactory.createSingleConstraintPuzzle(set([1,2]), 2, UniqueValueConstraint)
        
        cells = puzzle.grid.getCells()
        cells[0].setValue(1)

        self.assertEqual(cells[1].getPossibleValues(), set([2]), 
                         ""Incorrect exclusion of value"")
    
    def testSimpleTSVC(self):
        """"""Test the basic properties of the total sum value constraint""""""
        puzzle = PuzzleFactory.createSingleConstraintPuzzle(set([1,2]), 2, TotalSumValueConstraint)
        
        for cg in puzzle.getConstraintGroups():
            for constraint in cg.getConstraints():
                constraint.setTotalValue(3);
                constraint.applyConstraint()
                
        possibleValues = puzzle.grid.getCells()[1].getPossibleValues()",constraints/tests/constraintTest.py,JoostvanPinxten/ConstraintPuzzler,1
"body belongs to the signature.
""""""

from numpy import genfromtxt
from sklearn.svm import LinearSVC
from sklearn.externals import joblib


def init():
    """"""Inits classifier with optimal options.""""""
    return LinearSVC(C=10.0)


def train(classifier, train_data_filename, save_classifier_filename=None):
    """"""Trains and saves classifier so that it could be easily loaded later.""""""
    file_data = genfromtxt(train_data_filename, delimiter="","")
    train_data, labels = file_data[:, :-1], file_data[:, -1]
    classifier.fit(train_data, labels)

    if save_classifier_filename:",talon/signature/learning/classifier.py,agussman/talon,1
"				feature = []
				feature.extend(tp_feature[k][i])
				feature.extend(sp_feature[k][i])
				feature.extend(md_feature[k][i])
				feature.extend(st_feature[k][i])
				X.append(feature)
				y1.append(0 if cnt[k][i] == 0 else 1)
				y2.append(cnt[k][i])
	# clf1 = pipeline.Pipeline([
	# 	('feature_selection', linear_model.LogisticRegression(penalty='l1')),
	# 	# ('feature_selection', LinearSVC(penalty=""l1"",dual=False)),
	# 	# ('classification', tree.DecisionTreeClassifier())
	# 	# ('classification', ExtraTreesClassifier())
	# 	# ('classification', RandomForestClassifier())
	# 	('classification', GradientBoostingClassifier())
	# 	])
	clf1 = GradientBoostingClassifier()
	clf2 = tree.DecisionTreeRegressor()
	clf1 = clf1.fit(X, y1)
	clf2 = clf2.fit(X, y2)",data_process/critical_forecast/prediction.py,OMNILab/CCFBDC2014,1
"    labels = np.hstack((labelsbtr,labelsbte,labelsnbtr,labelsnbte))

    print ""Testing...""
    print ""1 = Bread""
    print ""2 = Nonbread""

    train = np.vstack((breadtrain,nonbreadtrain))
    labels = np.hstack((labelsbtr,labelsnbtr))
    test = np.vstack((breadtest,nonbreadtest))

    lin_svc = svm.LinearSVC(C=1.0).fit(train, labels)
    predictionsSVM = lin_svc.predict(test)

    cfr = RandomForestClassifier(n_estimators=120)
    cfr.fit(train,labels) # train

    gtruth = np.hstack((labelsbte,labelsnbte))
    predictionsRF = cfr.predict(test) # test

    print dirListbte",tests/test_real_fake.py,rbaravalle/imfractal,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0910_2015_pre_activation_rectifier_didnotwork.py,magic2du/contact_matrix,1
"
dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})
	]",scripts/histograms/non-normalised-ml-search.py,jmrozanec/white-bkg-classification,1
"        ax.imshow(trial_info['sample_direction'],interpolation='none',aspect='auto')
        ax = f.add_subplot(1, 3, 2)
        ax.imshow(trial_info['test_direction'],interpolation='none',aspect='auto')
        ax = f.add_subplot(1, 3, 3)
        ax.imshow(trial_info['match'],interpolation='none',aspect='auto')
        plt.show()
        1/0  
        
    def calculate_svms(self, num_reps = 3, DMC = [False], decode_test = False):
        
        lin_clf = svm.SVC(C=1,kernel='linear',decision_function_shape='ovr', shrinking=False, tol=1e-4)
        num_neurons, trial_length, num_trials = self.rnn_outputs.shape
        spike_decoding = np.zeros((trial_length,self.num_rules,num_reps))
        synapse_decoding = np.zeros((trial_length,self.num_rules,num_reps))
        spike_decoding_test = np.zeros((trial_length,self.num_rules,num_reps))
        synapse_decoding_test = np.zeros((trial_length,self.num_rules,num_reps))
        N = self.num_motion_dirs
        
        sample_cat = np.floor(self.sample_dir/(self.num_motion_dirs/2)*np.ones_like(self.sample_dir))
        if self.ABBA:",neural_analysis.py,nmasse/Short-term-plasticity-RNN,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load_server_with_ten_folds.py,magic2du/contact_matrix,1
"                        fptr.write(make_graphviz_string(rec_tree))
                    #export_to_pdf(rec_tree, 'rec_tree%d,dataset%d.pdf'%(f_number+1,count))
                
                for j,fraction in enumerate([0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]):
                    new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
    
                    from sklearn.svm import SVC
                    from sklearn.neighbors import KNeighborsClassifier
                    from sklearn.tree import DecisionTreeClassifier
    
                    clf= SVC(kernel='linear', C=100)
                    clf.fit(new_trn, trn_lbl)
                    tst_predict= clf.predict(new_tst)
                    errs_svm[count, d, j]= mean(tst_predict!=tst_lbl)

                    clf= KNeighborsClassifier(n_neighbors=1)
                    clf.fit(new_trn, trn_lbl)
                    tst_predict= clf.predict(new_tst)
                    errs_knn[count, d, j]= mean(tst_predict!=tst_lbl)
",problems/techTCrun.py,lioritan/Thesis,1
"def build_svcs(random_state=None):
    print('Building SVM models')

    Cs = np.logspace(-7, 2, 10)
    gammas = np.logspace(-6, 2, 9, base=2)
    coef0s = [-1.0, 0.0, 1.0]

    models = []

    for C in Cs:
        models.append(SVC(kernel='linear', C=C, probability=True,
                          cache_size=1000))

    for C in Cs:
        for coef0 in coef0s:
            models.append(SVC(kernel='sigmoid', C=C, coef0=coef0,
                              probability=True, cache_size=1000))

    for C in Cs:
        for gamma in gammas:",model_library.py,dclambert/pyensemble,1
"          'ytick.labelsize': 10}
plt.rcParams.update(params)


def make_classification_example(axis, random_state):
    X, y = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=2.7, random_state=random_state)

    axis.scatter(X[y == 0, 0], X[y == 0, 1], color=""red"", s=10, label=""Disease"")
    axis.scatter(X[y == 1, 0], X[y == 1, 1], color=""blue"", s=10, label=""Healthy"")

    clf = LinearSVC().fit(X, y)

    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 7)
    yy = a * xx - (clf.intercept_[0]) / w[1]

    # plot the line, the points, and the nearest vectors to the plane
    axis.plot(xx, yy, 'k-', color=""black"", label=""Model"")",figures/figure.classification.vs.regression.py,aldro61/microbiome-summer-school-2017,1
"    Output
    ------
    F: {numpy array}, shape (n_features, )
        index of selected features
    """"""

    n_samples, n_features = X.shape
    # using 10 fold cross validation
    cv = KFold(n_samples, n_folds=10, shuffle=True)
    # choose SVM as the classifier
    clf = SVC()

    # selected feature set, initialized to be empty
    F = []
    count = 0
    while count < n_selected_features:
        max_acc = 0
        for i in range(n_features):
            if i not in F:
                F.append(i)",PyFeaST/function/wrapper/svm_forward.py,jundongl/PyFeaST,1
"        X2 = scaled[len(df1.columns):, :]
    else:
        X1 = jointdf.ix[:,:len(df1.columns)].T.values
        X2 = jointdf.ix[:,len(df1.columns):].T.values

    return X1, y1, X2, y2, jointdf.index

def predictor_svm(X1, y1, X2, y2):
    verbalise(""C"", ""Size of training set: %d\nSize of test set: %d"" % (len(y1), len(y2)))
    verbalise(""C"", ""Number of features: %d"" % X1.shape[1])
    clf = svm.SVC(kernel='linear')
    #print clf
    clf.fit(X1, y1)
    verbalise(""Y"", ""real: %s\npred: %s"" % (
            "" "".join([str(x) for x in y2]), "" "".join([ str(x) for x in clf.predict(X2)])
            ))
    cscore = clf.score(X2, y2)
    pval = prob( len(y2)-round(cscore*len(y2)) + 1, 0.5, len(y2) )
    verbalise(""G"", ""score: %.2f (p=%.5f)\n"" % (cscore, pval ))
    return clf.coef_, clf.score(X2, y2)",brain_machine.py,oxpeter/small_fry,1
"    gram=km[:,1:].todense()
    f.write(""Total examples ""+str(len(gram))+""\n"")
    f.write(""seed\t CV_test_acc\t std\n"")

    #print gram
    # normalization
    from math import sqrt
    #for i in range(len(gram)):
    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    clf = svm.SVC(C=c, kernel='precomputed')
    scores= cross_validation.cross_val_score(clf, gram, target_array, cv=kf)
        #print ""inner scores"", inner_scores
    print ""CV Accuracy: %0.4f (+/- %0.4f)"" % (scores.mean(), scores.std())
    f.write(str(rs)+"" ""+str(scores.mean())+""\t""+str(scores.std()))
",scripts/cross_validation_not_nested_from_matrix.py,nickgentoo/scikit-learn-graph,1
"        self.train_course=''
        self.test_course=''
        self.lead=''
        self.lag=''
        self.X_train=''
        self.Y_train=''
        self.X_test=''
        self.Y_test=''
        self.penal=1
        self.logreg=linear_model.LogisticRegression('l2',dual=False,C=self.penal)
        self.svm=svm.SVC(C=self.penal)

    def flattenAndLoad_train(self,train_course,lead,lag):
        self.train_course=train_course
        train_course.flattenAndLoad_traindata(lead,lag)
        self.X_train=train_course.X_train
        self.Y_train=train_course.Y_train
        self.lead=lead
        self.lag=lag
",Stopout-Prediction/classes.py,EDUlib/eTracesX,1
"    for train_index,test_index in kf:
        # print ""training set:"",train_index,test_index
        print ""set : "",set_index
        traing_feature = feature[train_index]
        traing_label = label[train_index]
        test_feature = feature[test_index]
        test_label = label[test_index]
        kernels = ['linear','rbf','sigmoid','poly']
        for kernel in kernels:
            print kernel
            clf = SVC(kernel=kernel)
            clf.fit(traing_feature,traing_label)
            predict = clf.predict(test_feature)
            sum_error = 0
            for index in range(len(predict)):
                if int(predict[index]) != int(test_label[index]):
                    sum_error += 1
            print 'rate: ',float(sum_error)/float(len(predict))
        print
        print",smoking/classifier/svm_geo_classify.py,luckyharryji/smoking-modeling,1
"
# RADIAL BASIS FUNCTIONS

def f(x):
    return sign(x[2] -x[1] + 1/4 * np.sin(np.pi * x[1]))

def generate_data():
    return random_set(100, f), random_set(100, f)

def svc_rbf_trial(training_set, testing_set):
    svc_rbf = SVC(kernel='rbf', C=10**4)
    svc_rbf.fit(training_set.z, training_set.y)
    svc_ein, svc_eout = [
            classified_error(svc_rbf.predict(t_set.z), t_set.y)
            for t_set in [training_set, testing_set] ]
    if svc_ein > 10**-6:
        return None
    return svc_eout

def reg_rbg_trial(training_set, testing_set, k, gammas):",ass9/hw9.py,zhiyanfoo/caltech-machine-learning,1
"    print('End data loading...')

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import LinearSVC
    from sklearn.ensemble import ExtraTreesClassifier
    
    clf = GaussianNB()
    clf1 = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
    clf2 = LogisticRegression()
    clf3 = LinearSVC(random_state=0)
    clf4 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0) 
    
    # copy css and javascript on project folder
    createproject(options.outputfolder)
    expirements(options.title, options.outputfolder + ""/index.html"", [clf, clf1, clf2, clf3, clf4], X, Y)

if __name__ == '__main__':
    try:
        start_time = time.time()",baseclassifier.py,theofilis/base-line-classifier,1
"    clf.fit(matrix_features, vector_targets)

    return [i for i, e in enumerate(clf.coef_) if e != 0 and abs(e) > 1e-6]


def extract_linear_features_indexes(matrix_features, vector_targets):
    """"""
    Perform feature selection using a simple linear classifier.
    """"""

    clf = LinearSVC(C=0.01, penalty=""l1"", dual=False)
    clf.fit(matrix_features, vector_targets)

    return [i for i, e in enumerate(clf.coef_[0]) if e != 0 and abs(e) > 1e-6]


def extract_foba_features_indexes():
    """"""
    Return features to be included according FOBA algorithm (algorithm
    executed offline and results added here).",feature_selection.py,trein/quora-classifier,1
"#tt.fit(X_train)
X_train = tt.fit_transform(X_train)
#tt.fit(X_test)
X_test = tt.fit_transform(X_test)
#print 'logistic regression', pipeline.score(X_test, y_test)

clf = SGDClassifier()
clf.fit(X_train, y_train)
print 'logreg', clf.score(X_test, y_test)

svm = LinearSVC()
svm.fit(X_train, y_train)",MasteringMLWithScikit-learn/8365OS_04_Codes/classifier.py,moonbury/pythonanywhere,1
"	return type('Enum', (), enums)

def mkdir_p(path):
	try:
		os.makedirs(path)
	except OSError as exc: # Python >2.5
		if exc.errno == errno.EEXIST and os.path.isdir(path):
			pass
		else: raise

def patch_static_MSVC(path):
	msvc_ns_prefix = ""{http://schemas.microsoft.com/developer/msbuild/2003}""
	ElementTree.register_namespace('', ""http://schemas.microsoft.com/developer/msbuild/2003"")
	tree = ElementTree.parse(path)
	root = tree.getroot()

	# Change build result to .lib
	list = root.findall(msvc_ns_prefix+""PropertyGroup"")
	for child in list:
		try:",external/ext_lib_build.py,lovewinds/story-project,1
"from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
pipe_lr = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components=2)), ('clf', LogisticRegression(random_state=1))])
pipe_lr.fit(X_train, y_train)
print 'Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test)

from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score
pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))])
param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_grid = [{'clf__C': param_range,
                           'clf__kernel': ['linear']},
                         {'clf__C': param_range,
                          'clf__gamma': param_range,
                          'clf__kernel': ['rbf']}]


# Algorithm selection with nested cross-validation",self_practice/Chapter 6 Part II.py,wei-Z/Python-Machine-Learning,1
"    y_pred = best_clf.predict(X_test_outer_cv)
    y_score = best_clf.predict_proba(X_test_outer_cv)[:, 1]
    return y_pred, y_score, best_clf_label


def get_models_to_check(covariates=COVARIATES, structural_covariance=STRUCTURAL_COVARIANCE, z_threshold=z_THRESHOLD,
                        parcellation=PARCELLATION):
    if covariates:
        return use_covariates(structural_covariance=structural_covariance, parcellation=parcellation)

    svm = SVC(kernel='linear', probability=True)

    pca = PCA(n_components=0.9)
    pca_svm = Pipeline([('pca', pca), ('svm', SVC(kernel='linear', probability=True))])

    feat_sel = FeatureSelector(z_thresh=z_threshold[parcellation])
    feat_sel_svm = Pipeline([('feat_sel', feat_sel), ('svm', SVC(kernel='linear', probability=True))])

    struct_cov_svm, struct_cov_label = [], []
    if structural_covariance:",FTD_classification.py,PaulZhutovsky/ftd_project,1
"    return X, Y


def main():
    global clf
    ### training
    X, y = readDataset(TRAIN_DATA)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=2)

    clf = SVC()
    ret = clf.fit(X_train, y_train)

    yt = clf.predict(X_test)
    print(""SVM:"", np.sum(yt==y_test)*100.0/len(yt))

    joblib.dump(clf, SAVE_TO)
    print('SVM Model is saved.')

    clf = GradientBoostingClassifier()",train.py,Zing22/uemscode,1
"        else:
            print(Fore.RED + Style.BRIGHT + ""[-] Model type ""+ string +"" does not exist."")
            exit(1)

# ----------------------------------------------------
# ML classifiers
# ----------------------------------------------------
class SVM():
    def __init__(self, name=""svc""):
        print(""[+] SVM Classifier"")
        self.m = SVC()
        self.name = name

    def _get_lora_id_labels(self, instances, oh_labels):
        result = []
        for i in range(0, len(oh_labels)):
            result.append(instances.mapping.oh_to_lora_id(oh_labels[i]))
        return result

    def _to_vendor(self, instances, lora_id_labels):",tf_train.py,rpp0/lora-phy-fingerprinting,1
"from sklearn.svm import SVC
import numpy as np

from data import graph_data, graph_surface, sample_gmm_2d, eval_perf_binary


class KSVMWrapper(object):

    def __init__(self, X, Y_, c=1, g='auto'):
        self.clf = SVC(C=c, gamma=g)
        self.clf.fit(X, Y_)

    def predict(self, X):
        return self.clf.predict(X)
    
    def get_scores(self, X):
        return self.clf.decision_function(X)
    
    @property",1. labos/ksvm_wrap.py,drakipovic/deep-learning,1
"import math
import MySQLdb as mdb
#import gensim as gs

tweet_subset = ""tweetSubset_danielle.csv""
transformed_set = ""features.csv""
train_test_set = ""dani_tweets.csv""
DB_NAME = ""twitter""
    
def classify(ml):
    return svm.SVC(kernel='linear')
    

def extract_features(conn, tbl_name, a, b):
    """"""PRE: Populates the table with NER data, extracts all data for models.

            The other data comes from the SQL table init_data.
            The Stanford NER server must be running as described in the README. 

    Args:",tweet_learn.py,bgold09/tweet_learn,1
"        pass

    def cv_kfold(self, feat_table, k=5, method=""decisiontree"", **kwargs):
        if not isinstance(feat_table, CandidateSet):
            raise TypeError('Input object should be of type CandidateSet')
        table = feat_table.candset_table
        data, target = self._get_data_target(table)
        if method == ""decisiontree"":
            clf = DecisionTreeClassifier()
        elif method == ""svm"":
            clf = SVC()
        elif method == ""randomforest"":
            clf = RandomForestClassifier()
        elif method == ""boosting"":
            clf = AdaBoostClassifier()
        elif method == ""bagging"":
            clf = BaggingClassifier()
        elif method == ""naivebayes"":
            clf = GaussianNB()
        elif method == ""sgd"":",magellan/matcher/ml_matcher.py,kvpradap/magellan-v0.0,1
"    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=20,
                                 random_state=generator, max_depth=2)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)

    # Check if the supports are equal
    assert_array_equal(rfe.get_support(), rfe_svc.get_support())


def test_rfe():
    generator = check_random_state(0)",projects/scikit-learn-master/sklearn/feature_selection/tests/test_rfe.py,DailyActie/Surrogate-Model,1
"    grid_search.decision_function(X)
    grid_search.transform(X)

    # Test exception handling on scoring
    grid_search.scoring = 'sklearn'
    assert_raises(ValueError, grid_search.fit, X, y)


def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)
    grid_search = GridSearchCV(clf, {'C': Cs})
    grid_search.fit(X, y)

    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
                                        scoring='accuracy')
    # smoketest grid search",venv/lib/python2.7/site-packages/sklearn/tests/test_grid_search.py,chaluemwut/fbserver,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
         verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS",net-p3/lib/python3.5/site-packages/sklearn/metrics/classification.py,uglyboxer/linear_neuron,1
"c_values = np.logspace(-6, 6, 10)
hp_combinations = list(product(gamma_values, c_values))

data_train = np.zeros((len(gamma_values), len(c_values)))
data_cv = np.zeros((len(gamma_values), len(c_values)))
data_test = np.zeros((len(gamma_values), len(c_values)))
for i in range(len(gamma_values)):
    for j in range(len(c_values)):
        print ""Fitting with gamma={0:.8f} and C={1:.8f}"".format(gamma_values[i], c_values[j])

        estimator = SVC(kernel=""rbf"", gamma=gamma_values[i], C=c_values[j])
        estimator.fit(X_train, y_train)

        data_train[i, j] = estimator.score(X_train, y_train)
        data_cv[i, j] = np.mean(cross_val_score(estimator, X_train, y_train, cv=5, n_jobs=-1))
        data_test[i, j] = estimator.score(X_test, y_test)

f, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharex=True, sharey=True)
sns.heatmap(data_train, ax=ax1, cbar=False, annot=True, annot_kws=dict(fontsize=6))
sns.heatmap(data_cv, ax=ax2, cbar=False, annot=True, annot_kws=dict(fontsize=6))",exercises/code/basics.model.selection.py,aldro61/microbiome-summer-school-2017,1
"correct = 0
for a,b in zip(y_logistic_predict, test_data_label):
	if a == b:
		correct = correct+1

exit()
print float(correct)/len(test_data_label)
#SVM

from sklearn import svm
#svm_model = svm.SVC(decision_function_shape='ovo')
svm_model = svm.SVC()
svm_model.fit(train_data, train_data_label)

print ""SVM""
#print svm_model.score(test_data,test_data_label)
y_svm_predict = svm_model.predict(test_data)
print ""After predict""
#print y_svm_predict
correct = 0",randomforest_1.py,siddharthhparikh/INFM750-project,1
"                ),
        'ExtraTrees':
            sklearn.ensemble.ExtraTreesClassifier(
                random_state=settings['R_SEED'],
                ),
        'AdaBoost':
            sklearn.ensemble.AdaBoostClassifier(
                random_state=settings['R_SEED'],
                ),
        'SVC':
            sklearn.svm.SVC(
                probability=True,
                random_state=settings['R_SEED'],
                ),
        'LinearSVC':
            # Use this instead of svm.LinearSVC even though it is
            # slower, because we use the sample_weights input
            sklearn.svm.SVC(
                random_state=settings['R_SEED'],
                kernel='linear',",python/utils.py,Neuroglycerin/hail-seizure,1
"        n_folds = self.cv if isinstance(self.cv, int) \
            else self.cv.n_folds if hasattr(self.cv, ""n_folds"") else None
        if n_folds and \
           np.any([np.sum(y == class_) < n_folds for class_ in self.classes_]):
            raise ValueError(""Requesting %d-fold cross-validation but provided""
                             "" less than %d examples for at least one class.""
                             % (n_folds, n_folds))

        self.calibrated_classifiers_ = []
        if self.base_estimator is None:
            base_estimator = LinearSVC()
        else:
            base_estimator = self.base_estimator

        if self.cv == ""prefit"":
            calibrated_classifier = _CalibratedClassifier(
                base_estimator, method=self.method)
            if sample_weight is not None:
                calibrated_classifier.fit(X, y, sample_weight)
            else:",net-p3/lib/python3.5/site-packages/sklearn/calibration.py,uglyboxer/linear_neuron,1
"
# Training and validation data (k-fold = 30%)
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3, random_state=23)


# With a RBF kernel, gamma=0.01, corpus_number_perdomain = 5000 and
# vectors_size = 20 to reach the maximum score, however we
# use a linear kernel because we have a 0.1 less precison but we less
# support vectors. This is important in classification time

clf = svm.SVC(kernel='linear', probability=False)
clf.fit(X_train, y_train)  ## classifier generated

# Save model to disk and also a vectorizer index
joblib.dump(clf, 'models/svm_model_tagger.pkl')
with open('models/vectorizer_tagger.pkl', 'wb') as o_file:
    pickle.dump(vectorizer, o_file)

# Dump info about the model
print(""Supported vectors length: %s"" % str(clf.support_vectors_.shape))",svm-training-tagger.py,rmaestre/SVM-for-domains-detection,1
"#    print x0    
    
    for attr in data.columns.values:
        
        if not (attr in attr_list):
        
            x1 = data[[attr]].as_matrix()[:n]
#            print x1
            x = np.concatenate((x0, x1), axis=1)
#            print x
#            clf = svm.SVC(kernel='rbf')
#            clf = tree.DecisionTreeClassifier()
            clf = ensemble.RandomForestClassifier(n_estimators=10)
                 
            score1 = cross_validation.cross_val_score(clf, x, train_only[:n, 0] , cv=5)
    #        score2 = cross_validation.cross_val_score(clf, x[:1000], train_only[:1000, 1] , cv=5)
            print attr, np.mean(score1)#, np.mean(score2)
        
add_attr(['click_quality', 'price_usd', 'random_bool'])       
",Assignment-2/Code/step_up.py,Ignotus/DataMining,1
"    model = LogisticRegression(penalty='l2', tol=0.0001, C=1.0)#opt
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=50)), ('model', LogisticRegression(penalty='l2', tol=0.0001, C=1.0))])
    #model = Pipeline([('filter', SelectPercentile(chi2, percentile=70)), ('model', LogisticRegression(penalty='l2', tol=0.0001, C=1.0))])
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=15)), ('model', KNeighborsClassifier(n_neighbors=150))])
    #model = Pipeline([('filter', SelectPercentile(chi2, percentile=20)), ('model', MultinomialNB(alpha=0.1))])
    #model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None)#opt kaggle params
    #model = LogisticRegressionMod(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None)#opt kaggle params
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=100)), ('model', LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=10.0, class_weight=None))])
    #model = AdaBoostClassifier(base_estimator=LogisticRegressionMod(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True,intercept_scaling=1.0),learning_rate=0.1,n_estimators=50,algorithm=""SAMME.R"")
    #model = KNeighborsClassifier(n_neighbors=10)
    #model=SVC(C=0.3,kernel='linear',probability=True)
    #model=LinearSVC(penalty='l2', loss='l2', dual=True, tol=0.0001, C=1.0)#no proba
    #model = SVC(C=1, cache_size=200, class_weight='auto', gamma=0.0, kernel='linear', probability=True, shrinking=True,tol=0.001, verbose=False)
    #model=   RandomForestClassifier(n_estimators=200,max_depth=None,min_samples_leaf=10,n_jobs=1,criterion='entropy', max_features='auto',oob_score=False)
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=80)), ('model', AdaBoostClassifier(n_estimators=100,learning_rate=0.1))])
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=50)), ('model', BernoulliNB(alpha=0.1))])#opt sparse 0.849
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=50)), ('model', RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=10,n_jobs=1,criterion='entropy', max_features='auto',oob_score=False))])
    #opt greedy approach
    #model = AdaBoostClassifier(n_estimators=500,learning_rate=0.1)
",competition_scripts/stumble.py,chrissly31415/amimanera,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ll_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",skfeature/example/test_ll_l21.py,jundongl/scikit-feature,1
"
@pytest.mark.skip(True, reason = 'Test uses sklearn, which should not be a requirement for artemis.  It does work though.')
def test_compare_predictors(hang_plot = False):
    from sklearn.svm import SVC
    dataset = get_synthetic_clusters_dataset()

    w_constructor = lambda rng = np.random.RandomState(45): .1*rng.randn(dataset.input_shape[0], dataset.n_categories)
    records = compare_predictors(
        dataset = dataset,
        offline_predictors={
            'SVM': SVC()
            },
        online_predictors={
            'fast-perceptron': Perceptron(alpha = 0.1, w = w_constructor()).to_categorical(),
            'slow-perceptron': Perceptron(alpha = 0.001, w = w_constructor()).to_categorical()
            },
        minibatch_size = 10,
        test_epochs = sqrtspace(0, 10, 20),
        evaluation_function='percent_correct'
        )",artemis/ml/predictors/test_predictor_comparison.py,QUVA-Lab/artemis,1
"    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)
    grid_search.fit(X, y)
    assert_true(hasattr(grid_search, ""best_params_""))


def test_grid_search_error():
    """"""Test that grid search will capture errors on data with different
    length""""""
    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)

    clf = LinearSVC()
    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
    assert_raises(ValueError, cv.fit, X_[:180], y_)


def test_grid_search_one_grid_point():
    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
    param_dict = {""C"": [1.0], ""kernel"": [""rbf""], ""gamma"": [0.1]}

    clf = SVC()",venv/lib/python2.7/site-packages/sklearn/tests/test_grid_search.py,GbalsaC/bitnamiP,1
"from pprint import pprint

from sklearn.grid_search import GridSearchCV
from sklearn.datasets import load_digits
from sklearn.cross_validation import train_test_split
from sklearn.svm import LinearSVC

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target % 2)

grid = GridSearchCV(LinearSVC(), param_grid={'C': np.logspace(-6, 2, 9)}, cv=5)
grid.fit(X_train, y_train)
pprint(grid.grid_scores_)
pprint(grid.score(X_test, y_test))


Cs = [10, 1, .01, 0.001, 0.0001]
for penalty in ['l1', 'l2']:
    svm_models = {}
    training_scores = []",day3-machine-learning/solutions/linear_models.py,lvrzhn/AstroHackWeek2015,1
"        M.append(np.atleast_2d(np.array(list(mm.values()))))

    M1=np.concatenate([M,0])

#     for features in range(24):
#         pl.hist(M1[pos_examples,features],100,normed=True)   
#         pl.hist(M1[neg_examples,features],100,normed=True)  
#         pl.pause(1)
#         pl.cla()
    from sklearn import svm
    clf = svm.SVC()
    X=M1[np.hstack([pos_examples[:70],neg_examples[:70]])]

    y=np.hstack([np.zeros_like(pos_examples[:70]),np.ones_like(neg_examples[:70])])
    clf.fit(X, y)
    lbs=clf.predict(M1)
    pl.imshow(np.max(masks_ws[lbs==1],0))
    pl.imshow(np.max(masks_ws[pos_examples],0)*10,alpha=.5)

#    with np.load(os.path.join(folder_in_check,'results_analysis.npz'))  as ld:",sandbox/scripts_labeling/neurofinder_template.py,simonsfoundation/CaImAn,1
"
    size = len(y)
    kf = KFold(size, n_folds=k, shuffle=True)
    fold = 1
    for train_idx, test_idx in kf:
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        pipeline = Pipeline([
                (""vert"", TfidfVectorizer(min_df = 1, binary = True, ngram_range = (1, 3),
                                         tokenizer = Tokenizer())),
                (""clf"", LinearSVC(loss='l1',
                                  penalty=""l2"",
                                  multi_class=""ovr"",
                                  class_weight=""auto"")),
                ])
        _logger.debug(""Training fold %d"" % fold)
        pipeline.fit(X_train, y_train)
        _logger.debug(""Predicting for fold %d"" % fold)
        y_pred = pipeline.predict(X_test)
        _logger.info(""fold %d got accuracy: %f"" % (fold, accuracy_score(y_test, y_pred)))",clean/kfold/clean.py,luanjunyi/cortana,1
"        :param idxrelamap: mapping from parsing action indices to
                           parsing actions

        :type clf: LinearSVC
        :param clf: an multiclass classifier from sklearn
        """"""
        self.vocab = vocab
        # print labelmap
        self.labelmap = idxlabelmap
        if clf is None:
            self.clf = LinearSVC()


    def train(self, trnM, trnL):
        """""" Perform batch-learning on parsing model
        """"""
        self.clf.fit(trnM, trnL)


    def predict(self, features):",model.py,jerryyeezus/nlp-summarization,1
"instance_n3 = makeFitInstance(file_n3)
instance_n4 = makeFitInstance(file_n4)
instance_n5 = makeFitInstance(file_n5)
instance_n6 = makeFitInstance(file_n6)
instance_n7 = makeFitInstance(file_n7)
instance_n8 = makeFitInstance(file_n8)
instance_n9 = makeFitInstance(file_n9)


gtruth = open(str(ground_truth),""a"")
clf = svm.SVC()
print(""Starting cross validation with 10 folds..."")
mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
all_tpr = []
###Fold 1
###
fit_data0 = instance_a0[0] + instance_a1[0] + instance_a2[0] + instance_a3[0] + instance_a4[0] + instance_a5[0] + instance_a6[0] + instance_a7[0] + instance_a8[0] + instance_n0[0] + instance_n1[0] + instance_n2[0] + instance_n3[0] + instance_n4[0] + instance_n5[0] + instance_n6[0] + instance_n7[0] + instance_n8[0]
fit_classes0 = instance_a0[1] + instance_a1[1] + instance_a2[1] + instance_a3[1] + instance_a4[1] + instance_a5[1] + instance_a6[1] + instance_a7[1] + instance_a8[1] + instance_n0[1] + instance_n1[1] + instance_n2[1] + instance_n3[1] + instance_n4[1] + instance_n5[1] + instance_n6[1] + instance_n7[1] + instance_n8[1]
",format_py/n_gram_svm_with_cv.py,doylew/detectionsc,1
"    xtrain,ytrain = getDATA(gtlabels,dataIMS,dataMBH,infileC3D,database,'training')
    print 'got training and shape is ',np.shape(xtrain)
    xval,yval = getDATA(gtlabels,dataIMS,dataMBH,infileC3D,database,'validation')
    print 'got validation and shape is ',np.shape(xval)
    
    numSamples = np.shape(xval)[0]
    bestclf = {};
    bestscore = 0;
    Cs = [0.001,0.01,0.1,1,10,100];    
    for cc in Cs:
        clf = LinearSVC(C = cc)#,probability=True)
        clf = clf.fit(xtrain, ytrain)
        preds = clf.predict(xval)
        correctPreds = preds == yval;
        score = 100*float(np.sum(correctPreds))/numSamples
        print 'Overall Accuracy is ',score, '% ', ' C = ',str(cc),' features = ',featType
        if score>bestscore:
            bestclf = clf
            bestscore = score
            ",processing/calssification-fusion.py,gurkirt/actNet-inAct,1
"    # uniqueData = unique_rows(dataWithLabel)
    # print dataWithLabel.shape
    # print uniqueData.shape
    data, label = delteIndexCondition(data, label)

    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.1, random_state=42)

    # kernel='linear'
    # kernel='rbf'
    # decision_function_shape='ovo'
    clf = svm.SVC(kernel='rbf', max_iter=100000).fit(X_train[:,0:featureDimension], y_train)
    # print clf
    error_count = 0.0
    result = clf.predict(X_test[:,0:featureDimension])

    for i, l in enumerate(result):
    #print l, label[i]
        if l != y_test[i]:
            error_count += 1
",dataset/pre-process/classifier.py,changkun/MotionTouch,1
"svm = SVC(kernel='linear')
svm.fit(Xtrain, ytrain)
",python_scripts_day1/train_svm_simple.py,NeuroStat/Python-scripts,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",usr/lib64/python3.4/test/test_email/test_email.py,ElUniversoMELA/MELAOS,1
"
dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})
	]",scripts/histograms/normalised-ml-search.py,jmrozanec/white-bkg-classification,1
"are unbalanced.

We first find the separating plane with a plain SVC and then plot
(dashed) the separating hyperplane with automatically correction for
unbalanced classes.

.. currentmodule:: sklearn.linear_model

.. note::

    This example will also work by replacing ``SVC(kernel=""linear"")``
    with ``SGDClassifier(loss=""hinge"")``. Setting the ``loss`` parameter
    of the :class:`SGDClassifier` equal to ``hinge`` will yield behaviour
    such as that of a SVC with a linear kernel.

    For example try instead of the ``SVC``::

        clf = SGDClassifier(n_iter=100, alpha=0.01)

""""""",projects/scikit-learn-master/examples/svm/plot_separating_hyperplane_unbalanced.py,DailyActie/Surrogate-Model,1
"        exit(1)
    
    training_data = {}
    training_data['labels'],training_data['features'],training_data['max_index'] = svmutil.svm_read_problem_ibcga(args.trainfile)
    
    if(int(args.begin) > int(training_data['max_index'])):
        print(""The desired feature number is %2d, while the file contains %2d features"" % ( int(args.begin), int(training_data['max_index'] )))
        exit(1)
    
    # run IBCGA
    IGAFrame.GeneticAlgorithm(IBCGA_SVC(training_data['max_index'], 
                                       training_data['labels'], 
                                       training_data['features'],
                                       nfold = args.nfold,
                                       limit = args.generation, 
                                       size = args.population_size,
                                       begin = args.begin,
                                       end = args.end
                                       )).run()
    pass",ibcga-svc-py/ibcga_svc_ma.py,markliou/IBCGA,1
"    ...     tree_root_node.merge_tree_store(each_tree_root)
    >>> ## Run map processes in single process
    >>> ## ===================================
    >>> #for input in input_list:
    >>> #    # input = input_list[0]
    >>> #    mapper.map(input)
    >>> ## Run reduce process
    >>> ## ==================
    >>> tree_root_node.reduce()
    ResultSet(
    [{'key': SelectKBest/SVC(C=1), 'y/test/score_f1': [ 0.6  0.6], 'y/test/score_recall_mean/pval': [ 0.5], 'y/test/score_recall/pval': [ 0.   0.5], 'y/test/score_accuracy/pval': [ 0.], 'y/test/score_f1/pval': [ 0.   0.5], 'y/test/score_precision/pval': [ 0.5  0. ], 'y/test/score_precision': [ 0.6  0.6], 'y/test/score_recall': [ 0.6  0.6], 'y/test/score_accuracy': 0.6, 'y/test/score_recall_mean': 0.6},
     {'key': SelectKBest/SVC(C=3), 'y/test/score_f1': [ 0.6  0.6], 'y/test/score_recall_mean/pval': [ 0.5], 'y/test/score_recall/pval': [ 0.   0.5], 'y/test/score_accuracy/pval': [ 0.], 'y/test/score_f1/pval': [ 0.   0.5], 'y/test/score_precision/pval': [ 0.5  0. ], 'y/test/score_precision': [ 0.6  0.6], 'y/test/score_recall': [ 0.6  0.6], 'y/test/score_accuracy': 0.6, 'y/test/score_recall_mean': 0.6}])

    '''
    def __init__(self,
                 Xy,
                 tree_root,
                 store_fs=None,
                 function=""transform""):
",epac/map_reduce/mappers.py,neurospin/pylearn-epac,1
"import numpy as np

import config

class Trainer:
    clf = None
    svm = None

    def __init__(self):
        if config.model is 'SVM':
            self.svm = svm.SVC(kernel='linear', shrinking=True, verbose=False)
            params = {
                'C': np.logspace(-5, -1, num=20), # Range of C values
            }
            self.clf = GridSearchCV(self.svm, params,
                cv      = 5,           # k-fold CV
                n_jobs  = cpu_count(), # Parallelize over CPUs
                verbose = 1,
            )
            self.clf_gist = GridSearchCV(self.svm, params,",suitability/Trainer.py,swook/autocrop,1
"from sklearn.metrics import accuracy_score
from mnist import mnist_loader

# load
training_data, validation_data, test_data = mnist_loader.load_data()

# train
print 'Start training'
e0 = cv2.getTickCount()

clf = svm.SVC()
clf.fit(training_data[0], training_data[1])

print 'End training'
e00 = cv2.getTickCount()
time0 = (e00 - e0) / cv2.getTickFrequency()
print 'Training data duration: %.3fs' % time0

# test
predictions = [int(a) for a in clf.predict(test_data[0])]",OpencvSample/Chapter3_Machine_Learning/SklearnSample.py,GeHongpeng/PythonProject,1
"        merged, data_ = merge_clusters(data_, densclust.labels_, class_id)
        merged_clusters.append(merged)

    merged, data_ = merge_clusters(data_, densclust.labels_, class_id+1, tail=True)
    merged_clusters.append(merged)

    # vd.plot_densclusters(merged_clusters, scales=scales)
    merged_p = np.random.permutation(np.concatenate(tuple(merged_clusters)))

    # clf = KNeighborsClassifier(n_neighbors=int(0.01*len(data)))
    # clf = svm.SVC(C=1, gamma=100.) #kernel='poly'
    merged_px, merged_py = ld.split_by_lastcol(merged_p)
    fitter = clf.fit(merged_px, merged_py)

    ids = range(len(dens_layers)+1)
    predict_haz = clf.predict(haz_test)
    predict_nohaz = clf.predict(nohaz_test)
    # print ""predict_haz:"", predict_haz[:10]
    # print ""predict_nohaz:"", predict_nohaz[:10]
",asterion_learn.py,nomad-vagabond/asterion,1
"    variable_id = Column(Integer, ForeignKey('variables.id'), primary_key=True)
    amount = Column(Integer)

    def __init__(self, user_id, script_id, variable_id, amount):
        self.user_id = user_id
        self.script_id = script_id
        self.variable_id = variable_id
        self.amount = amount

    def __repr__(self):
        return 'USVC(%s, %s, %s, %d)' % (self.user_id, self.script_id,
                self.variable_id, self.amount)",classes.py,MerlijnWajer/SRL-Stats,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Storing the predictions of the trained classifier on the testing dataset...""
predicted = clf.predict(X_test_vect_red)

#print ""Evaluation results of the content-based engine working alone, predicting the top-1 developer: ""
#
import numpy as np
#print ""Accuracy: %s"" % np.mean(predicted == y_test)",Archive/RecoDev-Evaluator/recodev_eval2.py,amirhmoin/recodev,1
"

class BaselineClassifiers:
    def __init__(self):
        self.name = ""Baseline Classifiers""


    def learn_binary_models(self,X,Y):

        # SVM Classifier with linear kernel.
        svc = SVC(C=1.0,kernel='linear')
        svc.fit(X,Y)

        # Multinomial Naive Bayes Classifier.
        bnb = BernoulliNB()
        bnb.fit(X,Y)

        # Decision Tree Classifier.
        dtree = tree.DecisionTreeClassifier()
        dtree.fit(X,Y)",textanalysis/classify/baseline_classifiers.py,arunreddy/text-analysis,1
"random.shuffle(idxes)

# Training data
train_matrix = feat_matrix[idxes[0: int(np.ceil(train_pc*num_examples)) ] ]
train_labels = labels[idxes[0: int(np.ceil(train_pc*num_examples)) ] ]

# Val data
val_matrix = feat_matrix[idxes[ int(np.ceil(train_pc*num_examples)) : ] ]
val_labels = labels[idxes[ int(np.ceil(train_pc*num_examples)) : ] ]

clf = LinearSVC(C=10.0, verbose=1)
#clf = SVC(C=10.0, verbose=1, kernel='poly', degree=2)
print ""About to train SVM""
clf.fit(train_matrix, train_labels)
print ""Done training""

val_pred = clf.predict(val_matrix)
acc = np.mean(val_labels == val_pred)

print ""Validation Accuracy is "", acc",semantic_glovem_boc.py,tadityasrinivas/quora_semantic_equivalence,1
"    '''
    return scaler, X_trans_norm, Y_tr


'''
This method runs on a single core during the reduction phase.
'''

def Learn(binary_features, binary_labels):
    #binary_features, binary_labels = load_svmlight_file(""./train_binary"")
    clf = LinearSVC(C=0.01)
    clf.fit(binary_features, binary_labels)
    weights = clf.coef_[0]
    #np.save(""./weights"", weights)
    return weights


def Prediction(X_test_tfidf, class_list, X_test, class_map, collection_vec, idfs, classes_length, D, scaler):
    nbrs = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=num_candidates).fit(class_centroids_arr)
    # print(""before"")",run_script_m2b_github.py,bikash617/Double-Sampled-Multiclass-to-Binary-Reduction,1
"    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    
    sc=[]
    for train_index, test_index in kf:
        #print(""TRAIN:"", train_index, ""TEST:"", test_index)
    
        #generated train and test lists, incuding indices of the examples in training/test
        #for the specific fold. Indices starts from 0 now
        
        clf = svm.SVC(C=c, kernel='precomputed')
        train_gram = [] #[[] for x in xrange(0,len(train))]
        test_gram = []# [[] for x in xrange(0,len(test))]
          
        #generate train matrix and test matrix
        index=-1    
        for row in gram:
            index+=1
            if index in train_index:
                train_gram.append([gram[index,i] for i in train_index])",scripts/cross_validation_from_matrix_norm.py,nickgentoo/scikit-learn-graph,1
"        if n_folds and \
           np.any([np.sum(y == class_) < n_folds for class_ in self.classes_]):
            raise ValueError(""Requesting %d-fold cross-validation but provided""
                             "" less than %d examples for at least one class.""
                             % (n_folds, n_folds))

        self.calibrated_classifiers_ = []
        if self.base_estimator is None:
            # we want all classifiers that don't expose a random_state
            # to be deterministic (and we don't want to expose this one).
            base_estimator = LinearSVC(random_state=0)
        else:
            base_estimator = self.base_estimator

        if self.cv == ""prefit"":
            calibrated_classifier = _CalibratedClassifier(
                base_estimator, method=self.method)
            if sample_weight is not None:
                calibrated_classifier.fit(X, y, sample_weight)
            else:",summary/sumy/sklearn/calibration.py,WangWenjun559/Weiss,1
"            targets.append(g2)
    return features, targets


#==============================================================================
# SVM
#==============================================================================
# SVM - Linear
def do_SVM_linear():
    features, targets = get_values_single_subject()
    clf = svm.SVC(kernel='linear')
    clf.fit(features, targets)
    class_pred = list(clf.predict(features))
    print classification_report(targets, class_pred)
    
    # SVM - Radial Basis Function kernel
    # Ref - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    #sklearn.svm.SVC.predict_proba
    clf = svm.SVC(kernel='rbf', probability=True)
    clf.fit(features, targets)",kernel_svm.py,sagarjauhari/BCIpy,1
"            X.append(x)
            y.append(k[-3:])  # HACK
            # y.append(k)
    return np.array(X, np.float32), np.array(y, np.int32)

if __name__ == '__main__':
    X, y = load_data(sys.argv[1])
    Xt, yt = load_data(sys.argv[2])

    # === train ===
    clf = OneVsOneClassifier(svm.LinearSVC(verbose=1,
                                           max_iter=10000,
                                           dual=False,
                                           ), 5)
    clf.fit(X, y)
    pickle.dump(clf, open('svm.pkl', 'wb'))

    # clf = pickle.load(open('svm.pkl', 'rb'))

    # === test ===",tools/svm.py,hrantzsch/signature-verification,1
"        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",classifier_university_title.py,denimalpaca/293n,1
"# We use the default selection function: the 10% most significant features
selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X, y)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
plt.bar(X_indices - .45, scores, width=.2,
        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')

###############################################################################
# Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

svm_weights = (clf.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()

plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',
        color='navy')

clf_selected = svm.SVC(kernel='linear')",examples/feature_selection/plot_feature_selection.py,ngoix/OCRF,1
"	iris = load_iris()
	X = iris.data
	y = iris.target
	#To make sure there are only two classes and then shuffling
	data=np.c_[X[:100,:],y[:100]]
	np.random.shuffle(data)
	X = data[:,:-1]
	y = data[:,-1]	
	#print(X.shape)
	#print(y)
	clf= SVC(probability=True)
	clf.fit(X[:50,:],y[:50])
	
	print(""KS p value : "",p_value_scoring_object(clf,X[50:,:],y[50:]))
	print(""AD p value : "",p_value_scoring_object_AD(clf,X[50:,:],y[50:]))

",learningml/GoF/p_value_scoring_object.py,weissercn/learningml,1
"
names = [
""KNN"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
         ""Quadratic Discriminant Analysis"", 
         ""MLP""]


classifiers = [
    KNeighborsClassifier(weights='distance', n_neighbors=121),
    SVC(kernel=""linear"", C=1, probability=True),
    SVC(C=1, probability=True),
    DecisionTreeClassifier(max_depth=10),
    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(solver='lsqr', shrinkage=""auto""),
    QuadraticDiscriminantAnalysis(),
    MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,75,50,25,15), max_iter=10000, random_state=1)
    ]",2-AlexNet/MLPClassifier.py,cs60050/TeamGabru,1
"    distribution,  then predictor should be binary or it will fail.
    metric: the metric for cross validation.
    
    Returns
    fitted GridSearchCV object
    """"""    
    
    # instantiate stratified k-fold
    cv = StratifiedKFold(target, n_folds=folds)
    # instantiate svm model
    svr = svm.SVC() 
    # Perform cross validation
    clf = GridSearchCV(estimator=svr, param_grid=grid, n_jobs=-1,
                       scoring=metric, cv = cv)
    clf.fit(predictor, target)
    return clf",helpers/models.py,plablo09/geo_context,1
"    features_train = pca.transform(features_train)
    features_test = pca.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_4(pca).py,dikien/Machine-Learning-Newspaper,1
"	LogisticRegressionClassifier=SklearnClassifier(LogisticRegression())
	LogisticRegressionClassifier.train(training_set)

	# print(""LogisticRegression Accuracy:"",nltk.classify.accuracy(LogisticRegressionClassifier,test_set))

	SGDClassifier=SklearnClassifier(SGDClassifier())
	SGDClassifier.train(training_set)

	# print(""SGDClassifier Accuracy:"",nltk.classify.accuracy(SGDClassifier,test_set))

	# SVCClassifier=SklearnClassifier(SVC())
	# SVCClassifier.train(training_set)

	# print(""SVC Accuracy:"",nltk.classify.accuracy(SVCClassifier,test_set))

	LinearSVCClassifier=SklearnClassifier(LinearSVC())
	LinearSVCClassifier.train(training_set)

	# print(""LinearSVC Accuracy:"",nltk.classify.accuracy(LinearSVCClassifier,test_set))
",roorkee-bot.py,abhishekjiitr/roorkee-bot,1
"    elif estimator == 'ExtraTreesClassifier':
        param_dist = {**parameters['ensemble'], **parameters['bootstrap'],
                      **parameters['criterion']}
        estimator = ExtraTreesClassifier(
            n_jobs=n_jobs, n_estimators=n_estimators)
    elif estimator == 'GradientBoostingClassifier':
        param_dist = parameters['ensemble']
        estimator = GradientBoostingClassifier(n_estimators=n_estimators)
    elif estimator == 'LinearSVC':
        param_dist = parameters['linear_svm']
        estimator = LinearSVC()
    elif estimator == 'SVC':
        param_dist = parameters['svm']
        estimator = SVC(kernel='rbf')
    elif estimator == 'KNeighborsClassifier':
        param_dist = parameters['kneighbors']
        estimator = KNeighborsClassifier(algorithm='auto')

    return param_dist, estimator
",q2_sample_classifier/utilities.py,nbokulich/q2-sample-classifier,1
"from text.document import Document
from text.sentence import Sentence

pp = pprint.PrettyPrinter(indent=4)
text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(7,20), min_df=0.2, max_df=0.5)),
                             #('vect', CountVectorizer(analyzer='word', ngram_range=(1,5), stop_words=""english"", min_df=0.1)),
                             #     ('tfidf', TfidfTransformer(use_idf=True, norm=""l2"")),
                                  #('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(6,20))),
                                  #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=5, random_state=42)),
                                  #('clf', SGDClassifier())
                                  #('clf', svm.SVC(kernel='rbf', C=10, verbose=True, tol=1e-5))
                                  #('clf', RandomForestClassifier(n_estimators=10))
                                    #('feature_selection', feature_selection.SelectFromModel(LinearSVC(penalty=""l1""))),
                                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))
                                  #('clf', DummyClassifier(strategy=""constant"", constant=True))
                                 ])
class SeeDevCorpus(Corpus):
    """"""
    Corpus for the BioNLP SeeDev task
    self.path is the base directory of the files of this corpus.",src/reader/seedev_corpus.py,AndreLamurias/IBRel,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = JMI.jmi(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_JMI.py,jundongl/scikit-feast,1
"import numpy as np
from scipy import sparse
import matplotlib.pyplot as plt
from sklearn import svm

w = np.array([[0, 1], [1, 2], [2, 1]])
a = sparse.lil_matrix((3, 3))
a[0, 0] = a[1, 1] = a[2, 2] = 1
x = a.dot(w)
y = np.arange(3)
svc = svm.SVC().fit(x, y)
x = np.r_[x, np.array([[3, svc.predict(np.array([[0, 0]]))]])]
plt.plot(x[:, 0], x[:, 1])
plt.show()",check.py,hamukazu/pyconjp2015tutorial,1
"			self.labels_train_noisy[ self.data_train.index(x) ] = random_label
			self.nbr_noisy_labels += 1
			print ""Perturbed = "", self.nbr_noisy_labels
			
		return random_label

	def queryNoisyLabel2(self, x, p = 0.3): # most probable label error
		random_label = self.getTrueLabel(x)
		if random.random() <= p:
			GAMMA, C, K = 0.1, 100, 10
			hh = svm.SVC(gamma=GAMMA, C=C, probability=True).fit(self.data_test, self.labels_test)
			for id_p in range( len( list(self.unic_labels) ) ):
				random_label = raf.getLabelOf(id_p+1, x, hh)
				if random_label != self.getTrueLabel(x): break

			self.labels_train_noisy[ self.data_train.index(x) ] = random_label
			self.nbr_noisy_labels += 1
			print ""Perturbed = "", self.nbr_noisy_labels
			
		return random_label",DatasetLoader.py,HTCode/SimpleML,1
"    return -1


def train_and_predict(train_data, predict_data):
    # 注意，训练数据的最后一条没有下一日涨跌幅
    x_array = [(r.k, r.d, r.j) for r in train_data[:-1]]
    y_array = [math.copysign(1, d2.close - d1.close) for (d1, d2) in zip(train_data[:-1], train_data[1:])]
    # 默认svm参数，默认归一化，训练和预测
    scaler = MinMaxScaler()
    x_array_scaled = scaler.fit_transform(x_array)
    model = SVC()
    model.fit(x_array_scaled, y_array)
    x_array_predict_scaled = scaler.transform([(r.k, r.d, r.j) for r in predict_data])
    y_array_predict = model.predict(x_array_predict_scaled)
    return y_array_predict


if __name__ == ""__main__"":

    # 读入csv",test/kdj_test/kdj.py,zsffq999/helloworld,1
"#                    'it is raining in britian and nyc',
#                    'hello welcome to new york. enjoy it here and london too'])
# target_names = ['New York', 'London']

# lb = preprocessing.LabelBinarizer()
# Y = lb.fit_transform(y_train_text)

# classifier = Pipeline([
#     ('vectorizer', CountVectorizer()),
#     ('tfidf', TfidfTransformer()),
#     ('clf', OneVsRestClassifier(LinearSVC()))])

# classifier.fit(X_train, Y)
# predicted = classifier.predict(X_test)
# all_labels = lb.inverse_transform(predicted)

# for item, labels in zip(X_test, all_labels):
#     print '%s => %s' % (item, ', '.join(labels))

# X = [[0], [1], [2], [3]]",test.py,bubae/gazeAssistRecognize,1
"
# we create 40 separable points
rng = np.random.RandomState(0)
n_samples_1 = 1000
n_samples_2 = 100
X = np.r_[1.5 * rng.randn(n_samples_1, 2),
          0.5 * rng.randn(n_samples_2, 2) + [2, 2]]
y = [0] * (n_samples_1) + [1] * (n_samples_2)

# fit the model and get the separating hyperplane
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X, y)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - clf.intercept_[0] / w[1]


# get the separating hyperplane using weighted classes",Statistiques/Algorithme du gradient stochastique/sgd_separating_hyperplane_unbalanced.py,NicovincX2/Python-3.5,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = bongo.external.email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",src/libs/python/bongo/external/email/test/test_email_renamed.py,bongo-project/bongo,1
"
C = np.linspace(0.5, 2.5, 10)
Gamma = np.linspace(1e-2, 0.2, 10)


def test_c_gamma(training_features, training_classes, test_features, test_classes, c, gamma):
    f_cv = []
    f_train = []
    for iii in c:
        for jjj in gamma:
            clf = svm.SVC(C=iii, gamma=jjj, cache_size=1000)
            clf.fit(training_features, training_classes)
            f_train = np.append(f_train, np.mean(F1Score(training_features, training_classes, clf).values()))
            f_cv = np.append(f_cv, np.mean(F1Score(test_features, test_classes, clf).values()))
        print iii
    f_cv_matrix = np.reshape(f_cv, (len(C), len(gamma)))
    f_train_matrix = np.reshape(f_train, (len(C), len(gamma)))
    return f_train_matrix, f_cv_matrix

",Evaluation/testC-gamma.py,andimarafioti/AIAMI,1
"    print 'loss_train: %s' % loss_train
    print 'loss_test: %s' % loss_test


def ctr_svm(model='sklearn-clicklog', from_cache=False, train_dataset_length=100000, test_dataset_length=100000):
    """"""
    Doesn't work
    """"""
    TRAIN_FILE, TEST_FILE = create_dataset(model, from_cache, train_dataset_length, test_dataset_length)

    prediction_model = LinearSVC(
        penalty='l1',
        loss='squared_hinge',
        dual=False,
        tol=0.0001,
        C=1.0,
        multi_class='ovr',
        fit_intercept=True,
        intercept_scaling=1,
        class_weight=None,",sklearn_experiments.py,kazarinov/hccf,1
"#                 te_set_principal, te_labels,
#                 log_reg,
#                 'linear model (with PCA)')

# Do the same thing without PCA
# fit_and_evaluate(tr_set, tr_labels, te_set, te_labels,
#                 log_reg,
#                 'linear model (without PCA)')

# Test radial basis SVM with PCA and without
rad_svm = SVC()
# fit_and_evaluate(tr_set_principal, tr_labels, te_set_principal, te_labels,
#                 rad_svm,
#                 'RBF SVM (with PCA)')

# Test radial basis SVM with PCA and without
# fit_and_evaluate(tr_set, tr_labels, te_set, te_labels,
#                 rad_svm,
#                 'RBF SVM (without PCA)')
",task1/model_testing.py,stefan-ptrvch/os4nm,1
"        db.close()
        db.close_connection()
    
        map_result['status'] = 0
        map_result['description'] = 'success'
        return map_result
     
# if __name__ == '__main__':
#     from sklearn.svm import SVC
#     import numpy as np
#     clf = SVC()
#     x_train = np.loadtxt('data/fselect.txt', delimiter=',', dtype=int)
#     y_train = np.loadtxt('data/fresult.txt', dtype=int)
#     clf = clf.fit(x_train, y_train)
#     lst = []
#     lst.append(3)
#     lst.append(0)
#     lst.append(0)
#     lst.append(0)
#     lst.append(0)",fbcredibility.py,chaluemwut/fbserver,1
"
def machinery(points, c):
    X = []
    y = []
    y_location = len(points[0]) -1 # y's location is assumed to be the last element in the list

    for point in points:
        X.append(numpy.array(point[:y_location]))
        y.append(point[y_location])

    machine = svm.SVC(kernel = 'linear', C=c)
    return machine.fit(X, y)

def estimator(points):
    X = []
    y = []
    y_location = len(points[0]) -1 # y's location is assumed to be the last element in the list

    for point in points:
        X.append(numpy.array(point[:y_location]))",Homework_7/Python/hw7_by_kirbs.py,nobel1154/edX-Learning-From-Data-Solutions,1
"Y = lb.fit_transform(l_train)
#print l_train
# print Y

if (X_train.size != len(l_train)):
    print ""Training sample size %d, Skills Labels %d Do not match"" % (X_train.size, len(l_train))
    sys.exit()

ex_classifier = Pipeline([
    ('tfidf', TfidfVectorizer()),  # Tfidtransformer combines countvectorize + tfid transformaer
    ('clf', OneVsRestClassifier(LinearSVC()))])

ex_classifier.fit(X_train, Y)

# save the classifier
_ = joblib.dump(ex_classifier, 'cf_ml_skill.pkl', compress=9)
_ = joblib.dump(lb, 'cf_ml_skill_binerizer.pkl', compress=9)

print ""Successfully generated the classfier and binarizer""",multilabel/online_training/cf_ml_skill_trainer.py,praveen049/text_classification,1
"from sklearn.feature_selection import SelectKBest
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline

# Potential pipeline steps
scaler = MinMaxScaler()
select = SelectKBest()
dtc = DecisionTreeClassifier()
svc = SVC()
knc = KNeighborsClassifier()

# Load pipeline steps into list
steps = [
		 # Preprocessing
         # ('min_max_scaler', scaler),
         
         # Feature selection
         ('feature_selection', select),",udacity/enron/ud120-projects-master/final_project/poi_id2.py,harish-garg/Machine-Learning,1
"    Output
    ------
    F: {numpy array}, shape (n_features, )
        index of selected features
    """"""

    n_samples, n_features = X.shape
    # using 10 fold cross validation
    cv = KFold(n_samples, n_folds=10, shuffle=True)
    # choose SVM as the classifier
    clf = SVC()

    # selected feature set, initialized to contain all features
    F = range(n_features)
    count = n_features

    while count > n_selected_features:
        max_acc = 0
        for i in range(n_features):
            if i in F:",PyFeaST/function/wrapper/svm_backward.py,jundongl/PyFeaST,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.NuSVC(gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='js').export()
print(output)

""""""
// Array.prototype.fill polyfill:
[].fill||(Array.prototype.fill=function(a){for(var b=Object(this),c=parseInt(b.length,10),d=arguments[1],e=parseInt(d,10)||0,f=0>e?Math.max(c+e,0):Math.min(e,c),g=arguments[2],h=void 0===g?c:parseInt(g)||0,i=0>h?Math.max(c+h,0):Math.min(h,c);i>f;f++)b[f]=a;return b});
",examples/classifier/NuSVC/js/basics.py,nok/sklearn-porter,1
"  if not isinstance(test, pd.core.frame.DataFrame):
    test = csv2DF(test, as_mtx=False, toBin=True)

  if smoteit:
    train = SMOTE(train, resample=True)
    # except: set_trace()
  if not tunings:
    if regress:
      clf = SVR()
    else:
      clf = SVC()
  else:
    if regress:
      clf = SVR()
    else:
      clf = SVC()

  features = train.columns[:-1]
  klass = train[train.columns[-1]]
  # set_trace()",src/tools/oracle.py,rahlk/RAAT,1
"	if i == [""Nom"",""Classe"",""Taille"",""Entropie1"", ""Entropie2"", ""Entropie3"", ""TailleAsm"", ""Entropie1Asm"", ""Entropie2Asm"", ""Entropie3Asm"", ""Sections""]:pass
	else:
		Val += [[float(i[2])*alpha,float(i[3])*beta,float(i[4])*beta,float(i[5])*beta,float(i[6])*alpha,float(i[7])*beta,float(i[8])*beta,float(i[9])*beta]]
		Fam += [int(i[1])]

X = np.array(Val)
y = np.array(Fam)

tmps1=time.time()

#lin_svc = svm.LinearSVC(C=1.0)
#scoresLIN_SVC = cross_validation.cross_val_score(lin_svc, X, y, cv=5)
#print scoresLIN_SVC
#print(""Accuracy LIN_SVC: %0.2f (+/- %0.2f)"" % (scoresLIN_SVC.mean(), scoresLIN_SVC.std() * 2))

#near_centroid = neighbors.NearestCentroid(shrink_threshold=None)
#scoresCENTROID = cross_validation.cross_val_score(near_centroid, X, y, cv=5)
#print scoresCENTROID
#print(""Accuracy CENTROID: %0.2f (+/- %0.2f)"" % (scoresCENTROID.mean(), scoresCENTROID.std() * 2))
",Tests/test-cross-validation.py,oubould/TestsML,1
"
  #C=[0.001,0.01,0,1,1,10,100,1000,10000,100000]
  #gamma=[1.0e-7,1.0e-6,1.0e-5,1.0e-4,0.001,0.01,0.1,1,10,100]

  C=[10000,100000,1000000]
  gamma=[1.0e-4,1.0e-5,1.0e-6,1e-7]

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/tune-rain-five.py,n7jti/machine_learning,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_30_2014_server_4.py,magic2du/contact_matrix,1
"class NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,dhalleine/tensorflow,1
"#for index, row in test.iterrows():
#    cur = test.get_value(index, ""cabin"")
#    if type(cur) == str:
#        if cur not in cabs:
#            cabs += [cur]
#        test = test.replace(to_replace=cur, value=cabs.index(cur))

train2 = train2.fillna(train2.mean())
test = test.fillna(test.mean())

dec = svm.SVC()
dec.fit(train2, train.get(""survived""))

print ""passenger_id,survived""

for index, row in test.iterrows():
    print ""%s,\""%s\""""%(index+785, dec.predict(row)[0])",lab6/alec.py,cycomachead/info290,1
"        yk_train, yk_validation = y_train[train_index], y_train[validation_index]

        for C in C_VALUES:
            acc = calculate_accuracy(Xk_train, Xk_validation, yk_train, yk_validation, C)
            accuracy_dict[C].append(acc)
    return accuracy_dict


def calculate_accuracy(X_train, X_validation, y_train, y_validation, C=1.0):
    # Training
    # svm = LinearSVC(C=C)
    svm = SVC(C=10, gamma=C)
    svm.fit(X_train, y_train)

    # Validate
    y_pred = svm.predict(X_validation)

    # Calculate accuracy
    tp = np.sum(y_validation == y_pred)
    accuracy = float(tp) / len(y_validation)",lab1/utils.py,emilioramirez/aacv2016,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.naive_bayes import BernoulliNB
from sklearn import svm
from get_data import *
from get_data_2 import *
from sklearn.feature_extraction.text import CountVectorizer
import scipy.sparse
import csv

ls = svm.LinearSVC()
labels, training_data_matrix, unique_ingredients = get_training_data_matrix(get_train_data())
ls = ls.fit(training_data_matrix, labels)

print(""Training Done"")

print(cross_val_score(ls, training_data_matrix, labels, cv=5).mean())

print(""CV done"")
",Easy/What's Cooking/linear_svc.py,AhmedHani/Kaggle-Machine-Learning-Competitions,1
"from sklearn.svm import SVC
# from sklearn import datasets
# import numpy as np

# iris = datasets.load_iris()
# X = iris.data
# Y = iris.target
# test_X = np.array([[100, 2, 3, 100]])

def svm(X, Y, test_X):
    svc = SVC()
    svc.fit(X, Y)",classical/svm.py,MihawkHu/Gene_Chip,1
"
    def get_result_field(self):
        raise NotImplementedError

class classifier_engine(engine):
    def __init__(self):
        self.type = ""classifier""
        engine.__init__(self)

    def get_model(self, params):
        return svm.SVC(**params)

    def get_result_field(self):
        return 'id'

class regression_engine(engine):
    def __init__(self):
        self.type = ""regression""
        engine.__init__(self)
",sim/train_sklearn.py,mapto/sprks,1
"y = df.loc[:, 1].values

# All malignant tumors will be represented as class 1, otherwise, class 0
le = LabelEncoder()
y = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, \
        test_size=0.20, random_state=1)

pipe_svc = Pipeline([('scl', StandardScaler()), \
        ('clf', SVC(random_state=1))])

param_range = [10**i for i in range(-4, 4)]
param_grid = [{'clf__C': param_range, \
        'clf__kernel': ['linear']}, \
        {'clf__C': param_range, \
        'clf__gamma': param_range, \
        'clf__kernel': ['rbf']}]

gs = GridSearchCV(estimator=pipe_svc, \",HPTuning/GridSearch.py,southpaw94/MachineLearning,1
"
    X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],
                      random_state=22)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    plt.suptitle(""decision_threshold"")
    axes[0, 0].set_title(""training data"")
    axes[0, 0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm)

    svc = SVC(gamma=.05).fit(X_train, y_train)
    axes[0, 1].set_title(""decision with threshold 0"")
    axes[0, 1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm)
    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,
                   ax=axes[0, 1])
    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 1])
    axes[0, 2].set_title(""decision with threshold -0.8"")
    axes[0, 2].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm)
    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 2], threshold=-.8)
    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,",mglearn/plot_metrics.py,amueller/advanced_training,1
"iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
lin_svc = svm.LinearSVC(C=C).fit(X, y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))",TeST/plot.py,sudhanshuptl/Machine-Learning,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",fail/335_test_email.py,mancoast/CPythonPyc_test,1
"#	roc_auc = auc(false_positive_rate, true_positive_rate)
  #  ax.set_title('ROC for %s' % m)
  #  ax.plot(false_positive_rate, true_positive_rate, \
  #              c='#2B94E9', label='AUC = %0.2f'% roc_auc)
 #       ax.legend(loc='lower right')
  #      ax.plot([0,1],[0,1],'m--',c='#666666')
  #  plt.xlim([0,1])
  #  plt.ylim([0,1.1])
  #  plt.show()

#y_true_svc, y_pred_svc = get_preds(stdfeatures, labels, LinearSVC())
#y_true_knn, y_pred_knn = get_preds(stdfeatures, labels, KNeighborsClassifier())

#actuals = np.array([y_true_svc,y_true_knn])
#predictions = np.array([y_pred_svc,y_pred_knn])
#models = ['LinearSVC','KNeighborsClassifier']

if __name__ == '__main__':

    if len(sys.argv) > 1:",code/prediction/data_visual.py,georgetown-analytics/housing-risk,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - Cparam:           SVM parameter C (cost of constraints violation)
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    svm = sklearn.svm.SVC(C = Cparam, kernel = 'linear',  probability = True)        
    svm.fit(X,Y)

    return svm


def trainRandomForest(features, n_estimators):
    '''
    Train a multi-class decision tree classifier.
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training",src/pyAudioAnalysis/audioTrainTest.py,belkinsky/SFXbot,1
"from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.grid_search import GridSearchCV


# t0 = time()
# t1 = time()

# for c in (10000):
c=10000
clf = SVC(kernel = ""rbf"",C = c)
print ""Training with C = %s""%c

# clf.fit(features_train_skinny,labels_train_skinny)
clf.fit(features_train,labels_train)
# print ""training time:"", round(time()-t0, 3), ""s""

pred = clf.predict(features_test)
# print ""predicting time:"", round(time()-t1, 3), ""s""
score = accuracy_score(pred,labels_test)",svm/svm_author_id.py,junhua/udacity_intro_to_ml,1
"        solver = ['lbfgs']
        max_iter = [1000]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(mlp, dict(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter, early_stopping=[False]), X, y)
        f = open('output/gender.mlp.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates SVM classifier
        svm = SVC()
        kernel = ['linear', 'rbf', 'poly', 'sigmoid']
        Cs = np.logspace(-3, 4, 8) # C = [0.001, 0.01, .., 1000, 10000]
        gamma = np.logspace(-3, 4, 8) # gamma = [0.001, 0.01, .., 1000, 10000]
        degree = [2, 3]
        coef0 = [0.0]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(svm, dict(kernel=kernel, C=Cs, gamma=gamma, degree=degree, coef0=coef0), X, y)
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        f = open('output/gender.svm.out', 'a')
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):",tests/test_gender.py,fberanizo/author-profiling,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_27_2014_server.py,magic2du/contact_matrix,1
"#feature reduction (on HOG part)
gain, j = mutual_info_classif(data[:, 8:-1], data[:, -1], discrete_features='auto', n_neighbors=3, copy=True, random_state=None), 0
for i in np.arange(len(gain)):
	if gain[i] <= 0.001:
		data = np.delete(data, 8+i-j, 1)
		j += 1

X_train, X_test, y_train, y_test = train_test_split(data[:, 0:-1], data[:, -1], test_size = 0.4, random_state = 0)

start = timer()
clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
clf2 = MLPClassifier(solver='lbfgs', activation = 'logistic', learning_rate = 'adaptive', hidden_layer_sizes = (5), random_state = 1)
clf2 = clf2.fit(X_train, y_train)
clf3 = LogisticRegression().fit(X_train, y_train)
eclf = VotingClassifier(estimators=[('svm', clf1), ('mlp', clf2), ('lr', clf3)], voting='soft')
eclf = eclf.fit(X_train, y_train)
y_pred = eclf.predict(X_test)
end = timer()

print(""Confusion Matrix: \n"")",Holdout/Voting.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"# Remove the labels
test = np.load('test_distribute.npy')[:,1:]

data = train[:,1:]
target = train[:,0]

# Originally, I thought removing these fields would be good, but I actually do better by not removing them, tragic
#data = np.delete(data, np.s_[[x for x in range(990) if x % 15 in [3,4,5,10,11,12,13,14]]], 1)
#test = np.delete(test, np.s_[[x for x in range(990) if x % 15 in [3,4,5,10,11,12,13,14]]], 1)

#clf = svm.LinearSVC()
pred = RandomForestClassifier(n_estimators=100).fit(data, target).predict(test)

f = open('predictions.csv', 'w')
f.write(""ID,Category\n"")

for i, res in enumerate(pred):
    f.write(""%d,%d\n"" % (i+1,res))

f.close()",Sign-Language/code.py,bcspragu/Machine-Learning-Projects,1
"                     

    # Ignore linear for now, too slow

    scores = [('f1',make_scorer(f1_score))]

    for score in scores:
        print(""# Tuning hyper-parameters for %s"" % score[0])
        print()

        clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=k,
                           scoring=score[1])
        clf.fit(X_train, y_train)

        print(""Best parameters set found on development set:"")
        print()
        print(clf.best_params_)
        print()
        #print(""Grid scores on development set:"")
        #print()",c2_grid.py,Tweety-FER/tar-polarity,1
"seed = 7
scoring = 'accuracy'

# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
	cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())",try-ml/try-v01.py,yvlasov/ConProbIN,1
"
    estimator = EstimatorWithFit()
    scorer = check_scoring(estimator, allow_none=True)
    assert_true(scorer is None)


def test_check_scoring_gridsearchcv():
    # test that check_scoring works on GridSearchCV and pipeline.
    # slightly redundant non-regression test.

    grid = GridSearchCV(LinearSVC(), param_grid={'C': [.1, 1]})
    scorer = check_scoring(grid, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    pipe = make_pipeline(LinearSVC())
    scorer = check_scoring(pipe, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    # check that cross_val_score definitely calls the scorer
    # and doesn't make any assumptions about the estimator apart from having a",projects/scikit-learn-master/sklearn/metrics/tests/test_score_objects.py,DailyActie/Surrogate-Model,1
"import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC()
clf.fit(X, y)",svm/test.py,askldjd/udacity-machine-learning,1
"    X_data=np.asarray(X_data)
    y_target=np.asarray(y_target)

    #evaluate model to make sure it is reasonable on the behavioral profile data
    linear_svm_classifier = SVC(kernel=""linear"", C=0.025)
    scores = sklearn.cross_validation.cross_val_score(OneVsRestClassifier(linear_svm_classifier), X_data, y_target, cv=2)
    print(""Accuracy using %s: %0.2f (+/- %0.2f) and %d folds"" % (""Linear SVM"", scores.mean(), scores.std() * 2, 5))


    #Do a full training of the model
    behavioral_profiler = SVC(kernel=""linear"", C=0.025)
    behavioral_profiler.fit(X_data, y_target)

    #Take it out for a spin
    print behavioral_profiler.predict(vectorizer.transform(['Some black shoes to go with your Joy Division hand bag']).toarray()[0])
    print behavioral_profiler.predict(vectorizer.transform(['Ozzy Ozbourne poster, 33in x 24in']).toarray()[0])

    #Now on to classifying our customers
    predicted_profiles=[]
    ground_truth=[]",bpro.py,zang0/bpro,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_04_27_2015_server_for_final.py,magic2du/contact_matrix,1
"    def train_svm(self):
        with open(DATA_PATH+'action_detection_training_set.txt') as f:
            data = f.readlines()
        X, y = [],[]
        for line in data:
            line = line.strip()
            if not line: continue
            line = line.split(' ',1)
            X.append(self.extract_feature(line[1]))
            y.append(int(line[0]))
        lin_clf = svm.LinearSVC()
        lin_clf.fit(X, y)
        self.model = lin_clf
        self.save_model()
        return

    def load_model(self):
        f = open(MODEL_PATH + 'action_detection.model', 'rb')
        self.model = pickle.load(f)
        f.close()",SLU/action_detection.py,raybrshen/robot_sds,1
"        elif clf['dimred'=='none']:
            dimred = None
        else:
            assert False,'FATAL: unknown dimred'

        if dimred is not None:
            dimred.fit(xtr)
            xtr = dimred.transform(np.asarray(xtr))

        ## tuning
        clf = svm.SVC(kernel=self._kernel,probability=True)

        ## train
        if self._kernel=='precomputed':
            assert self._simMat is not None
            simMatTr = cutil.makeComProKernelMatFromSimMat(xtr,xtr,self._simMat)
            clf.fit(simMatTr,ytr)
        else:
            clf.fit(xtr,ytr)
",predictor/imbalance/ensembled_svm.py,tttor/csipb-jamu-prj,1
"    """"""
    from sklearn.base import clone
    from sklearn.utils import check_random_state
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import check_cv

    if clf is None:
        scaler = StandardScaler()
        svc = SVC(C=1, kernel='linear')
        clf = Pipeline([('scaler', scaler), ('svc', svc)])

    info = epochs_list[0].info
    data_picks = pick_types(info, meg=True, eeg=True, exclude='bads')

    # Make arrays X and y such that :
    # X is 3d with X.shape[0] is the total number of epochs to classify
    # y is filled with integers coding for the class to predict
    # We must have X.shape[0] equal to y.shape[0]",mne/decoding/time_gen.py,jaeilepp/eggie,1
"    y_train = np.asarray(fid['y'], dtype=float)

gamma = 1 / np.median(pdist(X_train, 'euclidean'))
C = compute_crange(rbf_kernel(X_train, gamma=gamma))

# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': list(2 ** np.arange(-3, 4.) * gamma), 'C': list(C),
                     'class_weight': [{1: int((y_train == -1).sum() / (y_train == 1).sum())}]},
                    ]

clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy', n_jobs=10, verbose=True)
clf.fit(X_train, y_train)

best = clf.best_estimator_
print(""Best estimator has training accuracy of %.4g"" % clf.best_score_)
joblib.dump(best, args.svmfile)",train_SVM.py,cajal/pupil-tracking,1
"X = data[:, :-1]
X_scaled = preprocessing.scale(X)
y = data[:, -1]

# Initialize several learners, to be compared momentarily
logistic1 = LogisticRegression(C=.1)
logistic2 = LogisticRegression(C=.3)
logistic3 = LogisticRegression(C=1)
logistic4 = LogisticRegression(C=5)
logistic5 = LogisticRegression(C=10)
linsvc = svm.LinearSVC()
knn = KNeighborsClassifier(4, weights='distance')
mytree = tree.DecisionTreeClassifier()
myforest = RandomForestClassifier()

for clf,name in [(logistic1, ""Logistic Regression C=.1""), (logistic2, ""Logistic Regression C=.3""), (logistic3, ""Logistic Regression C=1""), (logistic4, ""Logistic Regression C=5""), (logistic5, ""Logistic Regression C=10""), (linsvc, ""Linear SVC""), (knn, ""K-nearest Neighbors""), (mytree, ""Decision Tree""), (myforest, ""Random Forest"")]:

    scores = cross_validation.cross_val_score(clf, X_scaled, y, scoring='recall', cv=400)
    print(name + "":"")
    print(""Accuracy: {:0.4f} - {:0.4f}"".format(scores.mean()-scores.std()/2, scores.mean()+scores.std()/2))",crossvalidate.py,nrpeterson/mse-closure-predictor,1
"                X = stats.zscore(X, axis=0)
            elif pre_process == ""PCA"":
                pca = PCA()
                (X, V) = fit_transform(pca, X)
                pfield = [pfield[0]] + ['PCA_%d' % (d + 1) for d in range(len(pfield[1:]))]
            elif pre_process == ""Whitened PCA"":
                pca = PCA(whiten=True)
                (X, V) = fit_transform(pca, X)
                pfield = [pfield[0]] + ['PCA_%d' % (d + 1) for d in range(len(pfield[1:]))]

            clf = svm.SVC(C=regularizer, kernel='linear', probability=True, random_state=0, verbose=False)
            clf.fit(X[1:TrainingSize], Y[1:TrainingSize])
            yhat = clf.decision_function(X[TrainingSize:])
            yhat = [yhat[i][0] for i in range(len(yhat))]

            # summary results
            results = evaluate(yhat, Y[TrainingSize:])
            info[""results""].append(
                'Predicting %s using %2.2f %% of the data %s' % (pfield[0], ratio * 100, orderbyMessage))
            info[""results""].append('Number of samples: %d' % len(X))",modules/ml_svm_linear.py,garthee/gnot,1
"#! /usr/bin/python3
from sklearn import datasets

iris = datasets.load_iris()
digits = datasets.load_digits()
print(digits.data)
print(digits.data.shape)
print(digits.images[0])

from sklearn import svm
clf = svm.SVC(gamma=0.001, C=100.)
clf.fit(digits.data[:-1], digits.target[:-1])
result = clf.predict(digits.data[-1:])
print(result)",sklearnLearning/quickStart/LoadingExampleDataset.py,zhuango/python,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",notebooks/fig_code/svm_gui.py,solvie/sklearn_tutorial,1
"def classify_feature():
    train = torchfile.load('./features/feature_train.t7')
    train_y = torchfile.load('./features/train_label.t7')
    train_y = np.array(train_y, dtype = np.int32)
    seed = 42
    # shuffle the data
    np.random.seed(seed)
    np.random.shuffle(train)
    np.random.seed(seed)
    np.random.shuffle(train_y)
    clf = SVC(C = 1.0, probability=True)
    clf.fit(train, train_y)
    print('training finished ......')
    val = torchfile.load('./features/feature_val.t7')
    val_y = torchfile.load('./features/val_label.t7')
    val_y = np.array(val_y, dtype = np.int32)

    val_pred = clf.predict_proba(val)
    loss = log_loss(val_y, val_pred)
    print('loss on validation {}'.format(loss) )",generate_submit.py,ouceduxzk/kaggle_state_drive,1
"
from epitopes import imma2, features

print ""Loading data and transforming to toxin features""
imm, non = imma2.load_classes()
X, Y = features.toxin_features(imm, non, substring_length = 2, positional=False)

def run_classifiers(X,Y):
  print ""Data shape"", X.shape
  for c in [0.0001, 0.001, 0.01]:#, 0.1, 1, 10]:
    svm = sklearn.svm.LinearSVC(C=c)
    print ""SVM C ="", c
    print np.mean(sklearn.cross_validation.cross_val_score(svm, X, Y, cv = 10))

  n_classifiers = 2000
  rf = sklearn.ensemble.RandomForestClassifier(n_classifiers)
  print ""Random Forest""
  print np.mean(sklearn.cross_validation.cross_val_score(rf, X, Y, cv = 10))

""""""",Jan21_toxin.py,hammerlab/immuno_research,1
"# Make the dataset imbalanced
# Select only half of the first class
iris.data = iris.data[25:-1, :]
iris.target = iris.target[25:-1]

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    random_state=RANDOM_STATE)

# Create a pipeline
pipeline = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),
                         LinearSVC(random_state=RANDOM_STATE))
pipeline.fit(X_train, y_train)

# Classify and report the results
print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))",examples/applications/plot_multi_class_under_sampling.py,glemaitre/imbalanced-learn,1
"    std = np.std([tree.feature_importances_ for tree in forest.estimators_],
                 axis=0)
    indices = np.argsort(importances)[::-1]

    l = list(training_matrix.columns.values)
    for f in range(training_matrix.shape[1]):
        print(""%d. feature %d(%s) (%f)"" % (f + 1, indices[f], l[indices[f]], importances[indices[f]]))

    ##### Works well ######
    # SVM
    # svm = SVC(kernel=""linear"", C=0.06)
    # svm.fit(training_matrix, target)
    #
    # scores_svm = cross_validation.cross_val_score(svm, training_matrix, target, cv=5)
    # print(""(svm) Accuracy: %0.5f (+/- %0.2f)"" % (scores_svm.mean(), scores_svm.std() * 2))
    #
    # return svm
    ##### Works well ######

    # Random Forest",AV_Loan_Prediction/Main.py,kraktos/Data_Science_Analytics,1
"from helper_classes import Feature_Preprocessor

def SVM_predict_rank(features, classes, unknown, actual_classes):
    """"""
        Proviced a ranking of the different authors by likelyhood of having authored each unknown text.
    """"""
    FP = Feature_Preprocessor(features, True, False, 30)
    features = FP.batch_normalize(features)
    unknown = FP.batch_normalize(unknown)

    clf = SVC(probability=True, kernel='rbf', C=2.4, degree=1, gamma=0.7/len(features[0]))
    clf.fit(features, classes)

    # I'm sorry about the following lines:
    predictions = map(lambda x : zip(clf.classes_, x), clf.predict_log_proba(unknown))
    orderings = zip(map(lambda x : sorted(x, key = lambda s : s[1], reverse=True), predictions), actual_classes)

    orderings = [([ e[0] for e in l[0] ], l[1]) for l in orderings]
    rankings = map(lambda x : x[0].index(x[1]), orderings )
",classifications/att_classifiers.py,RemideZ/Stylometry,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,leggitta/mne-python,1
"            try:
                alphas = AlphaSelection(model())
            except YellowbrickTypeError:
                self.fail(""could not instantiate RegressorCV on alpha selection"")

    def test_only_regressors(self):
        """"""
        Assert AlphaSelection only works with regressors
        """"""
        with self.assertRaises(YellowbrickTypeError):
            model = AlphaSelection(SVC())

    def test_store_cv_values(self):
        """"""
        Assert that store_cv_values is true on RidgeCV
        """"""

        model = AlphaSelection(RidgeCV())
        self.assertTrue(model.estimator.store_cv_values)
",tests/test_regressor/test_alphas.py,pdamodaran/yellowbrick,1
"        step = max(min(0.99, step), 0.1)

        if num_features < 1:
            num_features = 1
        elif num_features > len(training_features.columns):
            num_features = len(training_features.columns)

        if len(training_features.columns.values) == 0:
            return input_df.copy()

        estimator = SVC(kernel='linear')
        selector = RFE(estimator, n_features_to_select=num_features, step=step)
        try:
            selector.fit(training_features, training_class_vals)
            mask = selector.get_support(True)
            mask_cols = list(training_features.iloc[:, mask].columns) + self.non_feature_columns
            return input_df[mask_cols].copy()
        except ValueError:
            return input_df[self.non_feature_columns].copy()
",tpot/tpot.py,bartleyn/tpot,1
"
pred={}
for c in range(len(ncomp)):
    pred['svm']=N.zeros((len(train_labels),len(svmparams)))
    pred['rbf']=N.zeros((len(train_labels),len(svmparams),len(rbfparams)))
    pred['lr']=N.zeros((len(train_labels),len(lrparams)))
    data=N.genfromtxt(os.path.join(melodic_dir,'datarun2_icarun2_%dcomp.txt'%ncomp[c]))

    for p in range(len(svmparams)):
        for train,test in skf:
            clf=LinearSVC(C=10**svmparams[p])
            clf.fit(data[train],labels[train])
            pred['svm'][test,p]=clf.predict(data[test])
            for r in range(len(rbfparams)):
                clf=SVC(C=10**svmparams[p],gamma=10**rbfparams[r])
                clf.fit(data[train],labels[train])
                pred['rbf'][test,p,r]=clf.predict(data[test])
                
            clf=LogisticRegression(C=10**lrparams[p],penalty='l2')
            clf.fit(data[train],labels[train])",openfmri_paper/5.1_estimate_parameters_from_run2.py,poldrack/openfmri,1
"
class SvmModel(ModelOutput):
    def run_model(self, data, targets, batch_size, epochs):

        #data = double_inverse_samples(data)
        #targets = double_inverse_samples(targets)

        # split the data up into multiple sets: training, testing
        train_data, test_data, train_target, test_target = train_test_split(data, targets, test_size=0.4, random_state=42)
        # create svm object using original one-vs-one (ovo)
        s_machine = svm.SVC(decision_function_shape='ovo')
        # http://stackoverflow.com/questions/34337093/why-am-i-getting-a-data-conversion-warning
        # fixed data to avoid compiler warning
        n = train_target.shape[0]
        y = train_target.reshape((n,))
        # fit the data int othe svm object
        s_machine.fit(train_data, y)
        # get score on training and test data
        train_score = str(s_machine.score(train_data, train_target))
",Dota_svm.py,MachineLearningTVEEP/DotaWinPredictions,1
"for i,e in enumerate(_all_data.keys()):
	Y += [i] * len(_all_data[e][:N]) #len(_all_data[e])

#[0] * 8 + [1] * 8

# figure number
fignum = 1

# fit the model
for kernel in ('linear', 'poly', 'rbf'):
    clf = svm.SVC(kernel=kernel, gamma=2)
    clf.fit(X, Y)

    # plot the line, the points, and the nearest vectors to the plane
    plt.figure(fignum, figsize=(4, 3))
    plt.clf()

    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
                facecolors='none', zorder=10)
    plt.scatter( map(lambda x:x[0], X)  , map(lambda x:x[1], X), c=Y, zorder=10)",src/figure_kernels.py,spacenut/vowel-svm,1
"
_armLength = 10;
_angle_err = 5
_n = 100

b = find_one_brick()
m_1 = Motor(b, PORT_B)
m_2 = Motor(b, PORT_C)
	
v = Vision()
clf = SVC(kernel='linear')

labels = np.zeros([_n,])
data = np.zeros([_n,2])

def main():	
	for i in range(10):
		print i , get_angle()

def get_angle():",4155/assignments/a7/a7.py,moriarty/csci-homework,1
"
    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM': make_pipeline(
        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM': make_pipeline(
        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4),
    'MultilayerPerceptron': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
        solver='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
        tol=1e-4, random_state=1),
    'MLP-adam': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,",python/bench_mnist.py,suresh/notes,1
"		# adding +self.b broadcasts the bias, adding it to each row, so the result is still of shape (rows,)
		self.hplaneproject = T.dot(input, self.W) + self.b
		
		# symbolic description of how to compute prediction as -1 or 1
		# the function sign() is not in Theano,
		# so I use (x>0)*2-1 using T.ge() which returns 1 when true and 0 when false
		self.y_pred = T.ge(self.hplaneproject, 0)*2 - 1
		
		
		# equivalent in scikit-learn:
		#self.scikitlearn_svc = svm.SVC(kernel='linear', C=C)
	
	
	def clearWeights(self):
		Wzeros, bzero = self.GetZeroWeights()
		self.W.set_value(Wzeros, borrow=True)
		self.b.set_value(bzero, borrow=True)
	
	def saveParams(self, filename):
		fout = file(filename,'wb')",Recognition/OCR/PythonOCR/ConvolutionalNeuralNet/Python/linear_svm_binary.py,UCSD-AUVSI/Heimdall,1
"#		i_uni2=file2.read()
		i_uni=i_uni[:-1]
#		i_uni2=i_uni2[:-1]
#		print i_uni,i_uni2
		os.rename(url+j,url+i_uni)

# balus code
def train_scikit_svm():
	# NOTE FROM SCIT KIT : All classifiers in scikit-learn do multiclass classification out-of-the-box. 
	# You don’t need to use the sklearn.multiclass module unless you want to experiment with different multiclass strategies.	
	clf = svm.SVC(decision_function_shape='ovo')
	SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)


  """"""

  SVC and NuSVC implement the “one-against-one” approach (Knerr et al., 1990) for multi- class classification. If n_class is the number of classes, then n_class * (n_class - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to aggregate the results of the “one-against-one” classifiers to a decision function of shape (n_samples, n_classes):",initial_temp.py,theidentity/lekha_OCR_1.0,1
"		C_values = [1, 2, 8, 32, 128, 512, 2048, 8192]
		gamma_values = [0.001,0.01,0.05,0.1,0.3,0.5,0.7]

		#C_values = [2]
		#gamma_values = [0.5]

		best_C = 0
		best_G = 0
		best_percentage = 0

		#clf = svm.SVC(C=2.0,gamma=0.5)

		C_range = np.logspace(-2, 10, num=13, base=2)
		gamma_range = np.logspace(-5, 1, num=7, base=10)
		param_grid = dict(gamma=gamma_range, C=C_range)
		cv = StratifiedShuffleSplit(trainingLabels, n_iter=3, test_size=0.11, random_state=42)
		grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
		grid.fit(trainingSet, trainingLabels)
		#C_range = np.logspace(-1, 1, num=2, base=2)
		#gamma_range = np.logspace(-1, 1, num=2, base=10)",myocollect/CrossValidate.py,JessMcintosh/EMG-classifier,1
"


sub_cortical_structures = [""BrStem"",""L_Accu"",""R_Accu"",""L_Amyg"",""R_Amyg"",""L_Caud"",
                         ""R_Caud"",""L_Hipp"",""R_Hipp"",""L_Pall"",""R_Pall"",""L_Puta"",
                         ""R_Puta"",""L_Thal"",""R_Thal""]

#sub_cortical_structures=[""L_Hipp"",""R_Hipp""]
#%%
## Feature Extraction
feature_linearSVC = svm.LinearSVC(penalty=""l1"", dual=False)
feature_RFECV = RFECV(feature_linearSVC, step=0.05, cv=10)
#feature_PCA=PCA(n_components=n_components)
#%%
svc = svm.SVC()
param_grid = dict(C=range(1,10,2),gamma=np.logspace(-6, 1, 10))

sites = pheno['site'].unique()
list_dfs = list()
for site in sites: ",different_pipelines.py,himalayajung/neuropy,1
"	print ""Vectorization completed!""
	#print (vectorizer.stop_words_)
	
	#testClassifiers(X_train, y_train)
	X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.33, random_state=42)
	print len(y_test)
	
	#Initialize a Random Forest classifier with 100 trees
	print ""Preparing training""
	#forest = RandomForestClassifier(n_estimators = 100)
	clf = svm.SVC()
	print ""Training in progress...""
	#forest = forest.fit(X_train.toarray(), y_train)
	clf.fit(X_train, y_train) 
	
	print ""Training completed!""
	
	print ""Preparing classification""
	result = clf.predict(X_test)
	print len(result)",classification/trainingAndTesting.py,pcomputo/webpage-classifier,1
"#deal with sample_weight
sample_weight = np.ones(y.shape[0])
classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))
 
###############################################################################
print(""PREPARE CLASSIFICATION"")
#-- classifier
if np.size(svm_C)>1:
    clf = GridSearchCV(svm.SVC(kernel='linear', probability=compute_probas),
    {'C': svm_C}, score_func=precision_score)
else:
    #clf = svm.SVC(kernel='linear', probability=True, C=svm_C)
    clf = svm.SVR(kernel='linear', C=svm_C)
 
#-- normalizer
scaler = StandardScaler()
 
#-- feature selection",JR_toolbox/skl_king_parallel_reg.py,kingjr/natmeg_arhus,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    concatenator = ConcatenateChannels()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,Odingod/mne-python,1
"    plot_2d(x, y, param=param)
    result_cv = cv(x, y, param=param)
    print result_cv.get_dict()
    print result_cv.mean_auc2


def get_algorithm(param=Default_param):
    """"""Get an algorithm for classification.""""""
    def _get_algorithm(name):
        if name == 'svm':
            clf = svm.SVC(random_state=0, kernel='linear', max_iter=param.svm_max_iter,
                          class_weight=param.class_weight)
        elif name == 'rsvm':
            clf = svm.SVC(random_state=0, kernel='rbf', max_iter=param.svm_max_iter,
                          class_weight=param.class_weight)
        elif name == 'psvm':
            clf = svm.SVC(random_state=0, kernel='poly', max_iter=param.svm_max_iter,
                          class_weight=param.class_weight)
        elif name == 'dt':
            clf = tree.DecisionTreeClassifier(random_state=0, class_weight=param.class_weight)",clinical_db/alg/classification.py,belemizz/mimic2_tools,1
"                           'max_iter': [500]}
        }

    if 'Nearest Neighbors' not in exclude:
        classifiers['Nearest Neighbors'] = {
            'clf': KNeighborsClassifier(),
            'parameters': {'n_neighbors': [1, 5, 10, 20]}}

    if 'SVM' not in exclude:
        classifiers['SVM'] = {
            'clf': SVC(C=1, probability=True, cache_size=10000,
                       class_weight='balanced'),
            'parameters': {'kernel': ['rbf', 'poly'],
                           'C': [0.01, 0.1, 1]}}

    if 'Linear SVM' not in exclude:
        classifiers['Linear SVM'] = {
            'clf': LinearSVC(dual=False, class_weight='balanced'),
            'parameters': {'C': [0.01, 0.1, 1],
                           'penalty': ['l1', 'l2']}}",polyssifier/poly_utils.py,alvarouc/polyssifier,1
"    estimators['ada boost '] = AdaBoostClassifier(tree.DecisionTreeClassifier(), algorithm=""SAMME"", n_estimators=400)
    estimators['forest_3000'] = RandomForestClassifier(n_estimators=3000)
    estimators['forest_2000'] = RandomForestClassifier(n_estimators=2000)
    estimators['forest_1000'] = RandomForestClassifier(n_estimators=1000)
    estimators['forest_900'] = RandomForestClassifier(n_estimators=900)
    estimators['forest_700'] = RandomForestClassifier(n_estimators=700)
    estimators['forest_500'] = RandomForestClassifier(n_estimators=500)
    estimators['forest_300'] = RandomForestClassifier(n_estimators=300)
    estimators['forest_100'] = RandomForestClassifier(n_estimators=100)
    estimators['forest_10'] = RandomForestClassifier(n_estimators=10)
    estimators['svm_c_rbf'] = svm.SVC()
    estimators['ada boost '] = AdaBoostClassifier(RandomForestClassifier(n_estimators=300), n_estimators=400)
    estimators['svm_c_sigmoid'] = svm.SVC(kernel='sigmoid') 
    estimators['svm_c_precomputed'] = svm.SVC(kernel='precomputed')
    estimators['svm_c_poly'] = svm.SVC(kernel='poly')
    estimators['svm_linear'] = svm.LinearSVC()
    estimators['svm_nusvc'] = svm.NuSVC()
    for k in estimators.keys():
        start_time = datetime.datetime.now()
        print '----%s----' % k",datamining_assignments/datamining_final/kaggle_homework.py,xuerenlv/PaperWork,1
"    :return: Three elements tuple respectively method used (String), best accuracy score on parameters grid in 5-folds 
    CV (float), accuracy score on test set
    """"""
    from sklearn import svm
    from sklearn.model_selection import GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    method = ""svm_linear""
    scaler = StandardScaler()
    scaled_feats_train = scaler.fit_transform(training_set_features)
    svr = svm.SVC(kernel='linear', random_state=10)
    parameters = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}
    clf = GridSearchCV(svr, parameters, cv=5, scoring='accuracy')
    clf.fit(scaled_feats_train, training_set_labels)
    scaled_feats_test = scaler.transform(testing_set_features)
    predicted_lab_test = clf.predict(scaled_feats_test)
    best_score = clf.best_score_
    test_score = accuracy_score(testing_set_labels, predicted_lab_test, normalize=True)
    return method, best_score, test_score
",problem3.py,swoldetsadick/MachineLearning,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_01_2014_server.py,magic2du/contact_matrix,1
"    grid_search.decision_function(X)
    grid_search.transform(X)

    # Test exception handling on scoring
    grid_search.scoring = 'sklearn'
    assert_raises(ValueError, grid_search.fit, X, y)


def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)
    grid_search = GridSearchCV(clf, {'C': Cs})
    grid_search.fit(X, y)

    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
                                        scoring='accuracy')
    # smoketest grid search",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/tests/test_grid_search.py,RPGOne/Skynet,1
"	np.save('trainX', trainX)
	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/arc_cosine.py,akhilpm/Masters-Project,1
"    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    'GBRT': GradientBoostingClassifier(n_estimators=250),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),
    'RandomForest': RandomForestClassifier(n_estimators=20),
    'CART': DecisionTreeClassifier(min_samples_split=5),
    'SGD': SGDClassifier(alpha=0.001, max_iter=1000, tol=1e-3),
    'GaussianNB': GaussianNB(),
    'liblinear': LinearSVC(loss=""l2"", penalty=""l2"", C=1000, dual=False,
                           tol=1e-3),
    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)
}


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('--classifiers', nargs=""+"",
                        choices=ESTIMATORS, type=str,",benchmarks/bench_covertype.py,kevin-coder/scikit-learn-fork,1
"
    print ""Training...""

    # commented out to delete .toarray() option, because len(data_X[0]) is not defined
    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.2.1.py,totuta/deep-supertagging,1
"
from sklearn.feature_selection import RFECV

sfk = cv.StratifiedKFold(labels, 10)
scores = []
for train, test in sfk:
    score = []
    train_set = feature[train]
    test_set = feature[test]
    clf = RFECV(
        LinearSVC(C=100),
        cv=cv.StratifiedKFold(labels[train], 10),
        scoring='f1')
    clf.fit(train_set, labels[train])
    pred = clf.predict(test_set)
    score.append(accuracy_score(labels[test], pred))
    score.append(precision_score(labels[test], pred))
    score.append(recall_score(labels[test], pred))
    score.append(f1_score(labels[test], pred))
    scores.append(score)",learning/RFE.py,fcchou/CS229-project,1
"        _attack_files_missing(wolves)
    print 'Loading attack samples from ""{}""'.format(wolves)
    malicious = utility.get_pdfs(wolves)
    if not malicious:
        _attack_files_missing(wolves)
    
    # Load an SVM trained with scaled data
    scaler = pickle.load(open(
                        config.get('datasets', 'contagio_scaler')))
    print 'Using scaler'
    svm = sklearn_SVC()
    print 'Loading model from ""{}""'.format(scenario['model'])
    svm.load_model(scenario['model'])
    
    # Load the training data used for kernel density estimation
    print 'Loading dataset from file ""{}""'.format(scenario['training'])
    X_train, y_train, _ = datasets.csv2numpy(scenario['training'])
    # Subsample for faster execution
    ind_sample = random.sample(range(len(y_train)), 500)
    X_train = X_train[ind_sample, :]",reproduction/common.py,srndic/mimicus,1
"perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
# sparsify
iris.data = sparse.csr_matrix(iris.data)


def test_svc():
    """"""Check that sparse SVC gives the same result as SVC""""""

    clf = svm.SVC(kernel='linear', probability=True).fit(X, Y)
    sp_clf = svm.SVC(kernel='linear', probability=True).fit(X_sp, Y)

    assert_array_equal(sp_clf.predict(T), true_result)

    assert_true(sparse.issparse(sp_clf.support_vectors_))
    assert_array_almost_equal(clf.support_vectors_,
            sp_clf.support_vectors_.todense())

    assert_true(sparse.issparse(sp_clf.dual_coef_))",python/sklearn/sklearn/svm/tests/test_sparse.py,seckcoder/lang-learn,1
"    classifierName = 'Support Vector Machine'
    C = np.logspace(-2, 10, 13)
    gamma = np.logspace(-9, 3, 13)

    def train(self):

        tuned_parameters = [{'kernel': ['rbf'], 'C': self.C, 'gamma':self.gamma}]

        print ('SVM Optimizing. This will take a while')
        start_time = time.time()
        clf = GridSearchCV(SVC(), tuned_parameters,
                           n_jobs=self.threadCount, cv=5)

        clf.fit(self.Xtrain, self.ytrain)
        print('Done with Optimizing. it took ', time.time() -
              start_time, ' seconds')

        self.model = clf.best_estimator_",infodens/classifier/svc_rbf.py,rrubino/B6-SFB1102,1
"    X = data[:, 1:]
    y = data[:, 0]
    y[y != x] = -1
    y[y >= 0] = 1
    return X, y


def q15():
    X, y = load_data('features.train', 0)
    from sklearn import svm
    clf = svm.LinearSVC(C=0.01, loss='l1', fit_intercept=False)
    clf.fit(X, y)
    print clf.coef_.dot(clf.coef_.T)


def q16():
    from sklearn import svm
    for i in range(1, 10, 2):
        X, y = load_data('features.train', i)
        clf = svm.SVC(C=0.01, kernel='poly', degree=2, gamma=1, coef0=1,",ntumltwo-001/hw1.py,hsinhuang/codebase,1
"
from imutils import paths
import numpy as np
import argparse
import imutils
import cv2
import os

data_path = ""DBIM/alldb""

model_pxl = CalibratedClassifierCV(svm.LinearSVC())
model_hst = CalibratedClassifierCV(svm.LinearSVC())


def image_to_feature_vector(image, size=(32, 32)):
	# resize the image to a fixed size, then flatten the image into
	# a list of raw pixel intensities
	return cv2.resize(image, size).flatten()

def extract_color_histogram(image, bins=(8, 8, 8)):",opencv/classifiers/svm.py,marcoscrcamargo/ic,1
"	print testX.shape

	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)

	#fit the svm model and compute accuaracy measure
	clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	#clf = svm.SVC(kernel='linear')
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithUVFS/mnistBackRandom/mnistRAND.py,akhilpm/Masters-Project,1
"            elif pre_process == ""PCA"":
                pca = PCA()
                (X, V) = fit_transform(pca, X)
                pfield = [pfield[0]] + ['PCA_%d' % (d + 1) for d in range(len(pfield[1:]))]
            elif pre_process == ""Whitened PCA"":
                pca = PCA(whiten=True)
                (X, V) = fit_transform(pca, X)
                pfield = [pfield[0]] + ['PCA_%d' % (d + 1) for d in range(len(pfield[1:]))]


            #clf = svm.SVC(C=regularizer, kernel='linear', probability=True, random_state=0,verbose=False)
            #clf.fit(X[1:TrainingSize], Y[1:TrainingSize])
            #yhat = clf.decision_function(X[TrainingSize:])
            #yhat = [yhat[i][0] for i in range(len(yhat))]

            clf = linear_model.Ridge(alpha=regularizer)
            clf.fit(X[1:TrainingSize], Y[1:TrainingSize])
            yhat = clf.decision_function(X[TrainingSize:])

            # summary results",modules/ml_ridge_linear.py,garthee/gnot,1
"import pylab as pl
from sklearn import svm

xx,yy = np.meshgrid(np.linspace(-3,3,500),np.linspace(-3,3,500))

np.random.seed(0)
X = np.random.randn(300,2)
Y = np.logical_xor(X[:,0] >0 ,X[:,1]>0)

#fit the model
clf = svm.NuSVC()
clf.fit(X,Y)

#plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(),yy.ravel()])
Z = Z.reshape(xx.shape)

pl.imshow(Z,interpolation='nearest',extent = (xx.min(),xx.max(),yy.min(),yy.max()),aspect='auto',origin='lower',cmap = pl.cm.PuOr_r)
contours = pl.contour(xx,yy,Z,levels=[0],linewidth=2,linetypes='--')
",1_supervised_classification/15-SVM/svm/plot_svm_nonlinear.py,PhenixI/machine-learning,1
"        elif args[i] == '-g':
            g = float(args[i+1])
            i += 2
        elif args[i] == '-d':
            d = float(args[i+1])
            i += 2
        else:
            raise Exception('invalid argument: ' + str(args[i]))

    X,Y = loadData(filename)
    f = SVC(C=c, kernel=k, degree=d, gamma=g, coef0=r)
    f.fit(X,Y)
    print >>sys.stderr, 'fit model:', f
    with open(model, 'w') as h:
        possv,negsv = 0,0
        if f.classes_[0] == -1:
            f.dual_coef_ *= -1
        for alpha in f.dual_coef_[0]:
            if alpha < 0: negsv += 1
            else: possv += 1",labs/lab6-svms/svm-train.py,barak/ciml,1
"    print 'tune SVM'

    pipeline = Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler())])
    scaler = pipeline.fit(X_train)
    rescaledX = scaler.transform(X_train)
    
    c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]
    kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']
    param_grid = dict(C=c_values, kernel=kernel_values)
    
    model = SVC()
    
    kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
    grid = GridSearchCV(n_jobs=N_JOBS, estimator=model, param_grid=param_grid, scoring=SCORING, cv=kfold)
    
    grid_result = grid.fit(rescaledX, Y_train)
    print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))    
        
    best_idx = grid_result.best_index_
",lib/eda3.py,FabricioMatos/ifes-dropout-machine-learning,1
"from masque.playground.runners import plain_classify, pretrain_classify


def _norm(X, y):
    """"""Scales data to [0..1] interval""""""
    X = X.astype(np.float32) / (X.max() - X.min())
    return X, y

rbm_svc = {
    'pretrain_model' : BernoulliRBM(n_components=1024, verbose=True),
    'model' : SVC(kernel='linear', verbose=True),
    'pretrain_data' : lambda: _norm(*datasets.ck_lm_series()),
    'data' : lambda: _norm(*datasets.ck_lm_series(labeled_only=True)),
}

# TODO: sample
dbn_svc = {
    'pretrain_model' : Pipeline([
        ('rbm0', BernoulliRBM(n_components=60, verbose=True)),
        ('rbm1', BernoulliRBM(n_components=80, verbose=True)),",masque/playground/lm_series.py,dfdx/masque,1
"y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1  
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),  
                     np.arange(y_min, y_max, h))  
  
''''' SVM '''  
# title for the plots  
titles = ['LinearSVC (linear kernel)',  
          'SVC with polynomial (degree 3) kernel',  
          'SVC with RBF kernel',  
          'SVC with Sigmoid kernel']  
clf_linear  = svm.SVC(kernel='linear').fit(x, y)  
#clf_linear  = svm.LinearSVC().fit(x, y)  
clf_poly    = svm.SVC(kernel='poly', degree=3).fit(x, y)  
clf_rbf     = svm.SVC().fit(x, y)  
clf_sigmoid = svm.SVC(kernel='sigmoid').fit(x, y)  
  
for i, clf in enumerate((clf_linear, clf_poly, clf_rbf, clf_sigmoid)):  
    answer = clf.predict(np.c_[xx.ravel(), yy.ravel()])  
    print(clf)  
    print(np.mean( answer == y_train))  ",ml/svm/run.py,maxis1314/pyutils,1
"    t_tc = time.clock()

    print ""Training...""

    X_dim = len(data_X[0])

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_count_stags.py,totuta/deep-supertagging,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_04_27_2015_parallel_for_final.py,magic2du/contact_matrix,1
"corpus = raw_data.speech

X = vectorizer.fit_transform(corpus)
X_tf_idf = transformer.fit_transform(X)


y = encoder.fit_transform(raw_data.party)


log_reg = LogisticRegression(penalty = ""l1"", verbose = 5)
svm_ = LinearSVC(penalty = 'l1', dual = False)
sgd_ = SGDClassifier(loss = ""hinge"", penalty = ""l1"", alpha = 0.0001)
svm_rbf = SVC()

cv = StratifiedKFold(y, n_folds = 6, random_state = 2)

accuracy_log_reg_list = []
accuracy_svm_list = []
accuracy_sgd_list = []
accuracy_svm_rbf_list = []",Code/supervised_learning.py,TimKreienkamp/TextMiningProject,1
"from sklearn.preprocessing import StandardScaler

FS = StandardScaler().fit_transform(FS)

from sklearn.svm import LinearSVC
from sklearn.cross_validation import cross_val_score
from sklearn.cross_validation import StratifiedKFold

cv = StratifiedKFold(aut_target, n_folds=5)

foldacc = cross_val_score(LinearSVC(),X=FS,y=aut_target,cv=cv,n_jobs=1)
acc = np.mean(foldacc)
print(acc)


# Also look at first 2 PCs of detrend coefs
""""""
acquisition_lengths = [len(mov) for mov in bunch.movement]
min_acq_length = np.min(acquisition_lengths)
from features import trend_coef",nilearn_private/simple_plot_movements.py,AlexandreAbraham/movements,1
"# Extract the hog features
list_hog_fd = []
for feature in features:
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier",projects/sklearn/downloaded/digitRecognition/generateClassifier_orig.py,bensinghbeno/design-engine,1
"        random_state = int(kwparams[""random_state""])
    else:
        random_state = None

    # Separating target from inputs
    X, y = design_matrix(train_filename=train_filename)

    print ""Training Support Vector Machine Classifier...""

    # Initializing SVM classifier
    clf = svm.SVC(probability=True,
                  C=C, kernel=kernel, degree=degree, gamma=gamma,
                  coef0=coef0, shrinking=shrinking, tol=tol, cache_size=cache_size,
                  class_weight=class_weight, max_iter=max_iter, random_state=random_state)

    # Fitting LR classifier
    clf.fit(X, y)

    # Pickle and save
    f = open(param_filename, 'wb')",scikit_svm/train.py,broadinstitute/ebola-predictor,1
"
#---------------------------------------------------------------
#very ugly way to bring vectors to the right shape for SVC fit()
a = []
for x in resize_set:
    a.append(x.tolist())
#----------------------------------------------------------------
X = a               #reshaped images (training)
y = resize_labels   #labels

clf = svm.SVC(gamma=1.0)  #load SVC
clf.fit(X, y)               #fit SVC

#-------------------------------------------------------------------
#very ugly way to bring vectors to the right shape for SVC predict()
a = []
for x in resize_test_set:
    a.append(x.tolist())
#-------------------------------------------------------------------    
",Exercises/06_SVM.py,peterwittek/qml-rg,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_08_2015_01.py,magic2du/contact_matrix,1
"from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def svm_linear(dataset, out):
    print('svm_linear')
    X = dataset[['x', 'y']]
    y = dataset.label
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
    tuned_parameters = [{'kernel': ['linear'], 'C': [0.1, 0.5, 1, 5, 10, 50, 100]}]
    
    clf = GridSearchCV(SVC(), tuned_parameters, cv=5)
    clf.fit(X_train, y_train)
    best_param = clf.best_params_
    print('best param: ' + str(best_param))
#     means = clf.cv_results_['mean_test_score']
#     stds = clf.cv_results_['std_test_score']
#     for mean, std, params in zip(means, stds, clf.cv_results_['params']):
#         print(""%0.3f (+/-%0.03f) for %r""
#               % (mean, std * 2, params))
    best_score = clf.best_score_",src/classification_sklearn/problem3_3.py,lucafon/ArtificialIntelligence,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/constraints_dses_dsessvc/setup.py,jpzk/evopy,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_04072015_2014_server.py,magic2du/contact_matrix,1
"        y = np.asarray(self.data)[:,2]
        if np.unique(y).size == 1:
            # only one-class
            self.clf = OneClassSVM(nu=params['nu'],
                             gamma=params['gamma'],
                             degree=params['degree'],
                             coef0=params['coef0'],
                             kernel=params['kernel'])
            self.clf.fit(X)
        else:
            self.clf = SVC(C=params['C'],
                           gamma=params['gamma'],
                           degree=params['degree'],
                           coef0=params['coef0'],
                           kernel=params['kernel'])
            self.clf.fit(X, y)
        self.is_fitted = True
        self.changed(""model_fitted"")

    def changed(self, status):",examples/svm_demo.py,feuerchop/increOCSVM,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,kingjr/mne-python-i,1
"				Xcv,Ycv = self.DBobj.GetXY(listchoice,1)
				Xtrain = []
				Ytrain = []
				for m in range(0,NumTraining,step) :
					self.verifygo()
					Xs,Ys = self.DBobj.GetXY(listchoice,0,step,m)
					Xtrain.extend(Xs)
					Ytrain.extend(Ys)
					for cost in Cs :
						self.verifygo()
						clf = SVC(C=cost,kernel='linear')
						clf.fit(Xtrain,Ytrain)
						self.verifygo()
						TrainScore = clf.score(Xtrain,Ytrain)
						CVScore = clf.score(Xcv,Ycv)
						self.PlotDataQ.put((clf,m+step,cost,TrainScore,CVScore))
						logging.debug('%d, %d, %f, %f, %f'%(jj, m+step, cost, TrainScore, CVScore))
						jj += 1
			except Exception as detail :
				logging.error(""Did not complete all the assigned SVM training: %s""%detail)",TrainSVMs.py,joecole889/spam-filter,1
"#print type(X)
#print type(Y)
#print Y
#print X

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))",kinect/pySVM/test/testKernelSVM.py,hackliff/domobot,1
"
    datasets = ['data/set4.out', 'data/set5.out']
    savefiles = ['data/clfset4.pickle', 'data/clfset5.pickle']

    for i, ds in enumerate(datasets):
        X, Y = get_features(ds)
        print(""Classifying dataset {} with training set size {}."".format(ds, trainsize))
        Xtrain, Xtest, Ytrain, Ytest = train_test_split(
            X, Y, test_size=testsize, train_size=trainsize, random_state=13)

        clf = LinearSVC()
        clf.fit(Xtrain,Ytrain)

        pred_test = [clf.predict([x]) for x in Xtest]
        acc_test = accuracy_score(Ytest, pred_test)

        pred_train = [clf.predict([x]) for x in Xtrain]
        acc_train = accuracy_score(Ytrain, pred_train)

        cm = confusion_matrix(Ytest, pred_test)",results.py,tjkemp/image-tractor,1
"	y_2d = y[y < 2]
	# Test part
	T_2d = np.delete(T,range(25,43)+range(50,64)+range(75,93),axis=0)
	yy_2d = yy[yy < 2]
	#------------------------------ Standardize data ------------
	scaler = StandardScaler()
	X_2d = scaler.fit_transform(X_2d)
	T_2d_scaled = scaler.transform(T_2d)
	#------------------------------ Create Classifier ------------------
	manual_param = {'C':100,'gamma':0.1}
	clf = SVC(gamma=manual_param['gamma'], C=manual_param['C'])
	clf.fit(X_2d, y_2d)
	#------------------------------ Create Classifier ------------------
	C_range = np.logspace(1, 3, 3)
	gamma_range = np.logspace(-3, -1, 3)
	param_grid = dict(gamma=gamma_range, C=C_range)
	cv = StratifiedShuffleSplit(y_2d, n_iter=100, test_size=0.2, random_state=42)
	grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
	#------------------------------ Best parameter Aprox.---------------
	grid.fit(X_2d, y_2d)",EpocArmData/Features/feat_extraction_original.py,maberyick/RPi-EPOC,1
"    @param a_neg_re - regular expression for matching negative terms

    @return list of terms sorted according to their polarity scores

    """"""
    a_pos = set(normalize(w) for w in a_pos)
    a_neg = set(normalize(w) for w in a_neg)

    vectorizer = DictVectorizer()
    # model for distinguishing between the subjective and objective classes
    so_clf = LinearSVC(C=0.3)
    so_model = Pipeline([(""vectorizer"", vectorizer),
                         (""LinearSVC"", so_clf)])
    # model for distinguishing between the positive and negative classes
    pn_clf = LinearSVC(C=0.3)
    pn_model = Pipeline([(""vectorizer"", vectorizer),
                         (""LinearSVC"", pn_clf)])
    # generate general training sets
    x_so = []
    y_so = []",scripts/severyn.py,WladimirSidorenko/SentiLex,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_08_2015_02.py,magic2du/contact_matrix,1
"
    def predict(self, xhat, yhat):
        pred = self.model.predict(xhat)
        print 'Precision, recall, F1:'
        for f in [precision_score, recall_score, f1_score]:
            print f(yhat, pred),
        print

    def run_model_tests(self, models=None):
        if not models:
            models = [LogisticRegression(), LogisticRegressionCV(), SVC(), LinearSVC(), RandomForestClassifier()]
        x, y, xhat, yhat = self.load_data()

        for model in models:
            print model
            print 'Training...'
            self.train_model(x, y, model=model)
            self.predict(xhat, yhat)",python/document_predictor.py,kiankd/events,1
"        features = range(KfoldDataSet[0][0])
    model = RandomForestClassifier(n_estimators=300, n_jobs=-1, min_samples_split=10, random_state=1,
                                   class_weight='auto')
    RFcrossValidationTest = None
    if toTestModel:
        RFcrossValidationTest = KappaOnCrossValidation(model, KfoldDataSet, features)
    rf_final_predictions = ActivateModelAndformatOutput(model, train, test, features)
    return RFcrossValidationTest, rf_final_predictions


def trainSVC(train, test, KfoldDataSet, features=None):
    if features is None:
        features = range(KfoldDataSet[0][0])
    scl = StandardScaler()
    svm_model = SVC(random_state=1, class_weight={1: 2, 2: 1.5, 3: 1, 4: 1})
    model = Pipeline([('scl', scl), ('svm', svm_model)])
    svcCrossValidationTest = None
    if toTestModel:
        svcCrossValidationTest = KappaOnCrossValidation(model, KfoldDataSet, features)
    svc_final_predictions = ActivateModelAndformatOutput(model, train, test, features)",modelCreation.py,Ilya-Simkin/NLP-crowdflower-assignment,1
"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB

import collections

import asl

training_data, test_data, training_target, test_target = train_test_split(asl.data, asl.target, test_size = 0.5, random_state = 0)

classifiers = {
		'SVCP': svm.SVC(gamma = 0.001, C = 10),
		'SVCR': svm.SVC(gamma = 0.0001, C = 50),
		'NB ': GaussianNB(),
		'BNB': BernoulliNB(),
		'NBU': neighbors.KNeighborsClassifier(5, weights = 'uniform'),
		'NBD': neighbors.KNeighborsClassifier(5, weights = 'distance'),
		'TRE': tree.DecisionTreeClassifier(),
		'GBC': GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 1, random_state = 0),
		'RFC': RandomForestClassifier()
	}",score_classifiers.py,paolo-torres/Sign-Language-Translator,1
"            assert p.shape == (len(X), 2)
        # checking that last iteration coincides with previous
        assert numpy.all(p == proba)


def test_folding():
    # base_ada = SklearnClassifier(AdaBoostClassifier())
    # folding_str = FoldingClassifier(base_ada, n_folds=2)
    # check_folding(folding_str, True, False, False)

    base_ada = SklearnClassifier(SVC())
    folding_str = FoldingClassifier(base_ada, n_folds=4)",tests/test_folding.py,nickcdryan/rep,1
"from sklearn import svm
from sklearn.externals import joblib

iris = datasets.load_iris()
digits = datasets.load_digits()
print(digits.data)

digits.target
digits.images[0]

clf = svm.SVC(gamma=0.001, C=100.)

clf.fit(digits.data[:-1], digits.target[:-1])  
clf.predict(digits.data[-1])

clf = svm.SVC()
iris = datasets.load_iris()
X, y = iris.data, iris.target
clf.fit(X, y)
",text/src/main/python/sklearn-spl.py,echalkpad/t4f-data,1
"import lemmatizer

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn import tree, svm
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

classifiers = {
				'rf': RandomForestClassifier(n_estimators = 150, min_samples_split = 2, n_jobs = -1),
				'dt': tree.DecisionTreeClassifier(), 
				'svm': svm.SVC(),
				'part': ExtraTreesClassifier(n_estimators = 30, min_samples_split = 2, n_jobs = -1),
				'featureselection': Pipeline([
					('feature_selection', LinearSVC(penalty=""l1"", dual = False)),
					('classification', RandomForestClassifier())
					])
			}

def LoadPronouns(filename):
	pronouns = {}",anaphoramllib.py,max-ionov/russian-anaphora,1
"import matplotlib.pyplot as plt

from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.learning_curve import validation_curve


cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]
training_scores, test_scores = validation_curve(LinearSVC(), X, y,
                                                param_name=""C"", param_range=cs)
plt.figure()
plot_validation_curve(range(7), training_scores, test_scores)


ks = range(1, 10)
training_scores, test_scores = validation_curve(KNeighborsClassifier(), X, y,
                                                param_name=""n_neighbors"", param_range=ks)
plt.figure()",day3-machine-learning/solutions/validation_curve.py,parejkoj/AstroHackWeek2015,1
"# LSA / SVD
svd = TruncatedSVD(n_components = 140)
X = svd.fit_transform(X)
X_test = svd.transform(X_test)

# Scaling the data is important prior to SVM
scl = StandardScaler()
X = scl.fit_transform(X)
X_test = scl.transform(X_test)

model = SVC(C=10.0)


# Fit SVM Model
model.fit(X, y)
preds = model.predict(X_test)

# Create your first submission file
submission = pd.DataFrame({""id"": idx, ""prediction"": preds})
submission.to_csv(""beating_the_benchmark_yet_again.csv"", index=False)",code/BenchmarkBeat2.py,jasonwbw/tmp_game,1
"	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(100):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_5_periods{1}D_sample_{0}.txt"".format(i,dim),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_5_periods{1}D_sample_1{0}.txt"".format(str(i).zfill(2),dim)))

	#originally adaboost learning_rate=0.01, n_estimators=983
        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=942)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        args=[str(dim)+ ""Dsin1diff_5_and_5_noCPV_optimised_bdt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/bdt_sin/bdt_Sin_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"
train_x = vectorizer.fit_transform(train['filename'])
train_y = train['target']

test_x = vectorizer.transform(test['filename'])
test_y = test['target']


# Use SVM as classifier

clf = LinearSVC()


# tf-idf unsupervised term weighting

transformer = TfidfTransformer()

train_x_t = transformer.fit_transform(train_x,train_y)
test_x_t  = transformer.transform(test_x)
",train.py,aysent/supervised-term-weighting,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                      gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",sandbox/plotting/svm_gui.py,MathYourLife/TSatPy-thesis,1
"        print ""In dir: "" + cur_dir
        cx, cy = calc_flow_dir(cur_dir)
        X += cx
        Y += cy

    return X, Y


def train_svm(root):
    trainX, trainY = collect_data(root)
    clf = svm.SVC(kernel=""rbf"", C=100.0)
    clf.fit(trainX, trainY)
    return clf


def classify(s, i1, i2):
    vec, have_flow = run_flow(i1, i2)
    if (have_flow):
        return s.predict([vec])[0]
",speed/speedTrain.py,quake0day/Dragonite,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MRMR.mrmr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_MRMR.py,jundongl/PyFeaST,1
"from sklearn.svm import SVC


def find_optimal_C_for_AUC(xTrain, yTrain, xTest, yTest):
    C_2d_range = [10.0 ** i for i in range(-3, 3)]

    accuracy = np.array([])
    auc_score = np.array([])

    for Ctry in C_2d_range:
        clf = SVC(C=Ctry, kernel=""linear"", probability=True)
        clf.fit(xTrain, yTrain)
        pred = clf.predict(xTest)
        pred_proba = clf.predict_proba(xTest)
        accuracy = np.append(accuracy, np.average(yTest == pred))
        auc_score = np.append(auc_score,
                              roc_auc_score(yTest, pred_proba[:, 1]))
        print ""C: {}"" .format(Ctry)
        print ""accuracy: {}"" .format(accuracy[-1])
        print ""AUC: {}"" .format(auc_score[-1])",convnet.py,ping133/dm-challenge,1
"
#Splitting the data into train and test set 

train_data = data[:TRAIN_SIZE]
train_labels = requester_pizza_status[:TRAIN_SIZE]
test_data = data[TRAIN_SIZE:]
test_labels = requester_pizza_status[TRAIN_SIZE:] 

#Initializing Classifiers

svm = SVC(kernel='rbf')
mnb = MultinomialNB()
rf = RandomForestClassifier(n_estimators=51)
ada = AdaBoostClassifier(n_estimators=100)

classifiers = [svm,mnb,rf,ada]
classifier_names = [""SVM"",""Multinomial NB"",""Random Forest"",""AdaBoost""]

for classifier,classifier_name in zip(classifiers,classifier_names):
    classifier.fit(train_data,train_labels)",PizzaPOSModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"    c.extend([i for i in crange])
    crange=frange(0.00007,7,10)
    c.extend([i for i in crange])
    crange=frange(0.00009,10,10)
    c.extend([i for i in crange])
    c.sort() #Cost parameter values; use a bigger search space for better performance
    
    cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0).split(x_train, y_train)
    # ids = readfeats('../data/election/output/id_train') # only for election data
    # cv = GroupKFold(n_splits=5).split(x_train, y_train, ids)
    clf = svm.LinearSVC()
    param_grid = [{'C': c, 'class_weight': ['balanced']}]

    twoclass_f1_macro = metrics.make_scorer(twoclass_fscore, greater_is_better=True)
    precision_macro = metrics.make_scorer(macro_averaged_precision, greater_is_better=True)
    grid_search = GridSearchCV(clf, param_grid=param_grid, cv=cv, verbose=0, scoring='accuracy')
    
    grid_search.fit(x_train, y_train)
    print(""Best parameters set:"")
    print '\n'",src/sklearnSVM.py,bluemonk482/tdparse,1
"                delete=False)

            self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # this is very slow.
            self._model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=False,
                probability=True)

            print('Training support vector machine from input data file{}...'.format(",coursera/nlpintro-001/Assignment1/code/providedcode/transitionparser.py,Alexoner/mooc,1
"    >>> from epac.stores import replace_values
    >>> from epac.stores import func_is_big_nparray
    >>> from epac.stores import TagObject
    >>>
    >>> conf.MEMM_THRESHOLD = 100
    >>> npdata1 = np.random.random(size=(2, 2))
    >>> npdata2 = np.random.random(size=(100, 5))
    >>>
    >>> store = StoreMem()
    >>>
    >>> r1 = Result('SVC(C=1)', a=npdata1, b=npdata2)
    >>> r2 = Result('SVC(C=2)', a=npdata2, b=npdata1)
    >>> set1 = ResultSet(r1, r2)
    >>> store.save('SVC', set1)
    >>> replaced_array, ext_obj, is_modified = extract_values(store, func_is_big_nparray)
    >>> isinstance(ext_obj.dict['SVC']['SVC(C=2)']['a'], TagObject)
    True
    >>> new_obj, _ = replace_values(ext_obj, replaced_array)
    >>> isinstance(new_obj.dict['SVC']['SVC(C=2)']['a'], TagObject)
    False",epac/stores.py,neurospin/pylearn-epac,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 5, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[5.0, 5.0]]),
        sigma = 1.0,
        beta = 0.9,
        meta_model = meta_model) 

    return method
",evopy/examples/problems/TR/CMAESSVC.py,jpzk/evopy,1
"    labels = pd.read_csv(fname, header=None).as_matrix()[:, 1]
    labels = map(itemgetter(1),
                 map(os.path.split,
                     map(os.path.dirname, labels)))  # Get the directory.
    fname = ""{}/reps.csv"".format(workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)
    labelsNum = le.transform(labels)
    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))
    clf = SVC(C=1, kernel='linear', probability=True)
    clf.fit(embeddings, labelsNum)
    fName = ""{}/classifier.pkl"".format(workDir)
    print(""Saving classifier to '{}'"".format(fName))
    with open(fName, 'w') as f:
        pickle.dump((le, clf), f)
        
#train(""./generated-embeddings"")",training.py,diningphilosophers5/Attendance-System-II,1
"y_test = labels_test


names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""SGDClassifier"",
         #""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""MLPClassifier"", ""AdaBoost"",
         ""Naive Bayes""]

classifiers = [
    KNeighborsClassifier(59),
    LinearSVC(),
    SVC(gamma=2, C=1),
    SGDClassifier(loss=""log"", n_iter=10),
    #GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=15),
    RandomForestClassifier(n_estimators=100, max_features='sqrt'),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(learning_rate=0.1),
    GaussianNB()]
",src/py/classifiers_experiment.py,samleoqh/machine-ln,1
"        this GridSearch instance after fitting.

    verbose: integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
        estimator=SVC(C=1.0, cache_size=..., coef0=..., degree=...,
            gamma=..., kernel='rbf', probability=False,
            shrinking=True, tol=...),
        fit_params={}, iid=True, loss_func=None, n_jobs=1,
            param_grid=...,",venv/lib/python2.7/site-packages/sklearn/grid_search.py,devs1991/test_edx_docmode,1
"	#
	# Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating 
	# hyperplane, even if that hyperplane misclassifies more points. 
	#
	# For very tiny values of C, you should get misclassified examples, 
	# often even if your training data is linearly separable.

	#TODO - try different values for C see what happens
	#TODO - try a differnet kernel and see if the results are different. Check out the link above
	C = 1.0
	svc = svm.SVC(kernel='linear', C=C, verbose=False).fit(trainingData, trainingLable)
	print 'Type of the support vector used: \n', type(svc)
	print 'Support Vector Classification: \n', svc

	# Extract the evaluation data into numpy arrays so that we can use them for the machine learning algorithms.
	evaluationData = extractDataAsNumPyArray(evaluationSet)
	evaluationLable = extractLabelAsNumPyArray(trainingSet)

	# Get the predicition
	predictedLable = svc.predict(evaluationData)",svm/spamfilter/spamFilterLab.py,r2m/machine-learning,1
"    # TASK: Build a vectorizer / classifier pipeline that filters out tokens
    # that are too rare or too frequent
    
    vect = TfidfVectorizer(stop_words='english')
    
    # TASK: Build a grid search to find out whether unigrams or bigrams are
    # more useful.
    # Fit the pipeline on the training set using grid search for the parameters
    
    clf = Pipeline([('vect', vect),
                    ('clf',LinearSVC())
                   ]) 
    
    parameters = {'vect__ngram_range': [(1, 1), (1, 2)]
                 }

    
    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)
    
    gs_clf = gs_clf.fit(docs_train, y_train)",machine_learning/movie_reviews_example.py,chrisjdavie/shares,1
"        # check pandas standard transform works

        df = pd.DataFrame({'A': ['A', 'B', 'A', 'A', 'A', 'B', 'B', 'B'],
                           'B': np.random.randn(8),
                           'C': np.random.randn(8)})

        mdf = pdml.ModelFrame(df)
        self.assert_frame_equal(df.groupby('A').transform('mean'),
                                mdf.groupby('A').transform('mean'))

    def test_grouped_estimator_SVC(self):
        df = pdml.ModelFrame(datasets.load_iris())
        df['sepal length (cm)'] = df['sepal length (cm)'].pp.binarize(threshold=5.8)
        grouped = df.groupby('sepal length (cm)')
        self.assertIsInstance(grouped, pdml.core.groupby.ModelFrameGroupBy)
        for name, group in grouped:
            self.assertIsInstance(group, pdml.ModelFrame)
            self.assertEqual(group.target_name, '.target')
            self.assertTrue(group.has_target())
            self.assert_index_equal(group.columns, df.columns)",pandas_ml/test/test_groupby.py,sinhrks/pandas-ml,1
"
#random generated data
positive = np.transpose(np.random.multivariate_normal(mean1,cov1,n_p).T)
negative = np.transpose(np.random.multivariate_normal(mean2,cov2,n_n).T)

data = np.vstack((positive,negative))
label = [1]*n_p + [-1]*n_n


# run classification
clf = svm.NuSVC()
clf.fit(data, label)

#xy grid of points to be evaluated with classifier
xx, yy = np.meshgrid(np.linspace(-10, 10, 200),  np.linspace(-10, 10, 200))

#calculate ""distances"" to hyperplane
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
",SVMClassification.py,lcreyes/PULearningSelfTutorial,1
"                                  ""my_data_processed/acceleration_train_x.csv"",
                                  ""my_data_processed/acceleration_train_y.csv"",
                                  ""my_data_processed/acceleration_train_z.csv"",
                                  ""my_data_processed/gyroscope_train_x.csv"",
                                  ""my_data_processed/gyroscope_train_y.csv"",
                                  ""my_data_processed/gyroscope_train_z.csv"")

data = ndarray(shape=(len(data), 39), dtype=float, buffer=np.asanyarray(data))
target = ndarray(shape=(len(target),), dtype=int, buffer=np.asanyarray(target))

clf = svm.SVC()
clf.fit(data, target)

target, data = load_data_enhanced(""my_data_processed/activity_test.csv"",
                                  ""my_data_processed/acceleration_test_x.csv"",
                                  ""my_data_processed/acceleration_test_y.csv"",
                                  ""my_data_processed/acceleration_test_z.csv"",
                                  ""my_data_processed/gyroscope_test_x.csv"",
                                  ""my_data_processed/gyroscope_test_y.csv"",
                                  ""my_data_processed/gyroscope_test_z.csv"")",Activity_and_Context_Recognition/Classifier/test_dataset_enhanced.py,VizLoreLabs/LCI-FIC2-SE,1
"            #tmp = HOGFeatures(x)
            tmp = LBPUMultiBlockAndHOGFeatures(x)
            feat = tmp.getFeatures()
            myfeat_test.append(feat)
            mylabel_test.append(i)

        print ""Features obtained for test class"", i

    # Train
    # Create a classifier: a support vector classifier
    svml = LinearSVC()
    rf = RandomForestClassifier()
    gnb = GaussianNB()
    tr = tree.DecisionTreeClassifier()
    dummy = DummyClassifier()

    print ""Training ...""
    # Train
    # Compute traing time
    ttime = []",handwriting_recognition/main.py,eusebioaguilera/cvsamples,1
"
  #C=[0.001,0.01,0,1,1,10,100,1000,10000,100000]
  #gamma=[1.0e-7,1.0e-6,1.0e-5,1.0e-4,0.001,0.01,0.1,1,10,100]

  C=[100000,1000000,10000000]
  gamma=[1.0e-5, 1e-6]

  tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

  print(""# Tuning hyper-parameters for accuracy"")
  clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='accuracy')
  clf.fit(X_train, y_train)

  print ""Best parameters set found on development set:""
  print
  print clf.best_estimator_
  print
  print ""Grid scores on development set:""
  print
  for params, mean_score, scores in clf.grid_scores_:",project/t2.py,n7jti/machine_learning,1
"    # tempX = np.column_stack((averages[1:],
    #                      rs[1:], amts[1:], cpi_ppi, fai_inverse, m2_m1))
    X = np.hstack((tempX, techs))
    y = upOrDowns[2:]  # 涨跌数组向后移一位,表当前周数据预测下一周涨跌
    y.append(upOrDowns[-1])  # 涨跌数组最后一位按前一位数据补上
    return X, y, actionDates[1:]


def optimizeSVM(X_norm, y, kFolds=10):
    clf = pipeline.Pipeline([
        ('svc', svm.SVC(kernel='rbf')),
    ])
    # grid search 多参数优化
    parameters = {
        'svc__gamma': np.logspace(0, 3, 20),
        'svc__C': np.logspace(0, 3, 10),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds, scoring='accuracy')
    gs.fit(X_norm, y)
    return gs.best_params_['svc__gamma'], gs.best_params_['svc__C'], gs.best_score_",finance/MonthTest/MonthDataPrepare.py,Ernestyj/PyStudy,1
"# 0.75 window=10, label_after=10
# 0.45  window=5, label_after=100
regr = linear_model.OrthogonalMatchingPursuit()
#regr = linear_model.Lars()  # 0.0 window=10, label_after=10
#regr = linear_model.Lasso()  # 0.60 window=10, label_after=10
#regr = linear_model.Ridge()  # 0.74 window=10, label_after=10
#regr = linear_model.LinearRegression()  # 0.74 window=10, label_after=10
#regr = linear_model.LogisticRegression()  # 0.09 window=10, label_after=10

# from sklearn import svm
# regr = svm.SVC()  # 0.09 window=10, label_after=10

# tree.DecisionTreeClassifier
# 0.64 window=10, label_after=10
# 0.62 window=10, label_after=100
# 0.26 window=100, label_after=100
# 0.73 window=5, label_after=100
#from sklearn import tree
#regr = tree.DecisionTreeClassifier()
",regression.py,javierarilos/market_session,1
"print(""%d categories"" % len(categories))
print()

print(""n_samples: %d, n_features: %d"" % X_train.shape)
print()


print('=' * 80)
print(""L2 penalty"")
# Train Liblinear model
# clf = benchmark(LinearSVC(loss='l2', penalty=""l2"",dual=False, tol=1e-3))

svr_lin = benchmark(SVR(kernel='linear'))

save_obj(svr_lin,""BaggedSVRClassifierModel"")",Project/BaggedClassifierTrainer.py,tpsatish95/Youtube-Comedy-Comparison,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = bongo.external.email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",src/libs/python/bongo/external/email/test/test_email.py,bongo-project/bongo,1
"def test_base_estimator():
    """"""Test different base estimators.""""""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)
    clf.fit(X, y_regr)

    clf = AdaBoostRegressor(SVR(), random_state=0)",BCI_Framework/test_weight_boosting.py,lol/BCI-BO-old,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",tools/project-creator/Python2.6.6/Lib/email/test/test_email.py,babyliynfg/cross,1
"    #del x
    #del y    
    listModel = []
    nest = [10,20,30]
    mfea = [30,70,100]
    mdep = [3,5,8,10]
    for i in nest:
        for j in mfea:
            for k in mdep:
                listModel.append(RandomForestRegressor(n_estimators=i, bootstrap=False, max_depth=k, max_features=j))
    # GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=10, random_state=0),  svm.SVC()
    res =[]
    start_time = time.time()
    i=0
    for model in listModel:
        i=i+1
        #start_time = time.time()
        print(i)
        model.fit(xTrain, yTrain)
        #model.score(xTrain, yTrain)",fonction_py/robin.py,LaRiffle/axa_challenge,1
"        y = data[target].as_matrix()

        C = 1
        gamma = 0

        if kernel_type == 'rbf':

            if auto_detect:

                grid_search = GridSearchCV(
                    estimator=SVC(),
                    param_grid=self._get_param_grid(kernel_type),
                    scoring=perf_measure,
                    cv=n_grid_folds,
                    refit=True)
                grid_search.fit(X, y)
                C = grid_search.best_params_['C']
                gamma = grid_search.best_params_['gamma']

            else:",pyminer/network/classifiers.py,rbrecheisen/pyminer,1
"# Reduce the problem size so that the plotting finishes in this eon.
X = [datum[2:4] for i,datum in enumerate(_data_values) if i % 100 == 0]
y = [key for i,key in enumerate(_data_keys) if i % 100 == 0]

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
print ""Charging linear kernel SVC...""
svc = svm.SVC(kernel='linear', C=C).fit(X, y)
print ""Charging rbf SVC...""
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
print ""Charging poly SVC...""
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
print ""Charging linear SVC...""
lin_svc = svm.LinearSVC(C=C).fit(X, y)
print ""Done.""

x0s = [x[0] for x in X]",src/figure_classes.py,spacenut/vowel-svm,1
"                               n_repeated=0, \
                               n_classes=8, \
                               n_clusters_per_class=1, \
                               random_state=0)
    return X, y



def createRFECV(y):
    # create the RFE objects and compute a cross-validated score
    svc = SVC(kernel='linear')
    rfecv = RFECV(estimator=svc, \
                  step=1, \
                  cv=StratifiedKFold(y, 2), \
                  scoring='accuracy')
    return rfecv

def predict(rfecv, X, y):
    return rfecv.fit(X, y)
",examples/scikit-learn/examples/general/recursive_feature_elimination_with_cross-validation.py,KellyChan/Python,1
"        self.is_fitted = False
        self.changed(""sample_cleared"")

    def fit(self, params):
        if len(self.data) <= 1:
            self.DEBUG_INFO = ""No samples are there!""
            self.changed(""alert_generated"")
            return
        X = np.asarray(self.data)[:, 0:2]
        y = np.asarray(self.data)[:, 2]
        self.clf = iSVC(C=params['C'],
                        gamma=params['gamma'],
                        degree=params['degree'],
                        coef0=params['coef0'],
                        kernel=params['kernel'])
        self.clf.fit(X,y)
        self.is_fitted = True
        self.changed(""model_fitted"")

    def changed(self, status):",Python/isvc_gui.py,feuerchop/IndicativeSVC,1
"SVM = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42)

SVM_scores = cross_validation.cross_val_score(SVM, texts_tfidf, labels, cv=5)
print SVM_scores


# GNB = GaussianNB()
# GNB_scores = cross_validation.cross_val_score(GNB, texts_tfidf.toarray(), labels, cv=5)
# print GNB_scores

SVM_Linear = svm.LinearSVC()
SVM_Linear_scores = cross_validation.cross_val_score(SVM_Linear, texts_tfidf, labels, cv=5)
print SVM_Linear_scores


# pickle.dump(SVM,open('SVM.p','wb'))


# SVM_rbf = svm.SVC(decision_function_shape='ovr', kernel = 'rbf')
# SVM_rbf_scores = cross_validation.cross_val_score(SVM_rbf, texts_tfidf, labels, cv=5)",Python_Projects/NLP/policy area prediction/TrashBin/preprosessing_bigram.py,YangLiu928/NDP_Projects,1
"    SVM using parameter c. You can also specify a reduced subset of
    features, if desired; this becomes useful when we're tuning
    and trying to identify the size of an optimal feature set.
    '''

    data = scaleddf.drop(fold)
    testdata = scaleddf.loc[fold, : ]
    trainingyvals = metadata.loc[~metadata.index.isin(fold), 'class']
    realyvals = metadata.loc[fold, 'class']

    # supportvector = svm.LinearSVC(C = c)
    supportvector = svm.SVC(C = c, kernel = 'linear', probability = True)
    supportvector.fit(data.loc[ : , 0: featurecount], trainingyvals)

    prediction = supportvector.predict(testdata.loc[ : , 0: featurecount])

    return prediction, realyvals

def svm_probabilistic_one_fold(metadata, fold, scaleddf, c, featurecount):
    ''' This constitutes a training set by excluding volumes in fold,",trainamodel.py,tedunderwood/20cgenres,1
"                    ""i like london better than new york""])
y_train = [[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[0,1],[0,1]]
X_test = np.array(['nice day in nyc',
                   'welcome to london',
                   'hello welcome to new york. enjoy it here and london too'])   
target_names = ['New York', 'London']

classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
classifier.fit(X_train, y_train)
predicted = classifier.predict(X_test)
for item, labels in zip(X_test, predicted):
    print '%s => %s' % (item, ', '.join(target_names[x] for x in labels))",supervised_sklearn.py,jannson/Similar,1
"        See SVC.__init__ for details.
    coef0 : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    degree : int
        Degree of kernel, if kernel is polynomial.
        See SVC.__init__ for details.
    """"""

    def __init__(self, C, kernel='rbf', gamma=1.0, coef0=1.0, degree=3):
        estimator = SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0,
                        degree=degree)
        super(DenseMulticlassSVM, self).__init__(estimator)

    def fit(self, X, y):
        """"""
        Fit underlying estimators.

        Parameters
        ----------",pylearn2/models/svm.py,TNick/pylearn2,1
"	'''print train.shape
	clf = ExtraTreesClassifier()
	clf = clf.fit(train, target)
	model = SelectFromModel(clf, prefit=True)
	train= model.transform(train)
	print train.shape'''
	'''alg=ExtraTreesClassifier( n_estimators=1200,max_features= 30,																																																																																																													criterion= 'entropy',min_samples_split= 2,
	                            max_depth= 30, min_samples_leaf= 2, n_jobs = -1)'''

	alg=Pipeline([
	  ('feature_selection', LinearSVC(C=0.1, penalty=""l1"", dual=False)),
	  ('classification', RandomForestClassifier( n_estimators=1200,																																																																																																													criterion= 'entropy',min_samples_split= 2,
	                            max_depth= 30, min_samples_leaf= 2, n_jobs = -1))
	])


	'''alg=Pipeline([
	  ('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=100,criterion= 'entropy'))),
	  ('classification', RandomForestClassifier(n_estimators=100,criterion= 'entropy'))
	])'''",paribas_trees_n_forests.py,souravsarangi/BNPParibasKaggle,1
"        X_digits = digits.data
        y_digits = digits.target
        n_samples = len(X_digits)
        X_train = X_digits[:int(.9 * n_samples)]
        y_train = y_digits[:int(.9 * n_samples)]
        X_test = X_digits[int(.9 * n_samples):]
        y_test = y_digits[int(.9 * n_samples):]
        svm = SVM(sparkSession, is_multi_class=True, transferUsingDF=True)
        mllearn_predicted = svm.fit(X_train, y_train).predict(X_test)
        from sklearn import linear_model, svm
        clf = svm.LinearSVC()
        sklearn_predicted = clf.fit(X_train, y_train).predict(X_test)
        self.failUnless(accuracy_score(sklearn_predicted, mllearn_predicted) > 0.95 )


if __name__ == '__main__':
    unittest.main()",src/main/python/tests/test_mllearn_df.py,dusenberrymw/systemml,1
"def train(X_train, y_train, mla=DECISION_TREE):
    # print 'train classifier'

    if mla == DECISION_TREE:
        model = MLDecisionTree(X_train, y_train)
    elif mla == RANDOM_FOREST:
        model = MLRandomForest(X_train, y_train)
    elif mla == K_NEIGHBORS:
        model = MLKNeighbors(X_train, y_train)
    elif mla == SVM_SVC:
        model = MLSVC(X_train, y_train)
    elif mla == AdaBoost:
        model = MLAdaBoost(X_train, y_train)
    elif mla == Gaussian_Naive_Bayes:
        model = MLGaussianNaiveBayes(X_train, y_train)
    # elif mla == Linear_Discriminant_Analysis:
    #     model = MLLinearDiscriminantAnalysis(X_train, y_train)
    # elif mla == Quadratic_Discriminant_Analysis:
    #     model = MLQuadraticDiscriminantAnalysis(X_train, y_train)
    else:",digoie/core/ml/classifier/base.py,ZwEin27/digoie-annotation,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_15_2014_server.py,magic2du/contact_matrix,1
"        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",publivore/doc_classifier_example.py,mlaneuville/publivore,1
"        versions = conf.get_msvc_versions()
    except Exception as e:
        pass
    else:
        for version, targets in versions:
            name, version = version.split()
            for target_name, target in targets:
                arch, flags = target
                batfile, args, path, includes, libdirs = flags
                cl = conf.detect_executable(name == 'intel' and 'icl' or 'cl', path)
                c = MSVC(cl, name, version, target_name, arch, batfile, args, path, includes, libdirs)
                if c.name() in seen:
                    continue
                seen.add(c.name())
                conf.compilers.append(c)
    conf.end_msg('done')


def build(bld):
    pass",mak/compiler/msvc.py,bugengine/BugEngine,1
"            print (pred_scores)
            print len(pred_scores), "" "", len(test_labels)
            quit()
            acc = round(metrics.accuracy_score(test_labels, pred_labels), 6)
            #print acc
            scores.append(acc)
        score = round(sum(scores)/len(scores),4)
        return score

    def svm_evaluator (self, subset):
        clf = svm.SVC(kernel=""linear"")
        scores = []
        #print ""subset "", subset
        sample_data = self.data.ix[:,subset]
        for i in range(len(self.train_indices)):
            train_data = sample_data.ix[self.train_indices[i]] ##train sample data
            train_labels = self.labels[self.train_indices[i]] ##train labels
            
            test_data = sample_data.ix[self.test_indices[i]] ##test data
            test_labels = self.labels[self.test_indices[i]] ##test labels",Asynchronous_SFS/ML_data.py,JSilva90/MITWS,1
"
letters = load_letters()
X, y, folds = letters['data'], letters['labels'], letters['folds']
# we convert the lists to object arrays, as that makes slicing much more
# convenient
X, y = np.array(X), np.array(y)
X_train, X_test = X[folds == 1], X[folds != 1]
y_train, y_test = y[folds == 1], y[folds != 1]

# Train linear SVM
svm = LinearSVC(dual=False, C=.1)
# flatten input
svm.fit(np.vstack(X_train), np.hstack(y_train))

# Train linear chain CRF
model = ChainCRF()
ssvm = FrankWolfeSSVM(model=model, C=.1, max_iter=11)
ssvm.fit(X_train, y_train)

print(""Test score with chain CRF: %f"" % ssvm.score(X_test, y_test))",examples/plot_letters.py,massmutual/pystruct,1
"Cov1 = np.array([[1, -0.5],
                [-0.5, 1]])
Cov2 = Cov1

X = np.vstack([np.random.multivariate_normal(mu1, Cov1, N1),
               np.random.multivariate_normal(mu2, Cov2, N2)])
y = np.hstack([np.zeros(N1), np.ones(N2)])

#------------------------------------------------------------
# Perform an SVM classification
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

xx = np.linspace(-5, 5)
w = clf.coef_[0]
m = -w[0] / w[1]
b = - clf.intercept_[0] / w[1]
yy = m * xx + b

#------------------------------------------------------------",book_figures/chapter9/fig_svm_diagram.py,eramirem/astroML,1
"	return gs

def _gs_SVC_r0( xM, yVc, params):
	""""""
	Since classification is considered, we use yVc which includes digital values 
	whereas yV can include float point values.
	""""""

	print(xM.shape, yVc.shape)

	clf = svm.SVC()
	#parmas = {'alpha': np.logspace(1, -1, 9)}
	kf5 = cross_validation.KFold( xM.shape[0], n_folds=5, shuffle=True)
	gs = grid_search.GridSearchCV( clf, params, cv = kf5, n_jobs = -1)

	gs.fit( xM, yVc)

	return gs

def gs_SVC( xM, yVc, params, n_folds = 5):",repository/_jgrid_r0.py,jskDr/jamespy_py3,1
"		if fileData[0] == 'valid_images':
			l = 1
		elif fileData[0] == 'invalid_images':
			l = 0
		feature.append(np.array(map(np.float64,fileData[1:])))
		labels.append(l)
	except Exception:
		pass


clf=svm.LinearSVC()

dataset = np.asarray(feature)
labels = np.asarray(labels)
clf.fit(dataset, labels)

labels=[]
feature=[]
featureFile=open(""./data/valid/histrogram.out"")
data=featureFile.read().split(""\n"")",classify.py,madan-ram/TagMe,1
"	print testX.shape

	#save the new featurset for further exploration
	#np.save('trainX', trainX)
	#np.save('testX', testX)
	#np.save('trainY', trainY)
	#np.save('testY', testY)

	#fit the svm model and compute accuaracy measure
	clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	#clf = svm.SVC(kernel='linear')
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithTreeFS/mnistBackRandom/mnistRANDwithFS.py,akhilpm/Masters-Project,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 15, 2)],
        cv_method = KFold(20, 5))

    meta_model = SVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = [100.0, 100.0],
        sigma = 1.0,
        beta = 0.80,
        meta_model = meta_model)

    return method
",evopy/examples/problems/SchwefelsProblem21/CMAESRSVC.py,jpzk/evopy,1
"#from sklearn.externals import joblib

format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# if want to output log to a file instead of outputting log to the console,
# replace ""stream=sys.stdout"" with ""filename='fcma.log'""
logging.basicConfig(level=logging.INFO, format=format, stream=sys.stdout)
logger = logging.getLogger(__name__)

def example_of_aggregating_sim_matrix(raw_data, labels, num_subjects, num_epochs_per_subj):
    # aggregate the kernel matrix to save memory
    svm_clf = svm.SVC(kernel='precomputed', shrinking=False, C=1)
    clf = Classifier(svm_clf, num_processed_voxels=1000, epochs_per_subj=num_epochs_per_subj)
    rearranged_data = raw_data[num_epochs_per_subj:] + raw_data[0:num_epochs_per_subj]
    rearranged_labels = labels[num_epochs_per_subj:] + labels[0:num_epochs_per_subj]
    clf.fit(list(zip(rearranged_data, rearranged_data)), rearranged_labels,
            num_training_samples=num_epochs_per_subj*(num_subjects-1))
    predict = clf.predict()
    print(predict)
    print(clf.decision_function())
    test_labels = labels[0:num_epochs_per_subj]",examples/fcma/classification.py,mihaic/brainiak,1
"import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel=""linear"")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data
clf.fit(features_train, labels_train)


#### store your predictions in a list named pred",udacity/svm/example01.py,harish-garg/Machine-Learning,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.NuSVC(gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='c').export()
print(output)

""""""
#include <stdlib.h>
#include <stdio.h>
#include <math.h>",examples/classifier/NuSVC/c/basics.py,nok/sklearn-porter,1
"    # scale and normalise (all features on similar scale) which is more accurate
    X = preprocessing.scale(X)

    return X,y

def Analysis():
    test_size = 1000
    X, y = Build_Data_Set()
    print(len(X))

    clf = svm.SVC(kernel=""linear"", C= 1.0)
    clf.fit(X[:-test_size],y[:-test_size])

    correct_count = 0

    for x in range(1, test_size+1):
        if clf.predict(X[-x])[0] == y[-x]:
            correct_count += 1

    print(""Accuracy:"", (correct_count/test_size) * 100.00)",linear_svc_finance.py,davidcunha/linear-svc-yahoo-finance,1
"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

x = np.array([[5, 1], [1, 5]])
y = [1, -1]
s = svm.LinearSVC()
s.fit(x, y)

x_grid, y_grid = np.meshgrid(np.arange(0, 5.1, 0.1), np.arange(0, 5.1, 0.1))
grid_points = np.c_[x_grid.ravel(), y_grid.ravel()]

y_predicted = s.predict(grid_points)
y_predicted = y_predicted.reshape(x_grid.shape)

plt.contourf(x_grid, y_grid, y_predicted, cmap = plt.cm.Paired, alpha=0.1, levels=[0, 0.5, 1])",python/sklearn/svm1.py,yuncliu/Learn,1
"    params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}
    clf1 = GridSearchCV(lr, param_grid=params, scoring='roc_auc', verbose=True, cv=5, n_jobs=-1)
    clf1.fit(X_train_1, y_train_1)
    clf_1_x_val_predictions = clf1.predict(X_test)
    class_rep_1 = classification_report(y_test, clf_1_x_val_predictions)
    print clf1.best_params_
    print class_rep_1


    #2
    svc = SVC()
    svc_param_dist = {""C"": uniform(),
                         ""gamma"": uniform(),
                         ""kernel"": ['linear', 'rbf'],
                         ""class_weight"": [{1: 1}, {1: 2}, {1: 5}, {1: 10}],
                         ""probability"": [True]
                         }
    #params = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],
    #           'kernel': ['linear'], 'class_weight': [{1: 1}, {1: 5}, {1: 2}, {1: 3}, {1: 10}]}]
",Kaggle2/python/acpigeon/predict_pizza.py,jkuruzovich/MGMT6963,1
"
    def train(self, documents, labels, identifiers):
        """"""
        Fits vectorizer and classifier
        :param documents: (list)
        :param labels: (list)
        :param identifiers: (list)
        """"""

        self.vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1, tokenizer=lemma_tokenizer)
        self.clf = OneVsRestClassifier(LinearSVC(random_state=0))

        self._get_label_dicts(labels)
        self.doc_ids = identifiers

        x = self.vectorizer.fit_transform(documents)
        y = self.label_vectorizer(labels)
        self.clf.fit(x, y)

    def predict(self, documents):",src/model/base/model.py,brainsqueeze/OpenAgClassifier,1
"    When decision_function_shape == 'ovr', we use OneVsRestClassifier(SVC) from
    sklearn.multiclass instead of the output from SVC directory since it is not
    exactly the implementation of One Vs Rest.

    References
    ----------
    http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
    """"""

    def __init__(self, *args, **kwargs):
        self.model = sklearn.svm.SVC(*args, **kwargs)
        if self.model.decision_function_shape == 'ovr':
            self.decision_function_shape = 'ovr'
            # sklearn's ovr isn't real ovr
            self.model = OneVsRestClassifier(self.model)

    def train(self, dataset, *args, **kwargs):
        return self.model.fit(*(dataset.format_sklearn() + args), **kwargs)

    def predict(self, feature, *args, **kwargs):",libact/models/svm.py,ntucllab/libact,1
"from make_benchmarks import *

def tune(data,labels, clf=None):
    clf = Pipeline([('num_features', 
               SelectKBest(f_classif,k=100)),
                    ('svm', svm.SVC(C=.01,kernel='rbf'))])
    param_grid = {
        'num_features__k':range(40,300),
        # 'svm__C':10.**np.arange(-3,4)
    }
    grid_search = GridSearchCV(clf, 
                               param_grid,
                               cv=5,
                               scoring=""roc_auc"",
                               n_jobs=-1)",tuner.py,gabegaster/kaggle_schizophrenia_2014,1
"             hist,_ = np.histogram(kcent,bins=[0,1,2,3,4,5,6,7,8,9,10],normed=True)
             treino_hist_X.append(hist)
             treino_hist_Y.append(y_train[i])


    treino_hist_X = np.array(treino_hist_X).reshape((-1,10))
    treino_hist_Y = np.array(treino_hist_Y).reshape((-1,))

    print(""Treinando SVM"")
    #Treina um classificador SVM com os pares histograma e label das imagens do conjunto de treinamento
    modSVM = SVC()
    modSVM.fit(treino_hist_X,treino_hist_Y.reshape((-1,)))

    matriz_conf_teste = np.zeros((10,10))

    #Aplica o classificador ao conjunto de teste
    #Extrai os descritores de cada imagem
    #Encontra o centroide mais proximo para cada decritor de cada imagem
    #cria histograma com o vetor de centroides de cada imagem
    #Preve a categoria da imagem com o classificador SVM",quiz5-BoF.py,eugeniopacceli/ComputerVision,1
"  '''
  Test different models with generated features
  '''
  df_train = pd.read_csv('data/train.csv', encoding=""ISO-8859-1"")
  trainData = pd.read_csv('data/train_features.csv', encoding=""ISO-8859-1"")
  X_train, X_test, y_train, y_test = train_test_split(trainData, df_train['relevance'], test_size=0.3, random_state=42)
  
  # Classifiers: weaker results than Regressors
  # model = ensemble.RandomForestClassifier(n_estimators=50, criterion='entropy')
  # model = ensemble.GradientBoostingClassifier()
  # model = svm.SVC()
  # model.fit(X_train, [str(n) for n in y_train])
  # print(""Train RMSE: %.3f"" % np.sqrt(np.mean(([float(n) for n in model.predict(X_test)] - y_test) ** 2)))
  
  # Regressors
  # model = linear_model.SGDRegressor()
  # model = ensemble.RandomForestRegressor()
  # model = svm.LinearSVR()
  # model = svm.NuSVR(kernel='poly')
  model = ensemble.GradientBoostingRegressor()",build_models.py,CSC591ADBI-TeamProjects/Product-Search-Relevance,1
"import matplotlib.pyplot as plt
from email_preprocess import preprocess
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# features_train and features_test are the features for the training
# and testing datasets, respectively
# labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

# clf = SVC(kernel=""linear"")
clf = SVC(kernel=""rbf"", C=10000)
t0 = time()
""""""

	One way to speed up an algorithm is to train it on a smaller training 
	dataset. The tradeoff is that the accuracy almost always goes down when 
	you do this. Let's explore this more concretely:
	
	original (linear):",002 - SVM/svm_author_id.py,mdegis/machine-learning,1
"		contrib_string3+= str(int((3+counter)%4))+""_0__""

	for i in range(1):

		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_5__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_0__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i)))

		#comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        args=[str(dim)+ ""Dlegendre4contrib_bdt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/bdt_legendre/bdt_Legendre_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"import optunity
import optunity.metrics
import sklearn.svm

# score function: twice iterated 10-fold cross-validated accuracy
@optunity.cross_validated(x=data, y=labels, num_folds=10, num_iter=2)
def svm_auc(x_train, y_train, x_test, y_test, C, gamma):
    model = sklearn.svm.SVC(C=C, gamma=gamma).fit(x_train, y_train)
    decision_values = model.decision_function(x_test)
    return optunity.metrics.roc_auc(y_test, decision_values)

# perform tuning
optimal_pars, _, _ = optunity.maximize(svm_auc, num_evals=200, C=[0, 10], gamma=[0, 1])

# train model on the full training set with tuned hyperparameters
optimal_model = sklearn.svm.SVC(**optimal_pars).fit(data, labels)",docs/examples/python/sklearn/svc.py,chrinide/optunity,1
"            sell += 1

    print ""Buy/Sell: {}/{}"".format(buy,sell)
    '''

    time_liblinear_train = t1-t0
    time_liblinear_predict = t2-t1

    '''
    # Print results in a nice table
    print(""Results for SVC(kernel=rbf)"")
    print(""Training time: %fs; Prediction time: %fs"" % (time_rbf_train, time_rbf_predict))
    print(classification_report(test_labels, prediction_rbf))
    print(""Results for SVC(kernel=linear)"")
    print(""Training time: %fs; Prediction time: %fs"" % (time_linear_train, time_linear_predict))
    print(classification_report(test_labels, prediction_linear))
    '''

    print len(test_labels),test_labels[:10]
    print len(prediction_liblinear),prediction_liblinear[:10]",classing.py,ocanosoup/BuzzWordTrading,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    y = y.astype(float)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx = alpha_investing.alpha_investing(X[train], y[train], 0.05, 0.05)

        # obtain the dataset on the selected features
        selected_features = X[:, idx]
",skfeast/example/test_alpha_investing.py,jundongl/scikit-feast,1
"for i in range(len(hogList)):
	# hogList[i] = hogList[i].flatten()
	hogVals[i,:] = hogList[i][0].flatten()[0]
hogLabels = np.zeros(len(hogs['face'])+len(hogs['l_hand']))
# hogLabels[0:len(hogs['face'])]+=1
hogLabels[len(hogs['face']):]+=1

# im = HOGpicture(hogList[1], hogRes)
# imshow(im, interpolation='nearest')

svm_ = svm.SVC(kernel='poly', probability=True, C=1, degree=2, gamma=1)
# svm_ = svm.SVC(kernel='rbf', probability=True, nu=.7)#, C=1)#, )
svm_.fit(hogVals, hogLabels)

score = svm_.score(hogVals, hogLabels)
probs = svm_.predict_proba(hogVals)
print score

from sklearn.ensemble import RandomForestClassifier	
from sklearn.ensemble import ExtraTreesClassifier",pyKinectTools/scripts/hogTests.py,colincsl/pyKinectTools,1
"
	if len(optimisation_args)>0:
                svm_C=float(optimisation_args[0])
                if len(optimisation_args)>1:
                        acoef0=float(optimisation_args[1])
                        if len(optimisation_args)>2:
			        if optimisation_args[2]==""auto"":
                        	        agamma = optimisation_args[2]
                        	else:
					agamma = float(optimisation_args[2])
	self.clf_blueprint = svm.SVC(probability=True,C=svm_C, coef0=acoef0, gamma=agamma)
        self.specific_type_of_classifier=""svm""
	super( svm_sklearn, self ).__init__(data,percentage_used_for_validation,no_permutations,name,sample1_name,sample2_name)
    def set_cache_size(self,a_cache_size):
        self.cache_size=a_cache_size
    def get_cache_size(self):
        return self.cache_size
    
#class nn_sklearn(sklearn_classifier):
    #def __init__(self,data,percentage_used_for_validation,no_permutations=0,name=""unnamed"",sample1_name=""sample1"",sample2_name=""sample2""):",Dalitz/classifier_eval.py,weissercn/MLTools,1
"    # Shuffle the data and split up the request subset of the training data    
    size = int(np.exp(s))
    s_max = y_train.shape[0]
    shuffle = np.random.permutation(np.arange(s_max))
    train_subset = X_train[shuffle[:size]]
    train_targets_subset = y_train[shuffle[:size]]

    # Train the SVM on the subset set
    C = np.exp(float(x[0, 0]))
    gamma = np.exp(float(x[0, 1]))
    clf = svm.SVC(gamma=gamma, C=C)
    clf.fit(train_subset, train_targets_subset)
    
    # Validate this hyperparameter configuration on the full validation data
    y = 1 - clf.score(X_val, y_val)

    c = time.time() - start_time

    return np.array([[np.log(y)]]), np.array([[c]])
",examples/example_fmin_fabolas.py,aaronkl/RoBO,1
"import pandas as pd
from sklearn.svm import LinearSVC


def svm(xTrain, yTrain, xTest):
    svmmodel = LinearSVC()
    svmmodel.fit(xTrain, yTrain)
    return svmmodel.predict(xTest)


def print_output(yTest, filename):
    output = {'Id': range(1, (len(yTest)+1)), 'Solution': yTest}
    withcounter = pd.DataFrame(output)
    withcounter.to_csv(filename, index=False)
",code/svm.py,Oscarlsson/kaggle-scikit,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"", ""Neural Net"", ""Naive Bayes"", ""QDA"",
         ""Decision Tree"", ""Random Forest"", ""AdaBoost"", ""SCM-Conjunction"", ""SCM-Disjunction""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    MLPClassifier(alpha=1),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    SetCoveringMachineClassifier(max_rules=4, model_type=""conjunction"", p=2.0),",examples/decision_boundary.py,aldro61/pyscm,1
"
Len = np.shape(trainData)[0]
Size = np.size(trainData)

Width = Len/50000
print Len
print Width*50000
trainData = trainData.reshape((50000, Width))

# Training SVM
SVM = svm.LinearSVC(C=1)
# C=100, kernel='rbf')
print ""Training the SVM""
trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))
#print trainData
SVM.fit(trainData, trainLabel)
print(""Training Score = %f "" % float(100 * SVM.score(trainData, trainLabel)))
#print(""Training Accuracy = %f"" % (SVM.score(trainData, trainLabel) * 100))
eff = {}
eff['train'] = SVM.score(trainData, trainLabel) * 100",extreme/test_conv_destin.py,tejaskhot/deep-learning,1
"    print('Vectorization time:', vec_time)
    print('Data matrix size:', X_all.shape)

    y_train, X_train, ind_train, y_test, X_test, ind_test, X_unlab, ind_unlab =\
            dp.split_data(y_all, X_all, split=0.7, seed=0)
    me = ModelEvaluator()

    # LinearSVC (liblinear SVM implementation, one-v-all)
    cross_validate = True
    if cross_validate:
        model = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001,\
            C=1, multi_class='ovr', fit_intercept=True, intercept_scaling=1,\
            class_weight='balanced', verbose=0, random_state=None, max_iter=1000)
        param_grid = {'C':np.logspace(-2,2,24).tolist()}
        grid_info, grid_best, grid_time = me.param_search(model, param_grid,\
                y_train, X_train, num_folds=10)
        C = grid_best['C']
    else:
        C = 1
    print('C: ', C)",categorize.py,GautamShine/toxic-docs,1
"from time import time
import librosa

def svm_predict(training_samples, training_labels, test_samples, test_lables):
    from sklearn.svm import SVC
    from sklearn.svm import LinearSVC
    from sklearn.model_selection import GridSearchCV

    parameters = {'kernel':('linear','rbf','poly','sigmoid'), 'C':(1, 10,100,1000,10000)}

    # clf = GridSearchCV(SVC(probability=True), parameters)
    # clf.fit(training_samples,training_labels)
    # pred = clf.predict_proba(test_samples)
    # return pred

    clf = GridSearchCV(SVC(probability=False), parameters)
    clf.fit(training_samples,training_labels)
    pred = clf.predict(test_samples)
    from sklearn.metrics import accuracy_score
    acc = accuracy_score(test_lables,pred)",environmentalSoundClassification/soundClassifier.py,amogh3892/Environmental-sound-recognition-using-combination-of-spectrogram-and-acoustic-features,1
"
    def predict(self, X):
        return self._clf.predict(X)

    def get_params(self, deep=True):
        return dict(clf=self._clf)


def LinearSVC_Proba(probability=False, method='sigmoid', cv=5, **kwargs):
    if probability is True:
        base_estimator = LinearSVC(**kwargs)
        return CalibratedClassifierCV(base_estimator=base_estimator,
                                      method=method, cv=cv)
    else:
        return LinearSVC(**kwargs)


def SVC_Light(probability=False, method='sigmoid', cv=5, **kwargs):
    """"""
    Similar to SVC(kernel='linear') without having to store 'support_vectors_'",jr/gat/classifiers.py,kingjr/jr-tools,1
"					% (grid.best_params_, grid.best_score_))

			# Now we need to fit a classifier for all parameters in the 2d version
			# (we use a smaller set of parameters here because it takes a while to train)

			C_2d_range = [1e-2, 1, 1e2]
			gamma_2d_range = [1e-1, 1, 1e1]
			classifiers = []
			for C in C_2d_range:
				for gamma in gamma_2d_range:
					clf = SVC(C=C, gamma=gamma)
					clf.fit(X_2d, y_2d)
					classifiers.append((C, gamma, clf))

			##############################################################################
			# visualization
			#
			# draw visualization of parameter effects

			plt.figure(figsize=(8, 6))",Dalitz_simplified/classifier_eval_simplified_with_MLP_code.py,weissercn/MLTools,1
"        raise NotImplementedError

    def SMC(self, args):
        """""" Secure Monitor Call """"""
        raise NotImplementedError

    def SRS(self, args):
        """""" Store Return State """"""
        raise NotImplementedError

    def SVC(self, args):
        """""" Supervisor Call """"""",Execute/exception_handling.py,tdpearson/armdecode,1
"    num_instances = len(X_train)
    seed = 7
    scoring = 'accuracy'
    # Spot Check Algorithms
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC()))
    models.append(('RF', RandomForestClassifier(n_estimators=50)))
    # evaluate each model in turn
    results = []
    names = []
    for name, model in models:
        kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)",core/main.py,god99me/RandomRoughForest,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",projects/scikit-learn-master/sklearn/metrics/tests/test_classification.py,DailyActie/Surrogate-Model,1
"        
        return X, words


    def model(self, C=1.0):
        # clf = Pipeline([
        #                 ('feature_selection', RandomizedLogisticRegression()),
        #                 ('classification', SVC(probability=True))
        #                ])
        clf = RandomForestClassifier()
        #SVC(C=C, kernel='linear')
        # clf = LogisticRegression(C=C, penalty=""l1"")
        
        
        return clf




    def predict_population_text(self, text, clf):",bilearn.py,ijmarshall/cochrane-nlp,1
"train_data,mean0,std0 = preprocess(train_data)
valid_data,_,_ = preprocess(valid_data,mean0,std0)
test_data,_,_ = preprocess(test_data,mean0,std0)

pca_transf, pca_invtransf,m0,s0,var_fracs = pca(train_data, whiten=True, batchsize=train_data.shape[0])

trd = whiten(train_data,pca_transf,m0,s0,10)
vld = whiten(valid_data,pca_transf,m0,s0,10)
tsd = whiten(test_data,pca_transf,m0,s0,10)

svr = svm.SVC(probability=True)
param_grid = [{'C': [1, 10, 100, 1000, 10000], 'gamma': [0.001, 0.0001, 0.00001], 'kernel': ['rbf']},]
clf = grid_search.GridSearchCV(svr, param_grid)

R = np.random.permutation(train_data.shape[0])
R1 = np.random.permutation(valid_data.shape[0])

np.save('Clean_train_data.npy',train_data[R])
np.save('Clean_valid_data.npy',valid_data[R1])
np.save('Clean_test_data.npy',test_data)",Audio/data_loader.py,saebrahimi/Emotion-Recognition-EmotiW2015,1
"		'nsynapses': 392,
		'seg_th': 10,
		
		'syn_th': 0.5,
		'pinc': 0.001,
		'pdec': 0.002,
		'pwindow': 0.01,
		'random_permanence': True,
		
		'nepochs': 10,
		'clf': LinearSVC(random_state=seed),
		'log_dir': os.path.join('simple_mnist', '1-1')
	}
	
	# Seed numpy
	np.random.seed(seed)
	
	# Get the data
	(tr_x, tr_y), (te_x, te_y) = load_mnist()
	x, y = np.vstack((tr_x, te_x)), np.hstack((tr_y, te_y))",dev/sp_math_pub/mnist_simple.py,tehtechguy/mHTM,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/metrics/tests/test_classification.py,RPGOne/Skynet,1
"
    def test(self, descriptors,des_list, n):
        # Scale the features
        test_features = self.stdSlr.transform(descriptors)

        return self.clf.predict(test_features)

    def __init__(self,path,C,iter,c_w,probability):
        if not self.load(path):
            self.path = path
            self.clf = SVC()
            self.C = C
            self.gamma =0.1
            self.iter = iter
            self.prob = probability
            if c_w :
                self.class_weight = {0:1000,1:200}
            else:
                self.class_weight = 'balanced'
            self.stdSlr = None",src/Multimodal_Interaction/Interaction/SVM.py,pazagra/catkin_ws,1
"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]
",Classifrer/classifer.py,hitlonewind/PR-experiment,1
"

def plot_cross_val_selection():
    iris = load_iris()
    X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data,
                                                              iris.target,
                                                              random_state=0)

    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
                  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}
    grid_search = GridSearchCV(SVC(), param_grid, cv=5)
    grid_search.fit(X_trainval, y_trainval)
    scores = grid_search.grid_scores_[15:]

    best = np.argmax([x.mean_validation_score for x in scores])
    plt.figure(figsize=(10, 3))
    plt.xlim(-1, len(scores))
    plt.ylim(0, 1.1)
    for i, score in enumerate(scores):
        marker_cv, = plt.plot([i] * 5, score.cv_validation_scores, '^', c='gray', markersize=5, alpha=.5)",mglearn/plot_grid_search.py,amueller/advanced_training,1
"from mne.decoding import Vectorizer, FilterEstimator  # noqa


scores_x, scores, std_scores = [], [], []

# don't highpass filter because it's epoched data and the signal length
# is small
filt = FilterEstimator(rt_epochs.info, None, 40)
scaler = preprocessing.StandardScaler()
vectorizer = Vectorizer()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                              ('scaler', scaler), ('svm', clf)])

data_picks = mne.pick_types(rt_epochs.info, meg='grad', eeg=False, eog=True,
                            stim=False, exclude=raw.info['bads'])
ax = plt.subplot(111)
ax.set_xlabel('Trials')
ax.set_ylabel('Classification score (% correct)')",examples/realtime/plot_compute_rt_decoder.py,pravsripad/mne-python,1
"
def getPreAcc(trainCount, testCount, predTest, genre):
	clfMNB = MultinomialNB(alpha=.3).fit(trainCount, genre)
	predicted = clfMNB.predict(testCount)
	print ""Multinomial Naive Bayes Accuracy: "" + str(round(accuracy_score(predicted, predTest), 3))
	
	clfGNB = GaussianNB().fit(trainCount.toarray(), genre)
	predicted = clfGNB.predict(testCount.toarray())
	print ""Gaussian Naive Bayes Accuracy: "" + str(round(accuracy_score(predicted, predTest), 3))
	
	clfSVM = svm.SVC(kernel=""linear"").fit(trainCount, genre)
	predicted = clfSVM.predict(testCount)
	print ""Support Vector Classifier Accuracy: "" + str(round(accuracy_score(predicted, predTest), 3))
	
	clfDT = tree.DecisionTreeClassifier().fit(trainCount.toarray(), genre)
	predicted = clfDT.predict(testCount.toarray())
	print ""Decision Tree Accuracy: "" + str(round(accuracy_score(predicted, predTest), 3))

def main():
	genreTrain = [""./res/train/hiphop"", ""./res/train/country""]",main.py,aorti017/Genre-Identification,1
"iris_y_train = np.concatenate((iris_y_train_class1, iris_y_train_class2), axis=0)

iris_X_test_class1 = iris_X[iris_y == 1][-5:, :2]
iris_X_test_class2 = iris_X[iris_y == 2][-5:, :2]
iris_X_test = np.concatenate((iris_X_test_class1, iris_X_test_class2), axis=0)

iris_y_train_class1 = iris_y[iris_y == 1][:-5]
iris_y_train_class2 = iris_y[iris_y == 2][:-5]
iris_y_train = np.concatenate((iris_y_train_class1, iris_y_train_class2), axis=0)

svm.SVC(kernel='linear')
svc.fit(iris_X_train, iris_y_train)



# print type(iris_y[iris_y == 0])
# print len(iris_y[iris_y == 0])
# print len(iris_y[iris_y == 1])
# print len(iris_y[iris_y == 2])",scikitlearn-ex2.py,pieteradejong/python,1
"X_test = test_dt.copy()

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

models = ({
    # 'logreg': LogisticRegression(),
    # 'svc': SVC(),
    'random forest': RandomForestClassifier(),
    # 'knn': KNeighborsClassifier(n_neighbors=3),
    # 'GaussinNB': GaussianNB()
})

paramsDef = ({
    # 'logreg': { 'name': 'C', 'data': [1, 3, 10, 30, 100, 300] },
    # 'svc': { 'name': 'C', 'data': [1, 3, 10, 30, 100, 300] },
    'random forest': { 'name': 'n_estimators', 'data': [10, 20, 50, 100, 200] },",titanic/my.py,netssfy/kaggle-lab,1
"	features.extend(color_list)

targets = np.array(targets)
features = np.array(features)

print ""All data loaded!""

ratio = 0.9
train_targets, test_targets, train_features, test_features = train_test_split(targets, features, test_size=0.1, random_state=42)

clf = svm.SVC()

clf.fit(train_features, train_targets)

print ""Training Complete""

everything = Counter(test_targets)
correct = Counter()
for feature, target in zip(test_features, test_targets):
	if clf.predict(feature) == target:",PythonCode/ml.py,TheFightingMongooses/cloaked-octo-bugfixes,1
"        this GridSearchCV instance after fitting.

    verbose : integer
        Controls the verbosity: the higher, the more messages.

    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         degree=..., gamma=..., kernel='rbf', max_iter=-1,
                         probability=False, random_state=None, shrinking=True,
                         tol=..., verbose=False),
           fit_params={}, iid=..., loss_func=..., n_jobs=1,",sklearn/grid_search.py,loli/sklearn-ensembletrees,1
"
from gat.classifiers import force_predict, LinearSVC_Proba, SSSLinearClassifier
from gat.scorers import scorer_auc, scorer_spearman

scaler = StandardScaler()
clf = force_predict(SGDClassifier(loss='log', class_weight='auto'), axis=1)
clf = force_predict(LinearSVC(C=1, class_weight='auto'),
                    mode='decision-function')
clf = force_predict(LinearSVC_Proba(C=1, class_weight='auto',
                                    probability=True), axis=1)
clf = force_predict(SVC(kernel='linear', C=1, probability=True,
                        class_weight='auto'), axis=1)
clf = force_predict(SSSLinearClassifier(LogisticRegression(penalty='l2', C=1)),
                    axis=1)
reg = LinearSVR(C=1)
svc_pipeline = Pipeline([('scaler', scaler), ('clf', clf)])
svr_pipeline = Pipeline([('scaler', scaler), ('reg', reg)])

# MAIN CONTRASTS ##############################################################
unambiguous = dict(stim_category=[0.0, 1.0])",scripts/config.py,kingjr/meg_perceptual_decision_symbols,1
"
        # redefinition of model in else-clause is ok
        # pylint: disable=R0204
        if len(y.shape) == 1 or y.shape[1] == 1:
            # use regression
            y = y.squeeze()
            model = SVR()
        else:
            # use classification
            y = np.argmax(y, axis=1)
            model = SVC()

        model.fit(X, y)
        result[0] = model
        # pylint: enable=R0204


class OpSVMPredict(OpPredict, Regression, Classification):
    def execute(self, slot, subindex, roi, result):
        a = roi.start[0]",tsdl/classifiers/svm.py,burgerdev/hostload,1
"    
    
    
    tuned_parameters = [{'C': [1, 10, 100, 1000, 2000], 'gamma': [0.01, 0.001, 0.0001, .00001], 'kernel': ['rbf']},]
    scores = ['precision', 'recall']

    #for score in scores:
    #    print(""# Tuning hyper-parameters for %s"" % score)
    #    print()

    #    clf = GridSearchCV(SVC(cache_size=3000, C=1), tuned_parameters, cv=5, scoring=score)
    #    print (s.split_main()[1][:,0].shape)
    #    clf.fit(scaled_tr_data, s.split_main()[1][:,0])

    #    print(""Best parameters set found on development set:"")
    #    print()
    #    print(clf.best_params_)
    #    print()
    #    print(""Grid scores on development set:"")
    #    print()",algorithms/support_vector_machine.py,archonren/project,1
"def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    from sklearn import svm
    clf = svm.SVC(kernel='rbf', gamma =1.0,C=1)
    clf.fit(features_train,labels_train)
    return clf",SVM Quiz/ClassifyNB.py,AhmetHamzaEmra/Machine-Learning-,1
"from sklearn import svm

digits = datasets.load_digits()
print(digits.data)
print(digits.target)

# digits.target is the actual label we've assigned to the digits data. 
# Now that we've got the data ready, we're ready to do the machine learning.
# First, we specify the classifier:
# If you want, you can just leave parameters blank and use the defaults, like this:
# clf = svm.SVC()
# clf = svm.SVC(gamma=0.001, C=100)
# clf = svm.SVC(gamma=0.01, C=100)
clf = svm.SVC(gamma=0.0001, C=100)

X,y = digits.data[:-10], digits.target[:-10]
clf.fit(X,y)
print(clf.predict(digits.data[-5]))
plt.imshow(digits.images[-5], cmap=plt.cm.gray_r, interpolation='nearest')
plt.show()",p02.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"
norm = StandardScaler()
#best_score 0.932203389831
#C 1.55
#gamma 0.097
norm.fit(X_train)
X_train = norm.transform(X_train)
X_test = norm.transform(X_test)

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
score = model.score(X_test, y_test)
print(score)

best_score = 0

from sklearn.manifold import Isomap
for n_neighbors in range(2, 6):
    for n_components in range(4, 7):",Module6/assignment3.py,szigyi/DAT210x,1
"trainMatrixReduced = BigMatrixReduced[0:max(indexesIm), :]
testMatrixReduced = BigMatrixReduced[testIndexes[0]:BigMatrixReduced.shape[0], :]

#Divide dataset for cross validation purposes
X_train, X_test, y_train, y_test = cross_validation.train_test_split(
    trainMatrixReduced, y[0:24999], test_size=0.4, random_state=0) #fix this

#random grid search of hiperparameters

#create a classifier
clf = svm.SVC(verbose = True)

# specify parameters and distributions to sample from
params2Test = {'C': expon(scale=100), 'gamma': expon(scale=.1),
  'kernel': ['rbf'], 'class_weight':['auto']}

#run randomized search
n_iter_search = 5
random_search = RandomizedSearchCV(clf, param_distributions = params2Test, n_iter = n_iter_search)
",cats_vs_dogs.py,mblaauw/Kaggle_CatsVsDogs,1
"        breaths_to_stack = len(max_mins)
        modulo_idx = col % breaths_to_stack
        val = max_mins[modulo_idx]
        df.iloc[:, col] = (df.iloc[:, col] - val['min']) / (val['max'] - val['min'])
    return df


def non_spark(x_train, x_test, y_train, y_test, vents_and_files):
    for c in [12]:
        for gamma in [.02]:
            clf = SVC(cache_size=CACHE_SIZE, kernel=""rbf"", C=c, gamma=gamma)
            clf.fit(x_train, y_train['y'].values)
            print(""Params: "", c, gamma)
            predictions = clf.predict(x_test)
            print(""Accuracy: "" + str(accuracy_score(y_test['y'], predictions)))
            print(""Precision: "" + str(precision_score(y_test['y'], predictions)))
            print(""Recall: "" + str(recall_score(y_test['y'], predictions)))
            fpr, tpr, thresh = roc_curve(y_test['y'], predictions)
            print(""False pos rate: "" + str(fpr[1]))
            print(""True post rate: "" + str(tpr[1]))",learn.py,hahnicity/ecs251-final-project,1
"    """"""
    (options, args) = get_options()

    dataset = load_files(options.fold)
    x_train, x_test, y_train, y_test = train_test_split(
            dataset.data, dataset.target, test_size=0.4)

    clf = Pipeline([
            ('vect', TfidfVectorizer(decode_error='ignore',
                                     min_df=0.1, max_df=0.9)),
            ('svc', LinearSVC(dual=False))])
    parameters = {
            'vect__ngram_range': [(1,1), (1,2)],
            'svc__C': [i*0.5 for i in range(1, 10)],
            }

    gs_clf = GridSearchCV(clf, parameters, n_jobs=6)
    gs_clf = gs_clf.fit(x_train, y_train)

    y_pred = gs_clf.predict(x_test)",main.py,ShiehShieh/Code_Identifier,1
"def svmInfomapCluster(vdb,featureSubset=FEATURES,th=.34,C=.82,kernel='linear',gamma=1E-3):
	""""""
	The first argument is the validation data base, the rest of the training
	databases are used for training.
	""""""
	newWordList = pd.DataFrame()
	fitting = trainingVectors[trainingVectors.db!=vdb]
	validation = training[training.db==vdb].copy()
	X = fitting[featureSubset].values
	y = fitting.target.values
	svClf = svm.SVC(kernel=kernel,C=C,gamma=gamma,
					probability=True)
	svClf.fit(X,y)
	nprandom.seed(1234)
	random.seed(1234)
	svScores = svClf.predict_proba(validation[featureSubset].values)[:,1]
	validation['svScores'] = svScores
	scores = pd.DataFrame()
	wordlist = pd.DataFrame()
	concepts = validation.gloss.unique()",code/infer/base.py,evolaemp/svmcc,1
"
        X   = [chip.LCT.values() for chip in chips]
        gnd = [chip.gnd for chip in chips]            
        
        if grid_search:
            grid = { 'C': [1, 5, 10, 50, 100], \
                 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1] }

            print 'SVM: Grid search using parameter grid: ', grid

            self.model = GridSearchCV(SVC(kernel='rbf'), grid, n_jobs=4, \
                            fit_params={'class_weight': {1 : 1, -1 : 1}})
        else:
            self.model = SVC()
            
        self.scale_factors, Xstd = standardize(X)
        self.model.fit(Xstd, gnd)


    def predict(self, chip):",qikify/controllers/SVM.py,trela/qikify,1
"y_binarized = label_binarize(y, classes=range(noOfTotalClasses))
test_y_binarized = label_binarize(test_y, classes=range(noOfTotalClasses))

# Neural Classifiers
sgd_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
		activation = ""tanh"", solver = ""sgd"", max_iter = 1800, learning_rate = ""adaptive"", learning_rate_init=""0.01"",
		random_state=0)
adam_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
		activation = ""tanh"", solver = ""adam"", max_iter = 1000, random_state=0)
# SVM Classifiers
rbf_svc_clf = OneVsRestClassifier((svm.SVC(kernel='rbf', gamma=0.05, C=401, probability=True)))
lin_svc_clf = OneVsRestClassifier((svm.SVC(kernel='linear', C=801, gamma=0.01, probability=True)))

sgd = sgd_clf.fit(X,y_binarized)
adam = adam_clf.fit(X,y_binarized)
lin_svc = lin_svc_clf.fit(X, y_binarized)
rbf_svc = rbf_svc_clf.fit(X, y_binarized)

labels = ['Neural(SGD)', 'Neural(adam)', 'SVC(linear)', 'SVC(rbf)']
colors = ['black', 'blue', 'darkorange', 'violet', 'yellow', 'red', 'pink', 'green', 'magenta', 'cyan', 'grey', 'brown']",neural/main_roc.py,aroonav/furry-strokes,1
"from sklearn.svm import SVC


Builder.load_file('view/blocks/svmblock.kv')

class SVMBlock(Block):
    title2 = 'gds'
    out_1 = ObjectProperty()

    def function(self):
        self.out_1.val = SVC()",persimmon/view/blocks/svmblock.py,AlvarBer/Persimmon,1
"

    train_instances, train_labels, train_texts = Word2VecHelper.loadData(all, args, 'train')
    test_instances, test_labels, test_texts = Word2VecHelper.loadData(all, args, 'test')

    C = 0.5  # SVM regularization parameter

    """"""
    if args.classifier == 'poly':
        classifier_count = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                          (""extra trees"", svm.SVC(kernel=""poly"", degree=3, C=C))])

        classifier_tfidf = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                          (""extra trees"", svm.SVC(kernel=""poly"", degree=3,C=C))])
    elif args.classifier == 'linear':
        classifier_count = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                     (""extra trees"", svm.SVC(kernel=""linear"", C=C))])

        classifier_tfidf = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                                     (""extra trees"", svm.SVC(kernel=""linear"", C=C))])",train_model_W2v_grid.py,eamosse/word_embedding,1
"
class machinelearning():
    def __init__(self):
        if op.isfile('./model/face_model.pkl'):
            self.clf = joblib.load('./model/face_model.pkl')
        else:
            self.clf = None

    #create face type model in ./model/
    def create_model(self, X, Y):
        clf = svm.SVC(probability =True)
        clf.fit(X, Y)
        joblib.dump(clf, './model/face_model.pkl')
        print 'create face_model.pkl file'
        #self.clf = joblib.load('./model/face_model.pkl')

    #precit face type using model in ./model/
    def predic_data(self, target):
        if self.clf is None:
            print 'Error: cannot find face_model.pkl'",machinelearning.py,lheadjh/FaceTypeDetector,1
"
        expected_feature_name = PartialDependence.feature_column_name_formatter('sepal length (cm)')

        self.assertIn(expected_feature_name,
                      pdp_df.columns.values,
                      ""{0} not in columns {1}"".format(expected_feature_name,
                                                      pdp_df.columns.values))
        # 2. Using SVC
        from sklearn import svm
        # With SVC, predict_proba is supported only if probability flag is enabled, by default it is false
        clf = svm.SVC(probability=True)
        clf.fit(iris.data, iris.target)
        classifier_predict_fn = InMemoryModel(clf.predict_proba, examples=iris.data)
        interpreter = Interpretation()
        interpreter.load_data(iris.data, iris.feature_names)
        pdp_df = interpreter.partial_dependence.partial_dependence([iris.feature_names[0]], classifier_predict_fn,
                                                                   grid_resolution=25, sample=True)
        self.assertIn(expected_feature_name,
                      pdp_df.columns.values,
                      ""{} not in columns {}"".format(*[expected_feature_name,",skater/tests/test_partial_dependence.py,datascienceinc/Skater,1
"    bottom_inds=list(reversed(inds))[0:k]
    
    # Recommender System part
    if(plot_output):
        PR_fig = setup_plots()
    
    # Specify the classifiers
    clfs = [
            BernoulliNB(alpha=0.001),
            LogisticRegression(C=0.02, penalty='l1', tol=0.001),
            svm.SVC(C=1,kernel='rbf',probability=True),
            ensemble.RandomForestClassifier(),
            CollaborativeFilter(categories=False),
            Popularity(),
            RandomClassifier(),
            CollaborativeFilter(categories=case_categories)
            ]
    #plot_ops = ['k:','k--','k-','k-.','r-','b-','g-'] 
    plot_ops = [{'linewidth':8.0, 'linestyle':'-','color':PuBuGn4[1]},
                {'linewidth':8.0, 'linestyle':'-','color':PuBuGn4[2]},",paper_experiments.py,IDEALLab/design_method_recommendation_JMD_2014,1
"stds = stds.fit(train)
train = stds.transform(train)
test = stds.transform(test)

train = pd.DataFrame(data = train, columns = header_train, index = index_train)
test = pd.DataFrame(data = test, columns = header_test, index = index_test)

#print(train_labels)
#print(train)

lr = SVC(gamma = 0.003, verbose = True, C = 100.0)
lr = lr.fit(train, train_labels)

predictions = lr.predict(train)
predictions = list(predictions)
predictions_test = lr.predict(test)
predictions_test = list(predictions_test)

#print(predictions)
",exercise-scripts/Predict_SVM.py,Karl-Marka/data-mining,1
"
print(""read training data"")
path = '../Data/'
train = pd.read_csv(path+'train_tfidf.csv')
label = train['target']
trainID = train['id']
del train['id'] 
del train['target']

np.random.seed(131)
svc = svm.SVC(kernel='rbf',C=10,probability=True,verbose=True) 
svc.fit(train.values, label)
#calibrated_svc = CalibratedClassifierCV(OneVsRestClassifier(svc,n_jobs=-1), method='isotonic', cv=5)
#calibrated_svc.fit(train.values, label)

print(""read test data"")
test  = pd.read_csv(path+'test_tfidf.csv')
ID = test['id']
del test['id']
clf_probs = svc.predict_proba(test.values)",new/src/1st_level/svm_tfidf.py,puyokw/kaggle_Otto,1
"    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for logistic regression: ', auc)
    
    plot_curve(fpr, tpr, 'Logistic regression ' + str(auc))
    return clf


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_5.py,remigius42/code_camp_2017_machine_learning,1
"        
        cpt = 0
        score = []
        coeff = []
        for train, test in sss:
            x_train = x[train]
            y_train = y[train]
            x_test = x[test]
            y_test = y[test]
            
            svm = SVC(kernel='linear')
            svm.fit(x_train, y_train)
            score.append(svm.score(x_test, y_test))
            coeff.append(svm.coef_)
            cpt += 1
            print k, cpt
    scores.append(score)
    coeffs.append(np.mean(coeff, axis=0))

""""""",classification_fmri.py,mrahim/adni_petmr_analysis,1
"Xte = P.transform(Xte)

print 'logistic'
L = LogisticRegression()
L.fit(Xtr, ytr)
print L.score(Xte,yte)
print L.score(Xtr,ytr)
print getf1(L, Xte, yte)

print 'svm'
S = SVC(C = 0.1)
S.fit(Xtr,ytr)
print S.score(Xte,yte)
print S.score(Xtr,ytr)
print getf1(S, Xte, yte)

print 'forest'
R = RandomForestClassifier()
R.fit(Xtr,ytr)
print R.score(Xte,yte)",baseline.py,ricsoncheng/sarcasm_machine,1
"
    def store(self, model, store_path):
        if not os.path.exists(store_path):
            os.makedirs(store_path)
        logger.info(""Storing model at %s"", store_path)
        return joblib.dump(model, os.path.join(store_path, 'model.pkg'))

    def train(self, feature_tensor, correct):
        logger.info(""Training model..."")
        squeezed = feature_tensor.squeeze(axis=1)
        clf = svm.SVC(kernel=self.kernel, C=self.c)
        model = clf.fit(squeezed, correct)
        if self.store_path:
            self.store(model, self.store_path)
        self.model = model
        logger.info(""Training session done"")

    def test(self, feature_tensor):
        logger.info(""Loading model..."")
        self.model = self.restore(self.store_path)",architectures/svm_baseline/model.py,jimmycallin/master-thesis,1
"def main(args):
    data = pd.read_csv(sys.stdin)
    normalize_data(data)

    ndcg = compute_ndcg_without_w(data);
    print('Current NDCG: {}, std: {}'.format(np.mean(ndcg), np.std(ndcg)))
    print()

    x, y = transform_data(data)

    clf = svm.LinearSVC(random_state=args.seed)
    cv = cross_validation.KFold(len(y), n_folds=5, shuffle=True, random_state=args.seed)

    # ""C"" stands for the regularizer constant.
    grid = {'C': np.power(10.0, np.arange(-5, 6))}
    gs = grid_search.GridSearchCV(clf, grid, scoring='accuracy', cv=cv)
    gs.fit(x, y)

    w = gs.best_estimator_.coef_[0]
    ndcg = compute_ndcg_for_w(data, w)",search/search_quality/scoring_model.py,gardster/omim,1
"y = iris.target

n_features = X.shape[1]

C = 1.0

# Create different classifiers. The logistic regression cannot do
# multiclass out of the box.
classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),
               'L2 logistic': LogisticRegression(C=C, penalty='l2'),
               'Linear SVC': SVC(kernel='linear', C=C, probability=True,
                                 random_state=0)}

n_classifiers = len(classifiers)

plt.figure(figsize=(3 * 2, n_classifiers * 2))
plt.subplots_adjust(bottom=.2, top=.95)

for index, (name, classifier) in enumerate(classifiers.items()):
    classifier.fit(X, y)",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/examples/classification/plot_classification_probability.py,RPGOne/Skynet,1
"    pointList = np.c_[np.random.uniform(-1,1.00,numberOfPoints),np.random.uniform(-1,1.00,numberOfPoints)]
    return pointList

def applyFunction(points):
    return np.sign(points[:,1]-points[:,0]+0.25*np.sin(np.pi * points[:,0]))

def doAssignment13():
    experiments = 1000
    gama = 1.5
    numPoints = 100
    clf = svm.SVC(C= np.inf , kernel=""rbf"", coef0=1, gamma=gama)
    Ein0 = 0
    for i in range(experiments):    
        X = getPoints(numPoints)
        y = applyFunction(X)
        clf.fit(X,y)
        #print(clf.score(X,y))
        if(1-clf.score(X,y)==0):
            #print(""here"")
            Ein0 += 1",Final/t18.py,pramodh-bn/learn-data-edx,1
"    
    mn = train_x.mean(0)
    st = train_x.std(0)
    
    #train_x -= mn; #train_x /= st
    #test_x -= mn; #test_x /= st
    
    train_y = np.squeeze( splits_for_training.loc[train_bcs].values )
    test_y  = np.squeeze( splits_for_training.loc[test_bcs].values )
    #from sklearn.svm import LinearSVC
    #model = sklearn.svm.LinearSVC(penalty=penalty,C=C, intercept_scaling=2.0, fit_intercept=True)
    model = sklearn.linear_model.LogisticRegression(penalty=penalty,C=C, intercept_scaling=1.0, fit_intercept=False, class_weight = ""balanced"")
    model.fit( train_x, train_y )
    
    predict_train = model.predict( train_x )
    predict_test  = model.predict( test_x )
    predict_proba  = model.predict_proba( test_x )
    predict_proba  = model.predict_log_proba( test_x )
    
    test_prob[subset_ids]          = np.exp(predict_proba)",tcga_encoder/models/analyses.py,tedmeeds/tcga_encoder,1
"
    def _start(self, dsk):
        self.ran = True


def test_visualize():
    pytest.importorskip('graphviz')

    X, y = make_classification(n_samples=100, n_classes=2, flip_y=.2,
                               random_state=0)
    clf = SVC(random_state=0)
    grid = {'C': [.1, .5, .9]}
    gs = dcv.GridSearchCV(clf, grid).fit(X, y)

    assert hasattr(gs, 'dask_graph_')

    with tmpdir() as d:
        gs.visualize(filename=os.path.join(d, 'mydask'))
        assert os.path.exists(os.path.join(d, 'mydask.png'))
",dask_searchcv/tests/test_model_selection.py,jcrist/dask-searchcv,1
"        elif label == negative_class:
            X_train.append(example)
            y_train.append(-1.0)
    X_train = np.array(X_train)
    y_train = np.array(y_train)
    return X_train, y_train


def train_svm(X, y):
    X_train, y_train = filter_data(X, y)
    model = svm.SVC()
    model.fit(X_train, y_train)
    print(model.score(X_train, y_train))
    return model


if __name__ == '__main__':
    X, y = load_cifar('data', norm=True)
    model = train_svm(X, y)
    X_test, y_test = load_cifar('data', 'test.data', norm=True)",examples/tutorial/train_sklearn_svm.py,dubeyabhi07/clipper,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",kbe/src/lib/python/Lib/test/test_email/test_email.py,MQQiang/kbengine,1
"import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

timer = time.time()

classifiers = [LogisticRegression(), LinearSVC(), KNeighborsClassifier(), 
               RandomForestClassifier(), SVC()]

XandY = np.loadtxt('train1000.csv', delimiter=',', skiprows=1)
np.random.shuffle(XandY)
X = XandY[:,1:]
Y = XandY[:,0]
nX = np.shape(X)[0]

XTrain = X[:0.7*nX,:]
YTrain = Y[:0.7*nX]",scikit-learn/classifiersTest.py,SAGridOps/SoftwareTests,1
"        modelF2 = modelF.replace(""_""+str(numIts),""_""+str(numIts+1))
        
        with open(modelF2,'wb') as f:
            cp = cPickle.Pickler(f)
            cp.dump(clf)
        
    if (not isfile(modelF)) & (not useSGD):
        data, labels = getExamples(mod,sampsize)
        
        print data.shape, labels.shape
        clf = LinearSVC()
        clf.fit(data,labels)
        
        with open(modelF,'wb') as f:
            cp = cPickle.Pickler(f)
            cp.dump(clf)
    elif not useSGD:
        print ""loading LinearSVC""
        with open(modelF,'rb') as f:
            clf = cPickle.load(f)",scripts/proteinSVM.py,jamesmf/tokenFeature,1
"precision, recall, f1, accuracy, support, fn, roc_auc = 0, 0, 0, 0, 0, 0, 0
colors = ['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange']

k = 10
kf = KFold(n_splits = k)

start = timer()
for train, test in kf.split(data_ANN):
	X_train, X_test = data_SVM[train, 0:-1], data_SVM[test, 0:-1]
	y_train, y_test = data_SVM[train, -1], data_SVM[test, -1]
	clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
	y_prob1 = clf1.predict_proba(X_test)[:,1]
	y_pred1 = clf1.predict(X_test)
	y_acc1 = accuracy_score(y_test, y_pred1)

	X_train, X_test = data_ANN[train, 0:-1], data_ANN[test, 0:-1]
	y_train, y_test = data_ANN[train, -1], data_ANN[test, -1]
	clf2 = MLPClassifier(solver='lbfgs', activation ='logistic', learning_rate='adaptive', hidden_layer_sizes = (5,2), random_state=1).fit(X_train, y_train)
	y_prob2 = clf2.predict_proba(X_test)[:,1]
	y_pred2 = clf2.predict(X_test)",K-Fold/WeightedAveraging.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS

    # label junk food as -1, the others as +1
    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)

    # split the dataset for model development and final evaluation
    train_data, test_data, target_train, target_test = train_test_split(
        data, target, test_size=.2, random_state=0)

    pipeline = Pipeline([('vect', CountVectorizer()),
                         ('svc', LinearSVC())])

    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
        'svc__loss': ('l1', 'l2')
    }

    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)",sklearn/feature_extraction/tests/test_text.py,loli/semisupervisedforests,1
"vectorizers = [
    CountVectorizer(ngram_range=(1, 2),
                    stop_words='english'),
   #CountVectorizer(ngram_range=(1, 3),
   #                stop_words='english'),
   #CountVectorizer(ngram_range=(1, 4),
   #                stop_words='english'),
]

classifiers = [
    #LinearSVC(loss='squared_hinge', penalty='l2', dual=False, tol=1e-4),
    #LogisticRegression(penalty='l2', solver='liblinear', dual=False, tol=1e-4)
    SVC(C=1.5, tol=1e-4, probability=True)
]

pipelines = []


splitter = ShuffleSplit(csv.shape[0], n_iter=10, test_size=0.2)
for v in vectorizers:",classify.py,mrshu/tosdr-topic-suggester,1
"		for index in range(len(test_set)):
			document_sum = np.zeros(self.D)
			for word_index in range(len(test_set[index][0])):
				word = test_set[index][0][word_index]
				i = self.all_words[word]
				embedding = self.word_embedding[i]
				document_sum = np.add(document_sum, embedding)
			document_average = np.divide(document_sum, len(test_set[index][0]))
			self.test_documents.append(document_average)
			self.test_actual_labels.append(train_set[index][1])
		clf = svm.SVC()
		clf.fit(self.train_documents, self.train_labels)
		self.test_predicted_labels = clf.predict(self.test_documents)
		correct = 0
		for i in range(len(self.test_predicted_labels)):
			if self.test_predicted_labels[i] == self.test_actual_labels[i]:
				correct = correct + 1
		accuracy = (correct)/float(len(self.test_predicted_labels)) * 100.0
		print 'Accuracy : ',accuracy
",word2graph2vec/test.py,shashankg7/word2graph2vec,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_30_2014_server_2.py,magic2du/contact_matrix,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = RSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 5.0,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/sum_constraints_cmaes_cmaesrsvc/setup.py,jpzk/evopy,1
"    def __init__(self):
        self._args = None
        self._training_images = None
        self._dimensions = None
        self._score = None
        self._svc = None

    def train(self, kernel=None):
        images, labels = self._extract_images_and_labels()
        k = self.args.kernel if kernel is None else kernel
        self._svc = SVC(kernel=k)
        self._svc.fit(images, labels)
        self._score = cross_val_score(self.svc, images, labels, cv=30)

    @property
    def svc(self):
        return self._svc

    @property
    def score(self):",svm.py,jimrybarski/fylmtools,1
"            pair_list = pair_list[0:top]
        return pair_list, feat_list

    toclass_features = [annot_to_class_feats2(aid, aid2_nid, top=5) for aid in aids]
    aidnid_pairs = np.vstack(ut.get_list_column(toclass_features, 0))
    feat_list = np.vstack(ut.get_list_column(toclass_features, 1))
    score_list = feat_list.T[0:1].T
    lbl_list = [aid2_nid[aid] == nid for aid, nid in aidnid_pairs]

    from sklearn import svm
    #clf1 = svm.LinearSVC()
    print('Learning classifiers')

    clf3 = svm.SVC(probability=True)
    clf3.fit(feat_list, lbl_list)
    #prob_true, prob_false = clf3.predict_proba(feat_list).T

    clf1 = svm.LinearSVC()
    clf1.fit(score_list, lbl_list)
",ibeis/algo/hots/demobayes.py,SU-ECE-17-7/ibeis,1
"from sklearn import cross_validation
from math import sqrt
import timeit

start = timeit.default_timer()

data=np.loadtxt('D:/uw course/CSS 590C big data/project/factorized2.txt')
y=data[:,0]
x=data[:,1:]

clf=svm.SVC()'''or svm.LinearSVC() for linear kernel svm'''
clf.fit(x,y)
cv=cross_validation.cross_val_score(clf,x,y,scoring=""mean_squared_error"",cv=10)
rms=sqrt(abs(sum(cv)/10))
print(rms)

elapsed = (timeit.default_timer() - start)

",svmcv-classifier.py,evazyin/Capstoneproject,1
"# Use this movement as the baseline features. Yes it is brutal
# and shouldn't work
features = np.array(movement).reshape(len(movement), -1)


# Do a simple SVM on these features
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score, ShuffleSplit
from sklearn.dummy import DummyClassifier
svm = SVC(kernel=""rbf"", C=1.)
logreg = LogisticRegression(C=1.)

# Watch out with cross validation!
# Our first approach will not stratify across sites
# But this absolutely needs to be tested

# We also have slightly unbalanced classes

cv = ShuffleSplit(len(features), n_iter=100, test_size=0.1)",nilearn_private/baseline.py,AlexandreAbraham/movements,1
"    clf = None
    print('---------- Start training. ----------')
    if modeltype == 'lr':    ## Logistic Regression
        print('# Logistic Regression model')
        tuned_params = [{'C':np.logspace(-5, -4, 20),},]
        model = LogisticRegression()
    elif modeltype == 'rbf': ## SVM RBF kernel
        print('# SVM (RBF kernel) model')
        tuned_params = [{'kernel':['rbf'], 'C':np.logspace(0, 2, 20),
                         'gamma':np.logspace(-5, -3, 10)},]
        model = SVC(probability=False)
    elif modeltype == 'rf':  ## Random Forest
        print('# Random Forest model')
        tuned_params = [{'n_estimators': range(100, 200, 10),
                         'max_features': ['auto', 'log2']}]  ## auto == sqrt
        model = RandomForestClassifier(oob_score=True, n_jobs=jobs)
    else:
        print('model type: [lr|rbf|rf]')
        sys.exit(-1)
        ",classifier/bin/train_model.py,wellflat/cat-fancier,1
"from sklearn.svm import SVC
from bayes_opt import BayesianOptimization

# Load data set and target values
data, target = make_classification(n_samples=2500,
                                   n_features=45,
                                   n_informative=12,
                                   n_redundant=7)

def svccv(C, gamma):
    return cross_val_score(SVC(C=C, gamma=gamma, random_state=2),
                           data, target, 'f1', cv=5).mean()

def rfccv(n_estimators, min_samples_split, max_features):
    return cross_val_score(RFC(n_estimators=int(n_estimators),
                               min_samples_split=int(min_samples_split),
                               max_features=min(max_features, 0.999),
                               random_state=2),
                           data, target, 'f1', cv=5).mean()
",examples/sklearn_example.py,mpearmain/BayesBoost,1
"    return out

def Knn(train_data_x, train_data_y, test_data):
    knn = neighbors.KNeighborsClassifier()
    knn.fit(train_data_x, train_data_y)
    out = knn.predict(test_data).astype(int)
    # WriteFile('./result/Knn.csv', out)
    return out

def Svm(train_data_x, train_data_y, test_data, mode):
    svm_model = svm.SVC(kernel = mode)
    svm_model.fit(train_data_x, train_data_y)
    out = svm_model.predict(test_data).astype(int)
    # WriteFile('./result/Svm{}.csv'.format(mode), out)
    return out

if __name__ == '__main__':
    # get_title('Ahlin, Mrs. Johan (Johanna Persdotter Larsson)')
    test_data('./data/train.csv')
    print 'training...........'",predict.py,kdqzzxxcc/TitanicPredict,1
"    
    # create 'promotional video':
    #create_video_cv(os.path.join(config.experiment_output, 'frames_hmm_nomask'), max_frame = 3500)


def detect():

    positive_descriptors = get_descriptors(os.path.join(config.experiment_output, 'positive_examples'))
    negative_descriptors = get_descriptors(os.path.join(config.experiment_output, 'negative_examples'))
    from sklearn import svm
    clf = svm.SVC()
    X = np.array(positive_descriptors + negative_descriptors)
    y = np.hstack((np.ones(len(positive_descriptors)), np.zeros(len(negative_descriptors))))
    clf.fit(X, y) 
    
    positive_test = get_descriptors(os.path.join(config.experiment_output, 'positive_test'))
    clf.predict(positive_test)
    

def get_descriptors(image_dir):",video_analysis/src/process_video_data.py,hasadna/OpenTrain,1
"        self.dev_y_txt = argparams['dev_y_txt']
        self.model_folder = argparams['model_folder']
        self.prefix = argparams['prefix']
        self.task_name = argparams['task_name']  # 'oracle' or 'pipeline'
        self.id2token = argparams['id2token']

    def train(self, verbose=True):
        assert self.train_X is not None and self.train_y_vecBin is not None, 'train_X and train_y_vecBin are required.'
        assert self.dev_X is not None and self.dev_y_vecBin is not None, 'dev_X and dev_y_vecBin are required.'
        print('\ttraining ...')
        self.model = OneVsRestClassifier(SVC(kernel='linear', probability=True, verbose=verbose))
        self.model.fit(self.train_X, self.train_y_vecBin)
        probs = self.model.predict_proba(self.dev_X)
        # evaluation for user intent
        precision, recall, fscore, accuracy_frame, self.threshold = eval_intentPredict(probs, self.dev_y_vecBin)
        print('\teval_dev: precision={:.4f}, recall={:.4f}, fscore={:.4f}, accuracy_frame={:.4f}, threshold={:.4f}'.format(precision, recall, fscore, accuracy_frame, self.threshold))
        # write prediction results
        dev_txt = getActPred(probs, self.threshold, self.id2token)
        dev_pred_fname = '{}/{}_{}dev.pred'.format(self.model_folder, self.task_name, self.prefix)
        writeTxt(dev_txt, dev_pred_fname, prefix=self.prefix, delimiter=';')",BaselineModel.py,XuesongYang/end2end_dialog,1
"from sklearn.linear_model import LogisticRegression



names = [""Nearest Neighbors"", ""Linear SVM"", ""Gaussian SVM"", ""Polynomial SVM"", ""Decision Tree"",
		 ""Random Forest"", ""AdaBoost Classifier"", ""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(n_neighbors=15, weights='distance'),
	SVC(kernel=""linear"", C=3.4),
	SVC(kernel=""rbf"", C=3.4, gamma=0.1),
	SVC(kernel=""poly"", C=3.4, degree=2, gamma=0.1),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]

",UMKL/classifiers.py,akhilpm/Masters-Project,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIFS.mifs(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_MIFS.py,jundongl/scikit-feature,1
"    # OS = SMOTE(ratio=ratio, kind='regular')
    # m, targets = OS.fit_transform(m, targets)
    print ""Sum of targets: "", sum(targets)
    print ""Instances x features: "", m.shape

    skf = cross_validation.StratifiedKFold(targets, 5)
    # print ""Running stratified 5-fold CV""

    params_space = {}
    # clf = svm.SVC(gamma=0.001, C=100., class_weight='balanced')
    clf = svm.SVC(gamma=0.001, C=1000., probability=True)
    # clf = tree.DecisionTreeClassifier()
    # clf = clf.fit(m, targets)
    # print clf.feature_importances_

    # params_space = {'kernel': ['linear', 'poly', 'rbf'],
    #                 ""C"": [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4],
    #                 'gamma': np.logspace(-2, 2, 5)}

    # params_space = {""C"": [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]}",docaligner/train_classifier.py,ModernMT/DataCollection,1
"def Analysis():
  test_size = 1
  invest_amount = 10000 # dollars
  total_invests = 0
  if_market = 0
  if_strat = 0

  X, y, Z = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    invest_return = 0
    market_return = 0
    if clf.predict(X[-x])[0] == y[-x]: # test data
      correct_count += 1
",p26.py,cleesmith/sentdex_scikit_machine_learning_tutorial_for_investing,1
"    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg10_ohsumed_ver.py,lioritan/Thesis,1
"# @param data The data array object
# @param modelType The type of model to run (Linear SVM, Logistic Regression)
# @param outDir The output directory
def runModel(data, modelType, outDir):
    #store the means for our ROC curve
    meanTPRate = 0.0
    meanFPRate = np.linspace(0, 1, 100)

    #initialize our classifier
    classifier = Pipeline([
        ('feature_selection', LinearSVC()),
        ('classification', svm.SVC(kernel='linear', probability=True, random_state=0))
    ])

    #see if we wanted logistic regression
    if(modelType == 'Logistic Regression'):
        classifier = Pipeline([
            ('feature_selection', LinearSVC()),
            ('classification', linear_model.LogisticRegression(C=1000, random_state=0))
        ])",machine-learning/TRI/matrix_analyzer.py,dyermd/legos,1
"	kpca_train = arc_cosine(trainX[0:1000], trainX[0:1000])
	kpca.fit(kpca_train)

	kernel_train = arc_cosine(trainX, trainX[0:1000])
	kernel_test = arc_cosine(testX, trainX[0:1000])

	trainX_kpca = kpca.transform(kernel_train)
	testX_kpca = kpca.transform(kernel_test)
	print testX_kpca.shape

	#clf = svm.SVC(kernel=kernel.arc_cosine)
	clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	clf.fit(trainX_kpca, trainY)

	pred = clf.predict(testX_kpca)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",kpcaWithTreeFS/mnistBackImage/mnistIMAGE_withoutFS.py,akhilpm/Masters-Project,1
"    polarities = [
        [""Negative"", 0],
        [""Positive"", 1]
    ]
    crossed = [
        [""Negative -> Positive"", 0],
        [""Positive -> Negative"", 1],
    ]
    path = ""inf_spec_ner_liwc_speciteller.csv""
    rf = RandomForestClassifier(n_estimators=200, criterion='entropy')
    svc = LinearSVC(penalty=""l1"", dual=False, tol=1e-3)
    lr = LogisticRegression()
    for feature_set in feature_sets:
        print(feature_set[0])
        for polarity in polarities:
            print(polarity[0])
            X, C = get_data(path, feature_set[1], polarity[1])
            # run k-fold cv
            print(""--------- RF --------"")
            manual_cross_val(k, rf, X, C)",ml/mlapp.py,ben-aaron188/information_specificity,1
"clfs_graph = {1.5:make_pipeline(
    make_union(VotingClassifier([(""est"", GradientBoostingClassifier(max_depth=1, 
                                                                    max_features=0.2, 
                                                                    min_samples_leaf=5, 
                                                                    min_samples_split=2, 
                                                                    n_estimators=100, 
                                                                    subsample=0.45))]), 
                                                FunctionTransformer(copy)),
                                                LogisticRegression(C=0.5)),
        2.0:LogisticRegression(C=0.1, dual=False),
        2.5:LinearSVC(C=0.001, loss=""hinge"", penalty=""l2"", tol=0.1),
        3.0:make_pipeline(make_union(
                                    Normalizer(norm=""max""),
                                    FunctionTransformer(copy)),
                                    KNeighborsClassifier(n_neighbors=95, p=1)),
        3.5:LogisticRegression(C=5.0),
        4.0:make_pipeline(make_union(VotingClassifier([(""est"", BernoulliNB(alpha=100.0, 
                                                                                          fit_prior=True))]), 
                                                FunctionTransformer(copy)),
                                                LogisticRegression(C=0.5, penalty=""l2"")),",individual classification.py,adowaconan/Spindle_by_Graphical_Features,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",App/Lib/email/test/test_email_renamed.py,2uller/LotF,1
"        3 use best param and best clf to find recall for 100 percent precision
    """"""
    utils.print_success(""Find Recall for best Precision for each tag"")
    train = utils.abs_path_file(train)
    test = utils.abs_path_file(test)
    train_features, train_groundtruths = read_file(train)
    test_features, test_groundtruths = read_file(test)
    classifiers = {
        # ""RandomForest"": RandomForestClassifier(),#n_estimators=5
        ""DecisionTree"":DecisionTreeClassifier()#,#max_depth=10
        # ""SVM"":SVC(kernel=""linear"", C=0.0205),
        # ""ExtraTreesClassifier"":ExtraTreesClassifier(n_estimators=5, criterion=""entropy"", max_features=""log2"", max_depth=9),
        # ""LogisticRegression"":LogisticRegression()
    }
    tags = list(set(test_groundtruths))
    nb_tag = len(tags)
    step = 0.01
    # for index, tag in enumerate([""i""]):
    for index, tag in enumerate(tags):
        utils.print_success(""Tag "" + tag)",src/classify.py,ybayle/ISMIR2017,1
"selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X, y)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
pl.bar(X_indices - .45, scores, width=.2,
        label=r'Univariate score ($-Log(p_{value})$)',
        color='g')

###############################################################################
# Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

svm_weights = (clf.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()

pl.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',
        color='r')

clf_selected = svm.SVC(kernel='linear')",python/sklearn/examples/plot_feature_selection.py,seckcoder/lang-learn,1
"X_test = preprocesser.transform(X_test)

if dimensions != None:
  pca = PCA(n_components=dimensions)
  pca.fit(X_train)
  X_train = pca.transform(X_train)
  X_test = pca.transform(X_test)


# Create and fit the svm classifier
svc = svm.SVC(kernel='rbf')
svc.fit(X_train, y_train)
y_test_pred = svc.predict(X_test)
score = np.mean(y_test == y_test_pred)
print score
print confusion_matrix(y_test, y_test_pred)
",kaggle/default-of-credit-card/default-of-credit-card-svm.py,jajoe/machine_learning,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't instantiate with non-transformers on the way
    # Note that NoTrans implements fit, but not transform
    assert_raises_regex(TypeError,
                        'implement fit and transform or sample',
                        Pipeline, [('t', NoTrans()), ('svc', clf)])
",imblearn/tests/test_pipeline.py,glemaitre/imbalanced-learn,1
"# Load data set and target values
data, target = make_classification(
    n_samples=1000,
    n_features=45,
    n_informative=12,
    n_redundant=7
)

def svccv(gamma):
    val = cross_val_score(
        SVC(gamma=gamma, random_state=0),
        data, target, 'f1', cv=2
    ).mean()

    return val

def rfccv(n_estimators, max_depth):
    val = cross_val_score(
        RFC(n_estimators=int(n_estimators),
            max_depth=int(max_depth),",bayes_opt/bayes_opt_sklearn.py,vsmolyakov/opt,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CIFE.cife(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_CIFE.py,jundongl/scikit-feature,1
"        """"""
        Spectral Angle Mapper eqn between test and reference vectors T & R respectively
        """"""
        return np.arccos(np.dot(T,R)/(np.linalg.norm(T)*np.linalg.norm(R)))

    def trainSVM(self, kernel=None):
        if kernel is None:
            kernel = 'linear'
        elif kernel.lower() == 'sam':
            kernel = self.kernelSAM
        clf = svm.SVC(kernel=kernel(self.trainData, self.))
        clf.fit()




",Python/trainData.py,Crobisaur/HyperSpec,1
"                y_train (np.array) - label matrix
    OUTPUT:     nmb (MultinomialNB obj) - model trained on X_train, y_train
                svm (LinearSVC obj) - model trained on X_train, y_train
                ssvm (PyStruct chainCRF object) - trained Chain CRF model
    '''
    # Multinomial Naive Bayes Classifier:
    nmb = MultinomialNB()
    nmb.fit(np.vstack(X_train), np.hstack(y_train))

    # Support Vector Machine Classifier
    svm = LinearSVC(dual=False, C=.1)
    svm.fit(np.vstack(X_train), np.hstack(y_train))

    # Chain Conditional Random Field Classifier
    model = ChainCRF()
    ssvm = FrankWolfeSSVM(model=model, C=0.5, max_iter=15)
    ssvm.fit(X_train, y_train)
    return nmb, svm, ssvm

",Modeling/build_model.py,jscottcronin/PinkSlipper,1
"        'C': [0.01, .1, 1, 10, 100, 1000],
        'class_weight': [
            {0: 0.01}, {1: 1}, {1: 2}, {1: 10}, {1: 50}, 'balanced']},
        score='recall_weighted', iid=True, bagged=False, svm_results=True):
    """"""Build an SVM and return its scoring metrics
    """"""
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    # Find the Hyperparameters
    clf = search_type(SVC(C=1), parameter_space, cv=10,
                      scoring=score, iid=iid)

    # Build the SVM
    clf.fit(X_train, y_train)
    print(""Hyperparameters found:"")
    print(clf.best_params_)

    # Make the predictions
    y_pred = clf.predict(X_test)",winfault.py,lkev/wt-fdd,1
"    config_d = configParserWrapper.load_settings(open(args.config,'r'))

    penalty_names = config_d['SVM']['penalty_list'][::2]
    penalty_values = tuple( float(k) for k in config_d['SVM']['penalty_list'][1::2])

    for penalty_name, penalty_value in itertools.izip(penalty_names,penalty_values):
        if args.v:
            print '%s %s' % ('linear', penalty_name)

        if config_d['SVM']['kernel'] == 'linear':
            clf = svm.LinearSVC(C=penalty_value,
                                loss='l1')
            clf.fit(X,y)
        else:
            import pdb; pdb.set_trace()



        coef = clf.coef_.reshape(clf.coef_.size)
        y_hat = np.dot(X,coef) + clf.intercept_[0]",local/CTrainComponentSVMs_Bernoulli.py,markstoehr/phoneclassification,1
"        self.assertIs(df.grid_search.ParameterGrid, gs.ParameterGrid)
        self.assertIs(df.grid_search.ParameterSampler, gs.ParameterSampler)
        self.assertIs(df.grid_search.RandomizedSearchCV, gs.RandomizedSearchCV)

    def test_grid_search(self):
        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                             'C': [1, 10, 100]},
                            {'kernel': ['linear'], 'C': [1, 10, 100]}]

        df = expd.ModelFrame(datasets.load_digits())
        cv = df.grid_search.GridSearchCV(df.svm.SVC(C=1), tuned_parameters, cv=5, scoring='precision')

        with tm.RNGContext(1):
            df.fit(cv)

        result = df.grid_search.describe(cv)
        expected = pd.DataFrame({'mean': [0.974108, 0.951416, 0.975372, 0.962534,  0.975372,
                                          0.964695, 0.951811, 0.951811, 0.951811],
                                 'std': [0.01313946, 0.02000999, 0.01128049, 0.0202183, 0.01128049,
                                         0.0166863, 0.01840967, 0.01840967, 0.01840967],",expandas/skaccessors/test/test_grid_search.py,sinhrks/expandas,1
"
        #Store dictionary in a file
        joblib.dump(dictionary, path + ""_Genre_Dictionary"")
        
        #Store doc ids of trained data in a file
        myfile = open(r'doc_ids.pkl', 'wb')
        pickle.dump(doc_ids,myfile)
        myfile.close()

        #Initialize training models
        mod_1 = SVC(kernel='linear', C=1, gamma=1)
        mod_2 = LogisticRegression()
        mod_3 = GaussianNB()
        mod_4 = MultinomialNB()
        mod_5 = BernoulliNB()

        #Ensemble classifiers
        mod_6 = RandomForestClassifier(n_estimators=50)
        mod_7 = BaggingClassifier(mod_2, n_estimators=50)
        mod_8 = GradientBoostingClassifier(loss='deviance', n_estimators=100)",informationRetrival/classification/classify.py,BhavyaLight/information-retrival-search-engine,1
"    def rfPredict(self):
        # unique features from the entire cds df, convert to sparse mat
        uniqFeatures = self.cds[self.colNames].groupby(self.colNames).size().reset_index(name=""features"")[self.colNames]
        uniqFeaturesArr = sparse.csr_matrix(np.array(pd.get_dummies(uniqFeatures)))
        sltCdsX = self.selector.transform(uniqFeaturesArr)
        uniqFeatures[""asite""] = self.reducedClf.predict(sltCdsX)
        self.cds = pd.merge(self.cds, uniqFeatures, how=""left"", on=self.colNames)

    def svmFit(self):
        # grid search
        self.clf = svm.SVC()
        paramGrid = [{'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]}]
        cpus = 10 if multiprocessing.cpu_count() > 10 else max(1, multiprocessing.cpu_count() - 1)
        clfGs = GridSearchCV(estimator=self.clf, param_grid=paramGrid, n_jobs=cpus)
        clfGs.fit(self.X, self.y)
        print(""[result]\t best estimator parameters: c="", clfGs.best_estimator_.C, flush=True)
        # model fitting and cross validation
        self.clf = svm.SVC(C=clfGs.best_estimator_.C)
        scores = cross_val_score(self.clf, self.X, self.y, cv=10)
        print(""[result]\tAccuracy: %0.3f (+/- %0.3f)"" % (scores.mean(), scores.std() * 2), flush=True)",scikit_ribo/asite_predict.py,hanfang/scikit-ribo,1
"mms = MinMaxScaler()
rs = RobustScaler()

# define features selection for pipe
kbest = SelectKBest(k=9)
pbest = SelectPercentile(percentile=20)
pca = PCA(n_components=5)

# define clfs for pipe nb, svc, knn, rf and ada
nb = GaussianNB()
svc = SVC()
knn = KNeighborsClassifier()
rf = RandomForestClassifier()
ada = AdaBoostClassifier()

# pipe for gs1
pipe_gs = Pipeline([
    (""scaler"", None),
    (""feature_selection"", None),
    (""clf"", nb)",p5/final_project/poi_id.py,stefanbuenten/nanodegree,1
"
    # splits the dataset into training and validations ets with ratio 1/7 ratio
    # for training set and 6/7 ratio for validation set
    x_train, x_test, y_train, y_test = \
        train_test_split(inputs, targets, test_size=0.5)

    # trains an SVM model with default parameters and rbf kernel. in this stage
    # we're not worried about totally optimiozing the SVM model. default
    # settings work fine. once we optimize filter parameters, SVM grid search
    # will come to place to select the best model for chosen filter parameters
    model = svm.SVC(kernel='rbf', cache_size=1000)
    model.fit(x_train, y_train)

    # test the trained model on the test data and report back precision, recall
    # and f1 scores
    predicted = model.predict(x_test)

    score = precision_recall_fscore_support(y_test, predicted, average='macro')
    return score[:3]
",filteropt.py,dominiktomicevic/pedestrian,1
"    if output:
        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',
            Instances=n_instances, Results=metrics_results)

if __name__ == '__main__':
    from nltk.classify import NaiveBayesClassifier, MaxentClassifier
    from nltk.classify.scikitlearn import SklearnClassifier
    from sklearn.svm import LinearSVC

    naive_bayes = NaiveBayesClassifier.train
    svm = SklearnClassifier(LinearSVC()).train
    maxent = MaxentClassifier.train

    demo_tweets(naive_bayes)
    # demo_movie_reviews(svm)
    # demo_subjectivity(svm)
    # demo_sent_subjectivity(""she's an artist , but hasn't picked up a brush in a year . "")
    # demo_liu_hu_lexicon(""This movie was actually neither that funny, nor super witty."", plot=True)
    # demo_vader_instance(""This movie was actually neither that funny, nor super witty."")
    # demo_vader_tweets()",venv/lib/python2.7/site-packages/nltk/sentiment/util.py,MyRookie/SentimentAnalyse,1
"X = df[:, 0:256]
y = df[:, 256:266].argmax(1)

index = arange(len(y))
random.shuffle(index)

N = 1000
train_index = index[:N]
test_index  = index[N:]

clf = SVC()
clf.fit(X[train_index], y[train_index])
",src/python-lesson/semeion.py,minhouse/python-lesson,1
"        return X

def evaluate(cat, colds, txt_train, txt_test, y_train, y_test):
    if not os.path.exists(conf.BOCID_CLUSTFILE):
        print('Word-do-cluster-ID mapping file ""%s"" not found.' %
            conf.BOCID_CLUSTFILE)
        print('Did you run train_word2vec.py and bocid_clustering.py?')
        sys.exit(1)

    fe = BoCID_feature_extractor(conf.BOCID_CLUSTFILE)
    predictor = SVC(
        kernel=conf.SVM_KERNEL,
        class_weight=conf.SVM_CLWEIGHT,
        C=conf.SVM_C,
        random_state=conf.SEED,
    )
    fe.fit(txt_train)
    X = fe.transform(txt_train)
    predictor.fit(X, y_train)
    X_test = fe.transform(txt_test)",experiments/src/evaluate_bocid.py,OFAI/million-post-corpus,1
"import sys

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib


class EvaluateSentimentVec(object):
    def __init__(self):
        self.lin_clf_reg = linear_model.LogisticRegression()
        self.lin_clf_svm = svm.LinearSVC()
        self.tokenizer = RegexpTokenizer(r""[\w'-]+"")
        self.word2vec = {}
        self.dim = 0
        self.documentPos = []
        self.documentNeg = []

    def loadVector(self, pathVec, pathWord):
        print 'loading vector...'
        vec = np.loadtxt(open(pathVec, ""rb""), delimiter="","")  # , skiprows=1, usecols=tuple(range(1, 301)))",evaluate_sentiment_vec_csv.py,sidenver/ConstituentRetrofit,1
"        else:
            pca = PCA()
        x_train = pca.fit_transform(x_train, y_train)
        print x_train.shape, 'is the new dimensionality'
        print 'transforming...'
        pickle.dump(pca, open(PCA_PICKLE,'wb'))

    if rbf: 
        clstype = 'rbf'
        print 'Training rbf. This will take a while'
        cls = svm.SVC(kernel=clstype, C=20, gamma=0.001, 
                      cache_size=100000., probability=False) #<----
        #     cls = svm.SVC(kernel=clstype, C=20, gamma=0.001, cache_size=100000., probability=True)
        print 'fitting the classifier'
        cls.fit(x_train, y_train)
        print 'saving %s to disk' % clstype
        joblib.dump(cls, 'rbf-cls')

    if logistic:
        cls = LogisticRegression(C=1000, intercept_scaling=100)",classify.py,zmr/namsel,1
"from sklearn.learning_curve import validation_curve

from sklearn_evaluation import plot

digits = load_digits()
X, y = digits.data, digits.target

param_range = np.logspace(-6, -1, 5)
param_name = ""gamma""
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name=param_name,
    param_range=param_range,
    cv=5,
    scoring=""accuracy"",
    n_jobs=1)

plot.validation_curve(train_scores, test_scores, param_range, param_name,
                      semilogx=True)
plt.show()
",examples/validation_curve.py,edublancas/sklearn-evaluation,1
"            return ensemble.RandomForestClassifier(n_estimators=parameters['n_estimators'],
                                               max_features=parameters['max_features'],
                                               criterion=parameters['criterion'])

        return ensemble.RandomForestClassifier(n_estimators=parameters['n_estimators'],
                                               max_depth=parameters['depth'],
                                               max_features=parameters['max_features'],
                                               criterion=parameters['criterion'])

    elif model_name == 'SVM':
        return svm.SVC(C=parameters['C_reg'], kernel=parameters['kernel'])
        #return svm.NuSVC(kernel=parameters['kernel'])

    elif model_name == 'LogisticRegression':
        return linear_model.LogisticRegression(C=parameters['C_reg'])

    elif model_name == 'AdaBoost':
        return ensemble.AdaBoostClassifier(learning_rate=parameters['learning_rate'])
    else:
        raise ConfigError(""Unsupported model {}"".format(model_name))",blight_risk_prediction/model.py,dssg/cincinnati2015-public,1
"
from .. import logging, pd, np, time, pickle


logger = logging.getLogger(__name__)


class Brain(object):

    def __init__(self):
        self._lobes = {'svc': svm.SVC(gamma=0.001),
                       'sgd': SGDClassifier(),
                       'lsvc': svm.LinearSVC(),
                       'kn': KNeighborsClassifier(n_neighbors=3),
                       'rf': RandomForestClassifier(n_estimators=10, random_state=123),
                       'kmeans': KMeans(n_clusters=3, random_state=1)
                       }
        self.votinglobe = VotingClassifier(
            estimators=[(lobe, self._lobes[lobe]) for lobe in self._lobes],
            voting='hard',",marconi/tools/brain/__init__.py,s4w3d0ff/marconibot,1
"    features = extract_features(data)
    scale = MinMaxScaler()
    scale_feat = scale.fit_transform(features)
    return scale_feat

print ""scaling train feats""
train_feats = scale_features(X_train)
print "" scaling test feats""
test_feats = scale_features(X_test)

svc = SVC(cache_size=60000)

print ""Fitting svm""
svc.fit(train_feats, y_train)

print""Making predictions""
pred = svc.predict(test_feats)


def predictions(pred, y_test):",models/activ_cnn.py,ishank26/Kutils,1
"    print x.shape

    i = int(y.shape[0] * 0.7)
    train_x = x[:i]
    train_y = y[:i]
    test_x = x[i:]
    test_y = y[i:]

    if 1: # training

        clf = svm.SVC(kernel='rbf')
        print 'training...'
        clf.fit(train_x, train_y)
        joblib.dump(clf, 'clf.pkl')
    
    if 1: # testing
        clf = joblib.load('clf.pkl')
        res = clf.predict(test_x)
        print '%d / %d ' % ( (test_y != res).sum(), test_y.shape[0])",myo/finger_taps/skl.py,PiscesDream/Ideas,1
"                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            svm_lock.acquire()
            svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)
            svm_lock.release()
            print(""successfully trained svm {}"".format(svm))                    

    def processFrame(self, dataURL, name):
        global images
        global people
        global svm
        global training
        global svm_lock",server/openface-server/cloudlet-demo-openface-server.py,Jamesjue/FaceSwap-server,1
"    ressort_list = ''
    list_featurenames = set()

    def __init__(self, host, user, password, db, date):
        Learning.database = Database(host, user, password, db)
        Learning._lemmatizer = PatternParserLemmatizer()
        Learning.article_vector_hm = {}
        Learning.date = date
        Learning.ressort_list = Learning.database.get_ressort_list()
        #load data and learn model vor svm
        Learning.clf = svm.SVC(kernel='linear', C=0.1, probability=True)
        training_features = pickle.load(open(""resources/training_features.p"", ""rb""))
        training_annotations = pickle.load(open(""resources/training_features_annotations.p"", ""rb""))
        vec = DictVectorizer()
        for  s in training_features:
            for key in s:
                Learning.list_featurenames.add(key)
        feature_vectorized = vec.fit_transform(training_features)
        X = np.array(feature_vectorized.toarray())
        y = np.array(training_annotations)",Service/learning.py,swalter2/PersonalizationService,1
"            print ""=""*80

        if self.plot:
            self.plot_alpha_results(results)
        return results

    def build_classifiers(self):
        classifiers = [
            (""NaiveBayes"", NaiveBayesClassifier(alpha=0.3)),
            (""kNN"", KNeighborsClassifier()),
            (""Linear SVM"", LinearSVC()),
            (""Logistic\nRegression"", LogisticRegression())
        ]
        return classifiers

    def benchmark_experiment(self, dataset):
        if self.logging:
            print ""Running classifier benchmark experiment...""

        classifiers = self.build_classifiers()",benchmark.py,bocharov-ivan/naive,1
"    print( ""Avg. iterations: "" + str(np.mean(interations)) + "" : Avg. error probability: "" + str(np.mean(probability)) + "" scikit "",  str(np.mean(k)), "" avg "", np.mean(p), "" machine "", np.mean(myP), "" pc "", np.mean(pc))
    #pylab.hist(interations)
    #pylab.show()

# Runs one trial based on the number of test points desired and an iteration limit to cap run time.
# If showChart is set to True, this function with also return a chart of the points, target function and hypothesis.
# Returns the number of iterations perceptron took to converge, final weights, and the error probability.
def runTrial(numberOfTestPoints, iterationLimit, showChart = False):
    x1, y1, x2, y2, points = generatePoints(numberOfTestPoints)
    pclf = Perceptron()
    clf = SVC(C = 1000, kernel = 'linear')  
    sample = np.array(points)
    X = np.c_[sample[:,1], sample[:,2]]
    y = sample[:,3]
        #print(y)
    pclf.fit(X,y)
    clf.fit(X,y)
    
    iterations, w = train(points, iterationLimit)
    #print(""weights "", w)",Week 7/perceptron.py,pramodh-bn/learn-data-edx,1
"			x[j*100:(j*100)+100] = sp.y[:, 0]
		
		y = sp2.p * x[sp2.syn_map]
		w = np.zeros((nfeat, ms))
		for j in xrange(nfeat):
			a = y[sp2.syn_map == j]
			w[j][:a.shape[0]] = a
		te_x2[i] = np.mean(w, 1)
	
	# Classify
	clf = LinearSVC(random_state=123456789)
	clf.fit(tr_x2, tr_y)
	print 'SVM Accuracy : {0:2.2f} %'.format(clf.score(te_x2, te_y) * 100)

def execute(sp, tr, te):
	# Trains
	sp.fit(tr)
	
	# Test
	tr_x = sp.predict(tr)",dev/mnist.py,tehtechguy/mHTM,1
"  # DataToVector = NegAndPosTemplatesMagToVec(posTemplates, negTemplates)
  # DataToVector = CloseToOriginal()
  # DataToVector = ThreePartFeaturizer()
  DataToVector = WindowFeaturizer()
  # DataToVector = TestTemplateDifferencesMagToVec(posTemplates)

  trainX, trainY = GenerateData(runDataList, DataToVector)

  # print trainX

  # clf = sklearn.svm.LinearSVC()
  clf = sklearn.svm.SVC(kernel='linear')
  # clf = sklearn.linear_model.LogisticRegression()
  clf.fit(trainX, trainY)
  print clf.coef_

  detector = MLDetector()
  detector.clf = clf
  detector.MagnetToVectorObj = DataToVector
",data/test_detect.py,dodger487/MIST,1
"@ex.capture
def sample_data(X, y, sample_size, _rnd):
    indices = np.arange(X.shape[0])
    choice_indices = _rnd.choice(indices, sample_size)
    return X[choice_indices, :], y[choice_indices]


@ex.capture
def get_classifier(classifier):
    if classifier == 'svc':
        return LinearSVC()
    elif classifier == 'softmax':
        return SoftmaxClassifier()
    elif classifier == 'xgb':
        return xgb.XGBClassifier()
    else:
        return None


@ex.automain",sacred/mnist.py,shaform/experiments,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0829_2015_pre_activation_rectifier_didnotwork.py,magic2du/contact_matrix,1
"
        #read it and make machine learning on it

        data = pd.read_csv(path_to_file+'training_set_n%d.csv' %N)

        X = data.values[:,2:(N+2)] #x variables
        y = np.array(data['y']) #class variables


        #fit an SVM with rbf kernel
        clf = SVC( kernel = 'rbf',cache_size = 1000)
        #parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
        parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

        gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4) #grid search hyper parameter optimization
        gs_rbf.fit(X,y)

        #choose the best estimator 
        clf = gs_rbf.best_estimator_
",src/STEM.py,enricopal/STEM,1
"""""""
線形SVMでirisデータを分類
""""""

# データをロード
iris = datasets.load_iris()
X = iris.data[:, :2]
Y = iris.target

# 分類器を学習
clf = svm.SVC(C=1.0, kernel='linear')
clf.fit(X, Y)

pl.figure(1)

# メッシュの各点の分類結果を描画
h = 0.02
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),",sklearn/plot_svm_iris.py,TenninYan/Perceptron,1
"        """"""
        vectors = []
        answers = []
        for data in self.dataset:
            vector = self._vectorize(data)
            vectors.append(vector)
            answer = self.problem.target(data)
            answers.append(answer)
        if not vectors:
            raise ValueError(""Cannot train on empty set"")
        self.svm = svm.SVC()
        self._SVC_hack()
        self.svm.fit(vectors, answers)

    def classify(self, sentence_pair):
        """"""
        Classify if this SentencePair `sentence_pair` has sentences
        that are translations of each other.
        """"""
        self._SVC_hack()",yalign/svm.py,machinalis/yalign,1
"SVM_GAMMA_RANGE = [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]

TREE_SPLIT_RANGE = [1, 2, 4, 8, 16]

PARAMS = {'rbf_svm':{'C':SVM_C_RANGE, 'gamma':SVM_GAMMA_RANGE},
          'linear_svm':{'C':SVM_C_RANGE},
          'extra_trees':{'min_samples_split':TREE_SPLIT_RANGE}}

#Classifiers
CACHE_SIZE = 1024 * 4
CLFS = {'rbf_svm':svm.SVC(kernel='rbf', cache_size=CACHE_SIZE),
        'linear_svm':svm.LinearSVC(),
        'extra_trees':ensemble.ExtraTreesClassifier(n_estimators=20, 
                                                    compute_importances=True,
                                                    criterion='gini',
                                                    n_jobs=1)}

CLFS_SPARSE = {'rbf_svm':svm.sparse.SVC(kernel='rbf', cache_size=CACHE_SIZE),
               'linear_svm':svm.sparse.LinearSVC(),
               'extra_trees':CLFS['extra_trees']}",src/scripts/learn_base.py,flaviovdf/pyksc,1
"            #cv_obj = StratifiedKFold(n_splits=cv_n_folds, shuffle=False)
            cv_obj = cv_n_folds  # temporary hack (due to piclking issues otherwise, this needs to be fixed)
        else:
            cv_obj = None

        _rename_main_thread()

        if method == 'LinearSVC':
            from sklearn.svm import LinearSVC
            if cv is None:
                cmod = LinearSVC(**options)
            else:
                try:
                    from freediscovery_extra import make_linearsvc_cv_model
                except ImportError:
                    raise OptionalDependencyMissing('freediscovery_extra')
                cmod = make_linearsvc_cv_model(cv_obj, cv_scoring, **options)
        elif method == 'LogisticRegression':
            from sklearn.linear_model import LogisticRegression
            if cv is None:",freediscovery/categorization.py,kcompher/FreeDiscovUI,1
"    trn, trn_lbl, tst, feature_names= blor.get_new_table(test)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
    blah3= SVC(kernel='linear', C=inf)
#    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg10_globalfeatures.py,lioritan/Thesis,1
"

#----------------------------------------------------------
"""""" Applying classification techniques to create a predictive model.
    Here I spotcheck""""""
#----------------------------------------------------------
#Could use train_test_split, but this split isn't needed and we need all the data we can get
X_train = #Redacted information as requested.
Y_train = #Redacted information as requested.

linear_svc = LinearSVC()
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
linear_score = cross_val_score(linear_svc, X_train, Y_train, cv=cv, scoring='f1').mean()

svc = SVC(kernel='rbf')
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
svc_score = cross_val_score(svc, X_train, Y_train, cv=cv, scoring='f1').mean()

knn = KNeighborsClassifier(n_neighbors = 3)
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)",Online_Marketing_Analysis/dataset-classification.py,MichaelMKKang/Projects,1
"    n = input()
    testData = []
    for _ in xrange(n):
        data = json.loads(raw_input())
        testData.append(data)
    testData = transformer.transform(testData)
    return testData
    
def main():    
    trainData, trainLabel = read_train_data()
    svm = LinearSVC()
    svm.fit(trainData, trainLabel)
    testData = read_test_data()
    testLabel = svm.predict(testData)
    for e in testLabel: 
        print e
    
if __name__ == ""__main__"":",practice/ai/machine-learning/craigslist-post-classifier-the-category/craigslist-post-classifier-the-category.py,zeyuanxy/hacker-rank,1
"# y is filled with integers coding for the class to predict
# We must have X.shape[0] equal to y.shape[0]
X = [e.get_data()[:, data_picks, :] for e in epochs_list]
y = [k * np.ones(len(this_X)) for k, this_X in enumerate(X)]
X = np.concatenate(X)
y = np.concatenate(y)

from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score, ShuffleSplit

clf = SVC(C=1, kernel='linear')
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(X), 10, test_size=0.2)

scores = np.empty(n_times)
std_scores = np.empty(n_times)

for t in xrange(n_times):
    Xt = X[:, :, t]
    # Standardize features",examples/decoding/plot_decoding_sensors.py,kingjr/mne-python,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",website/demo/home/rfk/repos/pypy/lib-python/2.7/email/test/test_email_renamed.py,ArneBab/pypyjs,1
"
fo = open('data/drums.genres.json', 'r')
genres = json.loads(fo.read())
cmap = {'pop': 0, 'rock': 1, 'reggae': 2, 'jazz': 3, 'classical': 4}
classes = [cmap[genres[k]] for k in training_set.keys()]
fo.close()

X = np.array([p for k, p in training_set.items()])
Y = np.array(classes)
C = 1.5  # SVM regularization parameter
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)


# Test SVM

validate = np.array([p for k, p in testing_set.items()])

hits = 0.0
misses = 0.0
for s, p in [(s, p) for (s, p) in testing_set.items()]:",svm.py,henrikalmer/genre-recognition,1
"    # Initialize classifiers
    classifiers = {
        ""Naive Bayes""         : GaussianNB(),
        ""Gradient Boost""      : GradientBoostingClassifier(),
        ""Adaboost""            : AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)),
        ""Decision Tree""       : DecisionTreeClassifier(),
        ""Extra Random Trees""  : ExtraTreesClassifier(n_estimators=300),
        ""Logistic Regression"" : LogisticRegression(),
        ""K-Nearest-Neighbors"" : KNeighborsClassifier(),
        ""SGD""                 : SGDClassifier(),
        ""SVM""                 : LinearSVC(),
        ""Random Forest""       : RandomForestClassifier(n_estimators=300)
    }

    for c in classifiers:
      # cross validation
      scores = cross_val_score(classifiers[c], Xt, Yt, cv=cvf)

      # report
      print c,scores.mean()",src/five-fold.py,bravelittlescientist/kdd-particle-physics-ml-fall13,1
"# SVC example

# For more information: http://scikit-learn.org/stable/tutorial/basic/tutorial.html

import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC()
clf.fit(X, y) 
",Prototype/svc_example.py,TalaatHarb/PredictOceanHealth,1
"    def get_available_compilers(self, compiler_list):
        result = []
        for c in compiler_list:
            for regexp in self.SUPPORTED_TARGETS:
                if regexp.match(c.platform):
                    if 'Clang' in c.NAMES:
                        result.append((c, [], Windows_Clang()))
                    elif 'GCC' in c.NAMES:
                        result.append((c, [], Windows_GCC()))
                    elif 'msvc' in c.NAMES:
                        result.append((c, [], Windows_MSVC()))
                    else:
                        result.append((c, [], self))
        return result

    def load_in_env(self, conf, compiler):
        env = conf.env
        env.ABI = 'pe'
        env.VALID_PLATFORMS = ['windows', 'pc']
        env.pymodule_PATTERN = '%s.pyd'",mak/target/windows.py,bugengine/BugEngine,1
"    data = [
        [1,0,-1],
        [0,1,-1],
        [0,-1,-1],
        [-1,0,1],
        [0,2,1],
        [0,-2,1],
        [-2,0,1]
    ]

    machine = svm.SVC(kernel='poly', degree = 2, coef0 = 1, C=100000, gamma = 1)
    X, y = split(data)

    machine.fit(X, y)
    print machine.n_support_

##q12()

def q13Target(x1, x2):
    return numpy.sign(x2 - x1 + .25 * math.sin(math.pi * x1))",Final/Python/by_kirbs/final_exam.py,JMill/edX-Learning-From-Data-Solutions-jm,1
"
if __name__ == '__main__':
    data = load_data('./Problem2/ionosphere.txt')
    datacv = split_data(data,0.8)

    adaboost = list()
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=4, min_samples_leaf=1)))
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)))
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=BernoulliNB()))
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=LogisticRegression()))
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=svm.SVC(probability=True, kernel='linear')))
    adaboost.append(AdaBoostClassifier(n_estimators=100, base_estimator=svm.SVC(probability=True, kernel='rbf')))

    weakestimatornames = [""DecisionTreeClassifier (max_depth=4)"", ""DecisionStumpClassifier"", ""BernoulliNB"", ""LogisticRegression"", ""Linear SVM"", ""RBF SVM""]

    trainfeatures = datacv.train.g_features + datacv.train.b_features
    gtrainlen = len(datacv.train.g_features)
    btrainlen = len(datacv.train.b_features)
    trainlabel = numpy.ones(gtrainlen).tolist() + numpy.zeros(btrainlen).tolist()
",Homeworks/HW2/Python/hw2_B.py,MengwenHe-CMU/17S_10701_MachineLearning,1
"    plt.ylabel(""Score"")
    plt.semilogx(p_range, train_scores_mean, label=""Training score"", color=""#E29539"")
    plt.semilogx(p_range, test_scores_mean, label=""Cross-validation score"", color=""#94BA65"")
    plt.legend(loc=""best"")
    plt.show()

def blind_gridsearch(model, X, y):
    C_range = np.logspace(-2, 10, 5)
    gamma_range = np.logspace(-5, 5, 5)
    param_grid = dict(gamma=gamma_range, C=C_range)
    grid = GridSearchCV(SVC(), param_grid=param_grid)
    grid.fit(X, y)

    print(
        ""The best parameters are {} with a score of {:0.2f}"".format(
            grid.best_params_, grid.best_score_
        )
    )

def visual_gridsearch(model, X, y):",diagnostics/pycon/code/vizDr.py,rebeccabilbro/viz,1
"                paths_batch = paths[start_index:end_index]
                images = facenet.load_data(paths_batch, False, False, args.image_size)
                feed_dict = { images_placeholder:images, phase_train_placeholder:False }
                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)
            
            classifier_filename_exp = os.path.expanduser(args.classifier_filename)

            if (args.mode=='TRAIN'):
                # Train classifier
                print('Training classifier')
                model = SVC(kernel='linear', probability=True)
                model.fit(emb_array, labels)
            
                # Create a list of class names
                class_names = [ cls.name.replace('_', ' ') for cls in dataset]

                # Saving classifier model
                with open(classifier_filename_exp, 'wb') as outfile:
                    pickle.dump((model, class_names), outfile)
                print('Saved classifier model to file ""%s""' % classifier_filename_exp)",5Project/facenet/src/classifier.py,songjs1993/DeepLearning,1
"    #parse input and determin its type
    try:
        featureValue= float(input_dict[""featureIn""]) if '.' in input_dict[""featureIn""] else int(input_dict[""featureIn""]) #return int or float
    except ValueError:
        featureValue= input_dict[""featureIn""] #return string
    clf = tree.DecisionTreeClassifier(max_features=featureValue, max_depth=int(input_dict[""depthIn""]))
    output_dict={}
    output_dict['treeOut'] = clf
    return output_dict

def scikitAlgorithms_linearSVC(input_dict):
    from sklearn.svm import LinearSVC
    clf = LinearSVC(C=float(input_dict[""penaltyIn""]),loss=input_dict[""lossIn""],penalty=input_dict[""normIn""], multi_class=input_dict[""classIn""])
    output_dict={}
    output_dict['SVCout'] = clf
    return output_dict

def scikitAlgorithms_SVC(input_dict):
    from sklearn.svm import SVC
    clf = SVC(C=float(input_dict[""penaltyIn""]), kernel=str(input_dict[""kernelIn""]), degree=int(input_dict[""degIn""]))",workflows/scikitAlgorithms/library.py,janezkranjc/clowdflows,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the chi-square score of each feature
        score = chi_square.chi_square(X, y)

        # rank features in descending order according to score
        idx = chi_square.feature_ranking(score)
",skfeature/example/test_chi_square.py,jundongl/scikit-feature,1
"        'params': [
            {
                'solver': ['liblinear'],
                'penalty': ['l2'],
                'random_state': [77]
            },
        ]
    },
    {
        'name': 'SVM',
        'model': SVC(),
        'params': [
            {
                'kernel': ['poly'],
                'degree': [5],
                'random_state': [77]
            },
        ]
    },
    {",scripts/classifier.py,yohanesgultom/id-openie,1
"    layer_output = np.reshape(layer_output, (1, layer_output.shape[0], layer_output.shape[1]*layer_output.shape[2]))
    print (""Training output: "", layer_output)
    print (""Training output shape 0 : "", layer_output.shape[0])
    print (""Training output shape 1 : "", layer_output.shape[1])
    print (""Training output shape 2 : "", layer_output.shape[2])

    return (layer_output)
       
def evaluate_with_SVM(layer_output, train_X, train_Y, test_X, test_Y ):
    print (""Starting SVM"")
    clf = svm.SVC()
    print (""Training SVM"")
    train_X_SVM = clf.fit(layer_output, train_Y)
    print (""SVM Train Labels: "", train_Y)
    clf.predict(test_X) 
       

def main():
    embeddings_index = embedding_index(GLOVE_DIR, FILENAME)          # create embedding index with GloVe
    data, labels, labels_index = load_data(TEXT_DATA_DIR)             # load datasets",Model/20News/20news_LSTM-SVM.py,irisliu0616/Short-text-Classification,1
"
X_train = train_ml.values[:,2:-1] #x variables

y_train = np.array(train_ml['y']) #class variables

X_test = test_ml.values[:,2:-1] #x variables

y_test = np.array(test_ml['y']) #class variables

#fit an SVM with rbf kernel
clf = SVC( kernel = 'rbf',cache_size = 1000)

parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4, n_jobs = -1)
gs_rbf.fit(X_train,y_train)

#save the output

gs_rbf = gs_rbf.best_estimator_",validation/FEIII2016/ml/ml_validation.py,enricopal/STEM,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    vectorizer = EpochsVectorizer()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,dimkal/mne-python,1
"    best_raw_acc = [0]
    predictions = {}
    real = {}

    '0.5286 min_df=74 max_df=80 with SVC'
    '~0.35 min_df=35 max_df=180 with LR'
    # vect = TfidfVectorizer(min_df=74, max_df=80, ngram_range=(1, 3))
    vect = TfidfVectorizer(min_df=35, max_df=180, ngram_range=(1, 3))
    trainX = vect.fit_transform(shuffled_train_text)

    # model = SVC()
    model = LR()
    model.fit(trainX, shuffled_train_labels)

    joblib.dump(model, 'pickle/model.pkl')

    # test_vect = vect.transform(test_text)

    # pr_test_vect = model.predict(test_vect)
    # raw_acc = metrics.accuracy_score(test_labels, pr_test_vect)",model.py,aidankmcl/AdventureTimeClassifier,1
"from sklearn.datasets import load_svmlight_file
from sklearn.utils import check_random_state


data = load_svmlight_file(""leu"")
X_1 = data[0].todense().tolist()  # samples 72 features above 7129
y_1 = map(int,data[1])   # classes 2


#L1 SVM
l1svc = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

coef = l1svc.coef_.tolist()[0]

#print(len(l1svc.coef_.tolist()[0]))
",coeficient.py,narendrameena/featuerSelectionAssignment,1
"
print ""All data loaded!""

n_neighbors = 20

ratio = 0.3
train_targets, test_targets, train_features, test_features = train_test_split(targets, features, test_size=1-ratio)

clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
clf2 = ensemble.RandomForestClassifier(25, criterion=""entropy"")
clf3 = svm.SVC()

clf.fit(train_features, train_targets)
clf2.fit(train_features, train_targets)
clf3.fit(train_features, train_targets)

pickle.dump(clf, open('knn.pkl', 'w'))
pickle.dump(clf2, open('rfc.pkl', 'w'))
pickle.dump(clf3, open('svm.pkl', 'w'))
print ""Training Complete""",PythonCode/nearest.py,TheFightingMongooses/cloaked-octo-bugfixes,1
"
print('params: {0}'.format(args))

dataset = PennFudanDataset('dataset/PennFudanPed')
pipeline = create_pipeline(threshold=args.t)

process(dataset, pipeline)

inputs, targets = extractor.extract(dataset, w=args.w, N=args.n)

estimator = svm.SVC(C=args.c, gamma=args.g, cache_size=args.cache)

model = estimator.fit(inputs, targets)

joblib.dump(model, args.filename)",quick.py,dominiktomicevic/pedestrian,1
"	#y = y[idx]
	#X = X[idx]

	# split the data
	Xtrain = X[:nTrain,:]
	ytrain = y[:nTrain]
	Xtest = X[nTrain:,:]
	ytest = y[nTrain:]
	
	#linear
	clf = SVC(kernel='linear')
	clf.fit(Xtrain,ytrain)
	pred = clf.predict(Xtest)
	print ""RMSE linear = "", rmsle(ytest, pred)

	#polynomial
	clf = SVC(kernel='poly')
	clf.fit(Xtrain,ytrain)
	pred = clf.predict(Xtest)
	print ""RMSE poly = "", rmsle(ytest, pred)",svm/test_svms_count.py,agadiraju/519finalproject,1
"        return correct/len(results);
    
    
    
    def run(self):
        
        digits = datasets.load_digits()
        trainCount = 1500 #about 90%
        predictCount = len(digits.data) - trainCount
    
        clf = svm.SVC(gamma = 0.001, C= 100.)
        logistc = linear_model.LogisticRegression()
        neighbor = neighbors.KNeighborsClassifier()
    
        
    
        trainX = digits.data[:trainCount]
        trainY = digits.target[:trainCount]
        
        #import time",Scikit/Scikit/exercise_2_2_1.py,TechnicHail/COMP188,1
"    """"""
    def train(self, limit_data=None):
        if not hasattr(self, 'reviews'):
            print ""No data loaded""
            return

        if limit_data is None:
            limit_data = len(self.reviews)

        X = self.get_bag_of_ngrams(self.reviews[:limit_data])
        self.clf = LinearSVC(C=0.4).fit(X, self.labels[:limit_data])

    def __test(self, reviews, labels):
        X_training_counts = self.count_vect.transform(reviews)
        X_training_tfidf = self.tfidf_transformer.transform(X_training_counts)

        predicted = self.clf.predict(X_training_tfidf)
        self.cm = confusion_matrix(labels, predicted)

        return 1 - np.mean(predicted == labels)",classifier.py,lukedeo/cross-domain,1
"### set the random_state to 0 and the test_size to 0.4 so
### we can exactly check your result
features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(
features, labels,test_size=0.4,random_state=0)

features_train.shape, labels_train.shape
features_test.shape, labels_test.shape

###############################################################

clf = SVC(kernel=""linear"", C=1.)
clf.fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################
#def submitAcc():",intro_to_machine_learning/lesson/lesson_13_validation/Train-Test Split in sklearn.py,tuanvu216/udacity-course,1
"
    if print_cm:
        print ""confusion matrix:""
        print metrics.confusion_matrix(y_test,pred)
    print
    return score, train_time, test_time

for clf, name in ((RidgeClassifier(),""Ridge Classifier""),):
    print 80 * '='
    print ""%s penalty"" % penalty.upper()
    liblinear_results = benchmark(LinearSVC(loss='12',penalty=penalty, C=1000, dual=False, tol = 1e-3))
    #Train SGD model
    sgd_results = benchmark(SGDClassifier(alpha=.0001, n_iter=50,penalty=penalty))


#Train SGD with Elastic Net penalty
print 80 * '='
print ""Elastic-Net penalty""
sgd_results = benchmark(SGDClassifier(alpha=.0001,n_iter=50,penalty='elasticnet'))
",ml-samples-examples/Scikits_examples/doc_classify.py,softwaremechanic/Miscellaneous,1
"def fit_model(train_features, train_labels, svm_clf = False, RandomForest = False, nb = False):
    #Input: SVM, RandomForest, and NB are all boolean variables and indicate which model should be fitted
    #SVM: Linear Support Vector Machine
    #RandomForest: Random Forest, we set the max_depth equal to 50 because of prior tests
    #NB: LDA representation using an ngram range of (1,1)
    #train_features: Train reviews that have been transformed into the relevant features
    #train_labels: Labels for the training reviews, transformed into a binary variable
    #Output: A fitted model object

    if svm_clf == True:
        clf = svm.LinearSVC()
        clf.fit(train_features, train_labels)
        return clf
    elif RandomForest == True:
        clf = RandomForestClassifier(max_depth = 100, max_leaf_nodes=50, criterion='entropy')
        clf.fit(train_features, train_labels)
        return clf
    elif nb == True:
        clf = GaussianNB()
        clf.fit(train_features, train_labels)",machine_learning/yelp_ml.py,georgetown-analytics/yelp-classification,1
"					is_attr = True
				if line == '@data\n':
					is_attr = False
					continue
				if is_attr: continue
				tmp = line.strip('\n').split(',')
				tmp = map(lambda x:float(x),tmp)
				self.x.append(tmp[:-1])
				self.y.append(tmp[-1])
		print(len(self.x))
		self.clf = svm.SVC()
		print(self.labels)
	def train(self):
		self.clf.fit(self.x,self.y)
	def predict(self,input):
		return self.clf.predict(input)
	def crossValidate(self,test_size=None,confusion_matrux=False):
		if len(self.x)==0 or len(self.y) ==0:
			sys.stderr.write(""Uninitialized object"")
			return",SVM/SVM.py,frankdede/CMPUT466Project,1
"
    ###########
    # Train SVM
    ###########
    if (not exists(conf.modelPath)) | OVERWRITE:
        if VERBOSE: print str(datetime.now()) + ' training liblinear svm'
        if VERBOSE == 'SVM':
            verbose = True
        else:
            verbose = False
        clf = svm.LinearSVC(C=conf.svm.C)
        if VERBOSE: print clf
        clf.fit(train_data, all_images_class_labels[selTrain])
        with open(conf.modelPath, 'wb') as fp:
            dump(clf, fp)
    else:
        if VERBOSE: print 'loading old SVM model'
        with open(conf.modelPath, 'rb') as fp:
            clf = load(fp)
",phow_caltech101.py,thomasklau/pixy,1
"#print tagmatrix
for tag in tagsarray.keys():
	tagmatrix[tag] = []
	for othertag in tagsarray.keys():
		try:
			tagmatrix[tag].append((othertag,editDistanceFast(tag,othertag)))		
		except:
			tagmatrix[tag].append((othertag,editDistance(tag,othertag)))
	tagmatrix[tag] = sorted(tagmatrix[tag],key=lambda tup:tup[1])
	#tagmatrix[tag] = tagmatrix[tag][:100]
clf = svm.SVC(kernel='poly', C=1.0)
X = []
y = []
for tag in tagmatrix.keys():
	X.append(map(itemgetter(1),tagmatrix[tag])+[tagsarray[tag]['L'],tagsarray[tag]['P']])
	y.append(optionize(tagsarray[tag]))
X = scale(X)
print X
print y
pickle.dump(X,open(""X.p"",""wb""))",svm2.py,CreationLabs/IndixHackathon,1
"    ids = test_df['PassengerId'].values

    train_data = extract_feature(train_df)
    test_data = extract_feature(test_df)
    label_train = train_data[:, 0].astype(int)

    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5],
         'C': [1, 10, 100, 1000, 1500, 2000]}]

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='accuracy')
    clf.fit(train_data[:, 1:], label_train)
    print(clf.best_estimator_)

    result = clf.predict(test_data)

    with open(""result.csv"", 'w') as predictions_file:
        open_file_object = csv.writer(predictions_file)
        open_file_object.writerow([""PassengerId"", ""Survived""])
        open_file_object.writerows(zip(ids, result))",Kaggle/Titanic/svm.py,xueweuchen/pythonDemo,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    vectorizer = EpochsVectorizer()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,wronk/mne-python,1
"            raise TypeError(""gamma should be a list"")
    # creat svm model
    scaler = StandardScaler()
    train_x = scaler.fit_transform(train_x)
    Cs = c
    Gammas = gamma
    if c is None:
        Cs = list(np.logspace(-6, -1, 10))
    if gamma is None:
        Gammas = list(np.linspace(0.0001, 0.15, 10))
    svc = svm.SVC()
    clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs, gamma=Gammas),
                       n_jobs=-1)
    clf.fit(train_x, train_y)
    clf = clf.best_estimator_
    # fit the best model
    clf.fit(train_x, train_y)
    # predict the testing data and convert to data frame
    prediction = clf.predict(scaler.fit_transform((test_x)))
    prediction = pd.DataFrame(prediction)",mousestyles/classification/classification.py,berkeley-stat222/mousestyles,1
"Test_number=int(sys.argv[1])

#          *********** Load Testing Data ***************            

Pre=np.fromfile(cur_dir+""/vectors.txt"",float,dimension*Test_number,""  "")
Y=[0]*300+[1]*300# set Labels for training vectors
print len(Y)
Pre=Pre.reshape(Test_number,dimension)
dimension=len(X[0])# get new dimension after applying KPCA
Pre=np.array(pca.transform(list(Pre)))
lin_clf = svm.LinearSVC()
lin_clf.fit(X, Y) 
result=lin_clf.predict(Pre)
print ""Supervised Classifier Result""
for i in range(len(result)):
	print i,result[i]
number_of_test_vectors=Test_number

a=lin_clf.decision_function(Pre)# get decision function values to select most confident 
two_D=[]",bin/LDS_classifier.py,vaibhav9518/FaceReck,1
"vectorizer = TfidfVectorizer()
news_group.data = vectorizer.fit_transform(news_group.data)

# initialize valitador
cv = KFold(len(news_group.target), shuffle=True, n_folds=5, random_state=241)

# declare parameters
C_vals = [10**x for x in range(-5, 6)]
parameters = { 'kernel': ['linear'], 'C': C_vals, 'random_state': [241] }

svc = SVC(random_state=241)

model = GridSearchCV(svc, parameters, cv=cv)
model.fit(news_group.data, news_group.target)
best_params = model.best_params_
C = best_params['C']

model = SVC(kernel='linear', C=C, random_state=241)
model.fit(news_group.data, news_group.target)
",intro-into-ml/tasks/svm/svm-texts.py,universome/courses,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Storing the predictions of the trained classifier on the testing dataset...""
predicted = clf.predict(X_test_vect_red)

print ""Evaluation results of the content-based engine working alone (predicting only one developer): ""

import numpy as np
print ""Accuracy: %s"" % np.mean(predicted == y_test)",Archive/RecoDev-Evaluator/recodev_eval5.py,amirhmoin/recodev,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Python-3.5.0-main/Lib/test/test_email/test_email.py,Belxjander/Kirito,1
"##
## For an initial search, a logarithmic grid with basis
## 10 is often helpful. Using a basis of 2, a finer
## tuning can be achieved but at a much higher cost.
#    from sklearn.cross_validation import StratifiedShuffleSplit
#    from sklearn.grid_search import GridSearchCV
#    C_range = np.logspace(-2, 4, 7)
##    gamma_range = np.logspace(-9, 3, 13)
#    param_grid = dict( C=C_range)
#    cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.2, random_state=42)
#    grid = GridSearchCV(LinearSVC(), param_grid=param_grid, cv=cv,verbose=10)
#    print ""starting grid search""
#    grid.fit(encoded_features, y_train)
##    
#    print(""The best parameters are %s with a score of %0.4f""",scripts/Keras_CountMinSketch_deep_calculate_cv_allkernels.py,nickgentoo/scikit-learn-graph,1
"    tfidf = TfidfVectorizer(lowercase=False,use_idf=False,sublinear_tf=True)
    with open('./Resources/cities.txt','r') as ip:
        cities = ip.read().split()
    # For adding training data
    if extra_data is not None:
        X_extra,y_extra = extra_data
        X_extra,y_extra = np.array(X_extra),np.array(y_extra)
    for train_idx,test_idx in skf:
        if len(target_names) == 2:
            clfs = [
                LinearSVC(C=13),
                # SVC(C=11,kernel=""linear"",probability=True),
                # SGDClassifier(loss=""log""),
                LogisticRegression(),
                RandomForestClassifier(n_estimators=10,n_jobs=-1),

            ]
        else:
            clfs = [
                LinearSVC(C=13),",Subtask1-Revised/main.py,saatvikshah1994/hline,1
"    gamma_range = np.logspace(-15, 15, 31, base = 2.0)
     
#     param_grid = dict(gamma=gamma_range, C=C_range)
#     cv = StratifiedShuffleSplit(y, n_iter=10, test_size=0.2, random_state=42)
    roc_auc_scorer = get_scorer(""roc_auc"")
    scores = []
    for C in C_range:
        for gamma in gamma_range:
            auc_scorer = []
            for train, test in KFold(n=len(X), n_folds=10, random_state=42):
                rbf_svc = svm.SVC(C=C, kernel='rbf', gamma=gamma, probability=True)
                X_train, y_train = X[train], y[train]
                X_test, y_test = X[test], y[test]
                rbf_clf = rbf_svc.fit(X_train, y_train)
                auc_scorer.append(roc_auc_scorer(rbf_clf, X_test, y_test))
            scores.append(np.mean(auc_scorer))
#     grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
#     grid.fit(X, y)
#     scores = [x[1] for x in grid.grid_scores_]
    scores = np.array(scores).reshape(len(C_range), len(gamma_range))",kernel_selection2.py,lidalei/DataMining,1
"
for X, Y, labels in zip(Xs, Ys, Ls):
    lolo = LeaveOneLabelOut(labels)
    dt = tree.DecisionTreeClassifier()
    scores = cross_val_score(dt, X, Y[0], cv=lolo)
    print 'Decision Tree Accuracy, trained: {mean}% (+/- {std}%)'.format(
            mean=round(100*scores.mean(),1), std=round(100*scores.std(),1))
    scores = cross_val_score(dt, X, Y[1], cv=lolo)
    print 'Decision Tree Accuracy, surface: {mean}% (+/- {std}%)'.format(
            mean=round(100*scores.mean(),1), std=round(100*scores.std(),1))
    sv = svm.SVC(kernel='linear', C=1)
    scores = cross_val_score(sv, X, Y[0], cv=lolo)
    print 'SVM Accuracy, trained: {mean}% (+/- {std}%)'.format(
            mean=round(100*scores.mean(),1), std=round(100*scores.std(),1))
    scores = cross_val_score(sv, X, Y[1], cv=lolo)
    print 'SVM Accuracy, surface: {mean}% (+/- {std}%)'.format(
            mean=round(100*scores.mean(),1), std=round(100*scores.std(),1))
    nb = naive_bayes.GaussianNB()
    scores = cross_val_score(nb, X, Y[0], cv=lolo)
    print 'Naive Bayes Accuracy, trained: {mean}% (+/- {std}%)'.format(",classifyrunner/experiments/accuracy.py,toonn/mlii,1
"def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)
    clf.fit(X, y_regr)

    clf = AdaBoostRegressor(SVR(), random_state=0)",scikit-learn-0.18.1/sklearn/ensemble/tests/test_weight_boosting.py,RPGOne/Skynet,1
"    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
#    blah3= SVC(kernel='linear', C=inf)
    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg11_flatten.py,lioritan/Thesis,1
"    print ""get support...""
    col_index = rfecv.get_support(indices=True)
    print ""num features selected by RFE(CV)/Lasso: {}"".format(len(col_index))
    return col_index

def rfe_cv_f1(train_data, train_labels):
    # important toto!
    # todo: I think also for feature selection we should take care the 0 class is balanced!
    # todo: if you use it that way, scale the features
    print ""Recursive eleminate features: ""
    svc = svm.SVC(kernel=""linear"") #sklearn.linear_model.Lasso(alpha = 0.1)
    print ""scale data""
    values = train_data.values
    minmax = pp.MinMaxScaler()
    values = minmax.fit_transform(values)#pp.scale(values)
    print ""test fit.""
    svc.fit(values, np.array(train_labels).astype(int))
    print ""run rfecv..""
    rfecv = fs.RFECV(estimator=svc, step=0.05, verbose=2)
    rfecv.fit(values, np.array(train_labels).astype(int))",analysis/feature_selection.py,joergsimon/gesture-analysis,1
"if (X_train.size != len(skill_train_text)):
    print ""Training sample size %d, Skills Labels %d Do not match"" % ( X_train.size, len(skill_train_text))
    sys.exit()    
   
lb = preprocessing.MultiLabelBinarizer()
Y = lb.fit_transform(skill_train_text)

ex_classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

ex_classifier.fit(X_train, Y)
# Test a little
#X_test = np.array([""400 meter run   Overhead squat 95 lbs x 15"",
#                   ""Snatch 135 pounds"",
#                  ""Run 1 mile"",
#                   ""Ring Dips"",
#                   ""push up, pull ups"",
#                   ""Box jumps"",",multilabel/cf_ml_skill_trainer.py,praveen049/text_classification,1
"lcmin = -4
lcmax = 4
ng = 10
nc = 10
cs=map(lambda x:10**(x*(lcmax-lcmin)/(nc-1.0)+lcmin),range(nc))
gs=map(lambda x:10**(x*(lgmax-lgmin)/(ng-1.0)+lgmin),range(ng))
grid_pars = {'C':cs, 'gamma':gs}
#print grid_pars

# train classifier
svr = svm.SVC(kernel='rbf')
clf = grid_search.GridSearchCV(svr,grid_pars,verbose=2,cv=5,n_jobs=4)
clf.fit(xtrain, ytrain)

# print scores
print ""Grid scores:""
print ""\n score>0.96:""
for v in clf.grid_scores_:
    c=v.parameters['C']
    g=v.parameters['gamma']",runSVM.py,PBGraff/SwiftGRBpipe,1
"dfi = readWSDIndexFile(baseDir, instrument, startYear, yearNum)
dfmacro = readAndCombineMacroEconomyFile(baseDir, startYear, yearNum=yearNum)
dfmoney = readMoneySupplyFile(baseDir, 'money_supply.csv', startYear, yearNum=yearNum)
X, y, actionDates = prepareData(df, dfi, dfmacro, dfmoney)
print np.shape(X), np.shape(y)

normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
X_norm = normalizer.transform(X)
gamma, C, score = optimizeSVM(X_norm, y, kFolds=10)
print 'gamma=',gamma, 'C=',C, 'score=',score
clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)


pathName, df = readAndReWriteCSV(baseDir, instrument, startYear=startYear, yearNum=yearNum)
print pathName
# print df.sample(3)

feed = yahoofeed.Feed()
feed.addBarsFromCSV(instrument, pathName)
",finance/MonthTest/PyAlgoSVM.py,Ernestyj/PyStudy,1
"            
        
        def fit_transform(self, X, y, **fit_params):
            self.fit(X,y)
            return self.transform(X)
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    svm_model = OneVsRestClassifier(SVC(C=10.))
    
    # Create the pipeline 
    model = pipeline.Pipeline([('UnionInput', FeatureUnion([('svd', svd), ('dense_features', FeatureInserter())])),('scl', scl)])
    
    model.fit(X,y)
    
    Xtrh=model.fit_transform(X)
    Xtsh=model.fit_transform(X_test)
    ",Search_result/srr3.py,tanayz/Kaggle,1
"		if readInPlist[""interval""] == 0.01 and not readInPlist[""class""] == ""Unknown"":
			collectValidSample(readInPlist)

	""""""
	###############################################################################
	# Create a feature-selection transform and an instance of SVM that we
	# combine together to have an full-blown estimator

	transform = feature_selection.SelectPercentile(feature_selection.f_classif)

	clf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0, kernel='rbf', gamma=0.01))])

	###############################################################################
	# Plot the cross-validation score as a function of percentile of features
	score_means = list()
	score_stds = list()
	percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)

	for percentile in percentiles:
	    clf.set_params(anova__percentile=percentile)",Collected_Data/analysis.py,thepropterhoc/TreeHacks_2016,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CIFE.cife(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_CIFE.py,jundongl/PyFeaST,1
"
    def classify_data(self, training_set, events, test_data):
        """"""
        The data is classified based on the training data.
        :param plot: Input values to be processed for generating features
        :return: predictions for the entire data set
        """"""
        train_array = np.asarray(training_set)
        event_array = np.asarray(events)
        test_array = np.asarray(test_data)
        clf = svm.SVC()
        clf.fit(np.transpose(train_array), np.transpose(event_array))
        prediction = clf.predict(np.transpose(test_array))
        return prediction

'''
        model = SVM(max_iter=10000, kernel_type='linear', C=1.0, epsilon=0.001)
        model.fit(np.transpose(train_array), np.transpose(event_array))
        return model.predict(np.transpose(test_array))
'''",ClassifySlowWavesScikit.py,ssat335/GuiPlotting,1
"    pl.title('Confusion matrix for '+typeModel)
    pl.colorbar()
    pl.show()


# In[24]:

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

C=4
lin_svc = svm.LinearSVC(C=C).fit(X_train, Y_train)
print ""LibSVM fitted""

title = 'LinearSVC (linear kernel)'

predicted = lin_svc.predict(X_valid)
mcc= matthews_corrcoef(Y_valid, predicted)
print ""MCC Score \t +""+title+str(mcc)

cm = confusion_matrix(predicted, Y_valid)",scripts/train.py,vibhutiM/Production-Failures,1
"    with warnings.catch_warnings(record=True):
        gat.fit(epochs[0:6])
    gat.predict(epochs[7:])
    gat.score(epochs[7:])

    # Test wrong testing time
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,LauraGwilliams/mne-python,1
"import numpy as np
import sklearn
from sklearn import svm
from sklearn import metrics
from sklearn.model_selection import train_test_split
import matplotlib as plt
from sklearn.externals import joblib

def _svm(train_x,train_y,test_x,test_y):
    # train_x,test_x1,train_y,test_y1 = train_test_split(data_x,data_y,test_size=0, random_state=42)
    clf = svm.SVC(kernel='rbf')
    clf.fit(train_x,train_y)
    joblib.dump(clf, ""svm_model.m"")
    train_predict = clf.predict(train_x)
    test_predict = clf.predict(test_x)
    target_names = ['class 0', 'class 1']
    print ('train_predict',metrics. accuracy_score(train_y,train_predict))
    print ('test_predict',metrics. accuracy_score(test_y,test_predict))
    print ('report',metrics.classification_report(test_y,test_predict,target_names=target_names))
    #print clf.predict(test_x)",dongge/SVM.py,Li-Jiaqi/BugRiskPrediction,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Storing the predictions of the trained classifier on the testing dataset...""
predicted = clf.predict(X_test_vect_red)

#print ""Evaluation results of the content-based engine working alone, predicting the top-1 developer: ""
#
import numpy as np
#print ""Accuracy: %s"" % np.mean(predicted == y_test)",Archive/RecoDev-Evaluator/recodev_eval4.py,amirhmoin/recodev,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_05_07_2015_server_for_final.py,magic2du/contact_matrix,1
"lda = LDA(n_components=9)
X_train = X_train.toarray()
X_test = X_test.toarray()
lda.fit(X_train, y_map)
X_train = lda.transform(X_train)
X_test = lda.transform(X_test)
'''


print(""Treinando SVM multi classe..."")
clf = SVC(kernel=""linear"")
clf.fit(X_train, y_map)

print(""Calculando predições..."")
predict = clf.predict(X_test)
predict = list(predict)


'''
print(""Treinando MLP..."")",main.py,alan-mnix/MLFinalProject,1
"        ""#CBD5DC"", ""#BBCEDE"",""#ABC8DF"",""#9BC1E0"",""#8BBBE1"",""#7BB4E3"",""#6BAEE4"",""#5BA7E5"",""#4BA1E6"",""#3B9AE8"",""#2B94E9""]

DDL1 = colors.ListedColormap(ddl1)
DDL2 = colors.ListedColormap(ddl2)

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"",
         ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",diagnostics/pycon/code/classifiercompare.py,rebeccabilbro/viz,1
"# Stack the features together
feat = FeatureUnion([('words', words),
	                 ('char', char)
])

# Construct transformation pipeline
text_clf = Pipeline([('feat', feat),
	                 # ('select', select),
                     # ('clf', MultinomialNB()),
                     #('clf', SGDClassifier(penalty='l2'))
                     ('clf', LinearSVC(penalty='l2',C=0.5))
                     #('clf',RandomForestClassifier(n_estimators=300))
                     
])

# Set the parameters to be optimized in the Grid Search
parameters = {'feat__words__ngram_range': [(1,5), (1,6)],
			  # 'feat__words__stop_words': (""english"", None),
              'feat__words__min_df': (2,3),
              'feat__words__use_idf': (True, False),",SAM/model1.py,tanayz/Kaggle,1
"    print(""花费 %0.3fs"" % (time() - t0))

    # 训练SVM分类模型
    print(""使用分类器训练模型"")
    t0 = time()
    param_grid = {
        'C': [1e3, 5e3, 1e4, 5e4, 1e5],
        'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1],
    }
    # 训练
    clf = GridSearchCV(SVC(kernel='rbf'), param_grid)
    clf = clf.fit(X_train_pca, y_train)
    print(""花费 %0.3fs"" % (time() - t0))
    print(""搜索的最佳参数 "")
    print(""模型参数"", clf.best_estimator_)

    # 测试集上的模型质量的定量评估
    print(""在测试集预测人名"")
    t0 = time()
    y_pred = clf.predict(X_test_pca)",pyscript/ml/svm_face.py,jarvisqi/learn_python,1
"    
    #model = GradientBoostingRegressor(n_estimators=150,learning_rate=.1,max_depth=6,verbose=1,subsample=1.0)
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=150, learning_rate=0.05,max_depth=6,min_samples_leaf=100,max_features='auto',verbose=0)#with no weights, cutoff=0.85 and useProba=True
    #model = pyGridSearch(model,Xtrain,ytrain)
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=15)), ('model', GaussianNB())])
    #model = KNeighborsClassifier(n_neighbors=5,weights='distance',algorithm='ball_tree')#AMS~2.245
    print Xtrain.columns
    #model = KNeighborsClassifier(n_neighbors=10)
    #model = AdaBoostClassifier(n_estimators=200,learning_rate=0.1)
    #model=GaussianNB()
    #model = SVC(C=1.0,gamma=0.0)
    
    #model = ExtraTreesClassifier(n_estimators=250,max_depth=None,min_samples_leaf=5,n_jobs=1,criterion='entropy', max_features=10,oob_score=False)##scale_wt 600 cutoff 0.85
    #model = AdaBoostClassifier(n_estimators=100,learning_rate=0.1)    
    #analyzeLearningCurve(model,Xtrain,ytrain,wtrain)
    #odel = ExtraTreesClassifier(n_estimators=250,max_depth=None,min_samples_leaf=5,n_jobs=4,criterion='entropy', max_features=5,oob_score=False)#opt
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=15)), ('model', model)])
    #model = amsGridsearch(model,Xtrain,yt    #smoothWeights=Falserain,wtrain,fitWithWeights=fitWithWeights,nfolds=nfolds,useProba=useProba,cutoff=cutoff)
    model = GradientBoostingClassifier(loss='deviance',n_estimators=150, learning_rate=0.1, max_depth=6,subsample=1.0,verbose=False) #opt weight =500 AMS=3.548
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=500, learning_rate=0.05, max_depth=6,subsample=1.0,max_features=8,min_samples_leaf=100,verbose=0) #opt weight =500 AMS=3.548",competition_scripts/higgs.py,chrissly31415/amimanera,1
"- 1 green
- 2 orange
- 3 purple
- 4 yellow'''


class ImgRecognizer:
    def __init__(self):
        self.training_data = []
        self.target_values = []
        self.svc = svm.SVC(gamma=0.001, kernel='linear', C=100)
        self.downscale_res = (32, 32)

    def _load(self, path, target_value):
        training_imgs = os.listdir(path)
        for f in training_imgs:
            img = Image.open(path+'/'+f)
            img = img.resize(self.downscale_res, Image.BILINEAR)
            self.training_data.append(np.array(img.getdata()).flatten())
            self.target_values.append(target_value)",sklearn_decoder.py,AlexEne/CCrush-Bot,1
"    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS

    # label junk food as -1, the others as +1
    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)

    # split the dataset for model development and final evaluation
    train_data, test_data, target_train, target_test = train_test_split(
        data, target, test_size=.2, random_state=0)

    pipeline = Pipeline([('vect', CountVectorizer()),
                         ('svc', LinearSVC())])

    parameters = {
        'vect__ngram_range': [(1, 1), (1, 2)],
        'svc__loss': ('l1', 'l2')
    }

    # find the best parameters for both the feature extraction and the
    # classifier
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)",venv/lib/python2.7/site-packages/sklearn/feature_extraction/tests/test_text.py,chaluemwut/fbserver,1
"        self.scorelist = {}
        q = 'select page_title from categorylinks join page on cl_from = page_id where cl_type = ""page"" and cl_to = ""{0}"";'
        for priority in ['Top-', 'High-', 'Mid-', 'Low-']:
            prioritycategory = priority + self.projectcat
            self.scorelist[priority] = [(row[0].decode('utf-8'), self.score[row[0].decode('utf-8')]) for row in self.wptools.query('wiki', q.format(prioritycategory), None) if row[0].decode('utf-8') in self.score]

        X = np.array([(x[1]) for x in self.scorelist['Top-']] + [(x[1]) for x in self.scorelist['High-']] + [(x[1]) for x in self.scorelist['Mid-']] + [(x[1]) for x in self.scorelist['Low-']])

        y = np.array([0 for x in self.scorelist['Top-']] + [1 for x in self.scorelist['High-']] + [2 for x in self.scorelist['Mid-']] + [3 for x in self.scorelist['Low-']])

        model = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y)

        print(list(model.predict(X)))",predictor.py,harej/reports_bot,1
"        # check pandas standard transform works

        df = pd.DataFrame({'A': ['A', 'B', 'A', 'A', 'A', 'B', 'B', 'B'],
                           'B': np.random.randn(8),
                           'C': np.random.randn(8)})

        mdf = expd.ModelFrame(df)
        self.assert_frame_equal(df.groupby('A').transform('mean'),
                                mdf.groupby('A').transform('mean'))

    def test_grouped_estimator_SVC(self):
        df = expd.ModelFrame(datasets.load_iris())
        df['sepal length (cm)'] = df['sepal length (cm)'].pp.binarize(threshold=5.8)
        grouped = df.groupby('sepal length (cm)')
        self.assertTrue(isinstance(grouped, expd.core.groupby.ModelFrameGroupBy))
        for name, group in grouped:
            self.assertTrue(isinstance(group, expd.ModelFrame))
            self.assertEqual(group.target_name, '.target')
            self.assertTrue(group.has_target())
            self.assert_index_equal(group.columns, df.columns)",expandas/test/test_groupby.py,sinhrks/expandas,1
"import sklearn.svm as svm
import sklearn.metrics as metrics
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


##### hogeeee ###

def train_by_SVC_RBF(C, gamma, split_train_df, split_train_target_df):
    svc = svm.SVC(C=C, gamma=gamma)
    svc.fit(split_train_df, split_train_target_df)

    return svc



def predict_by_SVC_RBF(svc, predictor_df, file_path_save_results=None):
    predict_arr = svc.predict(predictor_df)
",Titanic/train_n_predict.py,LPacademy/kaggle-challenge,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    y = y.astype(float)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx = alpha_investing.alpha_investing(X[train], y[train], 0.05, 0.05)

        # obtain the dataset on the selected features
        selected_features = X[:, idx]
",skfeature/example/test_alpha_investing.py,jundongl/scikit-feature,1
"feature2, feature_names2 = extractor.feature_ps_ql_pair
#features = np.hstack((feature1, feature2))[labels != 2]
features =feature1[labels != 2]
features = normalizer.fit_transform(features).toarray()
labels = labels[labels != 2]
print features.shape

labels, features = shuffle(labels, features)
#clf = LogisticRegression()
#clf = MultinomialNB()
#clf = sklearn.svm.SVC()
clf = sklearn.svm.LinearSVC(C=10, class_weight='auto')

from sklearn.metrics import accuracy_score, f1_score


accu = []
f1 = []
cv = cross_validation.StratifiedShuffleSplit(labels, 60)
for train, test in cv:",learning/test.py,fcchou/CS229-project,1
"
    data = [munge(l.strip()) for l in open(""/home/chonger/Downloads/annotations.txt"")]

    labels = [x[0] for x in data]
    dicts = [x[1] for x in data]

    feats = v.fit_transform(dicts)

    ttsplit = int(len(labels) * .8)
    clf = svm.SVC(kernel='linear', class_weight={1: 10})
    #clf = svm.SVC()
    clf.fit(feats[:ttsplit],labels[:ttsplit])

    print clf.score(feats[ttsplit:],labels[ttsplit:])

    tot = defaultdict(int)
    tr = defaultdict(int)
    for ex in labels[ttsplit:]:
        tr[ex] += 1
",refinery/fact_classifier/classify_ex.py,daeilkim/refinery,1
"            self.y = encoder.fit_transform(self.y).astype(np.int32)

    def local_validation(self):
        print ""Training...""
        score = k_fold_validation(self.X, self.y, self.model)
        print score

    def submit(self):
        print ""Training...""
        if self.model in (1, 2, 3, 4, 7):
            clf1 = SVC(C=10.0, random_state=1234)
            clf2 = SVR(C=7.0)
        elif self.model == 5:
            clf = LogisticRegression(C=3.0, random_state=1234, class_weight='auto')
        elif self.model == 6:
            clf1 = LinearSVC(C=0.3, random_state=1234, class_weight='auto')
            clf2 = LinearSVR(C=0.6, random_state=1234)
        else:
            num_features = self.X.shape[1]
            num_classes = 4",src/SearchRelevanceModel.py,JimingAndYuqi/secret,1
"
# TODO: Import the three supervised learning models from sklearn
from sklearn import tree
from sklearn import svm
from sklearn.ensemble import GradientBoostingClassifier
#from sklearn.ensemble import RandomForestClassifier

# TODO: Initialize the three models
rand_state = 37
clf_A = tree.DecisionTreeClassifier(random_state=rand_state)
clf_B = svm.SVC(random_state=rand_state)
clf_C = GradientBoostingClassifier(random_state=rand_state)
models = [clf_A, clf_B, clf_C]


# TODO: Execute the 'train_predict' function for each classifier and each training set size
# train_predict(clf, X_train, y_train, X_test, y_test)
results = []

for model in models:",student_intervention/student_intervention.py,armandosrz/UdacityNanoMachine,1
"                             'gamma': [1e-3, 1e-2, 1e-1, 1, 1e+1, 1e+2, 1e+3],
                             'C': [1e-3, 1e-2, 1e-1, 1, 1e+1, 1e+2, 1e+3]
                            },
                        ]

        scores = ['precision', 'recall']

        for score in scores:
            print('# Tuning hyper-parameters for {}\n'.format(score))

            clf = GridSearchCV(svm.SVC(C=1), tunningParams, cv=5,
                                scoring=format('{}_macro'.format(score)))
            clf.fit(x, y)

            print('Best parameters set found on development set:\n')
            print(clf.best_params_)

            print('\nGrid scores on development set:\n')
            means = clf.cv_results_['mean_test_score']
            stds = clf.cv_results_['std_test_score']",fsdk/detectors/immersion.py,luigivieira/fsdk,1
"    y=[1 for j in range(0,nr_of_samples/2)]+[0 for j in range(0,nr_of_samples/2)]
    X_param=[parameter_estimation(X[i]) for i in range(0,nr_of_samples)]
    X=np.array(X)
    y=np.array(y)
    return X, y, X_param

X,y, X_param=data_setup(25,100)
v=string_matching_kernel(X,X)

for C in [0.01, 0.1, 0.5, 1.0, 2.0, 4.0]:
    svm_obj = svm.SVC(kernel='precomputed', C=C)
    svm_param_obj = svm.SVC(kernel='linear',C=C)
    z=svm_obj.fit(v, y) 
    scores = cross_validation.cross_val_score(svm_obj, v, y, cv=10)
    scores_param = cross_validation.cross_val_score(svm_param_obj, X_param, y, cv=10)    
    print C, np.mean(scores),np.std(scores), np.mean(scores_param), np.std(scores_param)

for C in [0.01, 0.1, 0.5, 1.0, 2.0, 4.0]:
    svm_obj = svm.SVC(kernel='linear', C=C)
    svm_param_obj = svm.SVC(kernel='linear',C=C)",markovchain_classification_experiment.py,mariakesa/FunComputationalExperiments,1
"        答案 = []
        for 類型 in sorted(listdir(訓練目錄)):
            類型目錄 = join(訓練目錄, 類型)
            for 音檔 in sorted(listdir(類型目錄)):
                音檔所在 = join(類型目錄, 音檔)
                音檔 = 聲音檔.對檔案讀(音檔所在)
                for 音框 in 音檔.全部音框():
                    特徵 = 恬音判斷.算特徵參數(音框)
                    題目.append(cls._特徵轉陣列(特徵))
                    答案.append(類型)
        cls.恬音模型 = svm.SVC()
        cls.恬音模型.fit(題目, 答案)

    @classmethod
    def _特徵轉陣列(cls, 特徵):
        if 特徵['相關係數'] is not None:
            return [特徵['平方平均'], 特徵['過零機率'], 特徵['相關係數']]
        return [特徵['平方平均'], 特徵['過零機率'], 100.0]

    @classmethod",bizu/切錄音檔.py,Taiwanese-Corpus/kaxabu_muwalak_misa_a_ahan_bizu,1
"    :param y_train: A dictionary with the following structure
            { instance_id : sense_id }

    :return: svm_results: a list of tuples (instance_id, label) where labels are predicted by LinearSVC
             knn_results: a list of tuples (instance_id, label) where labels are predicted by KNeighborsClassifier
    '''

    svm_results = []
    knn_results = []

    svm_clf = svm.LinearSVC()
    knn_clf = neighbors.KNeighborsClassifier()

    # implement your code here

    return svm_results, knn_results

# A.3, A.4 output
def print_results(results ,output_file):
    '''",Assignment3/A.py,ugaliguy/Intro-to-NLP,1
"    X_train, X_test, y_train, y_test = \
        sklearn.cross_validation.train_test_split(
            X, y, test_size=0.33, random_state=42)

    clf = sklearn.pipeline.make_pipeline(
        sklearn.preprocessing.MinMaxScaler(),
        sklearn.decomposition.RandomizedPCA(
            n_components=128,
            random_state=210,
        ),
        sklearn.svm.SVC(
            random_state=14,
            C=0.03,
            kernel='linear',
            class_weight={0: 7, 1: 2},
        ),
    )
    clf.fit(X_train, y_train)
    print(sklearn.metrics.classification_report(y_test, clf.predict(X_test)))
    clf.fit(X, y)",cities/train_clf.py,Alexander-Morgun/CityPic,1
"data = np.genfromtxt(""classification.txt"")
X = data[:, 0:2]
t = data[:, 2]

pl.figure(figsize=(18, 5))
no = 1

# それぞれのカーネルでSVMを学習
for kernel in ('linear', 'poly', 'rbf'):
    # 分類器を訓練
    clf = svm.SVC(kernel=kernel, C=10000)
    clf.fit(X, t)

    pl.subplot(1, 3, no)
    cmap1 = ListedColormap(['red', 'blue'])
    cmap2 = ListedColormap(['#FFAAAA', '#AAAAFF'])

    # 訓練データをプロット
    pl.scatter(X[:, 0], X[:, 1], c=t, zorder=10, cmap=cmap1)
    pl.scatter(clf.support_vectors_[:, 0],",sklearn/nonlinear_svm2.py,sylvan5/PRML,1
"    return np.asarray([np.dot(self.W, trial) for trial in X])


class ChanVar(base.BaseEstimator, base.TransformerMixin):
  def fit(self, X, y): return self
  def transform(self, X):
    return np.var(X, axis=2)  # X.shape = (trials, channels, time)


pipe = pipeline.Pipeline(
  [('csp', CSP()), ('chan_var', ChanVar()), ('svm', svm.SVC(kernel='linear'))])

# Create mask for same train and test data as used in competition:
test = folds == 1
train = ~test

# train model
pipe.fit(trials[train], y[train])

# make predictions on unseen test data",examples/ex_csp_motor_imagery.py,dreamyourweb/eegtools,1
"from sklearn import svm
from sklearn.ensemble import ExtraTreesClassifier
import csv
import ingestor
import validator

training_data, training_target, test_data, test_target = ingestor.get_data(
    '../clean_data.csv')


clf = svm.SVC()
clf.fit(training_data, training_target)

print validator.validate(clf, test_data, test_target)",models/svm.py,ssundarraj/music_genre_classifier,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",python-2.7.12-lib/email/test/test_email.py,wang1352083/pythontool,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.SVC(C=1., gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf).export()
# output = Porter(clf, language='java').export()
print(output)

""""""
class Brain {
",examples/classifier/SVC/java/basics.py,nok/sklearn-porter,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = fisher_score.fisher_score(X[train], y[train])

        # rank features in descending order according to score
        idx = fisher_score.feature_ranking(score)
",PyFeaST/example/test_fisher_score.py,jundongl/PyFeaST,1
"            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'linear-svm':
                from sklearn.svm import LinearSVC
                self.estimator_ = LinearSVC(
                    random_state=self.random_state, **self.kwargs)
            else:
                raise NotImplementedError
        else:
            raise ValueError('Invalid parameter `estimator`. Got {}.'.format(
                type(self.estimator)))

        self.logger.debug(self.estimator_)
",imblearn/ensemble/balance_cascade.py,scikit-learn-contrib/imbalanced-learn,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    vectorizer = EpochsVectorizer()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,choldgraf/mne-python,1
"

def svm_class(X_train, y_train, X_test, y_test):
    # # logistic = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg', n_jobs=multiprocessing.cpu_count())
    # svc_lin = SVC(kernel='linear', class_weight='balanced')
    # svc_lin.fit(X_train, y_train)
    # y_lin = svc_lin.predict(X_test)
    # y_tlin = svc_lin.predict(X_train)
    # print ""SVM Performance""
    # print_scores(y_tlin, y_train, y_lin, y_test)
    model = svm.LinearSVC(penalty='l1', dual=False)
    model.fit( X_train, y_train )
    pred_train_classes = model.predict( X_train )
    pred_test_classes = model.predict( X_test )
    print_scores(pred_train_classes, y_train, pred_test_classes, y_test)


def pre_classify_text(X_train, y_train, X_test, y_test=None):
    corpus = np.append(X_train, X_test)
    from sklearn.feature_extraction.text import HashingVectorizer",data_util.py,wtgme/labeldoc2vec,1
"Created on Mar 16, 2017

@author: Leo Zhong
'''
from sklearn import svm

X = [[2, 0], [1, 1], [2,3]] #data point
y = [0, 0, 1] #define type

#clf: classifier
clf = svm.SVC(kernel = 'linear') #get function
clf.fit(X, y) #set model

print (clf)

# get support vectors
print (clf.support_vectors_)

# get indices of support vectors
print (clf.support_ )",Support_Vector_Machine(SVM)/SVM_Sample.py,LeoZ123/Machine-Learning-Practice,1
"sample_weight = np.ones(y.shape[0])
classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))
 
###############################################################################
print(""PREPARE CLASSIFICATION"")
#-- classifier
if np.size(C)>1:
    if clf_type == ""SVC"":
        clf = GridSearchCV(svm.SVC(kernel='linear', probability=compute_probas),
        {'C': C}, score_func=precision_score)
    elif clf_type == ""SVR"":
        clf = GridSearchCV(svm.SVR(kernel='linear'),
        {'C': C}, score_func=precision_score)
        compute_distance = False
        compute_probas = False
        compute_predict = True
    elif clf_type == ""LogisticRegression"":
        clf = GridSearchCV(linear_model.LogisticRegression(penalty='l2'),",JR_toolbox/skl_king_parallel.py,kingjr/natmeg_arhus,1
"			else:
				train_data[count_train] = data[x]
				train_label[count_train] = label[x]
				count_train += 1

		if count_eval != size_sample:
			print (""ATTENTION FOR INT"")
		return train_data, train_label, eval_data, eval_label

	def svm_train(self, data, label):
		clf = svm.SVC()
		print (""training"")
		clf.fit(data, label)
		print (""done"")
		return clf

	def svm_predict(self, clf, data):
		result = clf.predict(data)
		return result
",work/ML/tensorflow/practice/tool.py,ElvisLouis/code,1
"    from scipy.io import loadmat,savemat
    import cProfile
    # X = np.random.multivariate_normal([0, 0], [[0.5, 0], [0, 0.5]], 100)
    # y = -np.ones(X.shape[0])
    # Xa = np.random.rand(5, 2)
    # ya = np.ones(Xa.shape[0])
    # Xtr, ytr = np.r_[X, Xa], np.r_[y, ya]
    # data = savemat(""syndata.mat"", mdict={'Xtr': Xtr, 'ytr': ytr})
    data = loadmat('../datasets/syndata.mat')
    Xtr, ytr = data['X'], data['Y']
    clf1 = H3iSVC(C=0.005, gamma=0.5, kernel='linear', display=False, labeling=False)
    cProfile.run(""clf1.fit(Xtr)"")
    # from H3iSVC_QP import H3iSVC_QP as svcqp
    # clf2 = svcqp(C=0.05, gamma=1.5, kernel='rbf')
    # clf2.fit(Xtr)
    print '--- done! ---'
    plt.subplot(1,1,1)
    clf1.draw_model(Xtr)
    # # plt.subplot(1,2,2)
    # # clf2.draw_model(Xtr)",Python/H3iSVC.py,feuerchop/IndicativeSVC,1
"    if output:
        output_markdown(output, Approach='Vader', Dataset='labeled_tweets',
            Instances=n_instances, Results=metrics_results)

if __name__ == '__main__':
    from nltk.classify import NaiveBayesClassifier, MaxentClassifier
    from nltk.classify.scikitlearn import SklearnClassifier
    from sklearn.svm import LinearSVC

    naive_bayes = NaiveBayesClassifier.train
    svm = SklearnClassifier(LinearSVC()).train
    maxent = MaxentClassifier.train

    demo_tweets(naive_bayes)
    # demo_movie_reviews(svm)
    # demo_subjectivity(svm)
    # demo_sent_subjectivity(""she's an artist , but hasn't picked up a brush in a year . "")
    # demo_liu_hu_lexicon(""This movie was actually neither that funny, nor super witty."", plot=True)
    # demo_vader_instance(""This movie was actually neither that funny, nor super witty."")
    # demo_vader_tweets()",lib/python2.7/site-packages/nltk/sentiment/util.py,akhilari7/pa-dude,1
"Xtrain, ytrain, Xtest, ytest = cub.get_train_test(feature_extractor.extract_one)
Xtrain_c, ytrain_c, Xtest_c, ytest_c = cub.get_train_test(feature_extractor_c.extract_one)

print Xtrain.shape, ytrain.shape
print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(numpy.concatenate((Xtrain, Xtrain_c), 1), ytrain)
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(numpy.concatenate((Xtest, Xtest_c), 1))
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_concat.py,yassersouri/omgh,1
"            ""tst-Common.kern.bin"")

        # Load training/testing kernels
        trn_kern = np.fromfile(trn_kern_fname,
                               dtype=""double"").reshape(N,N)
        tst_kern = np.fromfile(tst_kern_fname,
                               dtype=""double"").reshape(T,N)


        # Train/test classifier to get predictions
        clf = svm.SVC(kernel=""precomputed"",C=1)
        clf.fit(trn_kern,trn_lab)
        lab_hat = clf.predict(tst_kern)

        # Show confusion matrices
        cm = confusion_matrix(lab_hat, tst_lab)
        print ""Confusion matrix for %d-th CV run"" % cv
        print cm

        # Keep track of nr. of SVs / nr. training samples",Examples/TubeGraphKernels/permtest.py,sumedhasingla/TubeTK,1
"            for line in outString.split('\n'):
                if 'Accuracy' in line:
                    accuracy = float(line.split(':')[1].strip())
                    accSvmPerm[counter] = accuracy
                    print('Accuracy', accuracy)

            print('********************************')

            counter += 1;

        # svc = SVC(C=1.0, kernel='linear')
        # svc.fit(X_Tr, Y_Tr)
        # svcLabelsS = svc.predict(X_Tr)
        # svcLabelsT = svc.predict(X_Te)
        # print(""SVM:Linear"", accuracy_score(Y_Te, svcLabelsT))
        # svc = SVC(C=1.0,kernel='linear')
        # svc.fit(XS,labelsS)
        # svcLabelsS = svc.predict(XS)
        # svcLabelsT = svc.predict(XT)
        # print(""SVM:Linear"",accuracy_score(labelsT,svcLabelsT))",textanalysis/data/multiDomainSentiment.py,arunreddy/text-analysis,1
"- 1 green
- 2 orange
- 3 purple
- 4 yellow'''


class ImgRecognizer:
    def __init__(self):
        self.training_data = []
        self.target_values = []
        self.svc = svm.SVC(gamma=0.001, kernel='linear', C=100)
        self.downscale_res = (32, 32)

    def _load(self, path, target_value):
        training_imgs = os.listdir(path)
        for f in training_imgs:
            img = Image.open(path+'/'+f)
            img = img.resize(self.downscale_res, Image.BILINEAR)
            self.training_data.append(np.array(img.getdata()).flatten())
            self.target_values.append(target_value)",sklearn_decoder.py,vlaufer/candy_crush_bot_mac,1
"_data_keys            = [key for key in _class_labels for vec in _all_data[key]]
_d_values             = { key : [vec for vec in _all_data[key]] for key in _class_labels }
_d_keys               = { key : [key] * len(_all_data[key]) for key in _class_labels }
X_train, X_test, y_train, y_test = train_test_split(_data_values, _data_keys, test_size=0.5, random_state=0)

############################################
## Train a new support vector classifier. ##
############################################

_kernel               = 'poly' # ['linear', 'poly', 'rbf', 'sigmoid']
_svc                  = SVC(kernel=_kernel)
_svc                  .fit(X_train, y_train)

##############################
## Print out accuracy data. ##
## Create confusion matrix. ##
##############################

print ""Classifier trained with the %s kernel."" % _kernel
score = cross_val_score(_svc, X_test, y_test, cv=5)",src/svc.py,spacenut/vowel-svm,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Storing the predictions of the trained classifier on the testing dataset...""
predicted = clf.predict(X_test_vect_red)

print ""Evaluation results of the content-based engine working alone (predicting only one developer): ""

import numpy as np
print ""Accuracy: %s"" % np.mean(predicted == y_test)",Archive/RecoDev-Evaluator/recodev_eval1-2.py,amirhmoin/recodev,1
"
class Classifier(object):

    def __init__(self):
        data = Fish.objects.values('size', 'color', 'name')
        self.X = []
        self.y = []
        for e in data:
            self.X.append([e['size'], e['color']])
            self.y.append(e['name'])
        self.clf = svm.SVC()

    def train(self):
        self.clf.fit(self.X, self.y)

    def classify(self, targets):
        if len(targets) == 0:
            return False
        else:
            self.train()",www/classifier.py,andres-root/fishfrontend,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",fail/340_test_email.py,mancoast/CPythonPyc_test,1
"	f_score   = 2*( (precision*recall) / (precision+recall) )
	return (accuracy, precision, recall, f_score)
	
delimiter = ""\t""

with codecs.open(""aggregated_goldsmith_features.txt"",""r"",""utf-8"") as goldy:
    
	goldy = goldy.read().replace(""\r"","""").split(""\n"")[1:-1]
	
	#populate a list of classifiers 
	support_vector_c      = SVC(C=1)
	decision_tree_c       = DecisionTreeClassifier(max_depth=5) #with 4 features, depth of 2 is best; with 6 features depth of 4
	nearest_neighbors_c   = KNeighborsClassifier(6)
	naive_bayes_c         = GaussianNB()
	lda_c                 = LDA()
	qda_c                 = QDA()
	random_forests_c      = RandomForestClassifier(max_features=2, max_depth=2)
	ada_boost_c           = AdaBoostClassifier()
	logistic_regression_c = LogisticRegression()
	",classify/multi_classifier.py,duhaime/detect_reuse,1
"
	matrix_w = eig_pairs[0][1].reshape(featureSize,1)
	for i in range(200):
		matrix_w = numpy.hstack((matrix_w, eig_pairs[i+1][1].reshape(featureSize,1)))
	print matrix_w.shape

	transformed = matrix.dot(matrix_w)
	print transformed.shape

	#Start to train SVM
	Z = OneVsRestClassifier(LinearSVC()).fit(transformed, labels).predict(transformed)
	print Z
	correct = 0.0
	for x in range(len(Z)):
		if labels[x] == Z[x]:
			correct = correct +1

	print correct/len(Z)
	#recData = transformed.dot(matrix_w.T) + matrix.mean(axis=1)[:, None]
	#plot(recData[0].reshape((32,32)))",myproject/myproject/myapp/pca.py,motian12ps/Bigdata_proj_yanif,1
"Y = dataset[:, -1]

split = 0.75

spltv = int(split * len(Y))
X_train = X[:spltv,:]
Y_train = Y[:spltv]
X_test  = X[spltv:,:]
Y_test  = Y[spltv:]

svc = svm.SVC(kernel='rbf', gamma=0.01, cache_size=1000)
svc.fit(X_train, Y_train)

print(""testing data..."")
predicted = svc.predict(X_test)
print(metrics.classification_report(Y_test, predicted))
print(metrics.confusion_matrix(Y_test, predicted))",aula8/svm2.py,elmadjian/pcs5735,1
"df.max()
df.min()
type(predictors)
    
#==============================================================================
# Model fitting part 2?
#==============================================================================

#==============================================================================
# # fit the model
# clf = svm.NuSVC()
# clf.fit(predictors, outcomes)
# 
# # plot the decision function for each datapoint on the grid
# Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
# Z = Z.reshape(xx.shape)
# 
# plt.imshow(Z, interpolation='nearest',
#            extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
#            origin='lower', cmap=plt.cm.PuOr_r)",ModelFitting/single_SVM.py,georgetown-analytics/nba-tracking,1
"n_iter = 5
cv = ShuffleSplit(n_samples, n_iter=n_iter, train_size=500, test_size=500,
    random_state=0)

train_scores = np.zeros((n_gammas, n_iter))
test_scores = np.zeros((n_gammas, n_iter))
gammas = np.logspace(-7, -1, n_gammas)

for i, gamma in enumerate(gammas):
    for j, (train, test) in enumerate(cv):
        clf = SVC(C=10, gamma=gamma).fit(X[train], y[train])
        train_scores[i, j] = clf.score(X[train], y[train])
        test_scores[i, j] = clf.score(X[test], y[test])",grid_search_implementation.py,dropofwill/author-attr-experiments,1
"        words = p.get_words(filter=lambda x: x[""num""], flatten=True)
        
        return X, words


    def model(self, C=1.0):
        # clf = Pipeline([
        #                 ('feature_selection', RandomizedLogisticRegression()),
        #                 ('classification', SVC(probability=True))
        #                ])
        # clf = SVC(C=C, kernel='linear', probability=True)
        clf = LogisticRegression(C=C, penalty=""l1"")
        
        
        return clf




    def predict_population_text(self, text, clf):",bilearn_supervised.py,ijmarshall/cochrane-nlp,1
"#    print_debug(train_labels, ""train_labels"")

    for j in range(TEST_SIZE):
        test_labels[j] = np.sum(np.multiply(converter, mnist.test.labels[j, :]))

#    print_debug(test_features, ""test_features"")
#    print_debug(test_labels, ""test_labels"")

    initial_time = time.time()

    clf = svm.SVC(kernel=krnl)
    clf.fit(train_features, train_labels)
    training_time = time.time()-initial_time
    print(""\nTraining Time = "", training_time)

    accuracy = clf.score(test_features, test_labels)
#    test_time = time.time() - (training_time + initial_time)
#    print(""\nTest Time = "", test_time)

    print(""\n"", krnl, ""kernel SVM accuracy ="", accuracy)",cnn_main.py,isabel-schwende/one-class-svm-rnn,1
"#!/usr/bin/python

from sklearn import svm
from numpy import genfromtxt, savetxt
import uniqueNames

def main():
    clf = svm.SVC(gamma=0.001, C=100)    

    #create the training & test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('Data/trainPrep.csv','r'), delimiter=',', dtype='f8')[1:]    
    target = [x[-1] for x in dataset]
    train = [x[:-1] for x in dataset]
    test = genfromtxt(open('Data/testPrep.csv','r'), delimiter=',', dtype='f8')[1:]
    test =  [x[1:] for x in test]

    svc=clf.fit(train, target)",poker/makeSubmission.py,tdvance/kaggle_submissions,1
"    va_accs = []
    te_accs = []
    for _ in tqdm(range(10), leave=False, ncols=80):
        idxs = np.arange(len(trX))
        classes_idxs = [idxs[trY==y] for y in range(10)]
        sampled_idxs = [py_rng.sample(class_idxs, 100) for class_idxs in classes_idxs]
        sampled_idxs = np.asarray(sampled_idxs).flatten()

        trXt = features(trX[sampled_idxs])

        model = LSVC(C=c)
        model.fit(trXt[:1000], trY[sampled_idxs])
        tr_pred = model.predict(trXt)
        va_pred = model.predict(vaXt)
        tr_acc = metrics.accuracy_score(trY[sampled_idxs], tr_pred[:1000])
        va_acc = metrics.accuracy_score(vaY, va_pred)
        tr_accs.append(100*(1-tr_acc))
        va_accs.append(100*(1-va_acc))
    mean_va_accs.append(np.mean(va_accs))
    print 'c: %.4f train: %.4f %.4f valid: %.4f %.4f'%(c, np.mean(tr_accs), np.std(tr_accs)*1.96, np.mean(va_accs), np.std(va_accs)*1.96)",svhn/svhn_semisup_analysis.py,Newmu/dcgan_code,1
"        showSaveReport(report);
    
    return peaks
    
#########################################################################################
##################################################################################
###################################################################################
###Test for runEvaluation###
##Initialize the classifier
##clf = KNeighborsClassifier(1);
#clf = svm.LinearSVC()
##clf = LogisticRegression
#clfName = 'linearSVC'
#clfName = 'RandomForest'
###classifier =DecisionTreeClassifier();
#classifier = RandomForestClassifier();
##################################to run
#dirPath = 'C:\\Users\\LLP-admin\\workspace\\weka\\token-experiment-dataset\\';
#fname = 'user_' + str(0)
#fmt = '.csv'",simpleBatch_v2.py,cocoaaa/ml_gesture,1
"	fig.set_tight_layout(True)
	plt.show()



X_train, X_test, y_train, y_test = load('Datasets/optdigits.tes', 'Datasets/optdigits.tra')
peekData()

print ""Training SVC Classifier...""

model = SVC(kernel ='rbf',C=1,gamma=0.001)
model.fit(X_train,y_train)

print ""Scoring SVC Classifier...""
score = model.score(X_test,y_test)
print ""Score:\n"", score

drawPredictions()

true_1000th_test_value = X_test.iloc[1000]",MLandDS/MachineLearning/SVC-optical.py,adewynter/Tools,1
"        \\end{cases}

    The provided SVM classifier must be a sklearn's SVM classifier (SVC, LinearSVC, NuSVC)
    providing the decision_function() which computes the distance to decision boundary. This nonconformity works
    only for binary classification problems.

    Examples:
        >>> from sklearn.svm import SVC
        >>> train, test = next(LOOSampler(Table('titanic')))
        >>> train, calibrate = next(RandomSampler(train, 2, 1))
        >>> icp = InductiveClassifier(SVMDistance(SVC()), train, calibrate)
        >>> print(icp(test[0].x, 0.1))
    """"""

    def __init__(self, classifier):
        assert isinstance(classifier, (SVC, LinearSVC, NuSVC)), \
            ""Classifier must be a sklearn's SVM classifier (SVC, LinearSVC, NuSVC).""
        self.clf = classifier
        self.model = None
",ConfPred/conformal-master/cp/nonconformity.py,JonnaStalring/AZOrange,1
"    config_d = configParserWrapper.load_settings(open(args.config,'r'))

    penalty_names = config_d['SVM']['penalty_list'][::2]
    penalty_values = tuple( float(k) for k in config_d['SVM']['penalty_list'][1::2])

    for penalty_name, penalty_value in itertools.izip(penalty_names,penalty_values):
        if args.v:
            print '%s %s' % ('linear', penalty_name)

        if config_d['SVM']['kernel'] == 'linear':
            clf = svm.LinearSVC(C=penalty_value,
                                loss='l1')
            clf.fit(X,y)
        else:
            import pdb; pdb.set_trace()



        coef = clf.coef_.reshape(clf.coef_.size)
        y_hat = np.dot(X,coef) + clf.intercept_[0]",local/CTrainComponentSVMs.py,markstoehr/phoneclassification,1
"### we can exactly check your result

#!->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(
    features, labels, test_size=0.4, random_state=0)
#!->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


###############################################################

clf = SVC(kernel=""linear"", C=1.)
clf.fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################
def submitAcc():",learn/sklearn/train-test-split.py,enlighter/learnML,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.pipeline import Pipeline

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,dengemann/mne-python,1
"
#Somente o nome do arquivo
if __name__=='__main__':

    C = range(1, 100)
    for c in C:
        for file in glob.glob(os.path.join(sys.argv[1], '*.mat')):
            data = scipy.io.loadmat(file)

            print(""\nTreinando SVM multi classe..."")
            clf = sklearn.svm.SVC(kernel=""linear"", C=float(c))

            ytrain = data['Ytrain'].T.reshape(data['Ytrain'].shape[1])
            clf.fit(data['Xtrain'], ytrain)
            predict = clf.predict(data['Xval'])

            yVal = data['Yval'].T.reshape(data['Yval'].shape[1])
            print ""Acuracia: "", sklearn.metrics.accuracy_score(yVal, predict)

            cm = confusion_matrix(yVal, predict)",svm_optimize.py,alan-mnix/MLFinalProject,1
"    s.output_path = osp.join(s.output_root, 'svm.h5')
    pf.mkdir_p(s.output_path)

  def step(s):
    s.train()
    s.predict()
    s.write()
    s.halt_switch = True

  def train(s):
    s.model = svm.LinearSVC()
    s.model.fit(s.X, s.y)

  def predict(s):
    s.prediction = s.model.predict(s.X)
    s.accuracy = metrics.accuracy_score(s.prediction, s.y)
    s.confusion_matrix = metrics.confusion_matrix(s.y, s.prediction)

  def write(s):
    with h5py.File(s.output_path, 'w') as f:",sparco/supervised.py,smackesey/sparco,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIM.mim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_MIM.py,jundongl/scikit-feature,1
"from sklearn import svm
import re
from featureHelpers.py import *

#testStringList = getTitles(""test_data"" + os.sep + ""merged.txt"")
#testStringList = getTitles(""test.txt"")
""""""This is a function designed to extract an attribute vector out of the text of
a Craigslist posting. These attribute vectors will be fed to the SciKit Learn
module to determine the quality of the posting itself.""""""

clf = svm.SVC()

def extractVectorsFromListOfPosts(postList):
    
    def extractVectorFromPost(postText):
        upperCaseText = string.upper(postText)
        count = len(postText)
        whiteCount, letterCount, symbolCount, lowerCaseCount = 0, 0 ,0, 0
        for i in xrange(count):
            if postText[i] in string.whitespace: whiteCount += 1",vectorExtractorWithTuples.py,bharadwajramachandran/TartanRoof,1
"
  X = preprocessing.scale(X)

  return X,y

def Analysis():
  test_size = 1000
  X, y = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    if clf.predict(X[-x])[0] == y[-x]: # test data
      correct_count += 1

  print(""correct_count=%s""%float(correct_count))
  print(""test_size=%s""%float(test_size))",p19.py,cleesmith/sentdex_scikit_machine_learning_tutorial_for_investing,1
"    print params
    #comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data_optimisation.0.0.txt"",os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data_optimisation.200.1.txt"")]
    #comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_5_periods10D_1000points_optimisation_sample_0.txt"",os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/accept_reject/legendre_data/data_sin1diff_5_and_6_periods10D_1000points_optimisation_sample_0.txt"")]
    #comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_4D_1000_0.6_0.2_0.1_optimisation_0.txt"",os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_4D_1000_0.6_0.2_0.075_optimisation_0.txt"")]    
    param_name = 'pTmin'
    param_dict = {'alphaSvalue': 0.125, 'pTmin': 0.8}
    comp_file_list=[(os.environ['monash']+""/GoF_input/GoF_input_udsc_monash_optimisation.txt"" , os.environ['monash']+""/GoF_input/GoF_input_udsc_{}_{}_optimisation.txt"".format(str(param_dict[param_name]),param_name))]

    name = ""monash_""+param_name

    clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)

    result= classifier_eval.classifier_eval(name=""svm_""+name,comp_file_list=comp_file_list,clf=clf,mode=""spearmint_optimisation"")

    with open(""svm_optimisation_values_""+name+"".txt"", ""a"") as myfile:
        myfile.write(str(params['aC'][0])+""\t""+ str(params['agamma'][0])+""\t""+str(result)+""\n"")
    return result",learningml/GoF/optimisation_and_evaluation/not_automated/svm_opt/classifier_eval_wrapper.py,weissercn/learningml,1
"





    modeler.models = {    ""KNeighbors_default"": sklearn.neighbors.KNeighborsClassifier()
                      , ""RandomForest"": sklearn.ensemble.RandomForestClassifier()
                      , ""LogisticRegression"": sklearn.linear_model.LogisticRegression(penalty='l1', C=0.1)
                      , ""GaussianNB"": GaussianNB()
                      , ""SVC_rbf"": SVC(kernel = 'rbf', probability = True, random_state = 0)
                      , ""SVC_linear"": SVC(kernel = 'linear', probability = True,  random_state = 0)
                      , ""SVC_poly"": SVC(kernel = 'poly', degree = 3, probability = True,  random_state = 0)
                      }

    #Different method for KNeighbors allows us to compare multiple k's
    for i in range(3,13):
        modeler.models[""KNeighbors_{}"".format(i)] = sklearn.neighbors.KNeighborsClassifier(n_neighbors=i)

",presentation/code_snippets.py,georgetown-analytics/housing-risk,1
"                           memory_level=1)
X = nifti_masker.fit_transform(dataset_files.func)
# Restrict to non rest data
X = X[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",plot_haxby_grid_search.py,ainafp/nilearn,1
"class Classifier(object):
  def __init__(self):
    self.classifier = self._NewClassifier()
    self.labels_enum = StrEnumerator()
    self.features_enum = StrEnumerator()

  def _NewClassifier(self):
    if args.classifier == ""RandomForest"":
      return ensemble.RandomForestClassifier(n_estimators=400)
    elif args.classifier == ""SVM"":
      return svm.SVC(kernel='rbf', gamma=0.0001, C=1000000.0)
    elif args.classifier == ""GBRT"":
      return ensemble.GradientBoostingClassifier(
          n_estimators=100, learning_rate=0.2, max_depth=2, random_state=0)
    else: 
      assert False, (""Unknown classifier:"", args.classifier)

  def FromSparse(self, Xsparse):
    X = []
    for sparse in Xsparse:",src/classify.py,ytsvetko/adjective_supersense_classifier,1
"raw_test.drop('Dates', axis=1, inplace=True)
raw_test.drop('Id', axis=1, inplace=True)

print(""Casting data"")
columns = ['Year', 'Month', 'Hour', 'DayOfWeek', 'X', 'Y']
np_train = np.array(raw_train[columns]).astype(float)
np_train_category = np.array(raw_train_category).astype(float)
np_test = np.array(raw_test[columns]).astype(float)

print(""Training"")
clf = svm.SVC()
clf.fit(np_train, np_train_category)

print(""Predicting"")
test_classification = clf.predict(np_test)

print(""Saving to csv"")
np.savetxt('../results/sf-crime-submission.csv', test_classification, fmt='%f', delimiter=',')

print(""Done!"")",src/adele_hacking.py,gnu-user/sf-crime-classification,1
"    top_fi = fi.head(5)
    print('Feature importances: {}'.format(top_fi))

    plt.figure(figsize=(10,4), dpi=80)
    plot_feature_importances(clf, X_test2.columns)
    plt.show()

def answer_six():

    param_range = np.logspace(-4,1,6)
    train_scores, test_scores = validation_curve(SVC(), X_subset, y_subset,  param_name='gamma', 
                                                 param_range=param_range, cv=3)
    
    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    return (train_scores_mean, test_scores_mean)

def answer_seven():
    return (10e-4, 10, 1)",pyml/assignment2_part2.py,polde-live/pyml,1
"    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]

    print ""Train data size: "" + str(int(DATA_SIZE*0.8))
    print ""Test  data size: "" + str(len(X_target)-int(DATA_SIZE*0.8))


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(linear_model.LinearRegression())
    # regr = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True))
    # regr = OneVsRestClassifier(svm.SVC(kernel='rbf'))
    # regr = OneVsRestClassifier(svm.SVC(kernel='poly', probability=True, degree = 5))
    # regr = cluster.KMeans(n_clusters=100)    
    regr = neighbors.KNeighborsClassifier()


    # train the model using the training data
    # regr.fit(X_train, X_target)
    regr.fit(data_X_train, data_Y_train)",DST_v1.04.py,totuta/deep-supertagging,1
"    
    # Initialize SVD
    svd = TruncatedSVD(n_components=300,random_state=None)
    X=np.hstack([svd.fit_transform(X),np.asarray(trv).reshape(10158,5)])
    X_test=np.hstack([svd.fit_transform(X_test),np.asarray(tsv).reshape(22513,5)])
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    svm_model = SVC()
    
    # Create the pipeline 
    clf = pipeline.Pipeline([('scl', scl),
                    	     ('svm', svm_model)])
    
    # Create a parameter grid to search for best parameters for everything in the pipeline
    param_grid = {'svm__C': [9]}
    
    # Kappa Scorer ",Search_result/srr4.py,tanayz/Kaggle,1
"from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from SecuML.Classification.Classifier import Classifier

class Svc(Classifier):

    def createPipeline(self):
        self.pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('model', svm.SVC(kernel = 'linear', probability = True))])",code/SecuML/Classification/Classifiers/Svc.py,ah-anssi/SecuML,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_forward.decision_tree_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeast/example/test_decision_tree_forward.py,jundongl/scikit-feast,1
"        print(""done in %fs"" % (time() - t0))
        print()
        feature_names = np.asarray(feature_names)
        #save_obj(ch2,path + Models[i]+""/ch2Model"")
        results = []

        for penalty in [""l2"", ""l1""]:
            print('=' * 80)
            print(""%s penalty"" % penalty.upper())
            # Train Liblinear model
            benchmark(LinearSVC(loss='l2', penalty=penalty,dual=False, tol=1e-3))

        # Train sparse Naive Bayes classifiers
        print('=' * 80)
        print(""Naive Bayes"")
        benchmark(MultinomialNB(alpha=.01))
        benchmark(BernoulliNB(alpha=.01))

        print('=' * 80)
        print(""LinearSVC with L1-based feature selection"")",Project/BaggedClassifierExplorer.py,tpsatish95/Youtube-Comedy-Comparison,1
"
if __name__ == ""__main__"":
    from sklearn.svm import SVC
    from sklearn.datasets import load_digits
    from sklearn.cross_validation import train_test_split

    digits = load_digits()
    X_train, X_test, y_train, y_test = train_test_split(
        digits.data, digits.target, random_state=0)

    clf = SVC(gamma=0.01).fit(X_train, y_train)

    summary = bootstrap_scorer('f1')(clf, X_train, y_train)
    print('f1 train score:')
    print(summary)

    summary = bootstrap_scorer('f1')(clf, X_test, y_test)
    print('f1 test score:')
    print(summary)",oglearn/scorer.py,ogrisel/oglearn,1
"        try:
            sample_weight = self.model.sample_weight
        except AttributeError:
            sample_weight = np.ones(labels.shape, dtype=np.float64)

        # FIXME add hyperparameter tuning via CV.
        if kernel == 'linear':
            params = {'C': 0.1}
        elif kernel == 'rbf':
            params = {'C': 1., 'gamma': 0.0005}
        clf = SVC(kernel=kernel, probability=True, random_state=13,
                  **params)
        clf.fit(samples, labels, sample_weight=sample_weight)

        train_err = 1.0 - clf.score(samples,
                                    labels, sample_weight=sample_weight)
        test_err = 1.0 - clf.score(self.model.test[:, :2],
                                   self.model.test[:, 2].ravel())
        X1, X2, Z = self.decision_surface(clf)
        self.model.set_trainerr(""%.2f"" % train_err)",checkerboard/checkerboard.py,pprett/dataset-shift-osdc16,1
"	with open('test_data.csv', 'r') as data_file:
		spamreader = csv.reader(data_file)
		for row in spamreader:
			test_data.append(row[1:])
	test_data = test_data[1:]
	for instance in test_data:
		instance[3] = int(instance[3])

	#start train with SVM
	if model_name == 'SVM':
		clf = svm.SVC()
	if model_name == 'decision_tree':
		clf = tree.DecisionTreeClassifier()
	if model_name == 'random_forest':
		clf = RandomForestClassifier()
	if model_name == 'logistic_regression':
		clf = LogisticRegression()
	if model_name == 'linear_regression' or model_name == 'neural_net':
		if model_name == 'linear_regression':
			clf = LinearRegression()",code/train_and_test.py,samfu1994/cs838webpage,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",ext_lib/email/test/test_email_renamed.py,sugarguo/Flask_Blog,1
"    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline

    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)

    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])

    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svn
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
",venv/lib/python2.7/site-packages/sklearn/pipeline.py,devs1991/test_edx_docmode,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_11_13_2014.py,magic2du/contact_matrix,1
"    thresholds.append(float(i))
    i += float(dist)
  return thresholds

def train_and_predict(xs, ys):
  accuracies = []
  f1_scores = []
  cs = range_array(0.05, 10, 0.05)

  for c in cs:
    clf = svm.SVC(kernel='linear', C=c, probability=True)
    clf.fit(xs[:TRAIN_SIZE], ys[:TRAIN_SIZE])

    logging.info(""current time %s"" % time.strftime(""%c"") )
    # logging.info(""support vectors:"")
    # logging.info(clf.support_vectors_)
    # logging.info(""number of support vectors for each class: "" + str(clf.n_support_))
    # logging.info(""threshold: ""+ str(clf._intercept_))

    ys_predicted = clf.predict(xs[TRAIN_SIZE:]) # Predict",wholesale_customers/optimize_c.py,Josephu/svm,1
"
#print(train.columns)
scl=preprocessing.MinMaxScaler()
X_train=scl.fit_transform(X_train)
X_val=scl.transform(X_val)
X_test=scl.transform(X_test)


from sklearn import svm, grid_search
#
cls=svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True,
            probability=True, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=100,
            decision_function_shape=None, random_state=0)
parameters = dict(C=[0.0001,0.001])


clf = grid_search.GridSearchCV(cls, parameters, cv=5,scoring='accuracy',verbose=3)

clf.fit(X_train, y_train)
",cervical-cancer-screening/models/src/SVConfeatures-new-NotGrouped.py,paulperry/kaggle,1
"
THREAD_POOL_SIZE = 6

OLD_DAYS = 30
FUTURE_DAYS = 10
TEST_DAYS = 10

DATA_Y_LEN = 1

#learn_model = LogisticRegression(C=1000.0, random_state=0)
#learn_model = SVC(kernel='linear', C=1.0, random_state=0)
learn_model = SVC(kernel='rbf', C=1.0, random_state=0, gamma=0.2)

# convert time series into supervised learning problem
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = pd.DataFrame(data)
    cols, names = list(), list()
    # input sequence (t-n, ... t-1)
    for i in range(n_in, 0, -1):",sklearn_stock.py,megahertz0/tusharedemo,1
"train_index = index[:N]
test_index  = index[N:]

X = df.ix[:, 1:9]
y = df[10]

imp = Imputer(strategy='mean', axis=0)
X = imp.fit_transform(X)

print 'Training..'
#clf = SVC()
clf = RandomForestClassifier()
clf.fit(X[train_index, :], y[train_index])

print 'Test'
print clf.predict(X[test_index, :])
print y[test_index]",glass.py,minhouse/python-lesson,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
        ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA()
    ]

X, y = make_classification(n_features=2, n_redundant=0,",python/sklearn/examples/plot_classifier_comparison.py,seckcoder/lang-learn,1
"                Xs.append(FindConditions(data, i, var))     # Find conditions for day 1
            X.append(Xs)                             
            
            y1 = PercentChange(data, i+1)                   # Find the stock price movement for day 2
            if y1 > 0: y.append(1)                          # If it went up, classify as 1
            else:      y.append(0)                          # If it went down, classify as 0
                
        XX = vstack(X)                                      # Convert to numpy array
        yy = hstack(y)                                      # Convert to numpy array
    
        model = SVC(C=self.C, gamma=self.gamma, probability=True)
        model.fit(XX, yy)
        
        # ====================== #
        #         Testing        #
        # ====================== #       
        
        testDay = testStart                                  
        while (testDay < testEnd):
            ",clairvoyant/Backtest.py,anfederico/Clairvoyant,1
"
        # the datasets are in ./training/ and ./test/
        x_training, y_training, training_cats = load_dataset('./training/', audio_len)

        print('categories:', file=sys.stderr)
        for value in training_cats.values():
            print(value)

        print('\nstarting SVM model fit...')

        clf = svm.LinearSVC()
        clf.fit(x_training, y_training)

        # test the classifier
        x_test, y_test, test_cats = load_dataset('./test/', audio_len)
        print('\ntest dataset accuracy score: ' + str(clf.score(x_test, y_test))+'\n')

        if(save_classifier_flag):
            save_classifier(clf, audio_len, test_cats, clf_filename)
    elif(predict_flag):",vci.py,wtanner/vci,1
"print(np.unique(iris_y))
np.random.seed(0)
trainSize = int(len(iris_X) * 0.9)
iris_X_train = iris_X[:trainSize]
iris_y_train = iris_y[:trainSize]
iris_X_test = iris_X[trainSize:]
iris_y_test = iris_y[trainSize:]



svc = svm.SVC(kernel='linear')
models = svc.fit(iris_X_train, iris_y_train)
print(models)
print(svc.predict(iris_X_test))",sklearnLearning/statisticalAndSupervisedLearning/linearSVMs.py,zhuango/python,1
"
    print(""Accuracy : "", float(len(correctly_predicted_rows))/len(test))


    
def train_SVM(data):

     predictors = [""overlapping"",""reoccur1"", ""reoccur2"", ""reoccur3"", ""reoccur4"", ""reoccur5"", ""reoccur6"",""euclidean"",""refuting_feature_count"",""char_length_headline"",""char_length_body""]
     response = data.Stance

     clf = sklearn.svm.LinearSVC().fit(data[predictors],response)

     return clf



def train_logistic(data):
    """"""
    
    Arguments:",code/python/classifier.py,srjit/fakenewschallange,1
"         feat_generator=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))),
    dict(name=""tfidf_ng3"",
         feat_generator=TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 3))),
]

# classifiers
classifiers = [   dict(name=""Logistic Regression"", parameter_tunning=False,
         tune_clf=GridSearchCV(LogisticRegression(), [{'penalty': ['l2'], 'C': [1, 10, 100]}], cv=3),
         clf=LogisticRegression(penalty='l2', C=1)),
    dict(name=""Passive Aggresive"", parameter_tunning=False, clf=PassiveAggressiveClassifier(n_iter=100)),
    dict(name=""SVM"", parameter_tunning=False, clf=LinearSVC(loss='l2', penalty=""l2"", dual=False, tol=1e-3)),
    dict(name=""Perceptron"", parameter_tunning=False, clf=Perceptron(n_iter=100)),
    #
    dict(name=""bnb"", parameter_tunning=False, clf=BernoulliNB(binarize=0.5)),
    dict(name=""mnb"", parameter_tunning=False, clf=MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)),
    dict(name=""sgd"", parameter_tunning=False, clf=SGDClassifier(loss=""hinge"", penalty=""l2"")),
    dict(name=""KNN"", parameter_tunning=False, tune_clf=GridSearchCV(KNeighborsClassifier(),
        [{'n_neighbors': [5, 10, 50, 100], 'metric': ['euclidean', 'minkowski'], 'p': [2, 3, 4, 5]}], cv=5),
         clf=KNeighborsClassifier(n_neighbors=5, metric='euclidean')),
",python/Definations.py,mahmoudnabil/ASTD,1
"def train_classifier(data_path, out_path, plot_title,  plots=True, n_jobs=1):
    """"""

    :param model: gansim.Word2Vec initialized with word vectors
    :param data: tuple - (x, y), where x is vector fo phrase, y is label for that vector
    :return: trained classifier
    """"""
    x_train, y_train, x_test, y_test = load_doc2vec_data(data_path)
    print 'Data loaded'

    clf = SVC()
    if plots:
        plot_learning_curve(clf, plot_title, x_train, y_train, n_jobs=n_jobs)
        plt.savefig(join(out_path, '%s.png' % plot_title))
    # x_train, x_test, y_train, y_test = train_test_split(data[0], data[1])

    clf.fit(x_train, y_train)
    predicted_test = clf.predict(x_test)
    acc_test = accuracy_score(y_test, predicted_test)
    print 'Classifier test accuracy: %f' % acc_test",salib/models/svm/svm.py,plazowicz/salib,1
"from sklearn import svm
from sklearn import cross_validation
from sklearn import metrics

from HashForestSVM import data, learn

n = 2000
X, y = data.genXOR(n)
Xtest, ytest = data.genXOR(2000)

clf = svm.SVC(gamma=0.1, probability=True)
clf.fit(X, y)

yhat = clf.predict(Xtest)
print(""SVM.Accuracy: %1.4f"" % np.mean( yhat==ytest ))

yprob = clf.predict_proba(Xtest)
precision, recall, th = metrics.precision_recall_curve(ytest, yprob[:,1])
auc = metrics.auc(recall, precision)
",Scripts/do-svm.py,jaak-s/HashForestSVM,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",fail/332_test_email.py,mancoast/CPythonPyc_test,1
"
    print(k + ': fitting model')
    ''' Best so far - clf = GradientBoostingClassifier(n_estimators=200, verbose=True)
    normal-1 conf matrix:
    [[246548    246]
    [  2506    450]]
    normal-1 roc auc 2: 0.917382608619
    '''
    clf = GradientBoostingClassifier(n_estimators=200, verbose=True)
    # clf = LogisticRegression(C=1,penalty='l1', class_weight={0:1,1:8})
    # clf = SVC(C=1, kernel='poly')
    clf.fit(X,y)
    s = pickle.dumps(clf)
    f = open(model_file, 'w')
    f.write(s)
    f.close()

    f = open(model_file, 'r')
    s2 = f.read()
    clf2 = pickle.loads(s2)",cnct_graph_analyze.py,ecodan/kaggle-connectomix,1
"# SVM results
from sklearn.svm import SVC
from sklearn import metrics

for kernel in ['rbf', 'linear']:
    clf = SVC(kernel=kernel).fit(Xtrain, ytrain)
    ypred = clf.predict(Xtest)
    print(""SVC: kernel = {0}"".format(kernel))
    print(metrics.f1_score(ytest, ypred))
    plt.figure()
    plt.imshow(metrics.confusion_matrix(ypred, ytest),
               interpolation='nearest', cmap=plt.cm.binary)
    plt.colorbar()
    plt.xlabel(""true label"")
    plt.ylabel(""predicted label"")",notebooks/solutions/04_svm_rf.py,brianjwoo/sklearn_pycon2014,1
"nuts = ImageSet(data_path + '/data/supervised/nuts')
nut_blobs = [n.invert().find(Blob)[0] for n in nuts]
for n in nut_blobs:
    tmp_data.append([n.area, n.height, n.width])
    tmp_target.append(1)

dataset = np.array(tmp_data)
targets = np.array(tmp_target)

print 'Training Machine Learning'
clf = LinearSVC()
clf = clf.fit(dataset, targets)
clf2 = LogisticRegression().fit(dataset, targets)

print 'Running prediction on bolts now'
untrained_bolts = ImageSet(data_path + '/data/unsupervised/bolts')
unbolt_blobs = [b.find(Blob)[0] for b in untrained_bolts]
for b in unbolt_blobs:
    ary = [b.area, b.height, b.width]
    name = target_names[clf.predict(ary)[0]]",machine-learning/machine_learning_nuts_vs_bolts.py,sightmachine/simplecv2-examples,1
"        X_train_vect_red = DepickleMe('svm_train_input.pkl')
        X_test_vect_red = DepickleMe('svm_test_input.pkl')
        
        
    
    
    # prepare svm for content engine
    if not os.path.exists('./pickles/cb_linsvm.pkl'):
        print 'training  LinearSVC'
              
        clf = LinearSVC(C=5.0)          
        clf.fit(X_train_vect_red, y_train)
        PickleMe(clf, 'cb_linsvm.pkl')
    else:
        clf = DepickleMe('cb_linsvm.pkl')
    
    
    """"""
    
    if not os.path.exists('./pickles/cb_svc.pkl'):",RecoDev-Prototype/Evaluation2/algorithm2.py,amirhmoin/recodev,1
"theater_dance = data[7].split("" "")[1:]      #
non_event = data[8].split("" "")[1:]          #
# fi # # # # # # # # # # # # # # # # # # # #

mostres = [concert,conference,exhibition,fashion,other,protest,sports,theater_dance,non_event]
#tipo_evento = ['concert', 'conference', 'exhibition', 'fashion', 'other', 'protest', 'sports', 'theater_dance', 'no_event'] 
tipus_event = [1,2,3,4,5,6,7,8,9] # en este ejemplo hay solo cinco pero para cubrir cada evento es necesario un numero


# crear una variable de tipus de svm
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=1.0, probability = True).fit(mostres, tipus_event) # li pasem dos arrais un amb mostres


# llegim les imatges de test i guardem la id per tal de donar la solució

text_id_sol = open(""id_sol.txt"", ""w"")

for i in llista_imatges:
        
    class_predicted = rbf_svc.predict(i.tags_svm)",codi_svm.py,ProjecteGDSA33/SED,1
"    y_cv_pred = np.empty(N)

    # loop through the cross-validation folds
    for ii in np.arange(K):
        # break your data into training and testing subsets
        X_train = X.ix[folds != ii,:]
        y_train = y.ix[folds != ii]
        X_test = X.ix[folds == ii,:]

        # build a model on the training set
        model = SVC(gamma=gam_vec.ravel()[param_ind], C=cost_vec.ravel()[param_ind])
        model.fit(X_train, y_train)

        # generate and store model predictions on the testing set
        y_cv_pred[folds == ii] = model.predict(X_test)

    # evaluate the AUC of the predictions
    AUC_all.append(roc_auc_score(y, y_cv_pred))

indmax = np.argmax(AUC_all)",examples/realWorldMachineLearning/Chapter+4+-+Evaluation+and+Optimization.py,remigius42/code_camp_2017_machine_learning,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Lib/test/test_email/test_email.py,maxdeliso/elevatorSim,1
"    ## ================
    X, y = datasets.make_classification(n_samples=options.n_samples,
                                        n_features=options.n_features,
                                        n_informative=options.n_informative)

    ## 2) Build Workflow
    ## =================
    time_start = time.time()
    ## CV + Grid search of a pipeline with a nested grid search
    cls = Methods(*[Pipe(SelectKBest(k=k),
                         SVC(kernel=""linear"", C=C))
                    for C in C_values
                    for k in k_values])
    pipeline = CVBestSearchRefit(cls,
                                 n_folds=options.n_folds_nested,
                                 random_state=random_state)
    wf = Perms(CV(pipeline, n_folds=options.n_folds),
               n_perms=options.n_perms,
               permute=""y"",
               random_state=random_state)",examples/run_somaworkflow_gui.py,neurospin/pylearn-epac,1
"	accuracy = clf.score(X_test, y_test)
	return accuracy

def KNN(X_train, y_train, X_test, y_test):
	clf = neighbors.KNeighborsClassifier()
	clf.fit(X_train, y_train)
	accuracy = clf.score(X_test, y_test)
	return accuracy

def SVMClass(X_train, y_train, X_test, y_test):
	clf = LinearSVC()
	clf.fit(X_train, y_train)
	accuracy = clf.score(X_test, y_test)
	return accuracy

def QuadDA(X_train, y_train, X_test, y_test): 
	clf = QDA()
	clf.fit(X_train, y_train)
	accuracy = clf.score(X_test, y_test)
	return accuracy",machineLearning.py,hmn21/stockMarketPrediction,1
"from sklearn import preprocessing
from sklearn import cross_validation
from b4msa.textmodel import TextModel
from multiprocessing import Pool
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s :%(message)s')


class SVC(object):
    def __init__(self, model):
        self.svc = LinearSVC()
        self.model = model
        self.num_terms = -1

    def fit(self, X, y):
        X = corpus2csc(X).T
        self.num_terms = X.shape[1]
        self.le = preprocessing.LabelEncoder()
        self.le.fit(y)
        y = self.le.transform(y)",b4msa/classifier.py,INGEOTEC/b4msa,1
"        :type idxrelamap:
        :param idxrelamap:

        :type clf:
        :param clf:
        """"""
        self.vocab = vocab
        # print labelmap
        self.labelmap = idxlabelmap
        if clf is None:
            self.clf = LinearSVC(C=1.0, penalty='l1',
                loss='squared_hinge', dual=False, tol=1e-7)
        else:
            self.clf = clf
        self.withdp = withdp
        self.dpvocab, self.projmat = None, None
        if withdp:
            print 'Loading projection matrix ...'
            with gzip.open(fdpvocab) as fin:
                self.dpvocab = load(fin)",code/model.py,jiyfeng/DPLP,1
"
    transformer = preprocessing.MinMaxScaler().fit(to_float(flatten(Xs)))
    Xs_train = transformer.transform(to_float(Xs_train))
    Xs_test = transformer.transform(to_float(Xs_test))

    if attribute_count is not None:
        Xs_test, Xs_train = _eliminate_features(Xs_test, Xs_train, attribute_count, ys_train)
        Xs_test = flatten(Xs_test)
        Xs_train = flatten(Xs_train)

    clf = SVC(**SVC_parameters)
    # clf = LinearSVC(class_weight='auto')
    clf.fit(to_float(Xs_train), ys_train)

    ys_pred = clf.predict(to_float(Xs_test))
    predicted_class = list(ys_pred)
    actual_class = ys_test

    print ""%d, %.3f"" % (test_index[0], accuracy_score(actual_class, predicted_class))
",source/classification/cv_subjects.py,jschavem/facial-expression-classification,1
"           and platform.architecture()[0] == '64bit' \
           and self.oBuild.sKind == 'development' \
           and os.getenv('VERSIONER_PYTHON_PREFER_32_BIT') != 'yes':
            print ""WARNING: 64-bit python on darwin, 32-bit VBox development build => crash""
            print ""WARNING:   bash-3.2$ /usr/bin/python2.5 ./testdriver""
            print ""WARNING: or""
            print ""WARNING:   bash-3.2$ VERSIONER_PYTHON_PREFER_32_BIT=yes ./testdriver""
            return False;

        # Start VBoxSVC and load the vboxapi bits.
        if self._startVBoxSVC() is True:
            assert(self.oVBoxSvcProcess is not None);

            sSavedSysPath = sys.path;
            self._setupVBoxApi();
            sys.path = sSavedSysPath;

            # Adjust the default machine folder.
            if self.fImportedVBoxApi and not self.fUseDefaultSvc and self.fpApiVer >= 4.0:
                sNewFolder = os.path.join(self.sScratchPath, 'VBoxUserHome', 'Machines');",src/VBox/ValidationKit/testdriver/vbox.py,miguelinux/vbox,1
"        else:
            ky = normalize_km(np.dot(y, y.T))
        w = ALIGNF(kernel_list, ky)
        res[:,t] = w
    return res

def svm(train_km, test_km, train_y, test_y):
    # train on train_km and train_y, predict on test_km
    # return prediction
    para_grid ={'C':[1e-2, 1e-1, 1, 10, 100]}
    svc = SVC(kernel='precomputed')
    clf = grid_search.GridSearchCV(svc, para_grid)
    clf.fit(train_km, train_y)
    pred = clf.predict(test_km)
    # for multi-class task, use acc other than F1
    #return f1_score(test_y, pred)
    return accuracy_score(test_y, pred)

def ALIGNFSOFT(kernel_list, ky, y, test_fold, tags):
    # Find best upper bound in CV and train on whole data",svm_code/run_mkl.py,aalto-ics-kepaco/softALIGNF,1
"from sklearn.pipeline import Pipeline  # noqa
from sklearn.cross_validation import cross_val_score, ShuffleSplit  # noqa
from mne.decoding import Vectorizer, FilterEstimator  # noqa


scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
vectorizer = Vectorizer()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                              ('scaler', scaler), ('svm', clf)])

data_picks = mne.pick_types(rt_epochs.info, meg='grad', eeg=False, eog=True,
                            stim=False, exclude=raw.info['bads'])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):
",examples/realtime/plot_compute_rt_decoder.py,jniediek/mne-python,1
"tf_transformer = sklearn.feature_extraction.text.TfidfTransformer(use_idf=True).fit(word_counts)
X = tf_transformer.transform(word_counts)

cx = scipy.sparse.coo_matrix(X)

for i,j,v in zip(cx.row, cx.col, cx.data):
	if i == 0:
	    print(""(%s, %d), %s"" % (voc[j],j,v))

#scipy.io.mmwrite(""data.matrix"",X)
clf = sklearn.svm.LinearSVC()

X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X, files.target, test_size = 0.2)

clf.fit(X_train, y_train)
y_predicted = clf.predict(X_test)

print(sklearn.metrics.classification_report(y_test, y_predicted, target_names=files.target_names))",tools/VerifyWithSklearn.py,lisency/TextClassify,1
"from sklearn.svm import SVC
from sklearn.cross_validation import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from analyze.dimension_reduction import get_file_stats_data, get_file_patch_data
import tensorflow as tf
import numpy as np


def _run_svc_on_file_stats():
    clf = SVC()
    label_encoder = LabelEncoder()
    X_all, y_all = get_file_stats_data()
    y_all = label_encoder.fit_transform(y_all)
    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25)
    clf.fit(X_train, y_train)

    print ""Training set size: {}"".format(len(X_train))
    print ""Accuracy for training set:""
    print classification_report(",analyze/predict.py,Brok-Bucholtz/Pyomize,1
"    random.seed(0)
    random.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",venv/lib/python2.7/site-packages/sklearn/metrics/tests/test_metrics.py,GbalsaC/bitnamiP,1
"    test_data_label = test[['volumetype']]
    test_data_list = test[['medianincome','collegedegree','houseowner']]

##logistic regression
    logistic = linear_model.LogisticRegression()
    logistic.fit(train_data_list, train_data_label['volumetype'])
    predict = logistic.predict(test_data_list)
    err += score(predict,test_data_label)
    
##SVM
    svm_model = svm.SVC()
    svm_model.fit(train_data_list, train_data_label['volumetype'])
    y_svm_predict = svm_model.predict(test_data_list)
    cnf_matrix = cnf_matrix + confusion_matrix(test_data_label, y_svm_predict, labels=[0,1,2])
    err_svm += score(y_svm_predict,test_data_label)
    
#Decision Tree
    dt = tree.DecisionTreeClassifier()
    dt = dt.fit(train_data_list, train_data_label['volumetype'])
    y_dt_predict = dt.predict(test_data_list)",zipcode_classifications.py,siddharthhparikh/INFM750-project,1
"
  def get_message_resource_file_ext(self):
    return ""res""


# Returns the toolchain with the given name
def get_toolchain(name, config):
  if name == ""gcc"":
    return Gcc(config)
  elif name == ""msvc"":
    return MSVC(config)
  else:
    raise Exception(""Unknown toolchain %s"" % name)


class AbstractNode(node.PhysicalNode):
  __metaclass__ = ABCMeta

  def __init__(self, name, context, tools):
    super(AbstractNode, self).__init__(name, context)",mkmk/extension/c.py,tundra/mkmk,1
"Xtest = np.array(TestVal)
ytest = np.array(TestFam)

print ""FIN DE LA CREATION, LANCEMENT DE SVC-LINEAR""

h = 10000

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
lin_svc = svm.LinearSVC(C=C).fit(X, y)

##########
res = lin_svc.predict(Xtest)

compteur = 0
for i in range(len(res)):
	if res[i] == ytest[i]:
		compteur += 1
print ""TAUX DE REUSSITE = ""+str(float(compteur)/len(res))",Old/9-class-SVM-LinearSVC.py,oubould/TestsML,1
"from hyperopt.pyll import scope, as_apply
from hyperopt import hp
from .vkmeans import ColumnKMeans

""""""
Sourceed from jaberg/hyperopt-sklearn
Additional models added by tadejs
""""""

@scope.define
def sklearn_SVC(*args, **kwargs):
    return sklearn.svm.SVC(*args, **kwargs)


@scope.define
def sklearn_LinearSVC(*args, **kwargs):
    return sklearn.svm.LinearSVC(*args, **kwargs)


@scope.define",autokit/hpsklearn/components.py,tadejs/autokit,1
"precision, recall, f1, accuracy, support, fn, roc_auc = 0, 0, 0, 0, 0, 0, 0
colors = ['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange']

k = 10
kf = KFold(n_splits = k)

start = timer()
for train, test in kf.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
	clf2 = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)
	clf3 = LogisticRegression().fit(X_train, y_train)
	clf4 = GaussianNB().fit(X_train, y_train)
	eclf = VotingClassifier(estimators=[('svm', clf1), ('knn', clf2), ('lr', clf3), ('gnb', clf4)], voting='soft')
	eclf = eclf.fit(X_train, y_train)
	y_pred = eclf.predict(X_test)
	
	#ROC curve
	y_prob = eclf.predict_proba(X_test)[:,1]",K-Fold/Voting.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"#XtestPos = scale.fit_transform(XtestPos)
#
scale = preprocessing.Normalizer().fit(XtrainPos)
XtrainPos = scale.fit_transform(XtrainPos)
XtestPos = scale.fit_transform(XtestPos)

#classification
clf = CDClassifier(penalty=""l1/l2"", loss=""squared_hinge"",multiclass=True,max_iter=20,C=1,
                   alpha=1e-4,tol=1e-3)

#clf = LinearSVC(penalty=""l2"")
clf = clf.fit(XtrainPos, YtrainPos)
print(metrics.classification_report(YtestPos, clf.predict(XtestPos)))

## Crossvalidation 5 times using different split
#scores = cross_validation.cross_val_score(clf_svm, posfeat, label, cv=5, scoring='f1')
#print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

# Visualization
#plt.hist(XtrainPos[:,0])",scikit_algo/gp.py,sankar-mukherjee/CoFee,1
"
    print ""Training...""

    # commented out to delete .to_array() option in DictVectorizer
    # X_dim = len(data_X[0])

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v3.0.py,totuta/deep-supertagging,1
"    if feature_names:
        feature_names = np.asarray(feature_names)

    results = []

    #for penalty in [""l2"", ""l1""]:
    penalty = 'l2'
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    clf = LinearSVC(loss='l2', penalty=penalty,dual=False, tol=1e-3)
    results.append(benchmark(clf))
        
    joblib.dump(vectorizer, 'vectorizer.pkl', compress=9)
    joblib.dump(ch2, 'feature_selector.pkl', compress=9)
    joblib.dump(clf, 'linearsvc_classifier.pkl', compress=9)



# parse commandline arguments",easyml-server/easymlserver/topic_classifier.py,delvv/easyML-lib,1
"    Output
    ------
    F: {numpy array}, shape (n_features, )
        index of selected features
    """"""

    n_samples, n_features = X.shape
    # using 10 fold cross validation
    cv = KFold(n_samples, n_folds=10, shuffle=True)
    # choose SVM as the classifier
    clf = SVC()

    # selected feature set, initialized to contain all features
    F = range(n_features)
    count = n_features

    while count > n_selected_features:
        max_acc = 0
        for i in range(n_features):
            if i in F:",skfeature/function/wrapper/svm_backward.py,jundongl/scikit-feature,1
"    if KERNEL == None:
        kernels = [""linear"", ""poly"", ""rbf"", ""sigmoid""]
    else:
        kernels = [KERNEL]
    kernel_scores = [0]*len(kernels)
    kernel_stds = [0]*len(kernels)
    for k in range(0,len(kernels)):
        kernel = kernels[k]
        print ""Training SVM with %s kernel"" % kernel

        cv_svm = svm.SVC(kernel=kernel, scale_C=True)
        scores = cross_validation.cross_val_score(cv_svm, X_train, Y_train, cv=CROSS_VALIDATION_FOLDS)

        print ""Kernel %s accuracy: %f (+/- %f)"" % (kernel, scores.mean(), scores.std() * 2)

        if scores.mean() > best_score:
            best_score = scores.mean()
            best_std = scores.std()
            best_kernel = kernel
        kernel_scores[k] = scores.mean()",periodic_gestures/reference-code/train-svm.py,AutonomyLab/husky,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",sklearn/metrics/tests/test_classification.py,CforED/Machine-Learning,1
"# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)

estimator = GaussianNB()
plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

title = ""Learning Curves (SVM, RBF kernel, $\gamma=0.001$)""
# SVC is more expensive so we do a lower number of CV iterations:
cv = ShuffleSplit(n_iter=10, test_size=0.2, random_state=0)
estimator = SVC(gamma=0.001)
plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

plt.show()",projects/scikit-learn-master/examples/model_selection/plot_learning_curve.py,DailyActie/Surrogate-Model,1
"
    X, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],
                      random_state=22)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

    fig, axes = plt.subplots(2, 3, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks': ()})
    plt.suptitle(""decision_threshold"")
    axes[0, 0].set_title(""training data"")
    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 0])

    svc = SVC(gamma=.05).fit(X_train, y_train)
    axes[0, 1].set_title(""decision with threshold 0"")
    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 1])
    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,
                   ax=axes[0, 1], cm=ReBl)
    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 1])
    axes[0, 2].set_title(""decision with threshold -0.8"")
    discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=axes[0, 2])
    plot_2d_separator(svc, X_train, linewidth=3, ax=axes[0, 2], threshold=-.8)
    plot_2d_scores(svc, X_train, function=""decision_function"", alpha=.7,",mglearn/plot_metrics.py,JoostVisser/ml-assignment2,1
"        :param idxrelamap: mapping from parsing action indices to
                           parsing actions

        :type clf: LinearSVC
        :param clf: an multiclass classifier from sklearn
        """"""
        self.vocab = vocab
        # print labelmap
        self.labelmap = idxlabelmap
        if clf is None:
            self.clf = LinearSVC()


    def train(self, trnM, trnL):
        """""" Perform batch-learning on parsing model
        """"""
        self.clf.fit(trnM, trnL)


    def predict(self, features):",model.py,jiyfeng/RSTParser,1
"    tfidf_transformer = TfidfTransformer(use_idf=True)
    X_train_dtm = tfidf_transformer.fit_transform(X_train_counts)
    # Transform test data
    X_test_counts = count_vect.transform(X_test)
    X_test_dtm = tfidf_transformer.fit_transform(X_test_counts)
    data_load_time = time.time() - start_time

    # Not optimized
    C = 1.0
    classifier_dict = {
        ""SVC with linear kernel"": svm.SVC(kernel='linear', C=C),
        ""SVC with RBF kernel"": svm.SVC(kernel='rbf', gamma=0.7, C=C),
        ""SVC with polynomial (degree 3) kernel"": svm.SVC(kernel='poly', degree=3, C=C),
        ""LinearSVC (linear kernel)"": svm.LinearSVC(C=C)
    }

    for key, clf in classifier_dict.iteritems():
        start_time = time.time()
        clf.fit(X_train_dtm, y_train)
        y_pred_class = clf.predict(X_test_dtm)",svm.py,anmolshkl/oppia-ml,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",opt/Python27/Lib/email/test/test_email.py,xxd3vin/spp-sdk,1
"
    dev_results = ()
    exp_descriptions = ()
    exp_description_id = 0

    for penalty_name, penalty_value in itertools.izip(penalty_names,penalty_values):
        if args.v:
            print '%s %s' % ('linear', penalty_name)

        if config_d['SVM']['kernel'] == 'linear':
            clf = svm.LinearSVC(C=penalty_value,
                                loss='l1')
            clf.fit(X,y)
        elif config_d['SVM']['kernel'] == 'polynomial':

            import pdb; pdb.set_trace()

        np.save('%s_%s_%s_coef.npy' % (args.output_fls_prefix,config_d['SVM']['kernel'],
                                           penalty_name),
                    clf.coef_[0])",local/CSVMTrain.py,markstoehr/phoneclassification,1
"            with open(svmTrainedData):
                print('  Opening SVM training model...\n')
                clf = joblib.load(svmTrainedData)
        else:
            raise ValueError('  Force retraining SVM model')
    except:
        #**********************************************
        ''' Retrain training model if not available'''
        #**********************************************
        print('  Retraining SVM data...')
        clf = svm.SVC(C = svmDef.Cfactor, decision_function_shape = 'ovr', probability=True)
        
        print(""  Training on the full training dataset\n"")
        clf.fit(A,Cl)
        accur = clf.score(A_test,Cl_test)
        print('  Mean accuracy: ',100*accur,'%')

        Z = clf.decision_function(A)
        print('\n  Number of classes = ' + str(Z.shape[1]))
        joblib.dump(clf, svmTrainedData)",Other/sample_code/SpectraLearnPredict_test-TF-new.py,feranick/SpectralMachine,1
"        bewertungen.append(bewertungen_1[i])
        features.append(features_0[i])
        bewertungen.append(bewertungen_0[i])

    pickle.dump(features, open(""resources/training_features.p"", ""wb""))
    pickle.dump(bewertungen, open(""resources/training_features_annotations.p"", ""wb""))
    vec = DictVectorizer()
    feature_vectorized = vec.fit_transform(features)
    X = np.array(feature_vectorized.toarray())
    y = np.array(bewertungen)
    clf = svm.SVC(kernel='linear', C=0.1)
    scores = cross_validation.cross_val_score(clf, X, y, cv = 10)
    value = 0.0
    for s in scores:
        value+=s
    print(scores)
    print(value/len(scores))

def ressort_mapping(ressort):
    result = ''",Service/feature.py,swalter2/PersonalizationService,1
"plt.show()


#############################################################################
print(50 * '=')
print('Section: Tuning hyperparameters via grid search')
print(50 * '-')


pipe_svc = Pipeline([('scl', StandardScaler()),
                     ('clf', SVC(random_state=1))])

param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]

param_grid = [{'clf__C': param_range,
               'clf__kernel': ['linear']},
              {'clf__C': param_range,
               'clf__gamma': param_range,
               'clf__kernel': ['rbf']}]
",code/optional-py-scripts/ch06.py,1iyiwei/pyml,1
"    vectorizer = CountVectorizer(
        min_df=10,
        max_df=0.5,
        ngram_range=(1, 3),
        max_features=10000,
        lowercase=True,
        stop_words=stopwords_list,
        tokenizer=tokenize
    )

    svm = LinearSVC(C=0.3, max_iter=300, loss='hinge')
    pipeline = Pipeline([
        ('union', FeatureUnion([
            ('sentiment', sentiment_extractor),
            ('temp', temp_extractor),
            ('wind', wind_extractor),
            ('vect', vectorizer),
        ])),
        ('cls', svm),
    ])",example8/train.py,jramcast/ml_weather,1
"X, y = datasets.make_classification(n_samples=100, n_features=200,
                                    n_informative=2)
X = numpy.random.rand(*X.shape)

from epac import Perms, CV, Methods
perms_cv_svm = Perms(CV(Methods(SVM(loss=""l1""), SVM(loss=""l2""))), n_perms=100)
perms_cv_svm.run(X=X, y=y)
perms_cv_svm.reduce()

self = perms_cv_svm
key = 'LinearSVC(loss=l1)'",examples/debug_wf.py,neurospin/pylearn-epac,1
"                        label.append(int(trainingList[l][k][0]))
                        if k != 0:
                            data.append(data_tmp)
                        data_tmp = []
                    else:
                        data_tmp.append(trainingList[l][k][i])
            data.append(data_tmp)

            # TODO: Grid search
            # URL : http://sucrose.hatenablog.com/entry/2013/05/25/133021
            estimator = LinearSVC()#C=1.0)
            estimator.fit(data, label)

            if l == 0 or l == 1:
                if l == 0:
                    dataList = rowList
                elif l == 1:
                    dataList = columnList

                if len(dataList) >= 0 and len(dataList) < 40:",game.py,CORDEA/encampment-game,1
"cvset = trsetfull[trsplit:]
trlabels = labels[:trsplit]
cvlabels = labels[trsplit:]
c=5.0
g=0.05
#counter=0
#paramholder=np.zeros([nparams,2])
#bestscore=-1
#for c in paramc:
#	for g in paramg:
#		model=SVC(C=c,gamma=g)
#		print('Fitting model with params c: %f, g: %f ...'%(c,g))
#		model=model.fit(trset,trlabels)
#		try:
#			scorei=model.score(cvset,cvlabels)
#		except:
#			scorei=0 # something went wrong...
#		scores[counter]=np.mean(scorei)
#		paramholder[counter,0]=c
#		paramholder[counter,1]=g",mnistSVM.py,kjford/MNIST,1
"                    max_depth=self.hyperparameters['max_depth'],
                    min_samples_split=self.hyperparameters['min_samples_split'],
                    n_jobs=self.n_cores),
                #Boosting parameters
                learning_rate=self.hyperparameters['learning_rate'],
                algorithm=self.hyperparameters['algorithm'],
                n_estimators=self.hyperparameters['n_estimators_boost']
                )

        elif self.model_type == 'SVM':
            return svm.SVC(C=self.hyperparameters['C_reg'],
                           kernel=self.hyperparameters['kernel'],
                           probability=True)

        elif self.model_type == 'LogisticRegression':
            return linear_model.LogisticRegression(
                C=self.hyperparameters['C_reg'],
                penalty=self.hyperparameters['penalty'])

        elif self.model_type == 'AdaBoost':",fpsd/classify.py,freedomofpress/fingerprint-securedrop,1
"X,y = load_trainingData()

# Neural network classifiers
adam_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
				activation = ""tanh"", solver = ""adam"", max_iter = 1000, random_state=0, alpha=0.001)
sgd_clf = MLPClassifier(hidden_layer_sizes = (noOfInputNodes, noOfHiddenNodes, noOfOutputNodes), 
				activation = ""tanh"", solver = ""sgd"", max_iter = 1800, learning_rate = ""adaptive"", 
				learning_rate_init=0.01, random_state=0, alpha=0.01)
# SVM Classifiers
# C is SVM regularization parameter
rbf_svc_clf = svm.SVC(kernel='rbf', gamma=0.05, C=401)
lin_svc_clf = svm.SVC(kernel='linear', C=801, gamma=0.01)

# Training of neural networks classifiers
sgd_clf.fit(X,y)
adam_clf.fit(X,y)
# Training of SVM classifiers
lin_svc = lin_svc_clf.fit(X, y)
rbf_svc = rbf_svc_clf.fit(X, y)
",live_demo/main.py,aroonav/furry-strokes,1
"        3 use best param and best clf to find recall for 100 percent precision
    """"""
    utils.print_success(""Find Recall for best Precision for each tag"")
    train = utils.abs_path_file(train)
    test = utils.abs_path_file(test)
    train_features, train_groundtruths = read_file(train)
    test_features, test_groundtruths = read_file(test)
    classifiers = {
        # ""RandomForest"": RandomForestClassifier(),#n_estimators=5
        ""DecisionTree"":DecisionTreeClassifier()#,#max_depth=10
        # ""SVM"":SVC(kernel=""linear"", C=0.0205),
        # ""ExtraTreesClassifier"":ExtraTreesClassifier(n_estimators=5, criterion=""entropy"", max_features=""log2"", max_depth=9),
        # ""LogisticRegression"":LogisticRegression()
    }
    tags = list(set(test_groundtruths))
    nb_tag = len(tags)
    step = 0.01
    # for index, tag in enumerate([""i""]):
    for index, tag in enumerate(tags):
        utils.print_success(""Tag "" + tag)",src/classify.py,ybayle/ReproducibleResearchIEEE2017,1
"    
    c = 1.0
    iterations = 10 ** 6
    
    weights = {
        'MANNER' : c / 3,
        'PATH' : c / 3,
        'COMPOUND' : c / 3
    }
    
    lin_svc = svm.LinearSVC(
        tol=10 ** -6,
        dual=False,
        multi_class='crammer_singer',
        max_iter=iterations,
        class_weight=weights,
        C=c
    )
    
    degree = 2",eval.py,zyocum/motion-type-classifier,1
"    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import ExtraTreesClassifier
    from sklearn import svm
    from sklearn.ensemble import GradientBoostingClassifier
    import xgboost as xgb
    from sklearn.linear_model import LogisticRegression
    #clf = LogisticRegression()
    #clf = ExtraTreesClassifier(n_estimators=100)
    clf = RandomForestClassifier(n_estimators=150)
    #clf = GradientBoostingClassifier(n_estimators=40,max_depth=5)
    #clf = svm.SVC(kernel='linear', C=1, probability=True)
    #clf = xgb.XGBClassifier(objective=""binary:logistic"",nthread=-1,scale_pos_weight=1,n_estimators=50, max_depth=6)

    scores = cross_val_score(clf, X, y, cv=10, scoring=""neg_log_loss"")
    print scores
    print sum(scores) / float(len(scores))
    scores = cross_val_score(clf, X, y, cv=5, scoring=""recall"")
    print scores
    print sum(scores) / float(len(scores))
    scores = cross_val_score(clf, X, y, cv=5)",dsb_create_voxel_model_predictions.py,tondonia/data-science-bowl-2017,1
"                            train_indices, valid_indices, test_indices,
                            alphas=[10**(-x) for x in range(3)]):
        best_acc = float('-inf')

        # try several values on alpha
        for alpha in alphas:
            for C in [(10**x) / Y.shape[0] for x in range(-7,7,2)]:
                #for C in [0.001]:
                self.fit(A, alpha=alpha)
                try:
                    self.model = LinearSVC(C=C)
                    self.model.fit(self.K[train_indices,:], Y[train_indices].argmax(1))
                    preds = self.predict(Y, test_indices, valid_indices)

                    acc = accuracy_score(Y[valid_indices], preds)
                    if acc > best_acc:
                        best_acc = acc
                        best_alpha = alpha
                        best_C = C
                    print """"",scnn/kernel.py,jcatw/scnn,1
"        for tr in [self] + self.prop_extractors:
            #print(""+++"",tr.filename)
            tr.extract()
            #print(""---"",tr.filename)
        if self.learn_coding is None:
            self.learning_params(teaching=True)
        x, y = self.prepare_params(teaching=True)

        # print(x, y)

        # clf = svm.SVC()
        # m = clf.fit(x, y)
        # clf = GaussianNB()
        # m = clf.fit(x,y)

        param_coding, target_coding, _ = self.learn_coding
        if debug:
            print(""Features: -----------------------"")
            print(param_coding)
            print(""Target features: ----------------"")",src/icc/studprogs/learn.py,eugeneai/icc.studprogs,1
"        #test_set = Sel.transform(feature[test])
        clf.fit(train_set, labels[train])
        pred = clf.predict(test_set)
        score.append(accuracy_score(labels[test], pred))
        score.append(precision_score(labels[test], pred))
        score.append(recall_score(labels[test], pred))
        score.append(f1_score(labels[test], pred))
        scores.append(score)
    avg = np.average(scores, axis=0)
    return avg
#print validate(SVC(kernel='poly', degree=2, C=1000000), feature, 500)
'''
print validate(SVC(C=10000, gamma=0.75), feature, 500)
print validate(LinearSVC(C=100), feature, 500)
print validate(LogisticRegression(C=100), feature, 500)
'''

#clf = SVC(C=10000, gamma=0.75, probability=True)
#clf.fit(feature, labels)
#prob = clf.predict_proba(feature)",learning/final_vis.py,fcchou/CS229-project,1
"    
    logistic_regression_l1 = sklearn.linear_model.LogisticRegression(penalty = 'l1')
    logistic_regression_l2 = sklearn.linear_model.LogisticRegression(penalty = 'l2')
    sgd_lr = sklearn.linear_model.SGDClassifier(loss = 'log')
    sgd_svm = sklearn.linear_model.SGDClassifier(loss = 'hinge')
    sgd_elastic_net = sklearn.linear_model.SGDClassifier(loss = 'log', penalty = 'elasticnet')
    perceptron = sklearn.linear_model.Perceptron()
    passive_aggresive_classifier = sklearn.linear_model.PassiveAggressiveClassifier()
    #polynomial_d2 = Pipeline([('poly', sklearn.preprocessing.PolynomialFeatures(degree = 2)), ('linear', sklearn.linear_model.LogisticRegression(penalty = 'l1'))])
    #polynomial_d3 = Pipeline([('poly', sklearn.preprocessing.PolynomialFeatures(degree = 3)), ('linear', sklearn.linear_model.LogisticRegression(penalty = 'l1'))])
    svc = sklearn.svm.SVC()
    nuSVC = sklearn.svm.NuSVC()
    linearSVC = sklearn.svm.LinearSVC()
    kneighbors = sklearn.neighbors.KNeighborsClassifier()
    gaussian_nb = sklearn.naive_bayes.GaussianNB()
    multinomial_nb = sklearn.naive_bayes.MultinomialNB()
    bernoulli_nb = sklearn.naive_bayes.BernoulliNB()
    decision_tree = sklearn.tree.DecisionTreeClassifier()
    
    models.append(logistic_regression_l1)",testing/ml_testing.py,jadielam/pingo,1
"from sklearn.svm.classes import NuSVC

from ..Classifier import Classifier
from ...language.JavaScript import JavaScript as JS


class NuSVCJSTest(JS, Classifier, TestCase):

    def setUp(self):
        super(NuSVCJSTest, self).setUp()
        self.mdl = NuSVC(kernel='rbf', gamma=0.001, random_state=0)

    def tearDown(self):
        super(NuSVCJSTest, self).tearDown()",tests/classifier/NuSVC/NuSVCJSTest.py,nok/sklearn-porter,1
"    # just a dummy uniform probability classifier for working purposes
    #clf = sklearn.dummy.DummyClassifier(strategy='uniform')
    
    #clf = sklearn.linear_model.SGDClassifier(n_jobs=-1,
    #                                         loss='log')
    
    #clf = sklearn.ensemble.RandomForestClassifier(n_jobs=-1,
    #                                              n_estimators=100,
    #                                              verbose=1)
    
    # clf = sklearn.svm.SVC(probability=True)
    
    clf = sklearn.linear_model.LogisticRegression()
    
    cv = sklearn.cross_validation.StratifiedShuffleSplit(y)
    
    # Try cross-validating
    results = []
    for train, test in cv:
        clf.fit(X[train], y[train])",train_attr.py,Neuroglycerin/neukrill-net-work,1
"        test_params = self.model.get_params
        params = {}
        params['alpha'] = np.arange(0.01, 3.,step=0.01,dtype=np.float64)
        #params['alpha'] = np.logspace(-1, 2, 30)
        params['fit_prior'] = [True, False]
        best_params = validator.search_BestParameter(self.create_Model(), params)
        validator.cross_validation(self.create_Model(best_params))

    def create_Model(self, params=None):
        model = MultinomialNB(1)
        #model = LinearSVC(C=0.1)
        if params is not None:
            model.set_params(**params)
        return model


def main():
    ocr = OCREngine()
    temp_file_name = '../base_binary.png'
    doc = ocr.recognize(temp_file_name)",src/naivebayes.py,umyuu/FEZUnofficialRankingBot,1
"        # axis=0 means independently standardize each feature, otherwise (if 1) standardize each sample

# how to get scaler parameters
#print scaler.mean_
#print scaler.scale_


# TRAIN
def train_model(train_data, train_classes, print_accuracy = True): # with_probabilities = False,
    '''train a SVM classifier model'''
    #model = svm.SVC() # defaults to kernel='rbf' (usually worse for RP/SSD/RH features)
    #model = svm.SVC(kernel='linear', probability=with_probabilities)

    # we use this syntax as it supports multi-class classification
    model = OneVsRestClassifier(SVC(kernel='linear'))

    model.fit(train_data, train_classes)

    if print_accuracy:
        pred_train = model.predict(train_data) # predictions on train set",rp_classify.py,tuwien-musicir/rp_extract,1
"X_dev, y_dev = load_data_set(args.phn_set1,args.phn_set2, args.data_path, 2)

tuned_parameters = [{'kernel':['rbf'], 'gamma':[1e-2,1e-3,1e-4,1e-5,1e-1],
                     'C':[.1,1,10,.01,100]}]


print 'commencing training'
error_values = []
for gamma_id, gamma in enumerate([1e-4]):
    for C_id, C in enumerate([100]):
        clf = SVC(C=C,gamma=gamma,kernel='rbf',tol=0.00001,verbose=True)
        clf.fit(X_train,y_train)
        s = clf.score(X_dev,y_dev)
        print ""C=%g\tgamma=%g\tscore=%g"" % (C,gamma,s)
        error_values.append((gamma,C,s))
        if gamma_id == 0 and C_id ==0:
            best_score = s
            print ""updated best score to %g"" % best_score
            best_C = C
            best_gamma = gamma",local/load_compare_phone_sets_msc_kernel.py,markstoehr/phoneclassification,1
"
    `intercept_` : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
            gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
            shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [ 1.]

    See also
    --------",python/sklearn/sklearn/svm/classes.py,seckcoder/lang-learn,1
"            if np.argmax(ss.values()[:3]) == 0:
                intensidad[texto].append(0)
            if np.argmax(ss.values()[:3]) == 1:
                intensidad[texto].append(1)
            if np.argmax(ss.values()[:3]) == 2:
                intensidad[texto].append(2)
    return intensidad

def train_linear_SVM(textos, intensidades, start):
    resultados = []
    svc = LinearSVC()
    try:
        clf.fit(textos[start], intensidades[start])
    except ValueError:
        raise Exception(""Ntextos tiene que ser = a Nintensidades"")
    print clf.named_steps['vectorizer'].get_feature_names()
    for i in xrange(len(textos)):
        try:
            resultados.append(clf.predict(textos[i]))
            print clf.score(textos[i][:len(intensidades[i])], intensidades[i])",sentimiento.py,MarsCrop/Twitter-analisis-y-NLP,1
"
    f.close()

clfs = []

# Through cv testing, I found the optimal number of estimators to be 15
clfs.append(ensemble.RandomForestClassifier(n_estimators=150))
clfs.append(ensemble.GradientBoostingClassifier(n_estimators=200))
clfs.append(ensemble.AdaBoostClassifier(n_estimators=135))
#clfs.append(neighbors.KNeighborsClassifier(n_neighbors=10))
#clfs.append(svm.SVC())

predictificate(data, target, test, clfs)

# I use the following code to find good hyperparameter values
#scores = cross_validation.cross_val_score(
    #clf, data, target, cv=5)
#print(""Accuracy: %0.2f (+/- %0.2f) %f"" % (scores.mean(), scores.std() * 2, x))",Beats/code.py,bcspragu/Machine-Learning-Projects,1
"import numpy as np

class SupportVecttorMachine():
    def __init__(self):
        pass

    def train_svm(self):
        xval = np.asarray(self.xval)
        yval = np.asarray(self.yval)

        s = svm.SVC(kernel=self.cfg[""prediction""][""svm_kernel""])
        s.fit(xval,yval)

        if self.plot:
            self.plot_confusion_matrix(s.predict(xval),yval)
        if self.save:",scripts/models/support_vector_machine/support_vector_machine.py,mhernan88/fo_detect,1
"
# online subgradient ssvm
start = time()
subgradient_svm.fit(X_train_bias, y_train)
time_subgradient_svm = time() - start
acc_subgradient = subgradient_svm.score(X_test_bias, y_test)

print(""Score with pystruct subgradient ssvm: %f (took %f seconds)""
      % (acc_subgradient, time_subgradient_svm))

libsvm = SVC(kernel='linear', C=10)
start = time()
libsvm.fit(X_train, y_train)
time_libsvm = time() - start
acc_libsvm = libsvm.score(X_test, y_test)
print(""Score with sklearn and libsvm: %f (took %f seconds)""
      % (acc_libsvm, time_libsvm))

# plot the results
fig, ax = plt.subplots(1, 2, figsize=(10, 5))",examples/plot_binary_svm.py,pystruct/pystruct,1
"        elif dataset == 'NCI1':
            ds = load_graph_datasets.load_graphs_NCI1()
        y = ds.target
    else:
        gram, y = load_svmlight_file(gram_file)
        new_gram = gram[:, 1:gram.shape[1]].todense()

#    print gram_file

    start = time.clock()
    clf = svm.SVC(C=c, kernel='precomputed')
    #clf.fit(new_gram, y)

    scores = cross_validation.cross_val_score(clf, new_gram, y, cv=10)
    end = time.clock()

    print(""Accuracy: %0.4f (+/- %0.4f)"" % (scores.mean(), scores.std() * 2))
    print(""Elapsed time: %0.4f s"" % (end - start))

    """"""",scripts/SVM_precomputed.py,nickgentoo/scikit-learn-graph,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0519_2015.py,magic2du/contact_matrix,1
"    cache = os.path.join(args.workDir, 'fisherFacesExp.pkl')
    fishFacesDf = cacheToFile(cache)(opencvExp)(lfwPpl, cls)

    print(""LBPH Experiment"")
    cls = cv2.createLBPHFaceRecognizer()
    cache = os.path.join(args.workDir, 'lbphExp.pkl')
    lbphFacesDf = cacheToFile(cache)(opencvExp)(lfwPpl, cls)

    print(""OpenFace CPU/SVM Experiment"")
    net = openface.TorchNeuralNet(args.networkModel, 96, cuda=False)
    cls = SVC(kernel='linear', C=1)
    cache = os.path.join(args.workDir, 'openface.cpu.svm.pkl')
    openfaceCPUsvmDf = cacheToFile(cache)(openfaceExp)(lfwPpl, net, cls)

    print(""OpenFace GPU/SVM Experiment"")
    net = openface.TorchNeuralNet(args.networkModel, 96, cuda=True)
    cache = os.path.join(args.workDir, 'openface.gpu.svm.pkl')
    openfaceGPUsvmDf = cacheToFile(cache)(openfaceExp)(lfwPpl, net, cls)

    plotAccuracy(args.workDir, args.largeFont,",evaluation/lfw-classification.py,cmusatyalab/openface,1
"	# Find optimal value of lambda through cross-validation
	betas_opt = linear_svm_squared_hinge_loss.mylinearsvm(optimal_lambduh2, eta_init, 100, x_train, y_train)
	error_train_lambda_opt = linear_svm_squared_hinge_loss.compute_misclassification_error(betas_opt[-1,:],x_train,y_train)
	error_test_lambda_opt = linear_svm_squared_hinge_loss.compute_misclassification_error(betas_opt[-1,:],x_test,y_test)
	# Print results using this implementation
	print('Using implementaion, misclassification error for the training set is: ', error_train_lambda_opt)
	print('Using implementaion, misclassification error for the testing set is: ', error_test_lambda_opt)


	# Use scikit-learn LinearSVC function with squared_hinge
	svm_l2 = svm.LinearSVC(penalty='l2', loss='squared_hinge')
	parameters = {'C':[10**i for i in range(-2, 2)]}
	clf_svm = GridSearchCV(svm_l2, parameters)
	clf_svm.fit(x_train, y_train)
	error_train_svm = 1 - clf_svm.score(x_train, y_train)
	error_test_svm = 1 - clf_svm.score(x_test, y_test)
	# Print results using scikit-learn
	print('Use sklearn, misclassification error for the training set is: ', error_train_svm)
	print('Use sklearn, misclassification error for the testing set is: ', error_test_svm)
",src/compare_to_sklearn.py,dianachenyu/linear-svm-squared-hinge-loss,1
"        """""" This method returns a pair (C, gamma) with classifcation rate
            is maximized. """"""

        tuned_parameters = [{
            'kernel': ['linear'], 
            'C': self._C_range}]

        X = array([f.getA1() for f in feasible] + [i.getA1() for i in infeasible])
        y = array([1] * len(feasible) + [-1] * len(infeasible))

        clf = GridSearchCV(SVC(), tuned_parameters, cv=self._cv_method, verbose=0)

        clf.fit(X, y)
        best_accuracy = clf.best_score_

        return feasible, infeasible, clf.best_estimator_.C, best_accuracy",evopy/metamodel/cv/svc_cv_sklearn_grid_linear.py,jpzk/evopy,1
"                 map(os.path.split,
                     map(os.path.dirname, labels)))  # Get the directory.
    fname = ""{}/reps.csv"".format(args.workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)
    labelsNum = le.transform(labels)
    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))

    if args.classifier == 'LinearSvm':
        clf = SVC(C=1, kernel='linear', probability=True)
    elif args.classifier == 'GridSearchSvm':
        print(""""""
        Warning: In our experiences, using a grid search over SVM hyper-parameters only
        gives marginally better performance than a linear SVM with C=1 and
        is not worth the extra computations of performing a grid search.
        """""")
        param_grid = [
            {'C': [1, 10, 100, 1000],
             'kernel': ['linear']},",src/classifier.py,d-xc/face_authentication_viewer,1
"		'global_inhibition': global_inhibition,
		'trim': 1e-4,
		'seed': seed,
		
		# Synapse parameters
		'syn_th': 0.5,
		'random_permanence': True,
		
		# Fitting parameters
		'nepochs': 30,
		'clf': LinearSVC(random_state=seed)
		# NOTE: The SVM's will be identical, despite being seeded now
	}
	
	# Come up with some parameters to search
	param_distributions = {
		# Region parameters
		'ncolumns':randint(100, 1001),
		'nactive':uniform(0, 0.2),
		# As a percentage of the number of columns",src/examples/mnist_parallel.py,tehtechguy/mHTM,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = fisher_score.fisher_score(X[train], y[train])

        # rank features in descending order according to score
        idx = fisher_score.feature_ranking(score)
",skfeast/example/test_fisher_score.py,jundongl/scikit-feast,1
"
    half_source, half_target = int(nb_source/2), int(nb_target/2)
    train_X = np.vstack((source_X[0:half_source, :], target_X[0:half_target, :]))
    train_Y = np.hstack((np.zeros(half_source, dtype=int), np.ones(half_target, dtype=int)))

    test_X = np.vstack((source_X[half_source:, :], target_X[half_target:, :]))
    test_Y = np.hstack((np.zeros(nb_source - half_source, dtype=int), np.ones(nb_target - half_target, dtype=int)))

    best_risk = 1.0
    for C in C_list:
        clf = svm.SVC(C=C, kernel='linear', verbose=False)
        clf.fit(train_X, train_Y)

        train_risk = np.mean(clf.predict(train_X) != train_Y)
        test_risk = np.mean(clf.predict(test_X) != test_Y)

        if verbose:
            print('[ PAD C = %f ] train risk: %f  test risk: %f' % (C, train_risk, test_risk))

        if test_risk > .5:",experiments_amazon.py,GRAAL-Research/domain_adversarial_neural_network,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 5, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/cv_ppv_dsesscv/setup.py,jpzk/evopy,1
"transl = build_translation(images)

test = [(""Gaussian"", GaussianNB()),
        (""Linear"", LinearRegression()),
        (""DTree"", DecisionTreeClassifier()),
        ('Knn 5', KNeighborsClassifier(5)),
        ('Knn 10', KNeighborsClassifier(10)),
        ('Knn 15', KNeighborsClassifier(15)),
        ('Knn 20', KNeighborsClassifier(20)),
        ('Logistic Regression', LogisticRegression()),
        ('Linear SVM', SVC(kernel='linear', probability=True)),
        ('Poly SVM', SVC(kernel='poly', degree=2, probability=True)),
        ('RBF SVM', SVC(kernel='rbf', gamma=2, C=1, probability=True)),
        ('Random Forest', RandomForestClassifier())]

def compute_score(model):
    # Warning : score computation over 1 class, and not ten most likely ?
    score_dir = ""../database""
    images = listdir(score_dir)
    images.sort()",src/exps.py,rmonat/princess-or-frog,1
"        params_used = params
    elif clf_used == 'svm':
        params_used = svm_params
    elif clf_used == 'ada_boost':
        params_used = rf_params
    elif clf_used == 'lr':
        params_used = lr_params
    else:
        params_used = rf_params
    if clf_used == 'svm':
        clf = SVC(**params_used)
    elif clf_used == 'ada_boost':
        rf = RandomForestClassifier(**rf_params)
        clf = AdaBoostClassifier(base_estimator=rf, **params_used)
    elif clf_used == 'lr':
        clf = LogisticRegressionCV(**params_used)
    else:
        clf = RandomForestClassifier(**params_used)
    return clf
",syconn/processing/learning_rfc.py,StructuralNeurobiologyLab/SyConn,1
"
ptr=20000
idxTrain=idx[0:ptr]
idxTest=idx[ptr:len(idx)]

xTrain = df.iloc[idxTrain,1:41]
yTrain = df.iloc[idxTrain,41]
xTest = df.iloc[idxTest,1:41]
yTest = df.iloc[idxTest,41]

clf = svm.SVC()
clf.fit(xTrain, yTrain) 

preds = clf.predict(xTest)

#divisor = sum(yTest==11)

tp = 0
tn = 0
fp = 0",test_discretization/test_svm.py,zedoul/AnomalyDetection,1
"    for img in range(len(test)):
        test_hog.append(descriptor.describe(test[img]))

    pos_hog = np.array(pos_hog)
    neg_hog = np.array(neg_hog)
    test_hog = np.array(test_hog)

    x = np.vstack((pos_hog, neg_hog))
    y = np.array(labels)

    clf = SVC(kernel=""linear"")
    clf = clf.fit(x, y)

    y_pred = clf.predict(test_hog)
    print time.time() - start
    print str(y_pred)
    print str(test_labels)",FaceRec.py,C0mpy/Soft-Computing,1
"#test_size= 0.4 # selecting number of samples

X_train, X_test, y_train, y_test = cross_validation.train_test_split(data[0],data[1], test_size=0.4, random_state=1)

#print(X_train.shape)
#print(y_train.shape)
#print(X_test.shape)
#print(y_test.shape)

#L1 SVM
clf = LinearSVC(penalty='l1', dual=False,C =c)
scores = cross_validation.cross_val_score(clf, data[0], data[1], cv=10)

print(scores)
print(""L1 SVM \n  Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))


#L2 SVM trained on all the features
clf = LinearSVC(penalty='l2',dual=False,C= c)
scores = cross_validation.cross_val_score(clf, data[0], data[1], cv=10)",crossValidationArcaene.py,narendrameena/featuerSelectionAssignment,1
"
    def test(self, descriptors,des_list, n):
        # Scale the features
        test_features = self.stdSlr.transform(descriptors)

        return self.clf.predict_proba(test_features)

    def __init__(self,path):
        if not self.load(path):
            self.path = path
            self.clf = SVC()
            self.class_weight = {'point':0.25,'show':0.5,'talk':0.25}
            self.stdSlr = None
            self.loaded = False
        else:
            self.path = path",src/RGBDHand/src/SVM.py,pazagra/catkin_ws,1
"    f = open(""more_data.json"", ""w"")
    json.dump(accuracies, f)
    f.close()

def run_svms():
    svm_training_data, svm_validation_data, svm_test_data \
        = mnist_loader.load_data()
    accuracies = []
    for size in SIZES:
        print ""\n\nTraining SVM with data set size %s"" % size
        clf = svm.SVC()
        clf.fit(svm_training_data[0][:size], svm_training_data[1][:size])
        predictions = [int(a) for a in clf.predict(svm_validation_data[0])]
        accuracy = sum(int(a == y) for a, y in 
                       zip(predictions, svm_validation_data[1])) / 100.0
        print ""Accuracy was %s percent"" % accuracy
        accuracies.append(accuracy)
    f = open(""more_data_svm.json"", ""w"")
    json.dump(accuracies, f)
    f.close()",neural-networks-and-deep-learning/fig/more_data.py,seanpquig/study-group,1
"    XList.append(XNew)
    yList.append(yOne)
  return XList, yList

# -------------------------------------------------------------------------------------------------
# Train and test classifier

def create_clfs(C, nb):
  clfs = []
  for i in range(nb):
    clf = LinearSVC(dual=True, C=C, loss='hinge', tol=1e-7, fit_intercept=True, max_iter=3000)
    #clf = SVC(kernel='linear',C=C)
    clfs.append(clf)
  return clfs

def train_multilabel(clfs, XList, yList):
  global log
  nbLabel = len(yList)
  for lbl in range(nbLabel):
    if log: print 'Fit...'",test.py,Cadene/Deep6Framework,1
"X1test=X1[int(trainPerc*len(X1)):len(X1)]
Y1test=np.ones(len(X1test))
print ""len(X0test)="" + str(len(X0test))
print ""len(X1test)="" + str(len(X1test))
Xtrain=np.vstack((X0train,X1train))
Ytrain=np.hstack((Y0train,Y1train))
print ""len(Xtrain)="" + str(len(Xtrain))
Xtest=np.vstack((X0test,X1test))
Ytest=np.hstack((Y0test,Y1test))
print ""len(Xtest)="" + str(len(Xtest))
clf = svm.SVC()
clf.fit(Xtrain, Ytrain)  
res=clf.predict(Xtest)
vect=np.abs(res - Ytest)
print str((len(Ytest) - np.sum(vect))/len(Ytest)*100) + ""%""",src/experiments/demos/ClassifyExperiment.py,ChileanVirtualObservatory/ASYDO,1
"ssmodel = CPLELearningModel(basemodel)
ssmodel.fit(X, ys)
print ""CPLE semi-supervised log.reg. score"", ssmodel.score(X, ytrue)

# semi-supervised score, WQDA model
ssmodel = CPLELearningModel(WQDA(), predict_from_probabilities=True) # weighted Quadratic Discriminant Analysis
ssmodel.fit(X, ys)
print ""CPLE semi-supervised WQDA score"", ssmodel.score(X, ytrue)

# semi-supervised score, RBF SVM model
ssmodel = CPLELearningModel(sklearn.svm.SVC(kernel=""rbf"", probability=True), predict_from_probabilities=True) # RBF SVM
ssmodel.fit(X, ys)",examples/example.py,carloslds/semisup-learn,1
"        assert_equal(l1_min_c(dense_X, Y1, ""l2""),
                     l1_min_c(dense_X, Y1, ""squared_hinge""))
        assert_equal(w[0].category, DeprecationWarning)


def check_l1_min_c(X, y, loss, fit_intercept=True, intercept_scaling=None):
    min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)

    clf = {
        'log': LogisticRegression(penalty='l1'),
        'squared_hinge': LinearSVC(loss='squared_hinge',
                                   penalty='l1', dual=False),
    }[loss]

    clf.fit_intercept = fit_intercept
    clf.intercept_scaling = intercept_scaling

    clf.C = min_c
    clf.fit(X, y)
    assert_true((np.asarray(clf.coef_) == 0).all())",projects/scikit-learn-master/sklearn/svm/tests/test_bounds.py,DailyActie/Surrogate-Model,1
"    train = pca.transform(train)
    test = pca.transform(test)

    #clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=False,C=2,tol=1e-9,class_weight=None, random_state=None, intercept_scaling=1.0)
    clf = GaussianNB()
    #clf = MultinomialNB()
    #clf = GradientBoostingClassifier(n_estimators=400)
    #clf = RandomForestClassifier(n_estimators=400)
    #clf = RandomForestClassifier(n_estimators=100,max_depth=8,min_samples_leaf=4,n_jobs=3)
    #clf = SGDClassifier(loss=""log"", penalty=""l2"",alpha=0.1)
    #clf = svm.SVC(C = 1.0, kernel = 'rbf', probability = True)
    if ctype == ""cv"":
        print ""交叉验证""
        hehe = cross_validation.cross_val_score(clf,train,y,cv=3,scoring='roc_auc',n_jobs=-1)
        print hehe
        print np.mean(hehe)

    elif ctype ==""predict"":
        clf.fit(train,y)
        predict = clf.predict_proba(test)[:,1]",python/train_model.py,lavizhao/shopping,1
"	choise='poly'
elif choise==""3"":
	choise='rbf'
elif choise==""4"":
	choise='sigmoid'
elif choise==""5"":
	choise='precomputed'

#Selftest
#print(list(train_results.values.ravel()))
machine = svm.SVC(gamma = 0.001, kernel = 'linear')

machine = machine.fit(features, target.astype(int))

#machine.fit(X, Y) #list(temp2.ravel()) <- Gives -some- effect..but doesn't work


predicted_hotness=machine.predict(train_set.values)

predictionSuccess=(1-np.mean(predicted_hotness != train_results.values))*100",Cloudera/Code/million_song_dataset/Spark_scripts/spark_svm.py,cybercomgroup/Big_Data,1
"iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))",speed/plot.py,quake0day/Dragonite,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = RSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 4.5,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/generations_cmaes_cmaesrsvc/setup.py,jpzk/evopy,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_11_03_2014_server.py,magic2du/contact_matrix,1
"from os.path import isfile, join
import numpy as np
models = [
Perceptron(fit_intercept=False, n_iter=10, shuffle=False),
Perceptron(fit_intercept=False, n_iter=3, shuffle=False),
Perceptron(fit_intercept=False, n_iter=5, shuffle=False),
Perceptron(fit_intercept=True, n_iter=10, shuffle=False),
Perceptron(fit_intercept=True, n_iter=3, shuffle=False),
Perceptron(fit_intercept=True, n_iter=5, shuffle=False),
#linear_model.Ridge(alpha = .5),
svm.LinearSVC(),
svm.SVR(),
SGDClassifier(loss=""hinge"", penalty=""l2""),
SGDClassifier(loss=""log""),
KNeighborsClassifier(n_neighbors=2),
KNeighborsClassifier(n_neighbors=6),
KNeighborsClassifier(n_neighbors=10),
NearestCentroid(), 
RandomForestClassifier(n_estimators=2), 
RandomForestClassifier(n_estimators=10), ",predictor.py,DGaffney/stop_sign_project,1
"import cPickle
import sklearn.preprocessing as PPC
import sklearn.cross_validation as CV
from sklearn.svm import SVC
import numpy as np

clf = SVC()
pos = cPickle.load(open('positive.pkl', 'rb'))
neg = cPickle.load(open('negtive.pkl', 'rb'))
posdata = PPC.scale([pos[k] for k in pos])
postarget = np.ones(posdata.size)
negdata = PPC.scale([neg[k] for k in neg])
negtarget = np.zeros(negdata.size)

data = np.concatenate(posdata,negdata)
target = np.concatenate(postarget, negtarget)",testAI.py,zhuww/planetclient,1
"from sklearn import svm

def my_kernel(x, xhyph):
    print(""x"", x)
    print(""xhyph"", xhyph)
    print(np.dot(x.T, xhyph))
    return (1 + np.dot(x.T, xhyph))**2
    

if __name__ == '__main__':
    clf = svm.SVC(C=10**10, kernel=""poly"", coef0=1, gamma=1, degree=2, verbose=True)
    print(clf.kernel)
    in_data = np.array([[1,0,-1],[0,1,-1], [0,-1,-1], [-1,0,1], [0,2,1],[0,-2,1], [-2,0,1]])
    X = np.c_[in_data[:,0], in_data[:,1]]
    y = in_data[:,2]
    #print(X)
    #print(y)
    clf.fit(X,y)
    print(clf.support_vectors_)
    ",Final/t12.py,pramodh-bn/learn-data-edx,1
"n_samples, n_features = iris.data.shape

n_samples
n_features

iris.target
list(iris.target_names)

X, y = iris.data, iris.target

clf = LinearSVC()
clf

clf = clf.fit(X, y)

clf.coef_
clf.intercept_ 

X_new = [[ 5.0,  3.6,  1.3,  0.25]]
l = clf.predict(X_new)",Lightning Talk @Movile - ML with Scikit-Learn/Recipes/SVM.py,fclesio/learning-space,1
"    >>> ## =================
    >>> local_engine = LocalEngine(tree_root_node,
    ...                            function_name=""transform"",
    ...                            num_processes=3)
    >>> tree_root_node = local_engine.run(**Xy)

    >>> ## Run reduce process
    >>> ## ==================
    >>> tree_root_node.reduce()
    ResultSet(
    [{'key': SelectKBest/SVC(C=1), 'y/test/score_f1': [ 0.6  0.6], 'y/test/score_recall_mean/pval': [ 0.5], 'y/test/score_recall/pval': [ 0.   0.5], 'y/test/score_accuracy/pval': [ 0.], 'y/test/score_f1/pval': [ 0.   0.5], 'y/test/score_precision/pval': [ 0.5  0. ], 'y/test/score_precision': [ 0.6  0.6], 'y/test/score_recall': [ 0.6  0.6], 'y/test/score_accuracy': 0.6, 'y/test/score_recall_mean': 0.6},
     {'key': SelectKBest/SVC(C=3), 'y/test/score_f1': [ 0.6  0.6], 'y/test/score_recall_mean/pval': [ 0.5], 'y/test/score_recall/pval': [ 0.   0.5], 'y/test/score_accuracy/pval': [ 0.], 'y/test/score_f1/pval': [ 0.   0.5], 'y/test/score_precision/pval': [ 0.5  0. ], 'y/test/score_precision': [ 0.6  0.6], 'y/test/score_recall': [ 0.6  0.6], 'y/test/score_accuracy': 0.6, 'y/test/score_recall_mean': 0.6}])

    '''
    tree_root_relative_path = ""./epac_tree""

    def __init__(self,
                 tree_root,
                 function_name=""transform"",
                 num_processes=-1):",epac/map_reduce/engine.py,neurospin/pylearn-epac,1
"		print( ""LogisticRegression: 1e5"" )
		self.clf = LogisticRegression(C=1e5).fit(X_train, y_train)
		print( self.clf.score(X_test, y_test) )

		""""""
		print( ""LDA: "" )
		clf = LDA().fit(X_train, y_train)
		print( clf.score(X_test, y_test) )

		print( ""SVC: "" )
		clf = SVC().fit(X_train, y_train)
		print( clf.score(X_test, y_test) )
		""""""

	def colorname_by_index( self, index ):
		return self.color_names[ index ]

	def predict_from_filename(self, filename):
		return self.predict_from_histogram( self.data_source.histogram_from_filename( filename ) )
",Classifier/__init__.py,h10r/scikit-learn-template,1
"from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

#create data with junk features
print ""generating random dataset with junk features""
X,y = make_classification(n_samples=20000, n_features=30, n_informative=10, n_redundant=5, n_repeated=2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
start = time.clock()
clf = SVC()
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test) * 100.
end = time.clock()
print ""test score of support vector machine trained on initial data %.2f%%"" % score
print ""time to train on initial dataset: %.4f seconds\n"" % (end - start)

#use univariate feature selection to eliminate junk features
print 'eliminating junk features using univariate feature selection'
features = []",Basic_ML/Feature_Selection/feature_selection.py,iamshang1/Projects,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.
    from sklearn.cross_validation import StratifiedShuffleSplit
    from sklearn.grid_search import GridSearchCV
    C_range = np.logspace(-2, 2, 5)
#    gamma_range = np.logspace(-9, 3, 13)
    param_grid = dict( C=C_range)
    cv = StratifiedShuffleSplit(y_train, n_iter=10, test_size=0.2, random_state=42)
    grid = GridSearchCV(LinearSVC(), param_grid=param_grid, cv=cv,verbose=10)
    print ""starting grid search""
    grid.fit(encoded_features, y_train)
#    
    print(""The best parameters are %s with a score of %0.4f""
          % (grid.best_params_, grid.best_score_))",scripts/Keras_deep_pretrain_calculate_cv_allkernels.py,nickgentoo/scikit-learn-graph,1
"    target     = dataset.target

    # Get training and testing splits
    splits     = cross_validation.train_test_split(data, target, test_size=0.2)
    data_train, data_test, target_train, target_test = splits

    load_time  = time.time()

    # Fit the training data to the model
    model      = KNeighborsClassifier()
    # model      = SVC()
    model.fit(data_train, target_train)

    build_time = time.time()

    print model

    # Make predictions
    expected   = target_test
    predicted  = model.predict(data_test)",code/wheat.py,DistrictDataLabs/machine-learning,1
"        pd.DataFrame(gidAttribDict).transpose().to_csv(""/tmp/gidAttribDict.csv"")
        
    return gidAttribDict


def getLearningAlgo(methodName, kwArgs):
    kwargs = kwArgs[methodName]
    if methodName == 'logistic':
        return LogisticRegression(**kwargs)
    elif methodName == 'svm':
        return svm.SVC(**kwargs)
    elif methodName == 'linear_svr':
        return svm.LinearSVR(**kwargs)
    elif methodName == 'svr':
        return svm.SVR()
    elif methodName == 'dtree':
        return tree.DecisionTreeClassifier(**kwargs)
    elif methodName == 'dtree_regressor':
        return tree.DecisionTreeRegressor()
    elif methodName == 'random_forests':",script/ClassiferHelperAPI.py,smenon8/AnimalWildlifeEstimator,1
"
#X = [[0, 0, 0], [1, 3,2],[2,5,3]]
#y = ['3.2', '2','4']
for x in range(size):
    example_ppm[x] = str(example_ppm[x][0])
    #print example_ppm[x]
    #example_ppm[x] = 'similer to atom: '+str(example_name[x]) +' with ppm: '+ #str(example_ppm[x])

#print example_feature
#print example_ppm
clf = svm.SVC()
#clf.fit(example_feature, example_ppm)  
clf.fit(example_feature, example_ppm[0:1200]) 
print '*****************'
print example_test
print '*****************'
size_test = len(example_test)
for k in range(size_test):
    new_atom_features = example_test[k]
    result =  clf.predict(new_atom_features)",NMR-T2-SVC.py,Team-T2-NMR/NMR-T2,1
"
        assert np.array_equal(tpot_obj._rfe(training_testing_data.ix[:, -3:], 0, 0.1), training_testing_data.ix[:, -3:])


def test_rfe_2():
    """"""Ensure that the TPOT RFE outputs the same result as the sklearn rfe when num_features>no. of features in the dataframe """"""
    tpot_obj = TPOT()

    non_feature_columns = ['class', 'group', 'guess']
    training_features = training_testing_data.loc[training_testing_data['group'] == 'training'].drop(non_feature_columns, axis=1)
    estimator = LinearSVC()
    rfe = RFE(estimator, 100, step=0.1)
    rfe.fit(training_features, training_classes)
    mask = rfe.get_support(True)
    mask_cols = list(training_features.iloc[:, mask].columns) + non_feature_columns

    assert np.array_equal(training_testing_data[mask_cols], tpot_obj._rfe(training_testing_data, 64, 0.1))


def test_select_percentile():",tests.py,bartleyn/tpot,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    with use_log_level('error'):
        assert_raises(ValueError, gat.score, epochs2)",mne/decoding/tests/test_time_gen.py,alexandrebarachant/mne-python,1
"    
    C_range = np.array([1])
    gamma_range = np.array([0])
#    C_range = np.logspace(-20,20,20)
#    gamma_range = np.logspace(-20,20,20)
    param_grid = dict(gamma=gamma_range, C=C_range) 
    
    if user['Datatype'] == 'Regression':
        grid = GridSearchCV(svm.SVR(), param_grid=param_grid, cv=kf)
    elif user['Datatype'] == 'Classification 2 classes':
        grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=kf)
    grid.fit(X,Y)
    
    estimator = grid.best_estimator_
    YpredCV, Q2, RMSE_CV = OP.IterCV(X,Y,estimator,kf)
    return YpredCV, Q2, RMSE_CV, estimator
    
def IterCV(X,Y,estimator,kf):
    import PredictivePerformance as PP
    import PCM_workflow as PCM",Optimize_Parameters.py,wiwatowasirikul/PCM,1
"    model.compile(loss='categorical_crossentropy', optimizer=sgd,
                  metrics=['accuracy'])

    print(""Done compiling."")
    return model

def getMLModel(algo):

    multi_nb = MultinomialNB()

    svc = SVC(kernel=""linear"", probability=True)
    
    oneVsSVC =  OneVsRestClassifier(SVC(kernel='linear', probability=True), n_jobs = 4)

    if algo == 'multi_nb':
        return multi_nb

    elif algo == 'svc':
        return svc
    ",research/CNNModelCreatorWordML.py,Ninad998/deepstylometry-python,1
"    print(""%d categories"" % len(categories))
    print()

    print(""n_samples: %d, n_features: %d"" % X_train.shape)
    print()


    print('=' * 80)
    print(""L2 penalty"")
    # Train Liblinear model
    # clf = benchmark(LinearSVC(loss='l2', penalty=""l2"",dual=False, tol=1e-3))
    svr_lin = benchmark(SVR(kernel='linear'))

    print(svr_lin.predict([1,0,3]))",Project/SandBox/BaggedClassifierTrainer.py,tpsatish95/Youtube-Comedy-Comparison,1
"
from sklearn import svm
from timed import Timed

print len(features_train[0])
features_train = features_train[:len(features_train)/100]
labels_train = labels_train[:len(labels_train)/100]

kernel = ""rbf""
C = 10000.
clf = svm.SVC(kernel=kernel, C=C)

with Timed(""Training""):
    clf.fit(features_train, labels_train)

with Timed(""Predicting""):
    pred = clf.predict(features_test)

from sklearn.metrics import accuracy_score
acc = accuracy_score(pred, labels_test)",projects/MOOCs/udacity/ud120-ml/projects/svm/svm_author_id.py,seansu4you87/kupo,1
"

def dropout_channels_monte_carlo(input_matrix: np.array, output_labels: np.array) -> np.array:
    """"""
    Perform 10 fold shuffle split on the data and
    do cross validation to find channels with
    highest correlation to the output variable.
    """"""
    from sklearn.svm import SVC

    clf = SVC(C=1, kernel='linear')

    trials, channels, samples = np.shape(input_matrix)

    def monte_carlo_channel(channel):
        from sklearn.cross_validation import ShuffleSplit, cross_val_score
        from .features import pool

        cross_validation = ShuffleSplit(trials, n_iter=5, test_size=0.2)
        input_pooled = pool(input_matrix[:, [channel]])",atone/preprocessing.py,wohlert/agnosia,1
"# -*- coding: utf-8 -*-

""""""Evaluate the tweet classifier: is this tweet about damage/casualty or not?

Results as of 8/8/13:

INFO:eval-clf:Reading labeled tweets from database...
INFO:eval-clf:Read 1045 tweets
model        ....f1....   pre....rec....acc....f1_std pre_std....rec_std....acc_std
KNeighborsCla....0.52....0.84....0.38....0.84....0.09.... 0.10....   0.08....   0.04
SVC(C=1, cach....0.45....0.89....0.32....0.83....0.14.... 0.07....   0.13....   0.05
DecisionTreeC....0.55....0.91....0.40....0.85....0.09.... 0.07....   0.08....   0.03
MultinomialNB....0.62....0.53....0.74....0.79....0.07.... 0.08....   0.08....   0.03
LogisticRegre....0.66....0.78....0.59....0.86....0.05.... 0.10....   0.06....   0.03
""""""

import argparse
import logging
import numpy as np
",tweedr/ml/pyslda/evaluate-classifier.py,dssg/tweedr,1
"for i in range(Tdata.shape[0]):
    # only use the first 3 components
    data.append([i for i in Tdata.iloc[i, :3]])

labels = training_data.breed

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, stratify=labels, random_state=42)


# first try of SVC
clf = SVC(random_state=42)
params = {
          'C': [0.05, 0.1, 0.2, 0.5, 1.0, 2.0],
         }
print('[INFO] SVC gridsearching', params)
model = GridSearchCV(clf, params, cv=3, refit=False)
model.fit(data, labels)
print('[INFO] best hyperparameters: {}'.format(model.best_params_))
model = SVC(C=0.05, random_state=42) # best params
model.fit(X_train, y_train)",process_ims/machine_learning.py,nateGeorge/IDmyDog,1
"        p.imshow(images[i], cmap=plt.cm.bone)
        
        # label the image with the target value
        p.text(0, 14, str(target[i]))
        p.text(0, 60, str(i))
        
print_faces(faces.images, faces.target, 20)

from sklearn.svm import SVC

svc_1 = SVC(kernel='linear')
print svc_1

from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
        faces.data, faces.target, test_size=0.25, random_state=0)

from sklearn.cross_validation import cross_val_score, KFold
from scipy.stats import sem",src/main/python/scikitlearn_test.py,Chaparqanatoos/kaggle-knowledge,1
"		random_state=seed)
	results = Parallel(n_jobs=-1)(delayed(train_score_clf)(
		RandomForestClassifier(random_state=i), x[tr], x[te], y[tr], y[te])
		for i, (tr, te) in enumerate(sss))
	print 'Random Forest: {0:.3f} %'.format(np.median(results))
	
	# Run SVM in parallel
	sss = StratifiedShuffleSplit(y, n_iter=nsplits, train_size=pct_train,
		random_state=seed)
	results = Parallel(n_jobs=-1)(delayed(train_score_clf)(
		LinearSVC(random_state=i), x[tr], x[te], y[tr], y[te])
		for i, (tr, te) in enumerate(sss))
	print 'Linear SVM: {0:.3f} %'.format(np.median(results))

def sp_one_level(base_dir, data_path='data.csv', seed=123456789):
	""""""
	Test the SP.
	""""""
	
	# Make a new directory",dev/car_evaluation/car_eval.py,tehtechguy/mHTM,1
"      current = sequence[i + n_previous_bars]
      history = []
      for j in range(n_previous_bars):
        history.extend([ int(history_numbers[j] == k) for k in range(k_means) ])
      # history is just ones and zeros
      output.append([history, current])

  '''for history, current in output:
    print history, "":"", current'''

  lin_clf = svm.LinearSVC()
  xy = zip(*output)
  return xy

def argmax(l):
  index, value = max(enumerate(l), key = itemgetter(1))
  return index

def get_decision_function(xy, accuracy = None):
",makeTrainingSet.py,yashsavani/rechorder,1
"
predictors_tr = tfidftr

targets_tr = traindf['cuisine']

predictors_ts = tfidfts


#classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
parameters = {'C':[1, 10]}
#clf = LinearSVC()
clf = LogisticRegression()

# classifier = grid_search.GridSearchCV(clf, parameters)
classifier = GridSearchCV(clf, parameters)

classifier=classifier.fit(predictors_tr,targets_tr)

predictions=classifier.predict(predictors_ts)
testdf['cuisine'] = predictions",deep_learn/whatiscooking/ReadCooking.py,zhDai/CToFun,1
"
    size  =min([len(yes[0]) for yes in yess] + [len(no[0]) for no in nos])

    vsi = yess[:-1] + nos[:-1]
    vsi2 = []
    dolzinaSignala = 700
    for i in vsi:
        vsi2 += [i[0][:dolzinaSignala]]
    pravilni = ['yes' for i in range(yeslen-1)]
    pravilni += ['no' for i in range(nolen-1)]
    clf = svm.SVC()
    clf.fit(vsi2, pravilni)
    print(clf.predict(yess[-1][0][:dolzinaSignala]))
    print(clf.predict(nos[-1][0][:dolzinaSignala]))
",put_in_groups.py,ntadej/DragonHack2015,1
"norm_trn_data, norm_tst_data = norm(trn_data, tst_data)

norm_trn_data0, norm_trn_data1 = split(norm_trn_data, trn_labels)
norm_tst_data0, norm_tst_data1 = split(norm_tst_data, tst_labels)
trn_data0, trn_data1 = split(trn_data, trn_labels)
tst_data0, tst_data1 = split(tst_data, tst_labels)

#################### CLASSIFICATION ################
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)
errors['lda'] = (1-sklda.score(norm_tst_data, tst_labels))
errors['knn'] = (1-skknn.score(norm_tst_data, tst_labels))
errors['svm'] = (1-sksvm.score(norm_tst_data, tst_labels))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",exps/fig_yj.py,binarybana/samcnet,1
"from sklearn.utils.testing import ignore_warnings

from sklearn.metrics.scorer import SCORERS


def test_rfe_set_params():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target
    clf = SVC(kernel=""linear"")
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    y_pred = rfe.fit(X, y).predict(X)

    clf = SVC()
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1,
              estimator_params={'kernel': 'linear'})
    y_pred2 = rfe.fit(X, y).predict(X)
    assert_array_equal(y_pred, y_pred2)
",venv/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_rfe.py,chaluemwut/fbserver,1
"from sklearn.model_selection import cross_val_score


from pyGPGO.GPGO import GPGO
from pyGPGO.surrogates.GaussianProcess import GaussianProcess
from pyGPGO.acquisition import Acquisition
from pyGPGO.covfunc import squaredExponential


def evaluateModel(C, gamma):
    clf = SVC(C=10**C, gamma=10**gamma)
    return np.average(cross_val_score(clf, X, y))
    

if __name__ == '__main__':
    np.random.seed(20)
    X, y = make_moons(n_samples = 200, noise = 0.3)

    cm_bright = ListedColormap(['#fc4349', '#6dbcdb'])
    ",examples/sklearnexample.py,hawk31/pyGPGO,1
"    f_left.close()
    f_right.close()

    return a

def main(training, test):
    tr_lenth, tr_dimension = training.shape
    te_lenth, te_dimension = test.shape

    for i in range(1,100):
        clf = svm.SVC(gamma=0.014, C=0.01*i)
        clf.fit(training[0:tr_lenth, 0:2], training[0:tr_lenth, -1])
    
        error_count = 0
        for t in test:
            if clf.predict(t[0:2]) == t[-1]:
                continue
            else:
                error_count += 1
",src/clf/load_data_left.py,changkun/AugmentedTouch,1
"    clf = None
    kf = KFold(len(labelDF.values), n_folds=5)
    score = 0
    for train_index, test_index in kf:
        X_train = d[train_index, :]
        X_ct = d[test_index, :]
        y_train = labelDF.values[train_index]
        y_ct = labelDF.values[test_index]
        # lin_clf = sklearn.linear_model.LogisticRegression()
        # lin_clf = sklearn.linear_model.LogisticRegression(class_weight='auto')
        # lin_clf = svm.LinearSVC()
        # lin_clf = svm.LinearSVC(class_weight='auto')
        # lin_clf = svm.SVR()
        # lin_clf = svm.SVC()
        # lin_clf = svm.SVC(class_weight='auto')
        lin_clf = svm.SVC(decision_function_shape='ovo')
        # lin_clf = sklearn.neighbors.nearest_centroid.NearestCentroid()
        # lin_clf = sklearn.linear_model.Lasso(alpha = 0.1)
        # lin_clf = sklearn.linear_model.SGDClassifier(loss=""hinge"", penalty=""l2"")
        # lin_clf = sklearn.linear_model.SGDClassifier(loss=""hinge"", penalty=""l2"", class_weight='auto')",main_old.py,joergsimon/gesture-analysis,1
"from sklearn.svm import LinearSVC
from sklearn.cross_validation import train_test_split
from sklearn.datasets import load_boston

#crime_data = np.loadtxt(""data/crime_data.csv"", delimiter="","")
#crime_target = np.loadtxt(""data/crime_target.csv"")

#crime_data_train, crime_data_test, crime_target_train, crime_target_test =\
    #train_test_split(crime_data, crime_target)

#crime_clf = LinearSVC().fit(crime_data_train, crime_target_train)

boston = load_boston()

boston_X_train, boston_y_train, boston_X_test, boston_y_test = train_test_split(boston.data, boston.target)

print(""Crime data:"", len(crime_data))",learn.py,hallfox/Cryme,1
"	#set the timer
	start = time.time()

	trainX = np.load('trainX_feat.npy')
	testX = np.load('testX_feat.npy')
	trainY = np.load('trainY_feat.npy')
	testY = np.load('testY_feat.npy')
	print('\n!!! Data Loading Completed !!!\n')
	
	#clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)
	print('fitting done')

	pred = clf.predict(testX)
	print(accuracy_score(testY, pred))
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",UMKL/textData/kernel.py,akhilpm/Masters-Project,1
"
def svc_example(n_samples = 10000, n_features = 4):
	from sklearn.svm import LinearSVC
	from sklearn.preprocessing import PolynomialFeatures
	from sklearn.datasets import make_classification
	
	X,Y = make_classification(n_samples, n_features)
	#pp = PolynomialFeatures(degree=3)
	
	#X = pp.fit_transform(X)
	m = LinearSVC()
	m.fit(X,Y)

def crash_unexpectedly(signum):
	print(""going to receive signal {}."".format(signum))
	pid = os.getpid()
	time.sleep(1)
	os.kill(pid, signum)
	time.sleep(1)
",unit_tests.py,sfalkner/pynisher,1
"# Load data
train = np.genfromtxt(os.path.join(sys.argv[1], ""digitsdata_17_train.csv""),\
	              delimiter="","", skip_header=1)
test = np.genfromtxt(os.path.join(sys.argv[1], ""digitsdata_17_test.csv""),\
	             delimiter="","", skip_header=1)
X = train[:, 1:]
y = train[:, 0]
Xtest = test[:, 1:] 

# Fit SVM
clf = svm.LinearSVC()
clf.fit(X, y)
pred = clf.predict(Xtest)
pred = pred.reshape(200, 1)

# Export output
np.savetxt(os.path.join(sys.argv[1], 'predict_test_python.csv'), pred, fmt='%f')",SAS_Base_OpenSrcIntegration/digitsdata_svm.py,sassoftware/enlighten-integration,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half8.py,mirjalil/ml-visual-recognition,1
"C = np.logspace(-2, 2, 10)
scores = [cross_val_score(SVC(C=Ci), X, y, cv=5, scoring='precision').mean() for Ci in C]",notebooks/solutions/06-3_5fold_crossval.py,brianjwoo/sklearn_pycon2014,1
"import matplotlib.pyplot as plt

df_train = pd.read_csv(""testwithrevs.csv"")
df_train.head()
feats = df_train.drop(""42"", axis=1) 
X = feats.values #features
y = df_train[""42""].values #target


# Create the RFE object and rank each pixel
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)

ranking = rfe.ranking_.reshape(digits.images[0].shape)
",Ari/Preprocessing/RFE.py,WesleyyC/Restaurant-Revenue-Prediction,1
"        return (train, trainLabels, test)
        test = selector.transform(test)
            
class Classifier():
    def __init__(self, train, trainLabels, test):
        self.train = train
        self.trainLabels = trainLabels
        self.test = test
        
    def LinearSVM(self):
        clf = svm.LinearSVC()
        clf.fit(self.train, self.trainLabels)
        predictions = clf.predict(self.test)
        
        self.predictScore(clf)
        return predictions
    
    def FancySVM(self):
        
        ",prediction.py,jreiberkyle/Kaggle_Data-Science-London,1
"
import matplotlib.pyplot as plt
from sklearn import datasets,svm

# Import the Iris dataset and keep the first two features
iris = datasets.load_iris()
x_train = iris.data[:,:2]
y_train = iris.target

# Instantiate the model and fit to data
clf = svm.SVC(C=1,kernel='rbf',probability=True)
clf.fit(iris.data[:,:2],iris.target)

# Predict (maximum) probabilities for plot
x = arange(4,9,0.05)
y = arange(1,6,0.05)
xx,yy = meshgrid(x,y)
zz = clf.predict_proba(np.vstack((xx.ravel(),yy.ravel())).T).max(axis=1).reshape(xx.shape)

# Create plot",model.py,coreynoone/iris,1
"    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.8, random_state=42
    )
   
    # Create the (parametrised) models
    print(""Hit Rates/Confusion Matrices:\n"")
    models = [(""LR"", LogisticRegression()), 
              (""LDA"", LDA()), 
              (""QDA"", QDA()),
              (""LSVC"", LinearSVC()),
              (""RSVM"", SVC(
              	C=1000000.0, cache_size=200, class_weight=None,
                coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
                max_iter=-1, probability=False, random_state=None,
                shrinking=True, tol=0.001, verbose=False)
              ),
              (""RF"", RandomForestClassifier(
              	n_estimators=1000, criterion='gini', 
                max_depth=None, min_samples_split=2, ",SAT eBook/chapter16/train_test_split.py,Funtimezzhou/TradeBuildTools,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(1,2):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_{1}D_1000_0.6_0.2_0.1_{0}.txt"".format(i,dim),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_{1}D_1000_0.6_0.2_0.085_{0}.txt"".format(i,dim)))

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=496.6,gamma=0.00767,probability=True, cache_size=7000)
        #args=[str(dim)+ ""Dgauss_dt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        clf=""This shouldnt be used as we are in Keras mode""
	args=[str(dim)+""Dgaussian_same_projection__0_1__0_085_nn_4layers_100neurons_onehot"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0,100,4]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,1,args)",Dalitz_simplified/evaluation_of_optimised_classifiers/nn_gaussian_same_projection/nn_Gaussian_same_projection_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(X_train, y_train)
print('Random Forest classifier: ', rf1)

# calculate predictions with probabilities
prediction_p = rf1.predict_proba(X_test)
plot_data(y_test, prediction_p[:,1], ""Random Forest"")


# SVC
svc = SVC(probability = True)
svc.fit(X_train, y_train)
#Y_pred = svc.predict(X_test)
acc_svc = round(svc.score(X_train, y_train) * 100, 2)
print('SVC- accuracy: ', acc_svc)

# calculate predictions with probabilities
prediction_p = svc.predict_proba(X_test)
plot_data(y_test, prediction_p[:,1], ""SVC"")
",examples/titanic-febi-test.py,remigius42/code_camp_2017_machine_learning,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from settings import *

names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]

def main():

	#set the timer",UMKL/textData/classifiers.py,akhilpm/Masters-Project,1
"    #close the sample email
    f.close()
    
    #train linear svm for spam classification
    
    #load the training data
    mat = io.loadmat(""spamTrain.mat"")
    X, y = mat['X'], mat['y']
    
    #train SVM with linear kernel and C = 0.1
    svc = svm.SVC(kernel='linear',C=0.1)
    svc.fit(X,y.ravel())
    #alternatively, use LinearSVC
    svl = svm.LinearSVC(C=0.1)
    svl.fit(X,y.ravel())
    
    #trainig set accuracy
    accuracy_train = svc.score(X,y.ravel())  # or accuracy_score(svc.predict(X), y.ravel())
    accuracy_train_l = svl.score(X,y.ravel())
    ",Ex6_SVM/ex6_spam.py,gpiatkovska/Machine-Learning-in-Python,1
"
# print(targets[3])
# plt.imshow(features[3].reshape(28, 28),cmap=plt.cm.binary)
# plt.show()

# ERROR: Column '2' was not expected (Line 1, Column 1)
# ERROR: Required column 'ImageId' could not be found
# ERROR: Required column 'Label' could not be found.


clf = SVC(kernel=""linear"")

print ""training start...""
clf = clf.fit(features_train, targets_train)
print ""trainging end""

print clf.score(features_test, targets_test)

print ""running predictions...""
predictions = clf.predict(test_features)",digit-recognizer/impl.py,eggie5/kaggle,1
"	labels.append(3)

#Convert to numpy
np.asarray(data)
np.asarray(labels)

#Split training for cross-validation
data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 0.4, random_state = 0)

#Create a Linear SVM and train it
clf = LinearSVC()
print ""Training a Linear SVM Classifier""
clf.fit(data_train, labels_train)
print ""Model trained""

#Perform cross validation and display mean and std
scores = cross_val_score(clf, data_test, labels_test, cv=5, scoring = 'f1_macro')
print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

#Save models and weights",ucf_sub/src/sub_utils/src/svmc.py,RoboticsClubatUCF/RoboSub,1
"	y_regis_train = y_regis[:nTrain]
	y_total_train = y_total[:nTrain]
	Xtest = X[nTrain:,:]
	y_casual_test = y_casual[nTrain:]
	y_regis_test = y_regis[nTrain:]
	y_total_test = y_total[nTrain:]

	
	#linear
	#param_grid = {'C': [1, 5, 10, 100],}
	#clf = GridSearchCV(SVC(kernel='linear'), param_grid,n_jobs=-1)

	clf_regis = SVR(kernel='linear')
	clf_regis.fit(Xtrain,y_regis_train)
	pred_regis = clf_regis.predict(Xtest)
	#print ""best estimator = "",clf.best_estimator_
	#print ""RMSLE linear registered = "", rmsle(y_regis_test, pred_regis)
	
	clf_casual = SVR(kernel='linear')
	clf_casual.fit(Xtrain,y_casual_train)",svm/test_linear_svm.py,agadiraju/519finalproject,1
"
Train_Data = pd.read_csv(""../PrepareData/ProducedData/PCA(25)_prepared_Train_Data.csv"",header=None)
Test_Data = pd.read_csv(""../PrepareData/ProducedData/PCA(25)_prepared_Test_Data.csv"",header=None)

Labels_index = len(Train_Data.iloc[0][:])-1
Train_Data_labels = Train_Data.iloc[:][Labels_index]
del Train_Data[Labels_index]
Test_Data_labels = Test_Data.iloc[:][Labels_index]
del Test_Data[Labels_index]

Model = SVC().fit(Train_Data,Train_Data_labels)

print(""Result on PCA with 25 components :"")
Predicted_Train = Model.predict(Train_Data)
print(""Accuracy on Training Data :"",metrics.accuracy_score(Train_Data_labels,Predicted_Train))

Predicted_Train = Model.predict(Test_Data)
print(""Accuracy on Testing Data :"",metrics.accuracy_score(Test_Data_labels,Predicted_Train))
print(""\nconfusion_matrix : \n"",metrics.confusion_matrix(Test_Data_labels,Predicted_Train,labels=[0,1]))
print(""\nclassification report : \n\n"",metrics.classification_report(Test_Data_labels,Predicted_Train,labels=[0,1]))",IDS/SVC.py,danialjahed/IDS-KDDcup,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    y = y.astype(float)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx = alpha_investing.alpha_investing(X[train], y[train], 0.05, 0.05)

        # obtain the dataset on the selected features
        selected_features = X[:, idx]
",PyFeaST/example/test_alpha_investing.py,jundongl/PyFeaST,1
"        Callable object that returns a scalar score; greater is better.

    Examples
    --------
    >>> from sklearn.metrics import fbeta_score, make_scorer
    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
    >>> ftwo_scorer
    make_scorer(fbeta_score, beta=2)
    >>> from sklearn.grid_search import GridSearchCV
    >>> from sklearn.svm import LinearSVC
    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
    ...                     scoring=ftwo_scorer)
    """"""
    sign = 1 if greater_is_better else -1
    if needs_proba and needs_threshold:
        raise ValueError(""Set either needs_proba or needs_threshold to True,""
                         "" but not both."")
    if needs_proba:
        cls = _ProbaScorer
    elif needs_threshold:",sklearn/metrics/scorer.py,johnowhitaker/bobibabber,1
"    #Convert features and answers into arrays:
    testFeatures = np.asarray(testFeatures)
    testAnswers = np.asarray(testAnswers)
    finalFeatures = np.asarray(finalFeatures)
    finalAnswers = np.asarray(finalAnswers)
    cvFeatures = np.asarray(cvFeatures)
    cvAnswers = np.asarray(cvAnswers)

    #Creation of the learners:
    lrLearner = LogisticRegression(penalty='l1', dual=False, C=1000.0)
    svmLearner = svm.SVC(C=5, kernel='poly', degree=4)
    knnLearner = neighbors.KNeighborsClassifier(n_neighbors=350, algorithm='auto')

    return findTrainerErrorParallel(lrLearner, svmLearner, knnLearner, finalFeatures, finalAnswers, testFeatures, testAnswers)



# file = open('lrPrecDown', 'rb')
# avelrPrecDown = np.asarray(pi.load(file))
",stockMarket/learning/averagedLearning.py,seba-1511/stockMarket,1
"            # well, *log* probs, anyway
            preds = [p[1] for p in clf.predict_log_proba(X_test)]
        else:
            preds = clf.predict(X_test)
        return preds, y_test
    

    @staticmethod 
    def _get_SVM():
        tune_params = [{""C"":[1,5,10,100,1000]}]
        return GridSearchCV(LinearSVC(), tune_params, scoring=""f1"")


    def train(self):
        features, y = self.features_from_citations()
        self.vectorizer = DictVectorizer(sparse=True)
        X_fv = self.vectorizer.fit_transform(self.features)
        
        self.clf = _get_SVM()
",supervised_learner.py,ijmarshall/cochrane-nlp,1
"
#p.figure()
#p.plot(tst_data0[:,0], tst_data0[:,1], 'g.',label='0', alpha=0.5)
#p.plot(tst_data1[:,0], tst_data1[:,1], 'r.',label='1', alpha=0.5)
#p.legend(fontsize=8, loc='best')

#p.show()
#################### CLASSIFICATION ################
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)
errors['lda'] = (1-sklda.score(norm_tst_data, tst_labels))
errors['knn'] = (1-skknn.score(norm_tst_data, tst_labels))
errors['svm'] = (1-sksvm.score(norm_tst_data, tst_labels))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",exps/karen.py,binarybana/samcnet,1
"# Written by Matt Warren
#

xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
                     np.linspace(-3, 3, 500))
np.random.seed(10)
X = np.random.randn(300, 2)
Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

# fit the model
clf = svm.NuSVC()
clf.fit(X, Y)

# plot the decision function for each datapoint on the grid
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
           origin='lower', cmap=plt.cm.PuOr_r)",2015/06/fc_2015_06_11.py,mfwarren/FreeCoding,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit
from mne.decoding import CSP

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,bloyl/mne-python,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Python27/Lib/email/test/test_email.py,sometallgit/AutoUploader,1
"if __name__==""__main__"":
    # Prepare train data and test data
    train_df, train_target_df, test_df, predict_df = pre_proc_all()
    
    # Train and evaluate the model
    C_set, gamma_set = 2.0**np.mgrid[-10.:10.:1.,-10.:10.:1.]
    cols_for_res = [""clf"", ""score_mean"", ""score_std""]   #######
    results_set  = pd.DataFrame(columns=cols_for_res)   #######
    for i, (C, gamma) in enumerate(zip(C_set.reshape(-1), gamma_set.reshape(-1))):
        # Define the classifyer model
        clf = svm.SVC(C=C, gamma=gamma)
        
        # Assess the model with cross validation data
        tgt_arr = train_target_df.as_matrix().reshape(-1)
        scores  = cross_val_score(clf, train_df, tgt_arr, cv=100)
        
        temp = pd.DataFrame([[clf, scores.mean(), scores.std()]], columns=cols_for_res, index=[i])
        results_set = results_set.append(temp) #######
        print ""{0:4d}/{1:4d} Param C: {2:0.2e}, gamma: {3:0.2e}, "".format(i, C_set.size, C, gamma),
        print ""Score mean: {0:0.3f}, std: {1:0.3f}"".format(scores.mean(), scores.std())",Titanic/train_n_predict_svc.py,a2takashi/kaggle-challenge,1
"
# import six
# import math
# import sklearn


def classifyUsingSVM(trainX, trainY, testX, testY):
    print('################# classifyUsingSVM() started ##################')
    start_time = time.time()
    
    clf = svm.SVC(kernel='rbf')
    print('SVM Initialized')
    
    clf.fit(trainX, trainY)
    print('SVM Trained')
    
    predictedY = clf.predict(testX)
    print('SVM prediction completed')
 
    accuracy = accuracy_score(testY, predictedY)",src/Classifier.py,md-k-sarker/ML-Analyze-Morality,1
"        from sklearn.svm import SVC
        from sklearn.model_selection import StratifiedKFold

        paramgrid = {""kernel"": [""rbf""],
                     ""C""     : np.logspace(-9, 9, num=25, base=10),
                     ""gamma"" : np.logspace(-9, 9, num=25, base=10)}

        random.seed(1)

        from evolutionary_search import EvolutionaryAlgorithmSearchCV
        cv = EvolutionaryAlgorithmSearchCV(estimator=SVC(),
                                           params=paramgrid,
                                           scoring=""accuracy"",
                                           cv=StratifiedKFold(n_splits=10),
                                           verbose=1,
                                           population_size=50,
                                           gene_mutation_prob=0.10,
                                           gene_crossover_prob=0.5,
                                           tournament_size=3,
                                           generations_number=10)",evolutionary_search/cv.py,rsteca/sklearn-deap,1
"	""""""
	
	seed = 123456789
	np.random.seed(seed)
	ntrain, ntest = 800, 200
	(tr_x, tr_y), (te_x, te_y) = load_mnist()
	x, y = np.vstack((tr_x, te_x)), np.hstack((tr_y, te_y))
	cv = MNISTCV(tr_y, te_y, ntrain, ntest, 1, seed)

	for tr, te in cv:
		clf = OneVsRestClassifier(LinearSVC(random_state=seed), -1)
		clf.fit(x[tr], y[tr])
		print clf.score(x[te], y[te])
		
		clf = LinearSVC(random_state=seed)
		clf.fit(x[tr], y[tr])
		print clf.score(x[te], y[te])
		
		clf = OneVsOneClassifier(LinearSVC(random_state=seed), -1)
		clf.fit(x[tr], y[tr])",dev/mnist_novelty_detection/OneVsRest.py,tehtechguy/mHTM,1
"
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]
                    '''



scores2 = ['precision', 'recall']

'''

clf = svm.SVC(

C=1.0, cache_size=200, class_weight=None, coef0=0.0,

  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',

  max_iter=-1, probability=False, random_state=None, shrinking=True,

  tol=0.001, verbose=False
",model_testing/georges-classifier-stuff-2.0 works parameter changes.py,rschenck/Capsid_IDP_Classifier,1
"                max_iter=100,
                random_state=None,
                solver='newton-cg',
                multi_class='ovr',
                verbose=0,
                warm_start=False,
                n_jobs=1
                )

    @staticmethod
    def defaultLinearSVC():
        """"""Default suport vector machine with linear kernel classifier.
        :returns: LinearSVC classifier.
        """"""

        return LinearSVC(
                loss='hinge',
                penalty='l2',
                multi_class='ovr',
                fit_intercept=True,",hr/defaults.py,LukaK/HackerRankFramework,1
"    vectorize = Vectorize()
    vectorize.gen_words_doc(""../data/weiboV2.tsv"")
    vectorize.dict_init_from_file(""../data/weiboV2.tsv"")
    vectorize.tfidf_init()
    
    feature_dict = feature_encoding()
    emoji_dict = get_all_emoji_dict()
    train_X, train_y = gen_data(feature_dict, emoji_dict, 4244, vectorize, ""train"")
    test_X, test_y = gen_data(feature_dict, emoji_dict, 4244, vectorize, ""test"")

    clf = SVC(degree=3,
              kernel=""rbf"",
              class_weight={1:0.2, 2:0.4, 3:0.4})
    print ""=== train SVM model... === ""
    clf.fit(train_X, train_y)
    pred_y_res = clf.predict(test_X)

    accuracy_cnt = 0
    for pred_y, label_y in zip(pred_y_res, test_y):
        if pred_y == label_y:",src/sksvm.py,IDRC-Tsinghua/Vectorize,1
"	print ""training...""

	C = 0.19
	gamma = 0.0028
	shrinking = True
	#auto_class_weights = False

	probability = True
	verbose = True

	svc = SVC( C = C, gamma = gamma, shrinking = shrinking, probability = probability, verbose = verbose )
	svc.fit( x_train, y_train )
	p = svc.predict_proba( x_test )

	p = p[:,1] 

	# make sure both y and p are of shape (n,1) and not (n,)
	ids_and_p = np.hstack(( ids.reshape(( -1, 1 )), p.reshape(( -1, 1 ))))
	np.savetxt( output_file, ids_and_p, fmt = [ '%d', '%.10f' ], delimiter = ',', header = 'UserID,Probability1', comments = '' )",vectorize_and_predict_inplace.py,zygmuntz/kaggle-happiness,1
"np.unique(iris_y)
np.random.seed(0)


indices = np.random.permutation(len(iris_X))
iris_X_train = iris_X[indices[:-10]]
iris_y_train = iris_y[indices[:-10]]
iris_X_test  = iris_X[indices[-10:]]
iris_y_test  = iris_y[indices[-10:]]

svc = svm.SVC(kernel='rbf')
svc.fit(iris_X_train, iris_y_train) 
print svc.predict(iris_X)
""""""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

# import some data to play with",Proyecto Final/prueba.py,Sealos/Sarcasm,1
"	N = len(Y)
	label_enc = LabelEncoder()
	label_enc.fit(list(causes))
	print 'found %d causes' % len(label_enc.classes_)
	Y = [list(label_enc.transform(yi)) for yi in Y]
	print 'labels:', label_enc.classes_
	# LabelBinarizer buggy with np arrays. See https://github.com/scikit-learn/scikit-learn/issues/856
	Y = LabelBinarizer().fit_transform(Y)
	#clf = OneVsRestClassifier(MultinomialNB())
	#clf = OneVsRestClassifier(LogisticRegression())
	clf = OneVsRestClassifier(LinearSVC(random_state=0))

	cv = cross_validation.KFold(len(Y), 10, shuffle=True, random_state=1234)
	losses = []
	for train, test in cv:
		truth = Y[test]
		pred = clf.fit(X[list(train)], Y[list(train)])
		pred = pred.predict(X[test])
		losses.append(metrics.precision_score(truth.reshape(-1), pred.reshape(-1)))
	print 'F1 score=%.3f stderr=%.3f' % (np.average(losses), np.std(losses))",givinggraph/companycause/company_cause_svm.py,erichilarysmithsr/givinggraph,1
"import sklearn.ensemble
import sklearn.svm


print ""Loading data and transforming to toxin features""
X,Y = data.load_toxin_features(substring_length=2)

def run_classifiers(X,Y):
  print ""Data shape"", X.shape
  for c in [0.0001, 0.001, 0.01]:#, 0.1, 1, 10]:
    svm = sklearn.svm.LinearSVC(C=c)
    print ""SVM C ="", c
    print np.mean(sklearn.cross_validation.cross_val_score(svm, X, Y, cv = 10))
 
  n_classifiers = 2000
  rf = sklearn.ensemble.RandomForestClassifier(n_classifiers)
  print ""Random Forest""
  print np.mean(sklearn.cross_validation.cross_val_score(rf, X, Y, cv = 10))

""""""",Jan21_toxin.py,iskandr/immuno,1
"@param.float(""C-Exp"", interval=[0, 4])
@param.float(""Gamma-Exp"", interval=[-6, 0])
def f(C_exp, gamma_exp):
    iris = datasets.load_iris()

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        iris.data, iris.target, test_size=0.4, random_state=0)

    C = 10 ** C_exp
    gamma = 10 ** gamma_exp
    clf = svm.SVC(C=C, gamma=gamma)
    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)


def main():
    from metaopt.core.optimize.optimize import optimize
    from metaopt.optimizer.pso import PSOOptimizer
",examples/showcase/svm_pso_global_timeout.py,cigroup-ol/metaopt,1
"        datasets_ori = [(feature, target)]
        #datasets_expand, indicies = sv.expand_2d(datasets_ori)
        datasets_expand, indicies = datasets_ori, range(len(datasets_ori))
        #print datasets_expand
        datasets = sv.preprocess(datasets_expand, scale=True, max_samples=10000, nshuffle=nshuffle)
        #print datasets
        print len(datasets)
        clf = [
            LogisticRegression(),
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LDA(),
            QDA()]
        data_names = [ 'X' + str(i) + '#' + str(j) for i in indicies for j in range(nshuffle) ]
        clf_names = ['LR', 'KNN', 'LSVM', 'RBF', ""Tree"",",myquant/strategy/LogisticStrategy.py,goldfire9/stock-ml,1
"
    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        
    X_scaler = StandardScaler().fit(X)
    scaled_X = X_scaler.transform(X)
    y = np.hstack((np.ones(len(car_image_list)), np.zeros(len(non_car_image_list))))

    rand_state = np.random.randint(0, 100)
    X_train, X_test, y_train, y_test = train_test_split(
        scaled_X, y, test_size=0.1, random_state=rand_state)

    svc = LinearSVC()
    svc.fit(X_train, y_train)

    fit_accuracy = svc.score(X_test, y_test)
    print(""Accuracy:"", fit_accuracy)

    with open(svc_pickle_file, 'wb') as f:
        pickle.dump({
                    ""svc"": svc,
                    ""X_scaler"": X_scaler",Term1/Project5/p5.py,DavidObando/carnd,1
"
    ndcgs = compute_ndcgs_without_ws(data);
    print('Current NDCG: {}, std: {}'.format(np.mean(ndcgs), np.std(ndcgs)))
    print()

    xs, ys = transform_data(data)

    if args.plot:
        plot_diagrams(xs, ys, FEATURES)

    clf = svm.LinearSVC(random_state=args.seed)
    cv = cross_validation.KFold(len(ys), n_folds=5, shuffle=True, random_state=args.seed)

    # ""C"" stands for the regularizer constant.
    grid = {'C': np.power(10.0, np.arange(-5, 6))}
    gs = grid_search.GridSearchCV(clf, grid, scoring='accuracy', cv=cv)
    gs.fit(xs, ys)

    ws = gs.best_estimator_.coef_[0]
    max_w = max(abs(w) for w in ws)",search/search_quality/scoring_model.py,mgsergio/omim,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx, feature_score, subset_score = trace_ratio.trace_ratio(X[train], y[train], num_fea, style='fisher')

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",skfeast/example/test_trace_ratio.py,jundongl/scikit-feast,1
"#TRAIN-TEST SPLIT
train_data, test_data = model_selection.train_test_split(dataframe, test_size = 0.1)

print (train_data.shape)
vectorizer = HandCraftedFeatureExtractor()
X = vectorizer.fit_transform(train_data['TEXT'])
print (np.shape(X))

#training
kfold = model_selection.KFold(n_splits=10)
results = model_selection.cross_val_score(LinearSVC(), X, train_data['SENTIMENT'], cv=kfold)
mean_acc = results.mean()
print (mean_acc)

'''

pipeline = Pipeline ([
        ('features', FeatureUnion([
            ('tfidf', TfidfTransformer(norm='l2', use_idf = True)),
            ('hand', HandCraftedFeatureExtractor())",testing_features.py,mab1290/GHOST,1
"print ""* Testing set has {} samples."".format(X_test.shape[0])

# print ""* X Train mean {}"".format(X_train.mean(axis=0))
# print ""* X Train stddev:"", X_train.std(axis=0)


print ""== 1st basic training on original data"", ""-""*40

#  Initialize the three models
clf_A = DecisionTreeClassifier(random_state=my_random_seed)
clf_B = SVC(random_state=my_random_seed)
clf_C = LinearSVC(C=0.1)
clf_D = GaussianNB()


# loop thru models
for clf in [clf_A, clf_B, clf_C, clf_D]:
    print ""\n{}: \n"".format(clf.__class__.__name__)
    train_predict(clf, X_train, y_train, X_test, y_test)
    scores = cross_validation.cross_val_score(clf, features, labels, cv=5)",reduce_learn_tune.py,stefan-bergstein/ml-capstone,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/problems/SchwefelsProblem26/ORIDSESSVC.py,jpzk/evopy,1
"#     knn.fit(X_train, y_train[:, col])
#     print X_train.shape, y_train[:, col].shape
#     print ""predicting...""
#     y_pred[:,col] = knn.predict_proba(X_val)[:,1]



# #svm's
# y_pred = np.zeros(y_val.shape)
# for col in range(1):
#     clf = SVC(probability=True)
#     print ""fitting...""
#     clf.fit(X_train, y_train[:, col])
#     print ""predicting...""
#     y_pred[:,col] = clf.predict_proba(X_val)[:,1]

",logistic_regression.py,HristoBuyukliev/dss-hackaton,1
"#
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)


#
# TODO: Create an SVC classifier named svc
# Use a linear kernel, and set the C value to C
#
from sklearn.svm import SVC
svc = SVC(C=C, kernel=kernel)

#
# TODO: Create an KNeighbors classifier named knn
# Set the neighbor count to 5
#
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)

",Module6/assignment4.py,szigyi/DAT210x,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,cle1109/mne-python,1
"        cpu._bitwise_instruction(lambda x, y: x ^ y, None, *operands)
        cpu.commitFlags()

    @instruction
    def TST(cpu, Rn, Rm):
        shifted, carry = Rm.read(withCarry=True)
        result = Rn.read() & shifted
        cpu.setFlags(N=HighBit(result), Z=(result==0), C=carry)

    @instruction
    def SVC(cpu, op):
        if op.read() != 0:
            logger.warning(""Bad SVC number: {:08}"".format(op.read()))
        raise Interruption(0)

    @instruction
    def CMN(cpu, src, add):
        result, carry, overflow = cpu._ADD(src.read(), add.read())
        return result, carry, overflow
",manticore/core/cpu/arm.py,trailofbits/manticore,1
"test_X = []
for elment in test_X:
	test_X.append(element.flatten())

Y = [1,1,1,1,1,1,1,1,1,0,0,0,0,0,0]

# print np.array(X).shape
# print X[0]


lin_svc = svm.LinearSVC(random_state = 0).fit(X, Y)
",task2_2_step_3.py,haiweiosu/Optical-Character-Recognition-using-Template-Matching-Object-Detection-in-Images,1
"from sklearn.metrics import accuracy_score
import numpy as np

from pykernels.basic import RBF

X = np.array([[1,1], [0,0], [1,0], [0,1]])
y = np.array([1, 1, 0, 0])

print 'Testing XOR'

for clf, name in [(SVC(kernel=RBF(), C=1000), 'pykernel'), (SVC(kernel='rbf', C=1000), 'sklearn')]:
    clf.fit(X, y)
    print name
    print clf
    print 'Predictions:', clf.predict(X)
    print 'Accuracy:', accuracy_score(clf.predict(X), y)
    print",example.py,gmum/pykernels,1
"        n1, n2 = nodes
        features[i,:] = feature_map_function(g, n1, n2)
        labels[i] = loss_function(g, n1, n2, gt)
        labeled_image.ravel()[list(g[n1][n2]['boundary'])] = 2+labels[i]
        g.merge_nodes(n1,n2)
    return features, labels, labeled_image

def select_classifier(cname, features=None, labels=None, **kwargs):
    if 'svm'.startswith(cname):
        del kwargs['class_weight']
        c = SVC(probability=True, **kwargs)
    elif 'logistic-regression'.startswith(cname):
        c = LogisticRegression()
    elif 'linear-regression'.startswith(cname):
        c = LinearRegression()
    elif 'random-forest'.startswith(cname):
        if sklearn_available:
            c = DefaultRandomForest()
        elif vigra_available:
            c = VigraRandomForest()",ray/classify.py,jni/ray,1
"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
         ""Quadratic Discriminant Analysis""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",Python/testscikit.py,Crobisaur/HyperSpec,1
"from sklearn.tree import DecisionTreeClassifier as DecisionTree
from sklearn.ensemble import RandomForestClassifier as RanomForest
from sklearn.neighbors import KNeighborsClassifier as KNeighbors

from ambiruptor.base.core import Learner

# Parametrized by 'C' - Penalty parameter of the error term.
class LinearSVMClassifier(Learner):
    def __init__(self, C=1.0):
        """"""Init the learning model""""""
        self.model = OneVsRestClassifier(LinearSVC(C=C))

# Parametrized by
# 'C' that trades off misclassification of training examples against simplicity of the decision surface
# 'gamma' that defines how far the influence of a single training example reaches
# model parameters
class RbfSVMClassifier(Learner):
    def __init__(self, C=1.0, gamma='auto'):
        """"""Init the learning model""""""
        self.model = OneVsRestClassifier(SVC(C=C, gamma=gamma, kernel='rbf'))",ambiruptor/library/learners/models.py,Ambiruptor/Ambiruptor,1
"    
    @abstractmethod
    def show_kernel_matrix(self,X,Y=None):
        K=self.kernel(X,Y)
        imshow(K, interpolation=""nearest"")
        show()
    
    @abstractmethod
    def svc(self,X,y,lmbda=1.0,Xtst=None,ytst=None):
        from sklearn import svm
        svc=svm.SVC(kernel=self.kernel,C=lmbda)
        svc.fit(X,y)
        if Xtst is None:
            return svc
        else:
            ypre=svc.predict(Xtst)
            if ytst is None:
                return svc,ypre
            else:
                return svc,ypre,1-svc.score(Xtst,ytst)",kerpy/Kernel.py,VirgiAgl/Auto_GP_updated,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",App/Lib/email/test/test_email.py,2uller/LotF,1
"    tre = []
    tee = []
    confusion = np.zeros((29,29))
    cfs = []

    muy = np.unique(ytrain[:mtrain[0]])
    if len(muy)==1:
        ytrain[0] = muy[0]-1
    
    for m in mtrain:
        clf = svm.SVC(kernel='linear')
        clf.fit(Xtrain[:m],ytrain[:m])
        ytrpred = clf.predict(Xtrain[:m])
        ytepred = clf.predict(Xtest)

        ntr = len(np.where(ytrain[:m]!=ytrpred)[0])
        nte = len(np.where(ytest!=ytepred)[0])
        
        print('Number of training errors with {0} examples: {1}'.format(m,ntr))
        print('Number of test errors with {0} examples: {1}'.format(m,nte))",plot_utils.py,wmorning/inDianajonES,1
"        print np.amin(gamma)
        
        
def generate_binary_crime_label():
    y = retrieve_crime_count(2013)
    threshold = np.median(y)
    label = [1 if ele >= threshold else 0 for ele in y]
    F = generate_corina_features()
    from sklearn import svm, tree
    from sklearn.model_selection import cross_val_score
    clf1 = svm.SVC()
    scores1 = cross_val_score(clf1, F[1], label, cv=10)
    print scores1.mean(), scores1
    clf2 = tree.DecisionTreeClassifier()
    scores2 = cross_val_score(clf2, F[1], label, cv=10)
    print scores2.mean(), scores2
    pickle.dump(label, open(""crime-label"", 'w'))
    return y, label, F[1]
    
    ",python/FeatureUtils.py,thekingofkings/chicago-crime,1
"LABELED = 'Labeled'
TRAIN = 'Train'
TEST = 'Test'

class Classifier(BiPlot):
    '''
    To hold methods and data to support classification of measurements in a STOQS database.
    See http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html
    '''
    classifiers = { 'Nearest_Neighbors': KNeighborsClassifier(3),
                    'Linear_SVM': SVC(kernel=""linear"", C=0.025),
                    'RBF_SVM': SVC(gamma=2, C=1),
                    'Decision_Tree': DecisionTreeClassifier(max_depth=5),
                    'Random_Forest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                    'AdaBoost': AdaBoostClassifier(),
                    'Naive_Bayes': GaussianNB(),
                    'LDA': LDA(),
                    'QDA': QDA()
                  }
    def getActivity(self, mpx, mpy):",contrib/analysis/classify.py,mikemccann/stoqs,1
"
#clf = BaggingClassifier(n_estimators = 2000)
#clf.fit(train_data, train_label)


#eclf2 = VotingClassifier(estimators=[('lr', clf1), ('gb', clf2), ('gnb', clf3)], voting='soft')

#clf  = RandomForestClassifier(n_estimators=6000, max_depth = 4, verbose=1).fit(train_data, train_label)
#knn = neighbors.KNeighborsClassifier()
#logistic = linear_model.LogisticRegression()
#clf = svm.SVC(probability = True)
#clf = tree.DecisionTreeClassifier()


#print('KNN score: %f' % knn.fit(train_data, train_label).score(valid_data, valid_label))
#result = knn.fit(train_data, train_label).predict_proba(test_data)
#train_data = train_data[0:5000,:]
#train_label = train_label[0:5000]
#result = logistic.fit(train_data, train_label).predict_proba(test_data)
#clf.fit(train_data, train_label)",liao/final.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"          'subsample' : subsample,
          'min_samples_split' :min_samples_split,
          'verbose': verbose,
          'random_state':random_state,
          'min_samples_leaf':min_samples_leaf
        
         # 'max_features': None,
           }
        #print self.params_
        p0 = self.params_.copy()
        #p0[""init""] = Adaptor(SVC(probability=True))
      
        
        
        p1 = self.params_.copy()
        #p1[""init""] = Adaptor(SVC(probability=True))
        
        self.svm = [linear_model.LogisticRegression(class_weight= ""auto""),  linear_model.LogisticRegression(class_weight= ""auto"")]
        self.clf = [Gbc(**p0),Gbc(**p1) ]
        ",causal_classifier.py,ssamot/causality,1
"    mat_content = sio.loadmat(data_file)
    
    X = mat_content['X']
    y = mat_content['y']
    y = y.ravel()

    print('Training Linear SVM (Spam Classification)')
    
    C = 0.1

    model = svm.SVC(C=C, kernel='linear', max_iter=200)
    model.fit(X, y)

    print('Training Accuracy: %f' % model.score(X, y))
    
    raw_input('Program paused. Press enter to continue')

    # =================== Part 4: Test Spam Classification ===================
    
    data_file = '../../data/ex6/spamTest.mat'",skeletons/ex6/ex6_spam.py,cameronlai/ml-class-python,1
"with open(""accuracy"", ""rb"") as f:
    for line in f:
        acc = line.strip()
        test_results.append(float(acc))
print ""\nScores: "", test_results
print ""Test  Average accuracy: %f \n"" % np.mean(test_results)
sys.stdout.flush()
os.remove(""accuracy"")


#     text_clf = LinearSVC(C=1)
#     _ = text_clf.fit(d2v_model.train_doc_vecs, d2v_model.train_labels)
#     perf = text_clf.score(d2v_model.test_doc_vecs, d2v_model.test_labels) 
#     perf2 = text_clf.score(d2v_model.train_doc_vecs, d2v_model.train_labels)
#     print "" Train accuracy:"" + str(perf2)
#     print "" Test accuracy:"" + str(perf)

#     # clf = SVC(kernel='rbf',C=c, gamma=g).fit(train, train_Label)
#     # perf2 = clf.score(train, train_Label)
#     # perf = clf.score(test, test_Label)",sentiment-analysis-w2v.py,linbojin/dv4sa,1
"    return X, y


def plot_rbf_svm_parameters():
    X, y = make_handcrafted_dataset()

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, C in zip(axes, [1e0, 5, 10, 100]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='rbf', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)

    fig, axes = plt.subplots(1, 4, figsize=(15, 3))
    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])
        svm = SVC(gamma=gamma, kernel='rbf', C=1).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""gamma = %f"" % gamma)",notebooks/figures/plot_rbf_svm_parameters.py,wavelets/scipy_2015_sklearn_tutorial,1
"
    #svm（Support Vector Machine） regression
    # 统计学习方法
    # PRML

    t = time.time()
    pca = PCA(n_components=0.8,whiten=True)
    train_x = pca.fit_transform(train_data)
    test_x = pca.transform(test_data)

    svc = svm.SVC(kernel='rbf',C=10)
    svc.fit(train_x, train_label)
    preds = svc.predict(test_x)    
    
    acc = float((preds == test_label).sum())/len(test_x)
    print(""SVM Validation Accuracy: %f, %.2fs"" % (acc, time.time() - t))

    # 22000, 97.8%",Code/Chapter6/machinelearning/knn2.py,heibanke/python_do_something,1
"""""""
__author__ = 'Allison MacLeay'


def q1a():
    """"""SVM on Spam Data
    length train: 4140 length test 461
    train acc: 0.806763285024 test acc: 0.819956616052
    """"""
    data = utils.pandas_to_data(utils.load_and_normalize_spam_data())
    svm_q1(data, svm.SVC(kernel='poly'))


def q1b():
    """"""SVM on haar dataset
       OneVsOneClassifier:
    Loading 9000 records from haar dataset
    Beginning analysis: (8100, 200)
    train acc: 0.91950617284 test acc: 0.81""""""
    multiclassSVC(LinearSVC(random_state=0), 5000)",Homeworks/hw6.py,alliemacleay/MachineLearning_CS6140,1
"        else:
            print('C = %r' % (C,))

        clf_all = sklearn.svm.SVC(kernel=str('linear'), C=C, class_weight='balanced',
                                  decision_function_shape='ovr', verbose=10)
        X_train = ds.data.take(train_idx, axis=0)
        clf_all.fit(X_train, y_train)
        ut.save_data(clf_fpath, clf_all.__dict__)
        clf = clf_all
    else:
        clf = sklearn.svm.SVC()
        clf.__dict__.update(**ut.load_data(clf_fpath))

    def test_classifier(clf, X_test, y_test):
        print('[problem] test classifier on %d data points' % (len(test_idx),))
        if len(clf.classes_) == 2:
            # Adapt _ovr_decision_function for 2-class case
            # This is simply a linear scaling into a probability based on the
            # other members of this query.
            X = clf._validate_for_predict(X_test)",ibeis/scripts/classify_shark.py,SU-ECE-17-7/ibeis,1
"# q = question id (for collation)
X, y, q = np.delete(wout, [8, 46], axis=1), wout[:, 8], wout[:, 46]
# border between test and training data
border = len(y) * 2/3

# Spacing of the parameters we are trying to visualize (C and gamma)
base = 10 ** (1/10.)
exp_range = range(-60, 61)

def svc((C, gamma)):
    s = SVC(C=C, gamma=gamma, probability=True)
    start = time.time()
    s.fit(X[:border], y[:border])
    train_time = time.time() - start
    pred = s.predict_proba(X[border:])[:, 0]
    test_time = (time.time() - start) - train_time

    # This is the literal is-it-the-right-answer  binary score.
    # This measure is what we try to maximize but its relation to question
    # accuracy is complicated",scripts/svm_graph.py,lake-of-dreams/Assist,1
"# in the outer cross-validation procedure
# we make the decorator explicitly so we can reuse the same folds
# in both tuned and untuned approaches
folds = optunity.cross_validation.generate_folds(data.shape[0], num_folds=3)
outer_cv = optunity.cross_validated(x=data, y=labels, num_folds=3, folds=[folds],
                                    aggregator=optunity.cross_validation.identity)
outer_cv = optunity.cross_validated(x=data, y=labels, num_folds=3)

# compute area under ROC curve of default parameters
def compute_roc_standard(x_train, y_train, x_test, y_test):
    model = sklearn.svm.SVC().fit(x_train, y_train)
    decision_values = model.decision_function(x_test)
    auc = optunity.metrics.roc_auc(y_test, decision_values)
    return auc

# decorate with cross-validation
compute_roc_standard = outer_cv(compute_roc_standard)
roc_standard = compute_roc_standard()
#print('Nested cv area under ROC curve of non-tuned model: ' + str(roc_standard))
",bin/examples/python/sklearn/svc.py,MarkAWard/optunity,1
"        if nworks:
            self._kb_works_norm = nworks
        else:
            self._kb_works_norm = self._get_normalized_works()

        self._crf_citation_extractor = crf_citation_extractor
        self._postaggers = postaggers

        self._citation_parser = CitationParser()
        self._feature_extractor = FeatureExtractor(self._kb_authors_norm, self._kb_works_norm)
        self._ranker = SVMrank(svm.SVC(kernel='linear', C=0.1))

    def _normalize(self, text, lang=None):

        def remove_punctuation(s):
            return re.sub(ur""\p{P}+"", """", s)

        def trim_spaces(s):
            return u' '.join(s.split())
",citation_extractor/ned/ml_matchers.py,mfilippo/CitationExtractor,1
"from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""rbf"", C=0.025, probability=True),
    NuSVC(probability=True),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    GradientBoostingClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]
",learn/utils/classify_test.py,taotaocoule/stock,1
"import numpy as np
import random
from sklearn.svm import SVC
from sklearn.metrics.pairwise import rbf_kernel,laplacian_kernel,chi2_kernel,linear_kernel,polynomial_kernel,cosine_similarity
from sklearn import preprocessing
import xlrd
from sklearn.model_selection import GridSearchCV
def Lsvm_patatune(train_x,train_y):
    tuned_parameters = [
        {'kernel': ['precomputed'], 'C': [0.01, 0.1, 1, 10, 100, 1000]}]
    clf = GridSearchCV(SVC(C=1, probability=True), tuned_parameters, cv=5, n_jobs=1
                       )  # SVC(probability=True)#SVC(kernel=""linear"", probability=True)
    clf.fit(train_x, train_y)
    return clf.best_params_['C']
def splitdata(X,Y,ratio,seed):
    '''This function is to split the data into train and test data randomly and preserve the pos/neg ratio'''
    n_samples = X.shape[0]
    y = Y.astype(int)
    y_bin = np.bincount(y)
    classes = np.nonzero(y_bin)[0]",AMKL/ACal20rbf.py,hongliuuuu/Results_Dis,1
"# SVM results
from sklearn.svm import SVC
from sklearn import metrics


fig = plt.figure(figsize=(6, 6))

i = 0
for kernel in ['rbf', 'linear']:
    clf = SVC(kernel=kernel).fit(Xtrain, ytrain)
    ypred = clf.predict(Xtest)
    label = ""SVC: kernel = {0}"".format(kernel)
    print('{0}: {1}'.format(label, metrics.f1_score(ytest, ypred))
    ax = fig.add_subplot(3, 2, i + 1, xticks=[], yticks=[])
    ax.imshow(metrics.confusion_matrix(ypred, ytest),
               interpolation='nearest', cmap=plt.cm.binary)
    #ax.colorbar()
    ax.set_xlabel(""true label"")
    ax.set_ylabel(""predicted label"")",notebooks/solutions/04_svm_rf.py,pprett/sklearn_pycon2014,1
"            index_test.append(index)
        else:
            semanticFeatures_train.append(numpy.array(temp))
            classes_train.append(labels[index])

    feature_train = csr_matrix(numpy.array(semanticFeatures_train))
    feature_test = csr_matrix(numpy.array(semanticFeatures_test))

    resultFile.flush()

    model = svm.SVC()
    model.fit(feature_train, classes_train)
    predictions = model.predict(feature_test)

    if len(predictions) != len(classes_test):
        print 'inference error!'

    for index, label in enumerate(predictions):
        if label == 0 and classes_test[index] == 1:
            print ids[index_test[index]]",caseStudy.py,renhaocui/adPlatform,1
"Y_pred_GBC = clf.predict(test_data.drop(droplist[1:], axis=1))
Y_pred_GBC = Y_pred_GBC.round().astype(float)
print(""GBC model has %d survivors"" %np.sum(Y_pred_GBC))

# Support Vector Machines
droplist = 'Survived PassengerId Age_Known Cabin_Known Pclass Sex Age Embarked'.split()
data = training_data.drop(droplist, axis=1)
# ensmeble training and test set
X, y = data, training_data['Survived']

svc = SVC()
svc.fit(X, y)
Y_pred_svm = svc.predict(test_data.drop(droplist[1:], axis=1)).astype(float)
print(""SVM model has %d survivors"" %np.sum(Y_pred_svm))

# combine the models
Y_pred = np.add(0.3333*Y_pred_MLP, 0.3333*Y_pred_svm, 0.3333*Y_pred_GBC)
Y_pred = Y_pred.round().astype(int)
print(""Combined model has %d survivors"" %np.sum(Y_pred))
",titanic_3_options.py,michael-hoffman/titanic,1
"def build_trained_model(training_data, classifier='svc'):
    alpha = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]
    ridge_params = {'alpha': alpha}

    c_s = [0.01, 0.1, 1.0, 10.0, 100.0]
    gamma = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]
    svc_params = [{'kernel': ['rbf'], 'gamma': gamma, 'C': c_s},
                  {'kernel': ['linear'], 'C': c_s}]

    if classifier == 'svc':
        clf = GridSearchCV(SVC(probability=True), svc_params, cv=5)
        # clf = GridSearchCV(SVC(probability=True, class_weight='balanced'), svc_params, cv=5)
    elif classifier == 'ridge':
        clf = GridSearchCV(RidgeClassifier(), ridge_params, cv=5)
    else:
        raise NotImplementedError(""Only 'svc' (default) and 'ridge' classifiers are supported"")

    pipe = Pipeline([
        ('standard_scalar', StandardScaler()),
        ('feature_selection', SelectFdr()),",isi_lib/utils.py,whitews/wa-isit,1
"class SoftMax(Classifier):
  def __init__(self, data_set, configs):
    parameters = {'C': configs['c_range']}
    clf=linear_model.LogisticRegression(penalty='l2', solver='newton-cg', multi_class='multinomial')
    self.clf = GridSearchCV(clf, parameters, n_jobs=1)
    self.clf.fit(data_set.X_train, data_set.y_train)
    logging.info(""Training Accuracy(C=%f) %.4f"" % (self.clf.best_params_['C'], self.clf.best_score_))

class SVM(Classifier):
  def __init__(self, data_set, parameters):
    svr = SVC()
    verbose = 9
    if 'verbose' in parameters:
      verbose = int(parameters['verbose'])
    self.clf = GridSearchCV(svr, parameters, verbose=verbose, n_jobs=-1)
    self.clf.fit(data_set.X_train, data_set.y_train)

class SVMGaussianKernel(SVM):
  def __init__(self, data_set, configs):
    parameters = {'kernel': ['rbf'], 'C': configs['c_range'], 'gamma': configs['gamma_range']}",leaf/classifier.py,albertpliu/learnDL,1
"        )
        images = io.load_images_from_dir(data_dir, extension)
        conditions = io.load_labels(epoch_file)
        data, labels = prepare_searchlight_mvpa_data(images, conditions)
        # the following line is an example to leaving a subject out
        #epoch_info = [x for x in epoch_info if x[1] != 0]
    num_subjs = int(sys.argv[5])
    # create a Searchlight object
    sl = Searchlight(sl_rad=1)
    mvs = MVPAVoxelSelector(data, mask, labels, num_subjs, sl)
    clf = svm.SVC(kernel='linear', shrinking=False, C=1)
    # only rank 0 has meaningful return values
    score_volume, results = mvs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        score_volume = np.nan_to_num(score_volume.astype(np.float))
        io.save_as_nifti_file(score_volume, mask_image.affine,
                                   'result_score.nii.gz')
        seq_volume = np.zeros(mask.shape, dtype=np.int)
        seq = np.zeros(len(results), dtype=np.int)",examples/fcma/mvpa_voxel_selection.py,lcnature/brainiak,1
"#        test_set = feature[test]
#        clf.fit(train_set, labels[train])
#        pred = clf.predict(test_set)
#        score.append(accuracy_score(labels[test], pred))
#        score.append(precision_score(labels[test], pred))
#        score.append(recall_score(labels[test], pred))
#        score.append(f1_score(labels[test], pred))
#        scores.append(score)
#    avg = np.average(scores, axis=0)
#    return avg
##print validate(SVC(kernel='poly', degree=2, C=1000000), feature, 500)
#print validate(SVC(C=10000, gamma=0.75), feature, 500)
#print validate(LinearSVC(C=100), feature, 500)
#print validate(LogisticRegression(C=100), feature, 500)

sfk = cv.StratifiedShuffleSplit(labels, 500)
clf1 = SVC(C=10000, gamma=0.75, probability=True)
all_prob = []
all_label = []
for train, test in sfk:",learning/final_plot.py,fcchou/CS229-project,1
"test = rawtest[:,feat_inds]
norm_test = (test - test.mean(axis=0)) / np.sqrt(test.var(axis=0,ddof=1))
N = test.shape[0]
D = data.shape[1]
#sys.exit()

trn_labels = np.hstack(( np.zeros(Ntrn/2), np.ones(Ntrn/2) ))
tst_labels = np.hstack(( np.zeros(N/2), np.ones(N/2) ))
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_data, trn_labels)
skknn.fit(norm_data, trn_labels)
sksvm.fit(norm_data, trn_labels)
print(""skLDA error: %f"" % (1-sklda.score(norm_test, tst_labels)))
print(""skKNN error: %f"" % (1-skknn.score(norm_test, tst_labels)))
print(""skSVM error: %f"" % (1-sksvm.score(norm_test, tst_labels)))

labels = np.hstack((np.zeros(N/2), np.ones(N/2)))
n,gext,grid = get_grid_data(np.vstack(( norm_data0, norm_data1 )))",tests/all_poisson.py,binarybana/samcnet,1
"    fh = open(path, 'w')
    fh.write(liac_arff.dumps(arff_data))
    fh.close()


def __available_classifiers():
    available_clfs = dict()
    # features of all available classifiers
    Classifier = collections.namedtuple('Classifier', ['idf', 'full_name', 'function_call',
                                                       'scaling_possible', 'predict_proba', 'numeric_labels'])
    available_clfs[""svm""] = Classifier(""svm"", ""Support Vector Machine"", svm.SVC(probability=True), True, True, False)
    available_clfs[""svm_gs1""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.1, coef0=0.01, gamma=0.01),
                                           True, True, False)
    available_clfs[""svm_gs2""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.01, coef0=0.01, gamma=0.0),
                                           True, True, False)
    available_clfs[""mnb""] = Classifier(""mnb"", ""Multinomial Naive Bayes"", naive_bayes.MultinomialNB(), False, True,
                                       False)  # MNB can't do default scaling: ValueError: Input X must be non-negative
    available_clfs[""knn""] = Classifier(""knn"", ""k Nearest Neighbour"", neighbors.KNeighborsClassifier(), True, True,",safeness/glad/glad-main.py,pasmod/obfuscation,1
"# print help(clf)
# param_dist = {""n_estimators"": [100, 200],
#               ""max_depth"": [None, 8, 10, 12, 20],
#               'learning_rate': [0.1],
#               # ""max_features"": sp_randint(1, 11),
#               # ""min_samples_split"": sp_randint(1, 11),
#               # ""min_samples_leaf"": sp_randint(1, 11),
#               }

param_dist = {'C': [12, 14, 100], 'cache_size': [2048]}
clf = SVC(probability=True)
# searhc = GridSearchCV(clf, param_dist, random_state=42, cv=2, scoring='log_loss', verbose=3, n_jobs=-1)
random_search = RandomizedSearchCV(clf, param_dist, random_state=42, cv=2, scoring='log_loss', verbose=3, n_jobs=2, n_iter=3)
fit = random_search.fit(X, target)",src/sklearn_random_search.py,ternaus/kaggle_otto,1
"    importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))
    plt.barh(range(len(name_list)),importance_list,align='center')
    plt.yticks(range(len(name_list)),name_list)
    plt.xlabel('Relative Importance in the Random Forest')
    plt.ylabel('Features')
    plt.title('%s \n Relative Feature Importance' %(dataName))
    plt.grid('off')
    plt.ion()
    plt.show()

def PlotPerfPercentFeatures(X,y,est=LinearSVC()):
    '''
    Display performance of a classifier (default: SVM),
    varying the percentile of features retained (F-test) .

    http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py
    '''
    transform = SelectPercentile(f_classif)

    clf = Pipeline([('anova', transform), ('est', est)])",ProFET/feat_extract/VisualizeBestFeatures.py,ddofer/ProFET,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_03_12_2015_parallel.py,magic2du/contact_matrix,1
"form sklearn.svm import linearSVC

svc = linearSVC()

# Train the SVC
svc.fit(X_train, y_train)

print(""Model accuracy: {}"".format(svc.predict()))",svm.py,ledrui/programming-problems,1
"    labelsNum = le.transform(labels)

    param_grid = [
        {'C': [1, 10, 100, 1000],
            'kernel': ['linear']},
        {'C': [1, 10, 100, 1000],
            'gamma': [0.001, 0.0001],
            'kernel': ['rbf']}
    ]
    svm = GridSearchCV(
        SVC(probability=True),
        param_grid, verbose=4, cv=5, n_jobs=16
    ).fit(embeddings, labelsNum)
    print(""Best estimator: {}"".format(svm.best_estimator_))
    print(""Best score on left out data: {:.2f}"".format(svm.best_score_))

    with open(""{}/classifier.pkl"".format(args.workDir), 'w') as f:
        pickle.dump((le, svm), f)

",demos/classifier.py,sahilshah/openface,1
"    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc=""lower right"")
    plt.show()
    plt.close('all')

def plot_svc_decision_function(data, label, ax=None):
    """"""Plot the decision function for a 2D SVC""""""
    data, label = splitBufferDataByLabel(data, label, classificationCondition=1)
    label = [float(value) for value in label]
    clf = svm.SVC(kernel='rbf').fit(data, label)
    fig = plt.figure()
    if ax is None:
        ax = plt.gca()
    xx, yy = np.meshgrid(np.linspace(-1,1,1000), np.linspace(-1,1,1000))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # plot the margins
    ax.contour(xx, yy, Z, colors='k',
               levels=[-1, 0, 1], alpha=0.5,",src/script/new/buffer.py,changkun/AugmentedTouch,1
"    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)
    grid_search.fit(X, y)
    assert_true(hasattr(grid_search, ""best_params_""))


def test_grid_search_error():
    """"""Test that grid search will capture errors on data with different
    length""""""
    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)

    clf = LinearSVC()
    cv = GridSearchCV(clf, {'C': [0.1, 1.0]})
    assert_raises(ValueError, cv.fit, X_[:180], y_)


def test_grid_search_one_grid_point():
    X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
    param_dict = {""C"": [1.0], ""kernel"": [""rbf""], ""gamma"": [0.1]}

    clf = SVC()",python/sklearn/sklearn/tests/test_grid_search.py,seckcoder/lang-learn,1
"def train(features, target):
    """"""Train a classifier on given dataset.

    Args:
        features (array): features.
        target (array): target values of classes.
    Returns:
        clf: Classifier
    """"""
    
    clf = svm.LinearSVC()
    clf.fit(features, target)

    return clf


def preprocess(data, sr):
    """"""Preprocess the data and find features
    """"""
",src/modules/classify.py,lepisma/audire,1
"    from sklearn.svm import SVC
    from sklearn import metrics
    import matplotlib.pyplot as plt
#    import pandas as pd
    
    training_data, test_data = form_data(stocks, init_param)
    std_scale = preprocessing.StandardScaler().fit(training_data.X)
    training_data.X = std_scale.transform(training_data.X)
    test_data.X = std_scale.transform(test_data.X)
    
    svm = SVC(kernel='rbf', random_state=0, gamma = gamma_in, C=C_in, probability=True)
    svm.fit(training_data.X, training_data.y)
    preds = svm.predict_proba(test_data.X)[:,1]
    fpr, tpr, _ = metrics.roc_curve(test_data.y, preds)

#    df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))
    roc_auc = metrics.auc(fpr,tpr)
    
    if verbose:
        plt.figure()",chelmbigstock/chelmbigstock.py,mikec964/chelmbigstock,1
"                               flip_y=0.01, 
                               class_sep=1.0, 
                               hypercube=True, 
                               shift=0.0, 
                               scale=1.0, 
                               shuffle=True, 
                               random_state=None)
    
    y[np.where(y == 0)[0]] = -1.
    
    estimator = SVC(C=10,
                    kernel='rbf',
                    gamma=0.4,
                    probability=True)
    pu_estimator = PUAdapter(estimator, hold_out_ratio=0.2)
    
    pu_estimator.fit(X, y)
    
    print pu_estimator
    print",src/examples/puAdapterExample.py,jayfans3/pu-learning,1
"import threading
import os.path

# basic parameters
my_kernel = 'linear'
my_max_iteration = 500000
my_test_size = 0.3
my_random_state = 42

def classifyModel(trainingData, trainingLabel, kernel='linear', max_iter=-1):
    clf = svm.SVC(kernel=kernel, max_iter=max_iter).fit(trainingData, trainingLabel)
    return clf
def testingWithModel(testData, testLabel, model):
    error_count = 0.0
    result = model.predict(testData)
    for i, la in enumerate(result):
        if la != testLabel[i]:
            error_count += 1
    return error_count/result.shape[0]
def classify(trainingData, trainingLabel, testData, testLabel, kernel='linear', max_iter=-1):",dataset/script/moment.py,changkun/MotionTouch,1
"    y_test = y[1000:2000]

    # Set the parameters by cross-validation
    C=[]
    gamma=[]
    for i in range(4): gamma.append( 4.5e-7 + i * 0.25e-7)
    for i in range(1,5): C.append( i * 10 )

    tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma, 'C': C}]

    clf = GridSearchCV(svm.SVC(kernel='rbf'), tuned_parameters, cv=10, scoring='accuracy')

    # We learn the digits on the first half of the digits
    clf.fit(x_train, y_train)

    # Now predict the value of the digit on the second half:
    y_true, y_pred = y_test, clf.predict(x_test)
    
    print""Best parameters set found on development set:""
    print",DigitRecognizer/digitr.py,n7jti/kaggle,1
"    
####################################################################
# Dalitz operaton
####################################################################

for i in range(100):
	comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.{0}.0.txt"".format(i), os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.2{0}.1.txt"".format(str(i).zfill(2))))
    
#clf = tree.DecisionTreeClassifier('gini','best',46, 100, 1, 0.0, None)
clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.95,n_estimators=440)
#clf = SVC(C=1.0,gamma=0.0955,probability=True, cache_size=7000)
args=[""dalitz_bdt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
#For nn:
#args=[""dalitz_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),71,1]

classifier_eval_simplified.classifier_eval(0,0,args)

",Dalitz_simplified/evaluation_of_optimised_classifiers/bdt_Dalitz/bdt_Dalitz_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"    unlabeled_items: list of (view1_feature, view2_feature)
    
    ...
    
    output_path: where the result is saved

    """"""
    clfs = [
        # MultinomialNB(),
        LogisticRegression(),
        # SVC(kernel='linear')
    ]

    print ""started training...""
    all_results = Parallel(n_jobs=len(clfs))(delayed(train) \
                                             (clf1=clf,
                                              clf2=clf,
                                              views_obj=views_obj,
                                              labeled_examples=seed_items,
                                              unlabeled_examples=unlabeled_items,",_cotrain.py,xiaohan2012/cotrain,1
"class LinearSVC(sklearn.svm.LinearSVC):
    def __init__(self, **params):
        self.name = 'OneVsRestSVMsClassifier'
        self.classifier = sklearn.svm.LinearSVC(**params)
    def set_params(self, **params):
        self.classifier.set_params(**params)
    def predict(self, X):
        #import pdb; pdb.set_trace()
        return self.classifier.predict(X)
    def clone(self):
        r = LinearSVC()
        r.classifier = sklearn.base.clone(self.classifier)
        return r
    def fit(self, X, y, parameters):
        cross_val_score = loadModule(parameters['cross_validation'])
        metric_func = loadModule(parameters['metric'])
        average_measure = loadModule(parameters['average_measure'])
        if parameters.has_key('beta'):
            average_measure = partial(average_measure, beta=parameters['beta'])
        hps1 = parameters['param_dict']",src/python/model/sklearn_wrapper.py,XiaoLiuAI/RUPEE,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features on training set
        idx = CFS.cfs(X[train], y[train])

        # obtain the dataset on the selected features
        selected_features = X[:, idx[0:num_fea]]
",skfeast/example/test_CFS.py,jundongl/scikit-feast,1
"from sklearn import svm
from wallace.predictive_models.sklearn_model import SklearnModel, TrainedSklearnModel
from wallace.parameters import ParametersGeneralValidityCheck

class SvmSvcRegression(SklearnModel):
    def train(self, dataset):
        model = svm.SVC(
                C=self.get_penalty_parameter(),
                kernel=self.get_kernel(),
                shrinking=self.get_shrinking(),
                )
        independent_data = self.get_independent_variable_data(dataset)
        dependent_data = self.get_dependent_variable_data(dataset)
        trained_regression = model.fit(independent_data, dependent_data)

        return TrainedSklearnModel(self, trained_regression)",wallace/predictive_models/svm_svc_regression.py,wangjohn/wallace,1
"  if cname == 'randomforest':
    C = RandomForestClassifier(n_estimators=10, max_depth=None,
                             min_samples_leaf=10, random_state=0)
  elif cname == 'logistic':
    C = LogisticRegression(penalty='l2', C=1.0)
  elif cname == 'nearestneighbor1':
    C = KNeighborsClassifier(n_neighbors=1, algorithm='brute')
  elif cname == 'nearestneighbor3':
    C = KNeighborsClassifier(n_neighbors=3, algorithm='brute')
  elif cname == 'svm-rbf':
    C = SVC(C=1.0, kernel='rbf', probability=True)
  elif cname == 'svm-linear':
    C = SVC(C=1.0, kernel='linear', probability=True)
  else:
    raise NotImplementedError('Not recognized: ' + cname)
  
  ## Unique name for classifier pipeline
  ## must uniquely identify the training set + features + classifier
  uname = '%s-%s-%s' % (Train['trainuname'], Train['featuname'], cname)
  Train['pipelineuname'] = uname",satdetect/detect/Classifier.py,michaelchughes/satdetect,1
"        xTrLocal = [i[int(not(refIdx))] for i in xTrLocal]
        xTestLocal = [i[int(not(refIdx))] for i in xTest]

        gramTr = self._makeGram(xTrLocal, xTrLocal,
                                simMatT[not(refIdx)], metaT[not(refIdx)])
        gramTest = self._makeGram(xTestLocal,xTrLocal,
                                  simMatT[not(refIdx)], metaT[not(refIdx)])

        # Predict
        yPred = None; yPredNII = None
        clf = svm.SVC(kernel='precomputed')
        if (len(set(yTrLocal))==2): # as for binary clf where nClass=2
            clf.fit(gramTr,yTrLocal)

            yPred = clf.predict(gramTest)
            yPredNII = [None]*len(xTest)
        else:
            clf.fit(gramTr,yTrLocalNII)

            yPred = [None]*len(xTest)",predictor/blmnii/blm_tor.py,tttor/csipb-jamu-prj,1
"    parse_data=parse_csv(Train_File_name)
    
    #input Feature & output label
    X,Y = create_dataset(parse_data)
    #for i in range(len(X)):
    #    print X[i],Y[i]

    #Draw(Y,X)
    
    #Now Create & Train Our Classifier
    clsf=SVC(kernel='rbf',gamma=0.1,C=1) #SVM Classier
    print 'Training Started ..'
    a = datetime.datetime.now()
    clsf.fit(X, Y)
    b=datetime.datetime.now()
    print 'Training Is completed, Time taken for training :',b-a
    
    #Now Load Testing dataset
    '''
    parse_data=parse_csv(Test_file_name,)",Kaggle/Titanic/MySVMPredicterTester.py,sudhanshuptl/Machine-Learning,1
"
        :param wav_file_list: , each files should be named  # TODO : replace that with a list of namedTuple (file, class) for example ?
        :param clf: default is SVC with rbf kernel
        :param confidence_threshold:
        :return:
        """"""
        if wav_file_list is None:
            wav_file_list = glob.glob('/mnt/protolab_innov/data/sounds/dataset/*.wav')
        if clf is None:
            # TODO : try with linearSVC .. and one vs all
            clf = sklearn.svm.SVC(kernel='rbf', probability=True, verbose=False)
        print(""CLF is %s"" % clf)
        self.to_sklearn_features = DataFrameMapper([('features', sklearn.feature_extraction.DictVectorizer())])
        self.scaler = None  # init during learn
        self.wav_file_list = wav_file_list
        self.nfft = 1024
        self.fs = 48000.  # for now we force it .. TODO

        self.clf = clf
        self.confidence_threshold = confidence_threshold",sound_classification/classification_service.py,laurent-george/protolab_sound_recognition,1
"  num_train_records = num_records * 2 / 3
  train_input_values = input_values[:num_train_records]
  test_input_values = input_values[num_train_records:]
  train_labels = labels[:num_train_records]
  test_labels = labels[num_train_records:]

  X = np.array(train_input_values)
  y = np.array(train_labels)

  if classifier_type == ""svm"":
    clf = svm.LinearSVC()
  else:
    raise ValueError(""Classifier type is '%s' but can only be: %s""
                     % VALID_CLASSIFIER_TYPES)

  clf.fit(X, y)

  X_test = np.array(test_input_values)
  y_test = np.array(test_labels)
",brainsquared/module_runners/sklearn_trainer_runner.py,CloudbrainLabs/htm-challenge,1
"stacking_create_training_set('ensemble_duke_output_raw_T2_n%d.txt' %N,'training_set_T2_n%d.csv' %N, gold_standard_name, N)

#read it and make machine learning on it

data = pd.read_csv('training_set_T2_n%d.csv' %N)

X = data.values[:,2:(N+2)] #x variables
y = np.array(data['y']) #class variables

#fit an SVM with rbf kernel
clf = SVC( kernel = 'rbf',cache_size = 1000)
#parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
gs_rbf.fit(X,y)
#save the output
output = np.reshape(gs_rbf.predict(X),(len(data),1))

#dump it to file",validation/FEIII2016/sfem_validation.py,enricopal/STEM,1
"	####################################################################

	for i in range(100):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.1_{0}.txt"".format(i,dim),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/gaussian_same_projection_on_each_axis/gauss_data/gaussian_same_projection_on_each_axis_redefined_{1}D_1000_0.6_0.2_0.1_1{0}.txt"".format(str(i).zfill(2),dim)))

	
	#for bdt originally we used learning_rate=0.01,n_estimators=983

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.0482,n_estimators=942)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        args=[str(dim)+ ""Dgaussian_same_projection_redefined__0_1__0_1_noCPV_optimised_bdt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),params['dimof_middle'],params['n_hidden_layers']]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/bdt_gaussian_same_projection/bdt_Gaussian_same_projection_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"    """"""Exception class to raise if estimator is used before fitting.

    This class inherits from both ValueError and AttributeError to help with
    exception handling and backward compatibility.

    Examples
    --------
    >>> from sklearn.svm import LinearSVC
    >>> from sklearn.exceptions import NotFittedError
    >>> try:
    ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
    ... except NotFittedError as e:
    ...     print(repr(e))
    ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    NotFittedError('This LinearSVC instance is not fitted yet',)

    .. versionchanged:: 0.18
       Moved from sklearn.utils.validation.
    """"""
",mlens/externals/sklearn/exceptions.py,flennerhag/mlens,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_08_2015_04.py,magic2du/contact_matrix,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half4.py,mirjalil/ml-visual-recognition,1
"    results = {}
    for classname in CLASSES:
        results[classname] = MODELS[classname].predict(data)
    return results


def train_class(x_train, y_train):
    """"""
    Trains a model for one class
    """"""
    svm = LinearSVC(C=0.3, max_iter=300, loss='hinge')
    pipeline = Pipeline([
        ('union', FeatureUnion([
            ('sentiment', sentiment_extractor),
            ('temp', temp_extractor),
            ('wind', wind_extractor),
            ('vect', vectorizer),
        ])),
        ('cls', svm),
    ])",example9/classifier.py,jramcast/ml_weather,1
"
# import the relevant code and make your train/test split
# name the output datasets features_train, features_test,
# labels_train, and labels_test

# set the random_state to 0 and the test_size to 0.4 so
# we can exactly check your result

###############################################################

clf = SVC(kernel=""linear"", C=1.).fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################
def submitAcc():
    return clf.score(features_test, labels_test)",learning/testing_example.py,dmytroKarataiev/MachineLearning,1
"#DATA_TRAIN_LABELS_FILE = 'A4Q2_train_labels.npy'
#DATA_TEST_FILE = 'A4Q2_test.npy'

#Training = np.load(DATA_TRAIN_FILE)
#Labels = np.load(DATA_TRAIN_LABELS_FILE)
#Test = np.load(DATA_TEST_FILE)

#train_data, test_data = Training[:-100], Training[-100:]
#train_labels, test_labels = Labels[:-100], Labels[-100:]

#clf = SVC()
#clf.fit(Training, Labels)
#p=svc.predict(test_data).astype(bool)

#print clf.predict(Test[:,:])

N = 300; seed(12345)
r1, r2 = 2+rand(N), randn(N)
a1, a2 = 2*pi*rand(N), 0.5*pi*rand(N)
figure(""predict the label"")",4155/assignments/a4/svm_example.py,moriarty/csci-homework,1
"    d=[]
    for row in dataset:
        row1 = row.split("","");
        y.append(row1[15])
        d.append(row1[1:14]+row1[16:])
    return (y,d) #tuple that returns the 1st elent, the second element)

y,X= split_class_and_data(dataset)


clf=svm.SVC(kernel='linear', C=1) #setting up svm to be trained. use linear as default
clf.fit(X,y) #first arg is data, second arg is the class
print ""clf="", clf
print""Now predicting""
t1=X[0]   #the first entry in the data, and without the class info and predict what class it belongs to.
r=clf.predict(t1) #pass in the item or entry that you want to make a prediction about
print ""Prediction result="",r
print ""\n\n""

",dataprocessing.py,VVLJyotsna/PipelineEmbolization,1
"from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC

# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,
                           n_redundant=2, n_repeated=0, n_classes=8,
                           n_clusters_per_class=1, random_state=0)

# Create the RFE object and compute a cross-validated score.
svc = SVC(kernel=""linear"")
# The ""accuracy"" scoring is proportional to the number of correct
# classifications
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),
              scoring='accuracy')
rfecv.fit(X, y)

print(""Optimal number of features : %d"" % rfecv.n_features_)

# Plot number of features VS. cross-validation scores",projects/scikit-learn-master/examples/feature_selection/plot_rfe_with_cross_validation.py,DailyActie/Surrogate-Model,1
"iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset
Y = iris.target

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))",unused_code/svm_example.py,davestanley/compnet-email-classifier,1
"
# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,
                                                    random_state=1)
### Grid Search CV
tfidf = TfidfVectorizer(strip_accents=None,
                        lowercase=False,
                        preprocessor=None)

svc_tfidf = Pipeline([('vect', tfidf),
                     ('clf', SVC(random_state=1))])

ngram_range = [(1, args.ngram)]
param_range = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_grid = [{'vect__ngram_range': ngram_range,
               'vect__stop_words': [stop, None],
               'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_twitter],
               'clf__C': param_range, 
               'clf__kernel': ['linear']},
              {'vect__ngram_range': ngram_range,",Notebook_gs_svc.py,alejandrox1/tweet_authorship,1
"
    clf = LogisticRegression(penalty='l1',C=30,tol=1e-9)

    x = clf.fit_transform(x,label)

    t = clf.transform(t)

    print ""x shape"",x.shape
    print ""t.shape"",t.shape
    
    #clf = svm.SVC(kernel='sigmoid',degree=9,gamma=10)
    #clf = svm.SVC(degree=9,gamma=0.001)
    #clf = KNeighborsClassifier(n_neighbors=1)
    #
    #clf = SGDClassifier(loss=""log"",n_iter=300, penalty=""l2"",alpha=0.0003)
    clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=False,C=3.2,tol=1e-9,class_weight=None, random_state=None, intercept_scaling=1.0)
    print ""交叉验证""
    print np.mean(cross_validation.cross_val_score(clf,x,label,cv=20,scoring='roc_auc'))
    clf.fit(x,label)
    #验一下自己的结果",tfidf.py,ezhouyang/class,1
"    
    numSamples = np.shape(xval)[0]
    bestclf = {};
    bestscore = 0;
    # Cs = [0.01,0.1,1,10,100];    
    C_2d_range = [100,1000]
    gamma_2d_range = [0.001,0.01, 1, 10]
    classifiers = []
    for C in C_2d_range:
        for gamma in gamma_2d_range:
            clf = SVC(C=C, gamma=gamma)
            # clf = LinearSVC(C = cc,gamm)
            clf = clf.fit(xtrain, ytrain)
            preds = clf.predict(xval)
            correctPreds = preds == yval;
            score = 100*float(np.sum(correctPreds))/numSamples
            print 'Overall Accuracy is ',score, '% ', ' C = ',str(C),' features = ',featType
            if score>bestscore:
                bestclf = clf
                bestscore = score",processing/mbhcls.py,gurkirt/actNet-inAct,1
"    return y, code_map_inverse

def get_formatted_data(dictionary, source, messages, feature_index_map, feature_num, use_tfidf=False, master_messages=[]):
    X = get_formatted_X(dictionary, source, messages, feature_index_map, feature_num, use_tfidf, master_messages)
    y, code_map_inverse = get_formatted_y(source, messages, master_messages)

    return X, y, code_map_inverse


def train_model(X, y, model_save_path=None):
    lin_clf = svm.LinearSVC()
    lin_clf.fit(X, y)

    if model_save_path:
        joblib.dump(lin_clf, model_save_path + ""/model.pkl"")

    return lin_clf


def get_prediction(lin_model, X):",msgvis/apps/coding/utils.py,hds-lab/coding-ml,1
"

# sample spam email from SpamAssassin public corpus
sample_email = u'''From mikeedo@emailisfun.com  Wed Jun 27 04:56:45 2001
Return-Path: <mikeedo@emailisfun.com>
Delivered-To: yyyy@netnoteinc.com
Received: from ns.mediline.co.in (unknown [203.197.32.212]) by
    mail.netnoteinc.com (Postfix) with ESMTP id 43F82130028 for
    <jm7@netnoteinc.com>; Wed, 27 Jun 2001 04:56:44 +0100 (IST)
Received: from gw02_[192.168.224.26] ([4.16.194.53]) by ns.mediline.co.in
    with Microsoft SMTPSVC(5.0.2195.1600); Wed, 27 Jun 2001 09:28:59 +0530
Received: from mail3.emailisfun.com by gw02 with ESMTP; Tue,
    26 Jun 2001 23:01:39 -0400
Message-Id: <00007409198a$00006e26$00006c63@mail3.emailisfun.com>
To: <mikeedo@emailisfun.com>
From: mikeedo@emailisfun.com
Subject: You Won The First Round! claim#	9462               27747
Date: Tue, 26 Jun 2001 23:01:34 -0400
MIME-Version: 1.0
Content-Transfer-Encoding: quoted-printable",ml_spam/test_preprocess.py,pomalley/ml_spam,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - Cparam:           SVM parameter C (cost of constraints violation)
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    svm = sklearn.svm.SVC(C = Cparam, kernel = 'linear',  probability = True)        
    svm.fit(X,Y)

    return svm


def trainRandomForest(features, n_estimators):
    '''
    Train a multi-class decision tree classifier.
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training",audioTrainTest.py,muthu1993/InteliEQ,1
"    def predict(self, data):
        return self.classifier.predict(data)

    def getName(self):
        return self.__class__.__name__


class AdaBoost:
    # def __init__(self):
    #     self.classifier = ensemble.AdaBoostClassifier(
    #         base_estimator=svm.SVC(probability=True,kernel='linear', decision_function_shape='ovr')
    #     )

    def __init__(self):
        self.classifier = AdaBoostClassifier(
            DecisionTreeClassifier(max_depth=7),
            algorithm=""SAMME.R"",
            n_estimators=100
        )
",please/models.py,hinshun/smile,1
"    dst = nmf_fit.coef()
    return np.concatenate( (src, dst.T), axis=1 )
    
    
    
def evaluation_with_classification():
    F = LINEfeatures()
    M = NMFfeatures()
    
    clf1 = tree.DecisionTreeClassifier()
    clf2 = svm.SVC()
    
    l1 = pickle.load(open(""../miscs/poi-label""))
    l2 = pickle.load(open(""../miscs/demo-label""))
    c = pickle.load(open(""../miscs/crime-label""))
    lehd = pickle.load(open(""../miscs/lehd-label""))
    l = dict(l1, **l2)
    l[""Crime""] = c
    l[""LEHD""] = lehd
    for k in l:",python/binaryClassification_CA.py,thekingofkings/embedding,1
"    y.append(0)
    
## covert list into numpy array
X = np.array(X)
y = np.array(y)
print X.shape
print y.shape

from sklearn import svm
print 'start learning SVM.'
lin_clf = svm.LinearSVC()
lin_clf.fit(X, y)  
print 'finish learning SVM.'",01_pedestrian_detector/notebooks/train_after.py,payashim/python_visual_recognition_tutorials,1
"    Params:
      x - input features
      y - 0-1 label matrix
  
    Returns:
      nothing, but model is fitted.
    '''
    self.models = []
    for k in range(y.shape[1]):
      if (y[:, k]).any():
        model = LinearSVC(C = self.c)
        model.fit(x, y[:, k])
      else:
        model = NullModel(self.null_dv)
      self.models.append(model)
    
  def predict(self, x):
    '''
    Prediction method predicts class membership of instances with decision 
    values above threshold t1 or within t2 of the highest decision value ",models.py,davidthaler/Greek_media,1
"#feature2 = feature2[labels != 2]
#feature1_norm = feature1_norm[labels != 2]
#feature2_norm = feature2_norm[labels != 2]
labels = labels[labels != 2]

#
# Grid search for parameters
sfk = cv.StratifiedShuffleSplit(labels, 40)
params = {'C':[1, 100, 10000, 100000]}
gs = GridSearchCV(
        SVC(), params, scoring='f1',
        n_jobs=8, cv=sfk)
gs.fit(feature, labels)
print gs.best_score_, gs.best_params_

#C_lsvc = 10
#
#C_logi_1 = 100
#C_logi_2 = 10000
#",learning/test_shellshell.py,fcchou/CS229-project,1
"Y = array[:,4]
validation_size = 0.20
seed = 12345
X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)

# training
models = []
models.append(('LogisticRegression', LogisticRegression(),dict(C=np.logspace(-6, -1, 10))))
models.append(('KNeighborsClassifier', KNeighborsClassifier(),dict(n_neighbors=np.arange(2,10))))
models.append(('DecisionTreeClassifier', DecisionTreeClassifier(), dict(criterion=['gini','entropy'],max_features=['sqrt','log2', None])))
models.append(('SVC', SVC(), dict(C=np.logspace(-6, -1, 10), kernel=['linear', 'poly', 'rbf', 'sigmoid'])))

for name, model, params in models:
    ### GRID SEARCH
    # Computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score
    # By default uses 3 fold cross validation
    print(""{}, {}"".format(name,params))
    clf = GridSearchCV(estimator=model, param_grid=params,n_jobs=-1)
    clf.fit(X_train, Y_train)
    print(""{}, Train score:{}, Val score:{}"".format(name,clf.best_score_, clf.score(X_validation, Y_validation)))",magicloop/_original_pipeline.py,eduardomtz/magicloop,1
"
'Python To Try Support Vector Machine(classification & regression) through scikit-learn!'

def main():
	# bi_classification_test1()
	multi_classification_test1()

def bi_classification_test1():
	X = [[0, 0], [1, 1]]
	y = [0, 1]
	clf = svm.SVC()
	fit_result = clf.fit(X, y)
	print(fit_result)
	result = clf.predict([[2., 2.]])
	print(result)
	print(clf.support_vectors_)	# get support vectors
	print(clf.support_)			# get indices of support vectors
	print(clf.n_support_)		# get number of support vectors for each class

def multi_classification_test1():",learn-python2.7/scikit-learn/SupportVectorMachineDemo.py,JoshuaMichaelKing/MyLearning,1
"        log_to_info('Training details')
        log_to_info('Classifier parameters: {}'.format(clf.get_params()))
        log_to_info('On training: {}'.format(clf.score(x_train, y_train) * 100.0))
        log_to_info('Predicting test value')
        y_test = clf.predict(x_test)
        log_to_info('Done!')
        return y_test


        # clf = LinearSVC(dual=False, verbose=True, C=mp.classifier_c, penalty=mp.classifier_penalty)  # C=0.00101
        # clf = SVC(gamma=2, C=1, verbose=True)
        # clf = LinearSVC(dual=False, verbose=True, C=1.0)
        # clf = LinearSVC(dual=False, verbose=True, C=1000.0)
        # clf = LinearSVC(dual=False, verbose=True, C=0.00101)
        # 88.2% clf = LinearSVC(dual=False, verbose=True, C=0.0195, penalty='l1') #C=0.00101
        # clf = LinearSVC(dual=False, verbose=True, C=0.0195, penalty='l1')  # C=0.00101
        # clf = LinearSVC(dual=False, verbose=True, C=0.0195, penalty='l1')
        # clf = LinearSVC(dual=True, verbose=True, C=0.005, loss='hinge')
        # clf = LinearSVC(dual=False, verbose=True, C=0.0005)
        # clf = LogisticRegression(penalty='l2', dual=True, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None,",code/classifiers/logistic_classifier.py,lukaselmer/hierarchical-paragraph-vectors,1
"            ######################################################
            # Use different algorithms to build models
            ######################################################

            # Add each algorithm and its name to the model array
            models = []
            models.append(('NB', GaussianNB()))
            models.append(('CART', DecisionTreeClassifier()))
            models.append(('RF', RandomForestClassifier()))
            models.append(('KNN', KNeighborsClassifier()))
            models.append(('SVM', SVC()))
            print(models)
            # Evaluate each model, add results to a results array,
            # Print the accuracy results (remember these are averages and std
            results = []
            names = []
            print(models[1:10])
            for name, model in models:
                kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
                cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)",scripts/CaryLou_projectpart2_chidata.py,aburkard/501proj,1
"def svm_optmize(data):
	
	# Calculates results for execution with non-optimized parameters.
	best_scores = list( )
	best_params = list( )
	for i in range( 0, TIMES ):
		# Cross-validation iterator.
		folder = cross_validation.KFold( n = len( data ), k = K_FOLDS, indices = True, shuffle = True, random_state = i )
												
		# Uses cross-validation to search for the best parameter set for the classifier.
		clf = grid_search.GridSearchCV( svm.SVC( ), parameters, cv = folder, n_jobs = -1 )
		clf.fit( data, target )
		best_scores.append( clf.best_score_ )
		best_params.append( clf.best_params_ )

		# Prints the results.
	best_scores = numpy.array( best_scores )
	print '\n\tOptimized classifier precision:'
	print '\t\t' + str( best_scores.mean( ) ) + ' +- ' + str( best_scores.std( ) )
",svm_benchmark.py,alessandro-sena/slearning-stackoverflow,1
"    print clf_boost, clf_boost.get_params()
    

    if(fe==1): #L1 norm based feature elimination
        clf_fe = LogisticRegression(C=1000,penalty='l1',random_state=0)
        clf_fe.fit(X_train, y_train)
        X_train = X_train[:,clf_fe.coef_.ravel()!=0]
        print ""Xtrain.shape: "", X_train.shape
        X_val = X_val[:,clf_fe.coef_.ravel()!=0]

        clf2_l = svm.SVC(kernel='linear', C=reg)
        clf2_l.fit(X_train, y_train)
        print ""Lasso Validation set score filtered coeff linear: "" , clf2_l.score(X_val, y_val)
        clf2 = svm.SVC(kernel='rbf', C=reg, gamma=g)
        clf2.fit(X_train, y_train)
        print ""Lasso Validation set score filtered coeff: "" , clf2.score(X_val, y_val)
    elif(fe==4): #Univariate feature selection F-score based
        selector = SelectPercentile(f_classif, percentile=75)
        selector.fit(X_train, y_train)
        scores = -np.log10(selector.pvalues_)",python_scripts_from_net/forest.py,sankar-mukherjee/DecMeg2014,1
"X = csvio.load_csv_data(""../data/train_inputs.csv"")
X = np.array(X)
y = csvio.load_csv_data(""train_outputs.csv"")

pca = PCA(n_components=500)
pcaX = pca.fit_transform(X)
X_kaggle = csvio.load_csv_data(""test_inputs.csv"")
X_kaggle = np.array(X_kaggle)
pcaKaggle = pca.transform(X_kaggle)

lin_clf = svm.LinearSVC()
pred = lin_clf.predict(pcaKaggle)
csvio.write_csv_output([int(x) for x in pred], ""svm_test.csv"")",svm.py,deepanjanroy/aml3,1
"elif choise==""5"":
	choise='precomputed'
	
###Data preparation
del train_set[""PassengerId""]		
titanic_results = train_set[""Survived""]
del train_set[""Survived""]
###End data preparation

##Selftest
machine=svm.SVC(kernel=choise)
machine.fit(train_set.values, titanic_results.values)
predicted_survival=machine.predict(train_set.values)

predictionSuccess=(1-np.mean(predicted_survival != titanic_results.values))*100
print(""Test against training set(self test): ""+str(predictionSuccess)+""% correctness"")
###End selftest


###Predict survival of test.csv",Cloudera/Code/SVM/SVM_prediction.py,cybercomgroup/Big_Data,1
"
from sklearn import svm
from faker import Factory
from bokeh.plotting import figure, show, output_file, ColumnDataSource
from bokeh.models import HoverTool, BoxSelectTool
from bokeh.embed import components
from bokeh.models.tools import BoxZoomTool, WheelZoomTool, ResetTool

# create an instance of SVM to fit data
C=1.0 # reg term
SVM = svm.SVC(kernel='linear',C=C)

##
# Function to generate pseudo-data
##
def generate_data():
    # parameters
    N = 200
    dim = 2
    N_users = N*2",python/svm_main.py,otoja/fyd,1
"        '''
        Defines all relevant parameters and classes for classfier objects.
        Edit these if you wish to change parameters.
        '''
        # These are the classifiers; add new classifiers here
        self.clfs = {
            'RF': RandomForestClassifier(n_estimators = 50, n_jobs = -1),
            'ET': ExtraTreesClassifier(n_estimators = 10, n_jobs = -1, criterion = 'entropy'),
            'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth = [1, 5, 10, 15]), algorithm = ""SAMME"", n_estimators = 200),
            'LR': LogisticRegression(penalty = 'l1', C = 1e5),
            'SVM': svm.SVC(kernel = 'linear', probability = True, random_state = 0),
            'GB': GradientBoostingClassifier(learning_rate = 0.05, subsample = 0.5, max_depth = 6, n_estimators = 10),
            'NB': GaussianNB(),
            'DT': DecisionTreeClassifier(),
            'SGD': SGDClassifier(loss = 'log', penalty = 'l2'),
            'KNN': KNeighborsClassifier(n_neighbors = 3),
            'NN': MLPClassifier()
            }
        # These are the parameters which will be run through
        self.params = {",pipeline/model_loop.py,aldengolab/acg-ml-pipeline,1
"    # Code source: Gael Varoqueux
    #              Andreas Mueller
    # Modified for Documentation merge by Jaques Grobler
    # Modified to serve as a MinCq example by Jean-Francis Roy
    # License: BSD 3 clause

    h = .02  # step size in the mesh

    names = [""Linear SVM"", ""RBF SVM"", ""AdaBoost"", ""Linear MinCq"", ""RBF MinCq"", ""Stumps MinCq""]
    classifiers = [
        SVC(kernel=""linear"", C=0.025),
        SVC(gamma=2, C=1),
        AdaBoostClassifier(),
        MinCqLearner(mu=0.01, voters_type=""kernel"", kernel=""linear""),
        MinCqLearner(mu=0.01, voters_type=""kernel"", kernel=""rbf"", gamma=2),
        MinCqLearner(mu=0.01, voters_type=""stumps""),
    ]

    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                               random_state=1, n_clusters_per_class=1)",mincq/example.py,GRAAL-Research/MinCq,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
    print(X_train)
    print(y_train)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_estimator_)
    print()
    print(""Grid scores on development set:"")",Kaggle/Titanic/gridexample.py,xueweuchen/pythonDemo,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_09_2014_server.py,magic2du/contact_matrix,1
"class EvalSVM(object):
    """"""docstring for EvalSVM""""""
    b_preict = []
    def __init__(self, gamma, c):
        super(EvalSVM, self).__init__()
        self.gamma_value = gamma
        self.c_value = c
  

    def init_classifier(self):
        clf = svm.SVC(kernel = 'rbf', gamma=self.gamma_value, C=self.c_value)
        # print ""SVM configuration... \n\n"", clf
        # clf = MultinomialNB()
        return clf



    def fit_train_data(self, clf, a_train, b_train):
        # clf = svm.SVC(kernel = 'rbf', gamma=gamma_value, C=c_value)
        # print ""SVM configuration... \n\n"", clf",implementation/evaluation/svm.py,imink/UCL_COMPIG15_Project,1
"  # weight = float(np.sum(Y == 0)) / len(Y)
  # weights = {0 : weight, 1 : 1 - weight}
  if load == False:
    if H['model'] == 'logistic':
      # model = LogisticRegression(penalty = H['penalty'], C = H['lambda'],
      #   class_weight = weights, tol  = H['tol'], n_jobs = H['jobs'], 
      #   verbose = H['verbose'])
      model = SGDClassifier(loss = 'log', penalty = H['penalty'], alpha = 1.0 / H['lambda'])
        # class_weight = weights, )
    elif H['model'] == 'svm':
      # model = SVC(C = H['lambda'], kernel = H['kernel'],
      #   tol = H['tol'], class_weight = weights, verbose = H['verbose'])
      model = SGDClassifier(loss = 'hinge', penalty = H['penalty'], alpha = 1.0 / H['lambda'])
    elif H['model'] == 'gbdt':
      model = GradientBoostingClassifier(learning_rate = H['lr'], 
        max_depth = H['depth'], verbose = H['verbose'])
    print('start training...')
    if H['sgd'] == False:
      samples = get_sample(H['subset'])
      X = np.array([samples[key]['dat'] for key in samples])",classical.py,yancz1989/cancer,1
"		####################################################################
		#	Write code here for computing non-conformity score



		####################################################################


    elif params['classifier'] == 2:
        #svm
        clf = svm.SVC()
        clf.fit(data,target)

        """"""while there are n choose 2 different SVMs trained in 1vs1,
        there are only n-1 by n_SV dimensions in dual_coef.
        I'm guessing that the first dimension corresponds to the SVMs relevant to the point (second dimension) label;
        if it is 0, with 2 classes, then the first dimension corresponds to 0vs1 and 0vs2.
        this makes sense to reduce the size and sparseness of the dual_coef_ matrix """"""

        #preallocate for dual_coef values for all points",src/computeNcs.py,cprml2015/pyCP,1
"#
# Get to know your data. It seems its already well organized in
# [n_samples, n_features] form. Our dataset looks like (4389, 784).
# Also your labels are already shaped as [n_samples].
# peekData(X_train)
#
# TODO: Create an SVC classifier. Leave C=1, but set gamma to 0.001
# and set the kernel to linear. Then train the model on the training
# data / labels:
print('Training SVC Classifier...')
model = SVC(kernel='linear', C=1, gamma=0.001)
model.fit(X_train, y_train)

# TODO: Calculate the score of your SVC against the testing data
print('Scoring SVC Classifier...')
score = model.score(X_test, y_test)
print('Score: %f\n' % score)

# Visual Confirmation of accuracy
# drawPredictions(X_train, X_test, y_train, y_test)",Module6/assignment2.py,Wittlich/DAT210x-Python,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0906_2015_pre_activation_rectifier_didnotwork.py,magic2du/contact_matrix,1
"    y = mat['Y']    # label
    y = y[:, 0]
    Y = construct_label_matrix_pan(y)
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the feature weight matrix
        Weight, obj, value_gamma = ls_l21.proximal_gradient_descent(X[train], Y[train], 0.1, verbose=False)

        # sort the feature scores in an ascending order according to the feature scores
        idx = feature_ranking(Weight)
",PyFeaST/example/test_ls_l21.py,jundongl/PyFeaST,1
"    """"""
    from sklearn.base import clone
    from sklearn.utils import check_random_state
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import check_cv

    if clf is None:
        scaler = StandardScaler()
        svc = SVC(C=1, kernel='linear')
        clf = Pipeline([('scaler', scaler), ('svc', svc)])

    info = epochs_list[0].info
    data_picks = pick_types(info, meg=True, eeg=True, exclude='bads')

    # Make arrays X and y such that :
    # X is 3d with X.shape[0] is the total number of epochs to classify
    # y is filled with integers coding for the class to predict
    # We must have X.shape[0] equal to y.shape[0]",mne/decoding/time_gen.py,christianbrodbeck/mne-python,1
"                trainLabel = np.concatenate((trainLabel,labels),axis=0)

        return trainData,trainLabel
        
    def svmClassifier(self):
        trainData,trainLabel = self.loadExperimentData()

        print ""total available data"",len(trainData)
        data_train,data_test,label_train,label_test = cross_validation.train_test_split(trainData,trainLabel,test_size=self.test_size)
        
        clf = SVC(C=1.9,gamma=0.001)
        clf = clf.fit(data_train,label_train)
        print ""prediction Accuracy"",clf.score(data_test,label_test)
        print ""Number of support vectors used:"",len(clf.support_vectors_)

        #Use the cross_validation score
        clf2 = SVC(C=1.9,gamma=0.001)
        cv = cross_validation.ShuffleSplit(len(trainData), n_iterations=10,test_size=self.test_size, random_state=0)
        scores = cross_validation.cross_val_score(clf2, trainData, trainLabel, cv=cv)
        print scores",classification_real_data_svm.py,haramoz/RND-ss14,1
"    from sklearn.grid_search import GridSearchCV
else:
    from sklearn.model_selection import train_test_split, GridSearchCV

R_TOL = 1e-2


def test_imblearn_classification_scorers():
    X, y = make_blobs(random_state=0, centers=2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    clf = LinearSVC(random_state=0)
    clf.fit(X_train, y_train)

    # sensitivity scorer
    scorer = make_scorer(sensitivity_score, pos_label=None, average='macro')
    grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=scorer)
    grid.fit(X_train, y_train).predict(X_test)
    assert_allclose(grid.best_score_, 0.92, rtol=R_TOL)

    scorer = make_scorer(sensitivity_score, pos_label=None, average='weighted')",imblearn/metrics/tests/test_score_objects.py,scikit-learn-contrib/imbalanced-learn,1
"droplist = 'Survived PassengerId Cabin_Known'.split()
data = training_data.drop(droplist, axis=1)
# Define features and target values
X, y = data, training_data['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
#
# # Set the parameters by cross-validation
# param_dist = {'C': scipy.stats.uniform(0.1, 1000), 'gamma': scipy.stats.uniform(.001, 1.0),
#   'kernel': ['rbf'], 'class_weight':['balanced', None]}
#
# clf = SVC()
#
# # run randomized search
# n_iter_search = 10000
# random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
#                                    n_iter=n_iter_search, n_jobs=-1, cv=4)
#
# start = time()
# random_search.fit(X, y)
# print(""RandomizedSearchCV took %.2f seconds for %d candidates""",titanic_MICE_RS_SVM.py,michael-hoffman/titanic,1
"                X_valid,y_valid = valid_set
                X_test,y_test = test_set
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_05202015_2.py,magic2du/contact_matrix,1
"        print >>stream, '\n**** Cross-validation scores\n'

        for fold in range(n_folds_test):
            if verbose:
                print '  FOLD:', fold
            X_train, X_test, y_train, y_test = train_test_split(X[monkey],
                                                                y[monkey],
                                                                test_size=0.1)
            if verbose:
                print 'training classifier...'
            clf = GridSearchCV(SVC(),
                               param_grid,
                               cv=n_folds_gridsearch,
                               scoring='f1',
                               verbose=1 if verbose else 0, n_jobs=-1)
            clf.fit(X_train, y_train)

            print >>stream, 'FOLD:', fold, clf.best_score_
            print >>stream, pformat(clf.best_params_)
",mielke_replication.py,bootphon/monkey_business,1
"import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# we create 40 separable points
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-4.5, 4)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",scripts/plot_separating_hyperplane.py,JohnAshburner/ISBI,1
"from sklearn.ensemble import ExtraTreesClassifier

columns = [""FareBin"", ""Gender"", ""Port"", ""AgeBin"", ""FamilySize"", ""Title"", ""Pclass"", ""CabinSide""]

labels = train_data[""Survived""].values
features = train_data[columns].values

test_features = test_data[columns].values

# clf = RandomForestClassifier(n_estimators = 128, class_weight='auto')
# clf = LinearSVC(C=1.0)
# clf = tree.DecisionTreeClassifier()
clf = ExtraTreesClassifier(n_estimators=200, max_depth=10, min_samples_split=1, random_state=0)

clf_score = cross_val_score(clf, features, labels, n_jobs=1).mean()
print(""{0} -> CLF: {1})"".format(x, clf_score))

imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit(features)
",titanic/other/v2.py,rspective/kaggle,1
"          (.2, -2.3),
          (0, -2.7),
          (1.3, 2.1)].T
Y = [0] * 8 + [1] * 8

# figure number
fignum = 1

# fit the model
for kernel in ('linear', 'poly', 'rbf', 'sigmoid'):
    #clf = SVC(kernel=kernel) 
    clf = SVC(kernel=kernel, gamma=2)
    clf.fit(X, Y)

    # plot the line, the points, and the nearest vectors to the plane
    plt.figure(fignum, figsize=(8, 6))
    plt.clf()

    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
                facecolors='none', zorder=10, s=300)",support-vector-machines-101/kernel-examples.py,murali-munna/Data-Science-45min-Intros,1
"import copy
import numpy as np
import pylab as pl

features_train, labels_train, features_test, labels_test = makeTerrainData()

########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC

clf = SVC(kernel=""linear"", gamma=1.0)

#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data
clf.fit(features_train, labels_train)

#### store your predictions in a list named pred
pred = clf.predict(features_test)
",learning/algorithms/svm/svm.py,dmytroKarataiev/MachineLearning,1
"	clf = DummyClassifier(strategy=""uniform"")
	clf.fit(X, y)
	return clf

def decision_trees(X, y, max_depth=25):
	clf = DecisionTreeClassifier(max_depth=max_depth)
	clf.fit(X, y)
	return clf

def linear_svm(X, y):
	clf = OneVsRestClassifier(LinearSVC(random_state=0))
	clf.fit(X, y)
	return clf

def gaussian_naive_bayes(X, y):
	clf = OneVsRestClassifier(GaussianNB())
	clf.fit(X, y)
	return clf

def classify(X, y, algorithm):",classification.py,daleloogn/utils,1
"
order = np.random.permutation(60000)
train_set = [mnist.data[order[:5000],:], mnist.target[order[:5000]]]
valid_set = [mnist.data[50000:60000,:], mnist.target[50000:60000]]

for i in range(20):
    # Get suggested new experiment
    job = scientist.suggest()

    # Perform experiment
    learner = svm.LinearSVC(**job)
    learner.fit(*train_set)
    accuracy = learner.score(*valid_set)

    # Inform scientist about the outcome
    scientist.update(job,accuracy)
    scientist.report()
",examples/mnist_linear_svc.py,schevalier/Whetlab-Python-Client,1
"        
        
        # forcando as classes serem 1 ou -1.
        for i in range(len(y)):
            if y[i] == c1:
                y[i] = -1
            else: y[i] = 1
            
        vec = y.copy()

        clf = svm.LinearSVC(loss='hinge')
        clf.fit(X, y)
        l1 = 0
        regression = 0
        
        # verificar essa parte de classes internas do SVM
        
        def f(x, y):
            return pow(x - y, 2)
",scripts/stats.py,ricoms/gpam_stats,1
"    def rfPredict(self):
        ## create df for cds
        self.cds = pd.read_table(self.cdsFn + "".txt"",  header=0)
        cdsX = np.array(pd.get_dummies(self.cds[self.colNames]) )
        ## selected a subset of features and predict a-site
        sltcdsX = self.selector.transform(cdsX)
        self.cds[""asite""] = self.reducedClf.predict(sltcdsX)

    def svmFit(self):
        ## grid search
        self.clf = svm.SVC()
        paramGrid = [{'C': [ 0.01, 0.1, 1, 10, 100, 1000, 10000]}]
        self.clfGs = GridSearchCV(estimator=self.clf, param_grid=paramGrid, n_jobs=-1)
        self.clfGs.fit(self.X, self.y)
        print(""[result]\t best estimator parameters: c="", self.clfGs.best_estimator_.C, flush=True)
        ## model fitting and cross validation
        self.clf = svm.SVC(C=self.clfGs.best_estimator_.C)
        scores = cross_val_score(self.clf, self.X, self.y, cv=10)
        print(""[result]\tAccuracy: %0.3f (+/- %0.3f)"" % (scores.mean(), scores.std() * 2), flush=True)
",scripts/asite_predict.py,hanfang/scikit-ribo,1
"
from sklearn.svm import LinearSVC

from .backend import Backend


class GenericSVMClassifier(Backend):

    @staticmethod
    def model():
        return LinearSVC()",fickle/predictors.py,norbert/fickle,1
"
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
dt_9_1 = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)
dt_20_1 = DecisionTreeClassifier(max_depth=20, min_samples_leaf=1)

clf_default = AdaBoostClassifier(n_estimators=100)
clf_stump = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=100)
clf_dt = AdaBoostClassifier(base_estimator=dt_9_1, n_estimators=100)
# clf_dt = AdaBoostClassifier(base_estimator=dt_20_1, n_estimators=100)

clf_svc_linear = AdaBoostClassifier(SVC(probability=True, kernel='linear'))
clf_svc_rbf = AdaBoostClassifier(SVC(probability=True, kernel='rbf'))
clf_svc_poly = AdaBoostClassifier(SVC(probability=True, kernel='poly'))

clf_sgd_hinge = AdaBoostClassifier(SGDClassifier(loss='hinge'), algorithm='SAMME')
clf_sgd_logistic = AdaBoostClassifier(SGDClassifier(loss='log'))
clf_sgd_modified_huber = AdaBoostClassifier(SGDClassifier(loss='modified_huber'))

clf_ridge = AdaBoostClassifier(RidgeClassifier(), algorithm='SAMME')
",finance/EnsembleTest.py,Ernestyj/PyStudy,1
"

Xtrain=train.ingredients_string.values

ytrain=train.cuisine.values

vec1=TfidfVectorizer(stop_words='english')

tfidfTrain=vec1.fit_transform(Xtrain).todense()

svm1=svm.LinearSVC()

parameters = {'C':[0.01,1,100]}

grid1=grid_search.GridSearchCV(svm1,parameters)


grid1.fit(tfidfTrain,ytrain)

grid1.best_estimator_",CrossValidation.py,lingcheng99/Kaggle-what-is-cooking,1
"
    fname_crossvalidate = ""/Users/kgeorge/Dropbox/cars/crossvalidate/build/hog_output.txt""
    crossvalidate_data = np.genfromtxt(fname_crossvalidate, delimiter="","")

    assert(good_data.shape[1] == bad_data.shape[1])
    assert(((good_data.shape[0] + bad_data.shape[0]) % 105) == 0)
    X = np.zeros(((good_data.shape[0] + bad_data.shape[0])/105, (good_data.shape[1]-2)*105))
    y = np.zeros(((good_data.shape[0] + bad_data.shape[0])/105))
    putDataInX(X, y, 0, good_data, 1)
    putDataInX(X, y, good_data.shape[0]/105, bad_data, -1)
    clf = skl.SVC(kernel=""linear"", verbose=True, probability=True)
    fitResult = clf.fit(X, y)
    validateData(clf, crossvalidate_data)
    return clf
    pass

def hog(img, hogScheme, aHistTemplate, smAngle, smMag, descriptor):
    #print img.width, img.height
    imgCopy = cv2.resize(img, (128, 64))
    numpyImg = np.asarray(imgCopy)",samples/hog/run.py,kgeorge/kgeorge-cv,1
"    """"""
        Esta función devuelve un diccionario con
        los clasificadores que vamos a utilizar y
        una rejilla de hiperparámetros
    """"""
    clfs = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) 
            }

    grid = { 
    'RF':{'n_estimators': [1,10,100,1000,10000], 'max_depth': [1,5,10,20,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},",Tareas/tarea_3/magic_loop_pipeline.py,rsanchezavalos/compranet,1
"    d = numpy.vstack((d1,d2))
    t = numpy.empty(d.shape[0])
    t[:] = 0
    t[0:d1.shape[0]] = 1
    return d[:,:2], t

def Plot2DwSVM(datax, datay):
    """"""
    plot data and corresponding SVM results.
    """"""
    clf = svm.SVC()
    clf.fit(datax, datay)
    step = 0.02
    x_min, x_max = datax[:,0].min(), datax[:,0].max()
    y_min, y_max = datax[:,1].min(), datax[:,1].max()
    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, step), numpy.arange(y_min, y_max, step))
    Z = clf.decision_function(numpy.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    pylab.contourf(xx, yy, Z, 10, cmap=pylab.cm.Oranges)
    pylab.scatter(datax[datay == 1,0], datax[datay == 1,1], c='b', s=50)",Neutrions.py,ylqk9/NeutrinoClassification,1
"x = np.array(p)

idx = set_group_indices(dataset['dx_group'])
idx_ = np.hstack((idx['EMCI'], idx['LMCI']))
img_idx = np.hstack((idx['AD'], idx_))

x = x[img_idx, :]
y = np.ones(x.shape[0])
y[len(y) - len(idx_):] = 0

svc = LinearSVC(penalty='l1', dual=False)
sss = StratifiedShuffleSplit(y, n_iter=100, test_size=.25, random_state=42)
s = Parallel(n_jobs=20, verbose=5)(delayed(train_and_test)\
                                          (svc, x, y, train, test)\
                                          for train, test in sss):


",classification_connectivity.py,mrahim/adni_petmr_analysis,1
"
xtrain = pd.read_pickle(open('X_train.pickle', 'rb'))
ytrain = pd.read_pickle(open('y_train.pickle', 'rb'))

xtrain = xtrain.iloc[:,-100:]
xtrain = xtrain.values



def main():
	model = OneVsRestClassifier(LinearSVC(random_state=1), n_jobs=-1)
	model.fit(xtrain, ytrain)

	filename = 'OvRmodel.sav'
	pickle.dump(model, open(filename, 'wb'))

if __name__ == ""__main__"":",models/OvR/ovrmodel.py,jpzhangvincent/InstacartKaggleCompetition,1
"    ----------

    key: str

    eval_args: boolean
      If true ""un-string"" the parameters, and return (name, argument) tuples:
      [(item_name1, [[argname, value], ...]), ...]

    Examples
    --------
    >>> key_split(key='SelectKBest(k=1)/SVC(kernel=linear,C=1)')
    ['SelectKBest(k=1)', 'SVC(kernel=linear,C=1)']
    >>> key_split(key='SelectKBest(k=1)/SVC(kernel=linear,C=1)', eval=True)
    [[('name', 'SelectKBest'), ('k', 1)], [('name', 'SVC'), ('kernel', 'linear'), ('C', 1)]]
    """"""
    signatures = [signature for signature in key.split(conf.SEP)]
    if eval:
        return [signature_eval(signature) for signature in signatures]
    else:
        return signatures",epac/workflow/base.py,neurospin/pylearn-epac,1
"
def test_pipeline_grid_search6():
    # Test that the number of estimator calls is less than the ones for regular GridSearchCV
    parts = [
        create_mock_estimator(""f0"",[]),
        create_mock_estimator(""f1"", [(""p1"",0),(""p2"",2)]),
        create_mock_estimator(""f2"",[]),
        create_mock_estimator(""f3"",[(""c"",0),(""d"",0)]),
        create_mock_estimator(""f4"",[]),
        create_mock_estimator(""f5"",[]),
        SVC() 
        ]

    cv_params = [
        ('f1__p1', [10,20]),
        ('f3__c', [10,20,30]),
        ('f3__d', [10,20,30,40]),
        ('SVC__C', [1.,10.,100.,1000.]),
        ('SVC__kernel', ['linear']),
    ]",tests/test_pipeline_grid_search.py,tkerola/pipeline_grid_search,1
"    #d1,d2 = mldata.enfeatures[0], mldata.enfeatures[1]
    #h1 = mldata.features[d1]['meshd']  # step size in the mesh
    #h2 = mldata.features[d2]['meshd']
    h1 = 1
    h2 = 1

    #names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
    #         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
    classifiers = {
        ""nearest_neighbors"": KNeighborsClassifier(5),
        ""linear_svm"": SVC(kernel=""linear"", C=0.025),
        ""poly_svm"": SVC(kernel=""poly"", degree=2),#, gamma=1, C=1),
        ""decision_tree"": DecisionTreeClassifier(max_depth=None),
        ""random_forest"": RandomForestClassifier(max_depth=None, n_estimators=5, max_features=5, criterion=""gini""),
        ""adaboost"": AdaBoostClassifier(),
        ""naive_bayes"": GaussianNB(),
        ""lda"": LDA(),
        ""qda"": QDA()}

    """"""",svm.py,malimome/game-auth,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= MODATTR_OBSESSIVE_HANDLER_ENABLED",shinken/external_command.py,h4wkmoon/shinken,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the gini_index score of each feature
        score = gini_index.gini_index(X[train], y[train])

        # rank features in descending order according to score
        idx = gini_index.feature_ranking(score)
",skfeast/example/test_gini_index.py,jundongl/scikit-feast,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",3rdparty/python/Lib/test/test_email/test_email.py,eihero/Sigil,1
"            continue
    else:
        if src.endswith('-win32.cc'):
            continue

    sources.append(src)

if platform.isWindows():
    sources.append('src/getopt.c')

if platform.isMSVC():
    cl = 'cl'
    vcdir = os.environ.get('VCINSTALLDIR')
    if vcdir:
        if options.x64:
            cl = os.path.join(vcdir, 'bin', 'x86_amd64', 'cl.exe')
            if not os.path.exists(cl):
                cl = os.path.join(vcdir, 'bin', 'amd64', 'cl.exe')
        else:
            cl = os.path.join(vcdir, 'bin', 'cl.exe')",bootstrap.py,TheOneRing/ninja,1
"    def run_svm(self, filename='svm.pkl'):
        if not self.load_svm:
            if self.use_sentiment:
                self.pcscores.index = range(len(self.pcscores))
                data = pd.concat([self.pcscores, self.sentiment], axis=1)
            else:
                data = self.pcscores
            df_train, df_test, train_label, test_label = train_test_split(data, self.labels,
                                                                          test_size=0.2, random_state=42)
            parameters = {'kernel': ['rbf'], 'C': [0.01, 0.1, 1, 10, 100], 'gamma': [1e-3, 1e-2, 1e-1, 1e0, 1e1]}
            svr = svm.SVC()
            clf = grid_search.GridSearchCV(svr, parameters, cv=5, error_score=0, verbose=1)
            clf.fit(df_train, train_label)
            print('Best parameters: {}'.format(clf.best_params_))
            prediction = clf.predict(df_test)
            self.svc = clf
            return prediction, test_label
        else:
            print('Loading model/{}'.format(filename))
            clf = joblib.load('model/'+filename)",analysis.py,dr-rodriguez/The-Divided-States-of-America,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
         verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS",venv/lib/python3.4/site-packages/sklearn/metrics/classification.py,valexandersaulys/airbnb_kaggle_contest,1
"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
        random_state=0)

#tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100,
#    1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.001, 0.00055, 0.0001], 'C':
    [10, 50, 100]}]

for score in ['precision', 'recall']:
    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='%s_weighted' %
            score)
    clf.fit(X_train, y_train)

    print score, clf.best_params_",find_params.py,ssaamm/sign-language-translator,1
"train_data = normalize(train_data, norm='l1', axis=0)
del f
f = sio.loadmat(os.path.join(stackdir, 'stack_test.mat'))
f.keys() # Muestra las claves del diccionario file. 
test_data = f['data_test'].astype(np.float32)
del f
n_samples=train_data.shape[0]
data_dim=train_data.shape[1]

# Feature selection
lsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(train_data, trainlabels)
model = SelectFromModel(lsvc, prefit=True)
train_data = model.transform(train_data)

DIM = 429
DIM_LAT= 10
ARCHITECTURE = [DIM, # Input dimension
                500, 500, 100, # intermediate encoding
                DIM_LAT] # latent space dims
                # 50]",challenge/challenge3.py,juanka1331/VAN-applied-to-Nifti-images,1
"Xtrain = Xtemp[0:cutpoint]
Ytrain = Ytemp[0:cutpoint]

# cross validation set
Xcv = Xtemp[cutpoint:]
Ycv = Ytemp[cutpoint:]
print('done.')

# train SVM from sklearn
print('Train SVM...')
clf = svm.SVC()
clf.fit(Xtrain, Ytrain)
print('done.')

# check accuracy - DEPRECATED use built in function instead
print('Training set: ' + str(100 * np.sum(clf.predict(Xtrain) == Ytrain)/Xtrain.shape[0]) + '% accuracy')
print('Cross validation set: ' + str(100 * np.sum(clf.predict(Xcv) == Ycv)/Xcv.shape[0]) + '% accuracy')",simple_sentiment_analysis.py,libeanim/simple-sentiment-analysis,1
"        n1, n2 = nodes
        features[i,:] = feature_map_function(g, n1, n2)
        labels[i] = loss_function(g, n1, n2, gt)
        labeled_image.ravel()[list(g[n1][n2]['boundary'])] = 2+labels[i]
        g.merge_nodes(n1,n2)
    return features, labels, labeled_image

def select_classifier(cname, features=None, labels=None, **kwargs):
    if 'svm'.startswith(cname):
        del kwargs['class_weight']
        c = SVC(probability=True, **kwargs)
    elif 'logistic-regression'.startswith(cname):
        c = LogisticRegression()
    elif 'linear-regression'.startswith(cname):
        c = LinearRegression()
    elif 'random-forest'.startswith(cname):
        try:
            c = RandomForest()
        except NameError:
            logging.warning(' Tried to use random forest, but not available.'+",ray/classify.py,stuarteberg/ray,1
"                'data': traffic_tweets + non_traffic_tweets,
                'target': [True] * len(traffic_tweets) + [False] * len(non_traffic_tweets),
            }

            training_vectors = count_vect.fit_transform(tweets['data'])

            # print(training_vectors.shape)
            # print(len(tweets['target']))

            # start_time = time.clock()
            clf = LinearSVC(max_iter=10000).fit(training_vectors, tweets['target'])
            # training_time = round(time.clock() - start_time, 2) / 100
            # print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)

            ## Open Test set
            with open(join(dirname(__file__), 'tweets_corpus/test_set_10000.csv'), newline='\n') as csv_input:
                dataset = csv.reader(csv_input, delimiter=',', quotechar='""')
                dataset = [(line[0], line[1]) for line in dataset]
                shuffle(dataset)
                test = {",svm.py,dwiajik/twit-macet-mining-v3,1
"
    if kernel not in ['poly', 'sigmoid'] and coef0 != 0.0:
        continue

    features = input_data.drop('class', axis=1).values.astype(float)
    labels = input_data['class'].values

    try:
        # Create the pipeline for the model
        clf = make_pipeline(StandardScaler(),
                            SVC(C=C,
                                gamma=gamma,
                                kernel=kernel,
                                degree=degree,
                                coef0=coef0,
                                random_state=324089))
        # 10-fold CV score for the pipeline
        cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
        accuracy = accuracy_score(labels, cv_predictions)
        macro_f1 = f1_score(labels, cv_predictions, average='macro')",model_code/SVC.py,rhiever/sklearn-benchmarks,1
"from sklearn import svm

    
def init(data, testpart):
    print 'Module Loaded'
    dataMat = data['Data']
    labelMat = data['Labels']
    train_dat, test_dat, train_lab, test_lab = cross_validation.train_test_split(dataMat, labelMat, test_size=testpart, random_state=0)

    print 'Training SVM'
    clf = svm.SVC(kernel='linear', C=1).fit(train_dat, train_lab)
    print 'Training Complete'
    print '-----------------'
    print 'Testing SVM' 
    score = clf.score(test_dat, test_lab)",Modules/Classification/Support Vector Machine/Module.py,blakebjorn/DeepLearnR,1
"from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as pl
from matplotlib.colors import ListedColormap
from sklearn.svm import SVC

np.random.seed(0)
X_xor = np.random.randn(200, 2)
y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)
y_xor = np.where(y_xor, 1, -1)

svm = SVC(kernel='rbf', C=10.0, gamma=0.10, random_state=0)
svm.fit(X_xor, y_xor)

def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # plot the decision surface",final/code/sk_learn/svm_kernel.py,Mageluer/computational_physics_N2014301040052,1
"
# We learn the digits on the first half of the digits
data_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]


# Now predict the value of the digit on the second half:
data_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:]
#data_test = scaler.transform(data_test)

# Create a classifier: a support vector classifier
kernel_svm = svm.SVC(gamma=.2)
linear_svm = svm.LinearSVC()

# create pipeline from kernel approximation
# and linear svm
feature_map_fourier = RBFSampler(gamma=.2, random_state=1)
feature_map_nystroem = Nystroem(gamma=.2, random_state=1)
fourier_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_fourier),
                                        (""svm"", svm.LinearSVC())])
",examples/plot_kernel_approximation.py,loli/sklearn-ensembletrees,1
"def training_and_test(token, train_data, test_data, num_classes, result):
    """"""Train and test

    Args:
        token (:obj:`str`): token representing this run
        train_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of training feature and label
        test_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of testing feature and label
        num_classes (:obj:`int`): Number of classes
        result (:obj:`pyActLearn.performance.record.LearningResult`): LearningResult object to hold learning result
    """"""
    svm_model = sklearn.svm.SVC(kernel='rbf')
    svm_model.fit(train_data[0], train_data[1].flatten())
    # Test
    predicted_y = svm_model.predict(test_data[0])
    # Evaluate the Test and Store Result
    confusion_matrix = get_confusion_matrix(num_classes=num_classes,
                                            label=test_data[1].flatten(), predicted=predicted_y)
    result.add_record(svm_model, key=token, confusion_matrix=confusion_matrix)
    return predicted_y
",examples/CASAS_single/casas_svm.py,TinghuiWang/pyActLearn,1
"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

x = np.random.rand(1000, 2)
x[:, 0] = x[:, 0] * 10

y = [ 1 if np.sin(i) > j else 0 for [i, j] in x]
s = svm.SVC()
s.fit(x, y)

""""""
以0.1作间隔，得到平面上的网格，计算每个网格上每个点的预测值
""""""
x_grid, y_grid = np.meshgrid(np.arange(0, 10.1, 0.1), np.arange(0, 2.1, 0.1))
grid_points = np.c_[x_grid.ravel(), y_grid.ravel()]

""""""",python/sklearn/svm_sin.py,yuncliu/Learn,1
"        sys.stdout.flush()

    print ""---- Num of Repetitions:"", n
    print ""---- Average Score:"", avg_score
    np.set_printoptions(linewidth=999999)
    print confusion_matrix(y_test_overall, pred_overall)

if __name__ == '__main__':

    # three classifiers to choose from omgz
    svm = svm.NuSVC(nu=0.02)
    ridge = linear_model.RidgeClassifierCV()
    knn = neighbors.KNeighborsClassifier()
    lr = linear_model.LogisticRegression(C=10.0)

    n = 40

    if len(sys.argv) == 2:
        n = int(sys.argv[1])
",analyze.py,fabeschan/midigeneration,1
"    if_market = 0
    if_strat = 0



    
    X, y, Z = Build_Data_Set()
    print(len(X))

    
    clf = svm.SVC(kernel=""linear"", C= 1.0)
    clf.fit(X[:-test_size],y[:-test_size])

    correct_count = 0

    for x in range(1, test_size+1):
        if clf.predict(X[-x])[0] == y[-x]:
            correct_count += 1

        if clf.predict(X[-x])[0] == 1:",svm.py,DEK11/SP500,1
"
    #print samples_train
    print
    print ""Train mean: {}, std: {}"".format(np.mean(samples_train, 0), np.std(samples_train, 0))
    print
    #print samples_test
    print ""Test mean: {}, std: {}"".format(np.mean(samples_test, 0), np.std(samples_test, 0))
    print

    from sklearn import svm
    clf = svm.SVC(kernel='linear')
    run_classifier(clf, samples_train, labels_train, samples_test, labels_test)

    print ""Feature extraction time: {}"".format(feature_extraction_time)
    print ""Training time: {}"".format(training_time)
    print ""Classification time: {}"".format(classification_time)


if __name__ == ""__main__"":
    main()",tools/dsp/time_svm.py,williampierce/cc,1
"#!/usr/bin/env python3
def getInput():
    theInput = """"""(19x14)(3x2)ZTN(5x14)MBPWH(112x2)(20x15)(2x15)AX(7x4)UDNOYNU(7x7)YGJJMBB(59x11)(1x10)Q(29x4)VGDXLQYSEUBZSCXVKJLIDXGHCSQXL(3x15)QMJ(2x15)GA(1x11)N(161x5)(10x8)DNMWSUEGYZ(60x12)(36x10)RFWPBFRPFUUERWOMFVIPLIIVNIKYBEPNAEMO(11x4)DJQYLWDSUYF(28x4)KMFEZNRDVFPALMIBTUSSIKBEDDES(25x4)WHBANBCBSMYYJJYMXMEHSVHLK(8x2)DXMYJAOA(157x8)(81x8)(16x13)UDZKAIWYGRMGTFEL(2x2)MX(4x10)UWEW(18x8)XFETJLTWLMXERLKYZE(10x15)ZZINBFHXMJ(58x7)(2x13)PU(6x9)EKETLU(4x14)PYWO(11x13)QPFDYVKMYQT(6x1)FXYXHT(1x9)UQKPHVIYMXGIJU(574x14)(567x3)(318x14)(311x8)(22x8)(15x13)IFVDNTIWLQZPKFY(20x9)PIAHRLMWBKSLGRMANIZG(38x3)(3x12)FJA(3x14)XBN(1x14)T(8x1)PAYAHPVW(62x13)(2x14)RP(47x12)(13x14)PAHQVKGAOCQSI(5x6)FWNZJ(11x6)UWJGVVVQNDU(137x4)(59x10)(2x15)LU(8x12)JFULFVHX(13x15)JGHVEPFJFRELS(5x9)FMEAD(2x2)UI(1x15)H(10x5)OLOGVCUNVA(42x9)(27x2)TSPUMQILTHKOYJEBYVGMIVNPGYJ(3x13)TOS(233x15)(225x13)(51x7)(32x5)ITOPJLZJXADLZUWWZODCJZSFARRBEVAW(7x13)(2x2)MY(161x7)(11x14)(5x15)NUANI(39x8)(3x2)WFP(24x14)YUXDAFGDGETSMQFKXLNAJUXB(92x1)(15x1)VVBNYVKPYIHGXVW(7x11)SHTQWTV(24x1)MFTWXBQKXYRYVHLNSHYCUCIX(8x10)DLJPXYPL(8x15)DFFGYJOZ(2413x13)(2147x9)(8x11)RQUCQBKE(189x2)(7x1)OFSVWLH(150x2)(142x12)(15x9)(9x14)LWQHHDQAO(59x14)(14x7)MXFGIEONQVAOWQ(3x9)NRO(8x3)KVVUCTPS(12x8)UTUHAKZBEWEN(5x5)XIZCQ(4x15)LVRW(29x2)(2x3)QF(1x11)E(1x14)Z(3x6)OGN(7x5)RQVWCKS(2x15)MY(725x14)(404x1)(225x5)(50x6)(12x5)TXWSSNMDRHRQ(8x5)BKJVIYJC(2x6)LT(6x13)AUEBNS(42x2)(3x14)ZVJ(12x1)IVOHWTXJGDJI(4x8)AVTY(1x3)U(50x1)(6x7)WAFFVQ(7x7)TVXBEKD(8x10)QZTUCAVH(8x3)UQQQWQRT(28x7)(12x12)AUEULVXQYLII(4x1)UNGS(25x2)VHMTBELOETJHHIMOCSSQFODHM(67x11)(37x9)(5x7)OOSOJ(21x8)IHVJDYDZMEYFJQZQYXXYU(7x8)FZRHJQZ(7x8)(2x7)MA(14x6)QHWLXSRHFUFUTP(71x14)(11x1)TUVSALWVHJO(25x11)(12x5)VROWLLAEHOLE(2x9)YN(16x9)(10x6)TSAAUESDGC(263x8)(29x13)MQGYMAWVEQTEKIHRECFCMOHOMNXLA(220x9)(2x15)KW(39x13)(2x11)HS(17x14)MDOUAHVKKAXCGWXLP(2x1)HO(36x8)(1x12)W(22x15)RQYBTSLNZTRISZUZSYVSCY(66x6)(4x11)RPRV(16x7)KCYZYRPHGWXCKTVI(11x8)QCRXXBUYCHS(5x1)QZWBP(2x6)SY(46x4)(8x6)AQQHWWDB(11x5)UPLVDZOZVOY(10x9)TQQDBCWKKV(27x9)(13x9)(7x11)BQSKLAU(3x8)ZRW(5x13)JBKRD(342x8)(120x13)(10x8)(5x2)GOBYG(36x7)CSHVGUVIIAEMDKAQLZSHTFFJKIIWHKZYMCQW(3x10)QKI(9x15)(4x9)KGHA(31x11)(3x10)QRN(5x13)ZKXNX(6x6)YHCNNJ(189x4)(9x5)ALDBNBHUT(168x2)(1x14)P(67x1)(18x1)VHYINNFXQTKRLVBXYY(29x10)QMSFREUKSLKASCUANUHBRCBOWPJMK(1x15)K(16x6)(5x8)YXIMM(1x8)A(51x10)(5x15)WQVQW(3x11)YFY(15x1)RBJODFPBSHDMRLQ(5x8)SFUQL(3x1)ARD(12x8)(6x11)VLUZSI(848x8)(214x12)(207x6)(19x4)(2x14)VA(6x2)VSFUJI(1x2)U(9x1)(4x1)GKPI(10x12)(5x9)ZRFAE(137x11)(9x12)YHHPKSKTM(16x5)CPGBRBWZGTEJCALY(14x10)YLIUSVWIMZAPRD(65x15)ZGDKFMFTSRZCNWSWPUPDVWZYXSLOISPNHEUAQJJREFBDCBRISOYQRQLDMKQGEVUQP(2x4)DK(203x8)(12x9)ZXCNCMDROASE(177x14)(7x2)NVKUHBU(2x2)GM(107x12)(3x8)OKV(16x3)LFVOGUTFSLMGBPXF(12x8)SVHILNSDJZMQ(28x5)YEEHKYMCKLSMMUBROUQFXTFCCEZT(19x1)FNZNMSIMOHDOZFVFVEC(10x10)(4x12)NGML(20x7)ICULAJEPWOHSMLJZOKJI(223x11)(31x4)(1x6)L(13x6)(8x8)CGTYOKUY(1x8)Z(167x8)(61x14)(4x9)WGDI(13x9)KKIVRFPRCORSQ(8x4)NVRMBAPP(4x1)XNCK(5x14)AJYDE(42x9)(3x13)VBK(20x9)OENBWZXYLYRJVOZATTRO(2x7)AQ(12x5)(7x7)OLOWHLP(27x2)(1x7)F(7x7)JMQKSCT(4x9)YCIT(7x7)JZSEJTP(177x13)(53x8)(46x11)(6x8)MCDKPD(15x10)AXTVGBJADSVXZQU(8x8)QVMAXBDP(111x7)(13x4)EVWQQTQLPKJGW(14x11)(9x7)QNFLRFACY(6x11)FMOZOW(1x3)K(46x10)(3x8)HFG(17x14)WVUZTQACXVRLDNCIP(1x3)F(2x15)AQ(3x13)WRD(241x14)(234x2)(182x6)(76x11)(49x12)(4x12)BKXA(27x7)VJZDTFMRHGWMWCPELDDGYSOHICE(1x5)L(7x8)LETVJYM(3x6)FOE(86x5)(3x11)RHG(3x7)PGU(3x12)HRA(54x1)(16x14)RUZDLXTFKCSHDMJX(2x15)KP(3x5)ERR(9x15)OXSUBLNDE(2x3)AO(22x13)(15x12)KVJPEBINSXCMIAJ(10x7)FDUTYGTPYO(7045x3)(1694x15)(1237x13)(2x7)TC(387x1)(123x10)(55x2)(11x3)FKYSUXQOUBT(3x3)AHD(4x5)XRKQ(5x11)MHSYF(4x10)FKAV(55x10)(15x3)PGTZRCQJTXZIDVM(13x13)IBZJCPEYTOJYK(9x6)KXUEJVAKP(249x3)(108x14)(7x11)VQJUCRC(7x15)HKKUUJF(7x4)CADYWRN(2x9)PF(57x3)ALPBVWITMJTLAEFYDFKYQZHWTPDTGDVFTWVOPTCAMEYWNPKUVBUZXXXZL(32x14)(4x6)EVRA(17x6)NBSTKJYEIRPRJBTXW(12x12)CZWVEDJMITSE(68x13)(13x9)YKEBNECKNDWYD(2x11)II(22x8)KLJENDMYESWDYZNDGWRQZA(8x6)NOYUPJEC(361x3)(88x6)(23x8)(3x12)GKU(9x9)ZJKUAVSTQ(41x5)(3x10)YTZ(26x6)UJQKFPXTJUDJQRZBIIXKOASSRR(7x5)MAXGECR(8x12)BTNQVUDH(150x8)(100x3)(11x11)DXJYIBQXEVC(17x5)DXRJKPEHAVIKKZJMZ(27x5)QXMAGKYSHNXUKCERPBJZLAMQTIP(20x8)QYNXKHBXLVXATJQMOQRZ(5x11)CDELG(26x4)WXGTJWIZQSNLWDVCWWLWFXTOFJ(10x7)VLFWYHBFIA(73x15)(66x10)(1x8)E(7x8)AGWFUEF(14x7)NUJRIZNQZGPMHS(21x15)ALPTMKCXUEHVZDAZDMDKH(131x12)(13x14)(8x8)PFDAAJLQ(6x9)HZBHJQ(12x12)(6x13)OTPHTI(13x8)(1x4)T(2x5)BG(55x15)(1x1)C(12x4)DOKEVDCYJERH(24x10)(1x6)D(5x9)FDXHK(2x10)RQ(322x4)(113x9)(11x12)CUGZEAYMHAW(89x3)(17x11)NAERBWAKWWYSWOWDR(13x15)XEGOGNUDJORCO(27x12)BDCVKNTKLNSEAWKTPYNZIAZYJJW(5x12)QMCNJ(55x11)(4x14)GIXM(38x15)(1x12)J(13x15)UTTZFEJCPXRWZ(6x4)UEVKLG(133x7)(38x6)MGNLNGQBUFNTSDODFJRNWBYXUJWOXMZSKPFSES(77x4)(9x12)OCPDSQAXL(4x15)BCRP(12x11)KAZCKBRPGBXQ(17x11)URTUNBJWKZUFUASCY(3x11)QDX(1x9)S(441x2)(285x14)(7x13)HASJKVW(6x2)GURASU(254x4)(40x8)(34x7)EMKSEGMUWUONRQLNHUJDNWFOSPQJUDEIVV(100x9)(14x2)NCNRTHZEQMSFPY(19x2)TFWPRMZOKFVALYACOJO(1x1)H(36x11)SUKGBMMXYFLOLBONANUKAAOXDIEUWYQGNMHF(1x2)R(95x1)(22x8)KINSVVMPHKGNNYBLPRRNDX(28x11)EERSHQYWXXRXFGCZPZVQSKNADFDH(15x11)ISRXKHOICZSGUYT(5x6)IAXEY(7x4)(1x13)A(5x14)LIYFY(105x8)(9x13)PVUDVCCHN(83x15)(17x14)(11x8)PKHOYTPFZPW(52x15)(1x2)R(5x15)LADVM(16x15)CVRXNFJKOYDTUNBB(6x13)ZSBETZ(7x10)JTWLXUN(1262x2)(1143x3)(368x5)(14x3)QKANPUZNJGORAQ(128x2)(46x15)(27x12)RPQVEQLOEFEQFLDEGAEPZAJURCI(7x8)MZTIAWL(61x7)(13x7)GIEHKXTFWLIIV(24x1)UBGZCPTFSJKNAKWRQCRHPUIV(6x13)BOTCBY(2x11)ST(34x8)KHKIVTCYSIYGODVLCOYFZZDYSCTRPSGLQX(165x12)(13x14)(8x5)TBBSQPIO(29x12)(9x13)JKDEMHKWM(2x7)CV(2x4)TP(102x4)(17x5)TCDDTQQQEOXSGWWKE(17x5)UEUALEIASUDELICWN(16x6)MMMWRNDZHBIBXKOM(28x5)HEIGFFENZZRRFTUVBUQORHWTPGOV(247x1)(17x9)(10x11)ESATUGNVTG(25x4)(18x14)YNMKZINZXPZLHQQIRR(2x4)IT(110x1)(21x14)GMZFGRDOTAEUSTJHZMLOP(60x12)(19x6)XOOHSVUKPPWUMJLUQHS(20x6)ZILZMQNAUSIRVRUIZZTM(3x14)DWI(1x3)L(3x13)DEZ(63x5)(47x15)(16x14)VRFJFDIWFJFTJGKT(8x8)ZOFGVHDC(6x1)VCQQVH(4x9)LQRG(494x3)(56x15)(50x9)(8x8)ZPJBEIIG(5x15)UJNQO(9x2)ONVTMWKNV(6x12)HUTGXL(268x11)(7x2)EWDPKKS(84x10)(37x5)UYUCMWOMGOYZGZXWNRVRUHXTFEWCOPKHADOEU(3x13)OAF(8x7)ZGCBECWP(3x10)OMO(4x13)KAFH(77x13)(15x14)EGPWQGKQEDDDOTO(2x12)JV(10x2)PCAHKAXYDX(2x8)FS(17x14)XUZQMQWRECWVDYLDL(21x8)YKRLUENASPLZJWIVYDMFB(47x10)KJRRWAWOMNUYCMEVOHMLFLKYIKLZFWXDZJINORLBJBWKNYQ(24x15)JHATNNZZXPHCXIKGNIHUSEBQ(100x13)(29x10)(4x8)POID(13x10)LQCWOSJVGLQQC(17x2)YTTHNDHRWMKRKXTUU(25x10)SPQMEFRJIUPXTZMMUJEPGKRJO(3x15)AOL(10x1)VLDBQNWERE(8x4)(2x10)ZV(104x4)(97x11)(43x10)(27x6)(2x15)HP(13x7)UFIZONIMVDCDF(4x10)SLRZ(16x3)WOGBTZXRVWHTCWAO(7x13)(2x6)XL(6x13)(1x6)X(48x2)(42x1)(36x9)DYLVFNGCJRJXEUOSSYOFSGKDMIHCGQCRXYXN(1750x2)(483x7)(467x11)(5x14)CDNBJ(85x13)(21x1)(9x13)UKLCTCFUI(1x6)J(9x7)RAITDZYEJ(21x4)FGQRBVJLQEVYSZAISQMAD(10x12)RGBXVGSOSR(202x15)(5x7)VJZBP(45x15)(31x8)DNLGOFUIWITQUZFFTPOHRMRVXYNIFSL(2x15)IM(56x13)(10x6)COOZPWTGPM(9x7)YJIILGZQT(4x15)HUKR(10x2)CMURLLTHBG(54x1)(7x14)PKUUDKU(11x15)FFJJILDMOAS(8x10)XHNHFOLS(3x14)ZAI(10x13)(4x10)JNNQ(112x7)(28x3)RXVJTWPTNOOLVKCHIPPPWMVBADPH(72x2)(32x7)UDNRKYALNRLKMDBJXTRRIMFKQFJVBWTH(1x9)Q(9x13)STRWFASIW(1x3)S(1x13)X(28x14)OFNSRVYFPSFPNJIARCUNLKMQIXPG(3x7)ZMT(113x3)(7x8)JILQNPH(95x3)(88x10)(7x4)UHLRLXX(8x14)YAQKCKWZ(10x12)(5x3)HUEJY(25x1)(19x2)EJASIEVVGPKVRDZZBHI(8x10)CSVAAVTT(1132x1)(253x4)(245x12)(64x9)(12x13)MYBUMZYQQGNB(14x9)OAEHKVUSGEJDJO(10x2)VKBQQKMWLD(4x9)PJBX(47x8)UQICXEKIQVMCVYGFVBDTOYLEKJQNFWWYMWPPIPKJRYDLNJY(7x8)JDOMLTT(92x6)(16x11)JZVSYKXQARJNCBOT(10x9)KEOWHTHIMB(2x9)IH(10x1)KOQIDZLQWE(23x12)ZASFEWYQTEMIRJMHFMKOUYD(6x12)EEZUYP(516x1)(75x2)(35x4)(7x5)TBOZCBW(1x3)K(11x7)ZAHCEIPCSLQ(27x13)WITJKURFYKCDZGLXQGTJTPILOPP(77x12)(35x13)(9x7)NGKJUFJPN(5x7)HEVKK(5x11)MRZRQ(11x12)QTZKFFPUJCM(11x2)(6x7)GOINTN(240x8)(17x15)VCSTHOVEMGKRCLAQO(85x10)(10x5)SFSFSXXZHZ(39x8)YHPHGOLEAICNEYFXRJZBEEPINXNONPSFEAEQLXR(17x10)NQOKSBBYTARGSUKAU(54x8)(29x8)XGZVIQEPZUZQXPWKPLVGVELWXEKZO(13x5)ABVOVHEUIFMCX(21x11)(8x9)PSCTVFMT(2x10)HG(29x13)(6x13)OIZWHI(10x13)UYJJQMXNEW(98x4)(92x1)(10x8)AMZNTRUYRJ(17x5)ZBOWCAIWECBUAUEEM(22x12)DQJCCJAUAWBUEDPYJWFATH(9x7)XNRAELWJX(4x13)LBKM(3x1)WSZ(334x3)(127x1)(21x3)ERKYTWYIAWUNQFCVGCVXG(13x13)ETKUWEVUDSUHY(10x3)ELMXXYOTMD(31x3)(1x12)E(3x12)WIS(9x11)RTUHDIEUN(21x4)(2x3)TC(3x6)BXB(1x2)M(19x7)QNWITDFPSLBAFFGYCNA(148x3)(20x4)FSSZIPFMPZFWEXIOHLPB(38x10)(8x5)UAIUVNOI(3x14)UBN(10x6)YWSRAYTVVH(32x15)(12x10)YRUFKQCOTDBG(7x13)OGFFBVM(25x3)(2x6)SV(12x8)OGPQUGBVHNPC(2x6)OW(13x10)YQUNEVPKGOAPQ(2252x4)(694x12)(250x12)(46x3)(1x3)P(21x15)(14x14)CBMUTIRGZRFPAQ(6x11)PXQNXN(160x6)(75x2)(2x2)WD(19x4)SDRKNDIDOXNJITECWVG(14x8)DKDNISRFFORICQ(10x5)WESURVPTOZ(1x13)V(72x13)(1x8)R(9x5)PAIEXGUAC(23x1)FHJZZBOFYERBKPOVZSRVXHA(3x6)AGX(9x11)VFIHMBVBV(24x14)(17x13)(11x6)ZYXEYJCXYRW(428x15)(50x10)(44x8)(14x12)YHRVJYBVASYNWC(8x6)DNMHGANB(5x3)AUZYC(6x10)DBAATO(176x1)(63x4)(12x11)MBJDMYYEOCJA(3x2)LCJ(6x1)QRUSJC(2x9)KN(12x9)DVZNCHHUDGDW(100x4)(9x4)IIZRYEAUC(19x5)NLNZCCJWUBUAPJISLSX(2x14)VR(27x13)KDJNNCAJVAERWHNQDXUNUHVVRQQ(13x5)GCIXBFKQTDTEU(168x14)(6x4)EWVTCB(34x11)(5x12)FFMBP(5x11)UXPXJ(6x10)HSECNG(5x15)IUUGP(66x11)(1x3)T(4x9)OFCQ(4x8)JMMI(36x9)CAMNVOZZVPYJNDYPIZXIFHRBIVJASQNCDNWZ(25x14)TKUWPVOTDZPWETHELIHWLEMYR(2x13)KT(579x9)(546x10)(157x14)(23x1)(1x14)B(3x8)ASI(2x13)YX(51x7)(14x3)TFZHYALQMHGPQP(6x1)SJIRBC(7x7)WDWLJIJ(3x7)WWF(1x4)N(50x12)(3x12)RLQ(5x1)EPNYQ(18x11)UAUIVHYLEXFIXMQOZO(1x2)R(3x9)IGZ(16x1)(2x4)XH(3x15)BMR(19x15)(13x2)HWIQKVODOFFQC(197x10)(35x2)(6x15)LCQRWS(16x13)BXVVYMMZZDFXPXPU(62x1)(3x3)ZAR(13x13)VKDSKLZOFTDGQ(12x4)UIFXQZAWEPYD(10x3)SJVMMCZQOD(22x9)(16x4)NINGDACSLHCILBTR(4x12)SCEN(43x10)(5x6)VMROP(9x3)BBMOWMAPY(12x12)HCJJNJQOFMBY(120x14)(102x7)(18x14)BUUKADNKOMHTIJDYEG(11x9)AIIFROSXCYW(34x15)FBJALEBNGFWQBBEZORTVGPUYKYFQIFVHUI(4x9)DWOK(4x12)PUYE(6x6)VDQBFE(19x4)CCUMDYGMGFDDIQWDVAL(948x12)(163x14)(18x12)(12x2)KDPCCMAMCABD(18x11)FPPLQNGQRLBMSBBTFG(99x8)(25x6)MSLIUSOYDOLZJRTIEOGMZIDIH(62x6)(8x12)EVGKTWID(2x3)GQ(11x2)UGQGKEPOVWY(17x11)VVPJOGPMWDXNUTZZC(3x9)EMT(393x12)(104x7)(72x5)(4x2)MKQY(3x11)FNO(24x1)YPSJAURBEWDJJMHKCZMNZCPN(18x6)JFLKSKXDKZUDARJSFW(1x11)I(12x11)MASFYNDTUUUO(133x3)(54x8)(16x9)HZJSKPDRMHSDLKWS(26x1)ONAIOWRXKXWGIPXHFPVXOWWTXV(59x11)(7x11)DHNWANP(1x9)C(13x10)PIUGQNICQPQIU(13x15)IVOPXOVIIOBYL(2x7)WO(127x13)(62x14)(6x10)ZAVFEI(4x8)STLP(11x12)FUCCXWZPALV(17x1)BMSSMCUZBCSWSQUUP(21x9)RXGCKKYMFYUCWLUPVKZGN(1x13)D(3x2)SNT(10x6)(5x9)IRHCC(1x10)R(47x8)(40x11)(2x5)IQ(3x11)LMP(6x8)(1x9)K(8x6)HLLGSFQT(316x7)(213x15)(113x10)(3x14)HKU(35x10)FHIQCKIPOYTKPLINYJFKUXNYHNXYTGDWPXZ(23x11)KRHKVKMFQQPNKAQYRZYNRPU(13x14)WOKSNIFCMECYU(7x5)SGTERJS(4x14)UXQX(6x15)(1x3)S(2x5)TU(57x8)(8x9)RPLYSWYI(6x10)XICJBU(1x9)X(20x9)ATAPYXISIGMKIIBNTULM(88x13)(1x4)E(9x13)NUJURGIXF(61x8)(18x12)DALJEKYLLCLSPZSKBZ(14x15)WCXYCPZPGKJBMR(2x13)GX(2x5)CO(6220x5)(1950x12)(1083x3)(267x1)(142x7)(63x6)(3x14)ALL(9x6)AMKZAHKHQ(6x12)EOSVKP(21x10)XJNRXDQENHCOBUXXTMZQZ(1x6)C(21x15)(14x12)IYMQWIYRVMSALV(32x11)(8x8)ABNSTYLH(1x6)S(2x9)BT(1x3)N(4x6)MVLQ(102x3)(22x13)EQBHHMWOYZQVTQZLIDDSFY(7x10)ZSPXWLN(54x9)(15x15)NHLKXTAMFXTLABT(4x5)ZXSE(8x14)IOLHOJLI(3x11)JFU(306x12)(39x2)(2x11)QF(1x13)K(5x6)PKURE(8x14)(2x15)BO(9x15)PVJTAXRXL(239x6)(96x3)(6x14)AYRKJN(10x12)UQOMVXMWMB(6x6)MEHBIG(6x13)TPYQAD(37x15)VRXCOGKSHRGKDIMEEWKYDTJOZSMTOOYMTIQAL(10x4)SNHIQWGRNW(18x9)(12x8)XEFUPQOWYBWX(3x8)ZMP(83x7)(20x8)YPUNCWPNFNHRELHRSUPX(19x11)XPJPRYIVBWLHJOEXORD(18x13)EEXUFYPAIOBMWNHZXH(1x8)E(487x12)(206x1)(2x12)TR(11x1)(6x5)IWGICM(40x10)(3x15)STY(6x13)EPDTGW(6x1)IVQYIZ(3x6)LMG(105x10)(18x7)JKERYRQOLQUIIKMLJM(7x12)DLXNYSN(35x6)FOKFBMDLAYUFJXRWYNRPTQNVCJSVAEQOSVC(8x10)SEVZYQTV(8x8)GKBPKOCD(15x5)LWERTYWMCBAEWHL(8x6)THHETSJF(127x14)(34x13)(2x10)FT(11x9)XPPKFXTKQDN(4x5)AFNN(80x2)(16x8)IXPPCOIUMXELYPCF(5x13)XUAZM(13x12)WZTXEDBDJBMSW(14x9)GPDHGANCKSAORD(2x9)JZ(34x15)(27x14)(21x8)ZNYOGHEJSDMXNYGNGKBYH(78x13)(3x11)MLH(9x14)JWNQCKWVK(7x15)FAFTQNI(35x4)(9x3)RGVLDDOGJ(4x9)QUBG(7x6)RTCATAL(191x15)(183x14)(175x13)(2x15)QX(101x7)(12x1)ZJYOHMXRPRRZ(3x5)PPF(11x5)BCIBNOGLZIV(21x1)NEWSLJUMKONJNVZKJHWCG(24x15)BYCEHYBSVYYZORDAVXVTQVIR(53x5)(5x13)JFLTG(23x6)LBFUCYDAFBQOUFISYUJOQID(8x5)FBVIEKWA(10x9)FYZURKLTPI(617x8)(3x7)RPT(602x5)(75x2)(4x3)WEEB(20x12)YFBAXJKVTMOGUNAVPHPJ(1x13)K(26x4)WBETTVSKASVIONCMNWLDPBJWJP(155x4)(9x1)WFWKVFGRV(99x3)(8x4)OYTCNIPO(2x7)TK(47x13)ZPCQDJKLJBZFRAOEVCRICHONYZDPQYPIBAZAMAUTQXFECMJ(10x8)ZEBAQGBAJM(3x13)CTH(30x5)(24x4)WNGAZRPZIVAOIAUEOADIMUZH(18x7)(2x10)GD(4x15)GVJO(92x1)(4x6)TSZH(24x8)OZTAYDZQDKWTAVHUSLJIYHJA(18x11)POWLTPGHWNTSUWUVIB(22x2)MAVRDHTYGVHYJVUURXIKHD(229x10)(6x8)IFNNZD(2x3)UN(81x5)(7x6)TZCQCCP(19x8)VCAPXKOHMQVBVWIKZPL(38x7)XUVSOXLAKSJVGSFREYSXZPICKSAESBBMNXYLER(19x6)RWNWHPVOUNYKIZXWDEU(93x5)(39x12)LKMRRQYYTLRVWUBQBRQYTUEJLWNVZECMGVZQLSI(5x14)QFEPQ(3x10)JCJ(12x2)WXLDSYATBGIW(3x11)SUE(13x14)PKIJONLXUANPI(2034x15)(601x9)(153x15)(137x15)(18x10)(2x4)XK(5x15)ZYJPJ(5x4)HAHKZ(14x8)SBFWAVGECLXPZG(63x5)(12x13)WMKTEGIOQDIO(12x1)PIFFUKEMLEHG(13x1)DNKNWIDXUNMHA(2x2)PT(8x7)DFEBPKUX(2x12)IS(155x7)(2x8)OD(2x11)MM(122x1)(65x5)(28x13)BRSKRLKHOUDCDOERWDPCHEJPRSWT(1x10)P(2x14)GH(9x10)AQRYRARFQ(45x1)(22x8)ATSCKEGSMYXFSJTCMUTPPE(10x15)KRSQOYZBMR(6x1)(1x4)G(86x10)(80x6)(28x11)(6x12)EOWFGX(3x1)CAQ(2x11)BY(29x3)WKLNUKTMDNXVQOEKOXOCOAETCVMYB(4x13)IEMT(178x2)(86x1)(44x11)(11x10)MTIOKFLCGYO(6x10)YALYGE(3x9)OFJ(1x5)J(10x12)XTNXMYFPMS(12x2)(7x8)DTVHZAC(34x7)(28x6)UNSUAFQCOTEQDLBXOKRCWXRUZDNA(40x1)(1x4)L(4x5)CMKN(19x5)XNZACQOQPXKFNXJZRRW(690x10)(224x2)(169x2)(2x13)OE(11x7)CBLHVYUEZJL(53x12)(7x11)DZMDXYZ(7x15)OQCQKPV(1x5)V(8x15)OFUHWXYC(2x8)RH(53x4)(7x7)YTJZFFK(7x11)VKFTYMQ(2x6)CW(15x1)HHWVGEDXIXHNZEC(19x8)(1x14)D(7x1)COKXISK(12x11)(6x14)EUFORX(22x15)(16x3)(10x1)XWUFYPOXDQ(444x15)(131x8)(13x2)ELWFYKDQUUTXY(6x4)RTHAIG(57x15)(7x10)TTEYCXM(1x10)S(10x2)ZRRFOMFAKP(14x14)JGNGIWLDMCSTRP(16x15)COAGPQBGKAHNYKEG(8x11)MPMRCZKA(86x13)(3x13)KMF(50x6)(7x8)BKPEJJO(5x1)ZLPNJ(14x10)OSTMKTEYXWPLXM(2x2)VJ(8x5)VEEVFOUR(2x12)GB(46x10)(9x4)QCUTJIDCA(14x9)ZTYDOZXUSMNXBF(6x11)KOONJV(20x12)(14x1)(8x15)VHFNUTKY(126x9)(7x1)(2x1)JS(1x1)T(92x12)(17x7)QGPXBGBVVHSPXGTOD(22x8)CINHYVENGXSJRJMZQYJDDO(9x3)PKHDXXMCM(21x7)BVCUZQJTLALQRXJIEEULS(4x7)AMWX(1x11)N(41x1)ARDAIXBZPOZZDDMVJRVAKWRXDCRWZHJNVTKBQENNI(137x4)(129x14)(122x6)(64x12)(26x2)ROKLQLSCIZLYQAJPTJHVKEVSMT(5x10)MQQKO(2x10)VP(7x12)NUIMAYJ(16x2)QGKTFKJTGYBWKQQB(22x13)(16x5)CBZENHLSYDZSFZHH(530x9)(164x15)(80x10)(7x3)CMJCERB(10x13)CWMRXJPGSG(13x14)(7x10)CIRUISS(9x8)(4x7)IRVQ(10x13)OGQXOHPBYK(71x2)(8x1)(2x13)QA(19x11)(3x10)VCK(5x9)KOAMK(9x2)AMGVLBSPI(12x7)(6x15)NHUWPB(350x10)(214x9)(19x14)(12x13)IOZNVGXCNCKG(70x15)(4x3)BSHO(14x6)VDILSQWNAWWNSF(5x5)EGKTN(2x15)IK(16x12)RLZLPKTDPJDVOVVU(87x9)(4x12)SBSP(21x15)QRYPMOLSULJWTWEBHOPIP(13x7)EELJZCAWLYING(24x4)OUPBNCPPYVWGQLPHFBHLFLHI(11x13)KUCQAROVMAV(112x2)(104x11)(26x14)AGSYXXHOODOCXVXEODQMGCBNFP(12x1)GCFQCETQZBOV(40x13)ZBOSZWQXUXSMKJBKGSZCURKKDMNPCPRDDNKNLIOI(1x9)F(5x3)SYLNJ(2160x9)(1124x2)(87x2)(80x10)(30x1)XODLGJEXHRGYWWETBMBDWUKRRAIZIC(14x15)DITBLTCVZYPDAH(6x14)HDAADE(6x1)UEERYO(260x10)(114x3)(32x9)(2x15)AM(7x1)ZKLONKC(6x12)TWEUFU(70x4)(15x3)PTXSEDFTSSOFZCQ(12x5)SPVKIQWORWQE(9x4)DLOUFOYKH(11x5)WKPQLMIERMA(131x15)(26x3)(6x2)BJNUED(9x15)MNROHJHAK(92x11)(5x5)OUIDX(35x10)EDARHYPBZKBSGWCDIBSKQMNPVSUCUFZNJXX(27x6)YQDCAUPUPSJMOEETGPBXEEXEHCH(2x7)LD(42x5)(3x12)ZSN(27x2)MWNWUNUMDOJVEZJRTIARFIXFAKY(708x5)(19x15)INPDEQAMXMYFNORSQAF(18x13)(11x15)CDEVAHIXUDQ(287x7)(59x1)(5x14)IRCIJ(10x5)ECQKNQICSB(18x8)XULECWJKBBPVZLGAMI(3x6)ZEG(67x3)(51x11)WOBTJCECZKEAMPUNADTDWGIUEBALLNTTPQXUXMSKUCKLDSMFFWE(3x13)FWG(51x13)(28x5)BWIRYOBBPRCKBGPUPOFXWJCAGKNG(10x14)JATDGJSCRI(15x14)ZHDEXOJIQSAHUKX(63x2)(9x9)CWNCSOTPX(3x5)CEL(8x7)VJOKSFQB(1x13)X(14x12)VVPDWLOGZFOQKH(192x15)(66x13)(24x8)UIVKTUMUTTWKCJBXUHEPRRGJ(2x1)XJ(10x2)IRCYLCZVLC(7x10)ARKRVPN(35x13)(1x9)U(16x13)NSKKVVSYKFWVEMQJ(1x1)J(48x15)(1x10)E(27x14)EDOEXQKEVWNOFOEAPPSQAIUZBGB(1x14)V(5x11)CTQMU(6x6)LKXFZN(156x1)(8x13)DFWSYVQQ(12x12)RHPCOUCLGADN(2x7)BU(31x5)(3x11)AFA(2x5)RJ(3x3)OVG(2x2)GG(73x3)(28x10)RRSWIAUUVVAPMRDKOLQJNENWEOOO(17x15)AJWJWWBZCYJMLTNHQ(8x13)WTCEHSBP(197x13)(189x13)(83x15)(22x3)(5x8)JJUNN(7x8)HBZVBUS(7x5)STHZRYT(25x1)SWGOLBEXDEDYMMHEYNUGOTPNU(1x5)X(1x3)E(10x7)LEFTYQSXKP(48x7)(5x4)JICYF(21x3)OCTBXKFVGRBCMMGKXODKH(5x11)SJUKW(9x11)UGMMXRERE(8x12)BOKVEIDJ(68x10)(62x9)(56x7)(43x5)(7x12)MFIHEQH(5x1)GLAYP(13x15)UTBQBFWWKMQOC(2x4)NG(717x15)(17x11)QUALCHVKEVAXUUVKX(8x13)IBTEPRMV(192x1)(19x2)AWQRWSKKWOXGJICCTKN(138x14)(90x13)(18x1)TVPWLYCAPUVOVLQWTT(17x15)ENPJMPJIZKMGRSUVD(20x15)AKXGALMORIJJGPYHRTJN(1x12)N(2x15)PS(18x10)AGHCGUWMXZDOVVTASF(10x3)WCKTOSJIZS(14x11)ILJHKTALBUBWIL(457x7)(198x5)(83x13)(2x4)MP(16x15)QFKTRPWECWUEVAYW(10x6)WPTCNHEKOS(31x4)ZQQHPRIDCHIYUKZLARMBJQDHKXUCWBD(45x5)(1x10)I(3x8)JQB(18x8)UWCVSYJNGXABCQBKCX(1x7)N(38x1)(24x11)AFJRIBIVWPLQLPWTUOTLREVT(1x15)W(8x6)DKNEKQOL(9x3)ZRDPXQKVR(13x15)(8x4)KDGOQAXL(211x9)(96x2)(37x3)YKGSAULQBELDOIDZOEDTWGYKSFVXWMZMHYGUV(8x3)BUORTCYM(33x15)NELWDCXCKNPBDREDWJZTWNTRPKKCGZXOR(8x5)HVMLQWMY(44x12)(11x8)JZXVXXQNOWZ(20x12)LKUODIKCVQKZMKIVFIKM(10x15)EOUHTLYSHS(22x3)DHLODIEKOAROSBTTAHVEVM(10x5)(5x5)QZNNF(16x13)(10x6)(5x3)KFIUY(43x13)(23x15)(8x7)EGWZMQBO(5x3)MTQWQ(7x13)MTPQZKX(14x3)PQSSIBHUPVETLZ(119x7)(7x5)(1x10)L(28x6)UFGLKEEGGQGKCNCZFZJOGSXQBVZW(67x2)(10x15)HQCQWGJUKJ(11x7)BAQUKCTSELT(4x4)IHPR(18x9)QJNPGYUDFYCHZSNPRQ(63x2)(1x1)T(51x6)(1x10)Z(21x4)EXVMGKZHCIEYWNYOOAAMF(5x11)LYPBQ(1x2)R(24x10)ETDLKFDMGOVMGWFUVGNRSDYU(4x7)DCYI(85x9)(6x5)AYSNJZ(50x8)(7x14)QRJDDCA(4x3)UHSC(13x8)JIBQGVVZWXRAR(4x2)NMQN(12x9)TXZBPVOMPAXW(246x6)(84x10)(34x6)DXNIWQLJXQXBCFKGIAXJNLFNCCKSQNQWFW(1x2)M(31x10)WZLMERNAUBXXBVXVSDZHGFPDETZOVUY(131x11)(13x6)AHBVWEJATPTNC(50x15)EKZFGPVIGZRLJEYNHPZVRDJTFJFYCMOZBJYVYVAHDEQDPBGCKR(17x8)KCMDUTXFIVHOIKSJK(8x2)HCIXTJJM(13x3)AIFGBDHAKCDVY(10x9)DVHZVHZISZ""""""
    return theInput",inputNine.py,znuxor/aoc2016,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't instantiate with non-transformers on the way
    # Note that NoTrans implements fit, but not transform
    assert_raises_regex(TypeError,
                        'All intermediate steps should be transformers'
                        '.*\\bNoTrans\\b.*',
                        Pipeline, [('t', NoTrans()), ('svc', clf)])",scikit-learn-0.18.1/sklearn/tests/test_pipeline.py,RPGOne/Skynet,1
"class SVCType(object):
    """"""
    Defines the recognised SVC addresses.

    `A` suffix stands for Anycast. It indicates the packet should go to a single
    instance of that service.
    `M` suffix stands for Multicast. It indicates the packet should go to all
    instances of that service.
    """"""
    # Beacon service
    BS_A = HostAddrSVC(0, raw=False)
    BS_M = HostAddrSVC(0 | HostAddrSVC.MCAST, raw=False)
    # Path service
    PS_A = HostAddrSVC(1, raw=False)
    # Certificate service
    CS_A = HostAddrSVC(2, raw=False)
    # SIBRA service
    SB_A = HostAddrSVC(3, raw=False)
    # No service, used e.g., in TCP socket.
    NONE = HostAddrSVC(0xffff, raw=False)",lib/packet/svc.py,FR4NK-W/osourced-scion,1
"from sklearn.svm.classes import SVC

from ..Classifier import Classifier
from ...language.C import C


class SVCCTest(C, Classifier, TestCase):

    def setUp(self):
        super(SVCCTest, self).setUp()
        self.mdl = SVC(C=1., kernel='rbf',
                       gamma=0.001,
                       random_state=0)

    def tearDown(self):
        super(SVCCTest, self).tearDown()

    def test_linear_kernel(self):
        self.mdl = SVC(C=1., kernel='linear',
                       gamma=0.001,",tests/classifier/SVC/SVCCTest.py,nok/sklearn-porter,1
"	n_sample = dataset[0]
	n_feature = dataset[1]
	from sklearn.neighbors import KNeighborsClassifier
	knn = KNeighborsClassifier().fit(n_sample,n_feature)
	print knn




   
def SVC(input_dict):
	dataset = input_dict[""data""]
	n_sample = dataset[0]
	n_feature = dataset[1]
	from sklearn.svm import SVC
	clf = SVC().fit(n_sample,n_feature)
	print clf
# SVC(data)

",test.py,darkoc/clowdflows,1
"        n1, n2 = nodes
        features[i,:] = feature_map_function(g, n1, n2)
        labels[i] = loss_function(g, n1, n2, gt)
        labeled_image.ravel()[list(g[n1][n2]['boundary'])] = 2+labels[i]
        g.merge_nodes(n1,n2)
    return features, labels, labeled_image

def select_classifier(cname, features=None, labels=None, **kwargs):
    if 'svm'.startswith(cname):
        del kwargs['class_weight']
        c = SVC(probability=True, **kwargs)
    elif 'logistic-regression'.startswith(cname):
        c = LogisticRegression()
    elif 'linear-regression'.startswith(cname):
        c = LinearRegression()
    elif 'random-forest'.startswith(cname):
        if sklearn_available:
            c = DefaultRandomForest()
        elif vigra_available:
            c = VigraRandomForest()",raveler/ray/ray/classify.py,VCG/gp,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",Code/ipynb/fig_code/svm_gui.py,everestso/ics1293,1
"        self.assertIs(df.pipeline.Pipeline, pipeline.Pipeline)
        self.assertIs(df.pipeline.FeatureUnion, pipeline.FeatureUnion)
        self.assertIs(df.pipeline.make_pipeline, pipeline.make_pipeline)
        self.assertIs(df.pipeline.make_union, pipeline.make_union)

    def test_Pipeline(self):

        iris = datasets.load_iris()
        df = expd.ModelFrame(iris)

        estimators1 = [('reduce_dim', df.decomposition.PCA()), ('svm', df.svm.SVC())]
        pipe1 = df.pipeline.Pipeline(estimators1)

        estimators2 = [('reduce_dim', decomposition.PCA()), ('svm', df.svm.SVC())]
        pipe2 = pipeline.Pipeline(estimators2)

        df.fit(pipe1)
        pipe2.fit(iris.data, iris.target)

        result = df.predict(pipe1)",expandas/skaccessors/test/test_pipeline.py,sinhrks/expandas,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the t-score of each feature
        score = t_score.t_score(X, y)

        # rank features in descending order according to score
        idx = t_score.feature_ranking(score)
",skfeast/example/test_t_score.py,jundongl/scikit-feast,1
"            reduce_ratio = 1
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if 1:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_24_2014_server.py,magic2du/contact_matrix,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = ICAP.icap(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_ICAP.py,jundongl/scikit-feast,1
"    random = np.random.RandomState(seed=0)
    E = random.normal(size=(len(X), 2200))
    X = np.c_[X, E]
    return X

def crossValidation(y):
    cv = StratifiedKFold(y, 2)
    return cv

def createSVM():
    svm = SVC(kernel='linear')
    return svm



def computeScore(svm, X, y, cv):
    score, permutation_scores, pvalue = permutation_test_score(svm, \
                                                               X, y, \
                                                               scoring='accuracy', \
                                                               cv=cv, \",examples/scikit-learn/examples/general/test_the_significance.py,KellyChan/Python,1
"from separar import Separar
from time import time

logger = logging.getLogger()

class L1LinearSVC(svm.LinearSVC):

    def fit(self, X, y):
        # The smaller C, the stronger the regularization.
        # The more regularization, the more sparsity.
        self.transformer_ = svm.LinearSVC(penalty=""l1"",
                                      dual=False, tol=1e-3)
        X = self.transformer_.fit_transform(X, y)
        return svm.LinearSVC.fit(self, X, y)

    def predict(self, X):
        X = self.transformer_.transform(X)
        return svm.LinearSVC.predict(self, X)

def main():",main.py,ebertti/nospam,1
"# We use the default selection function: the 10% most significant features
selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X, y)
scores = -np.log10(selector.pvalues_)
scores /= scores.max()
plt.bar(X_indices - .45, scores, width=.2,
        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')

###############################################################################
# Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(X, y)

svm_weights = (clf.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()

plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',
        color='navy')

clf_selected = svm.SVC(kernel='linear')",projects/scikit-learn-master/examples/feature_selection/plot_feature_selection.py,DailyActie/Surrogate-Model,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the chi-square score of each feature
        score = chi_square.chi_square(X, y)

        # rank features in descending order according to score
        idx = chi_square.feature_ranking(score)
",skfeast/example/test_chi_square.py,jundongl/scikit-feast,1
"    >>> from sklearn import svm
    >>> from sklearn.datasets import samples_generator
    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline
    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)
    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svm
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
    >>> prediction = anova_svm.predict(X)
    >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS",projects/scikit-learn-master/sklearn/pipeline.py,DailyActie/Surrogate-Model,1
"    print ""4) None""
    print """"
    sel = raw_input(""Please enter the number of the feature selector you want to use: "")
    if sel == ""1"":
        sel = raw_input(""Please enter the number of the features you want to use: "")
        selector = SelectKBest(f_classif, k=int(sel)).fit(walking_data_set, walking_labels)
    if sel == ""2"":
        clf = ExtraTreesClassifier()
        selector = clf.fit(walking_data_set, walking_labels)
    if sel == ""3"":
        selector = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(walking_data_set, walking_labels)
    if sel == ""4"":
        selector = None

    if selector is not None:
        walking_data_set = selector.transform(walking_data_set)

    print ""Training personal classifier.""
    print ""...""
    print """"",main.py,BavoGoosens/Gaiter,1
"    return int(1+numpy.log2(n))

if __name__ == '__main__':
    d = load_iris()
    X = d.data
    X = StandardScaler().fit_transform(X)
    y = d.target
    _n = X.shape[0]

    # http://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use
    clf = SVC()
    params = {
            'C': 2**numpy.linspace(-5,15),
            'gamma': 2**numpy.linspace(-15,3),
            'class_weight': [None, 'auto'],
            }
    cv = RandomizedSearchCV(clf, params, n_iter=1000, cv=best_cv_num(_n), n_jobs=2, verbose=1)
    cv.fit(X, y)
    print(cv.best_score_)
    print(cv.best_params_)",iris/ml.py,arosh/ml,1
"
		print 'adding training vectors'
		for p in train_keys:
			X.append(p_ppr[p])
			if p.find('.male.') > 0:
				y.append(1)
			else:
				y.append(0)
			
		### classifiersV
		#clf = svm.SVC(kernel='linear')
		#clf = svm.SVC(kernel='rbf', C=10000, gamma=0.1)
		#clf = svm.SVC(kernel='rbf', C=10, gamma=10)    
		clf = AdaBoostClassifier(DecisionTreeClassifier(criterion='gini', max_depth=conf['classification']['dtree_depth'], max_features=None, min_density=None, min_samples_leaf=1, min_samples_split=2), algorithm=""SAMME"", n_estimators=conf['classification']['ada_n_estimators'])
		
		print 'training classifier'
		clf.fit(X, y)  

		print 'testing'
		counter = 0.",train_and_test_binary_v2.py,jimbotonic/df_nlp,1
"droplist = 'Survived PassengerId Age_Known Cabin_Known'.split()
data = training_data.drop(droplist, axis=1)
# Define features and target values
X, y = data, training_data['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

# Set the parameters by cross-validation
# param_dist = {'C': scipy.stats.uniform(0.1, 1000), 'gamma': scipy.stats.uniform(.001, 1.0),
#   'kernel': ['rbf'], 'class_weight':['balanced', None]}
#
# clf = SVC()
#
# # run randomized search
# n_iter_search = 1000
# random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
#                                    n_iter=n_iter_search, n_jobs=-1, cv=4)
#
# start = time()
# random_search.fit(X, y)
# print(""RandomizedSearchCV took %.2f seconds for %d candidates""",titanic_tuned_SVM_random_search.py,michael-hoffman/titanic,1
"from sklearn.svm import *
import sklearn.cross_validation as skcv

def naive_svc_train(X, Y):
	Xtrain, Xtest, Ytrain, Ytest = skcv.train_test_split(X, Y, train_size=0.75)
	trainer = SVC(cache_size=1024)
	
	classifier = trainer.fit(Xtrain, Ytrain)
	Ypred = trainer.predict(Xtest)

	return classifier, Ypred, Ytest
",project2/naive_svc.py,ethz-nus/lis-2015,1
"from text.document import Document
from text.sentence import Sentence

pp = pprint.PrettyPrinter(indent=4)
text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(7,20), min_df=0.2, max_df=0.5)),
                             #('vect', CountVectorizer(analyzer='word', ngram_range=(1,5), stop_words=""english"", min_df=0.1)),
                             #     ('tfidf', TfidfTransformer(use_idf=True, norm=""l2"")),
                                  #('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(6,20))),
                                  #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=5, random_state=42)),
                                  #('clf', SGDClassifier())
                                  #('clf', svm.SVC(kernel='rbf', C=10, verbose=True, tol=1e-5))
                                  #('clf', RandomForestClassifier(n_estimators=10))
                                    #('feature_selection', feature_selection.SelectFromModel(LinearSVC(penalty=""l1""))),
                                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))
                                  #('clf', DummyClassifier(strategy=""constant"", constant=True))
                                 ])
class SeeDevCorpus(Corpus):
    """"""
    Corpus for the BioNLP SeeDev task
    self.path is the base directory of the files of this corpus.",src/reader/seedev_corpus.py,AndreLamurias/IBEnt,1
"class TestSimpleLogicWithSklearnSVM(unittest.TestCase):
    """"""
    Tets whether sklearn SVM behaves identically using its
    internal implementation of basic kernels and our implementation
    when facing simple binary logic problems
    """"""

    def setUp(self):
        self.datasets = [baseline_logic(operator) for operator in
                        (logical_or, logical_and)]
        self.models = [(SVC(kernel=Linear(), C=1000),
                        SVC(kernel='linear', C=1000)),

                       (SVC(kernel=Polynomial(bias=1, degree=2), C=1000),
                        SVC(kernel='poly', C=1000, coef0=1, degree=2)),

                       (SVC(kernel=RBF(), C=1000),
                        SVC(kernel='rbf', C=1000))]

    def tearDown(self):",tests/basic.py,gmum/pykernels,1
"y_30[rng.rand(len(y)) < 0.3] = -1
y_50 = np.copy(y)
y_50[rng.rand(len(y)) < 0.5] = -1
# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
ls30 = (label_propagation.LabelSpreading().fit(X, y_30),
        y_30)
ls50 = (label_propagation.LabelSpreading().fit(X, y_50),
        y_50)
ls100 = (label_propagation.LabelSpreading().fit(X, y), y)
rbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['Label Spreading 30% data',",projects/scikit-learn-master/examples/semi_supervised/plot_label_propagation_versus_svm_iris.py,DailyActie/Surrogate-Model,1
"classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))


###############################################################################
print(""PREPARE CLASSIFICATION"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
if fs_n < 99.00:
    fs = SelectPercentile(f_classif, percentile=fs_n)
elif fs_n > 99 and fs_n < 101:
    fs = SelectKBest(f_classif, k=n_features)",JR_toolbox/skl_ica.py,cjayb/kingjr_natmeg_arhus,1
"
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

svm = SVC(kernel='linear', C=1.0, random_state = 0)
svm.fit(X_train_std, y_train)

PlotFigures.plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150))
plt.show()

#for X_test_val in X_test_std:
#    print(lr.predict_proba(X_test_val.reshape(1, -1)))",Chapter3/SVM.py,southpaw94/MachineLearning,1
"	print ""training...""

	C = 18292
	gamma = 1.88e-07
	shrinking = True
	#auto_class_weights = False

	probability = True
	verbose = True

	svc = SVC( C = C, gamma = gamma, shrinking = shrinking, probability = probability, verbose = verbose )
	svc.fit( x_train, y_train )
	p = svc.predict_proba( x_test )

	p = p[:,1] 

	ids_and_p = np.hstack(( ids.reshape(( -1, 1 )), p.reshape(( -1, 1 ))))
	np.savetxt( output_file, ids_and_p, fmt = [ '%d', '%.10f' ], delimiter = ',', header = 'UserID,Probability1', comments = '' )
	
",amelia/vectorize_and_predict.py,zygmuntz/kaggle-happiness,1
"    Output
    ------
    F: {numpy array}, shape (n_features, )
        index of selected features
    """"""

    n_samples, n_features = X.shape
    # using 10 fold cross validation
    cv = KFold(n_samples, n_folds=10, shuffle=True)
    # choose SVM as the classifier
    clf = SVC()

    # selected feature set, initialized to be empty
    F = []
    count = 0
    while count < n_selected_features:
        max_acc = 0
        for i in range(n_features):
            if i not in F:
                F.append(i)",skfeature/function/wrapper/svm_forward.py,jundongl/scikit-feature,1
"
    Returns:
        best_params -- values for C and gamma that gave the highest accuracy with
                       cross-validation
        best_score -- highest accuracy with cross-validation
    """"""
    C_range = np.logspace(-2, 10, 13)
    gamma_range = np.logspace(-9, 3, 13)
    param_grid = dict(gamma=gamma_range, C=C_range)
    cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)
    grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
    grid.fit(X, y)
    print(""The best parameters are %s with a score of %0.2f""
        % (grid.best_params_, grid.best_score_))
    return grid.best_params_, grid.best_score_

def train_test_song_split(samples,labels,song_IDs,train_size,test_size = 0.2):
    """"""train_test_song_split:
    Splits samples from songs into training and test sets
",svm_rbf_test_utility_functions.py,NickleDave/hybrid-deep-finch,1
"        C_values = [1, 2, 8, 32, 128, 512, 2048, 8192]
        gamma_values = [0.001,0.01,0.05,0.1,0.3,0.5,0.7]

        #C_values = [2]
        #gamma_values = [0.5]

        best_C = 0
        best_G = 0
        best_percentage = 0

        #clf = svm.SVC(C=2.0,gamma=0.5)

        C_range = np.logspace(-2, 10, num=13, base=2)
        gamma_range = np.logspace(-5, 1, num=7, base=10)
        param_grid = dict(gamma=gamma_range, C=C_range)
        cv = StratifiedShuffleSplit(trainingLabels, n_iter=3, test_size=0.11, random_state=42)
        grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
        grid.fit(trainingSet, trainingLabels)
        #C_range = np.logspace(-1, 1, num=2, base=2)
        #gamma_range = np.logspace(-1, 1, num=2, base=10)",reference_test/VLC/CrossValidate.py,JessMcintosh/EMG-classifier,1
"    #=============================


    x1 = df_train['ACCELEROMETER_X'].values
    x2 = df_train['ACCELEROMETER_Y'].values
    x3 = df_train['ACCELEROMETER_Z'].values

    y = df_train['state'].values
    X = np.column_stack([x1, x2, x3])

    clf = svm.SVC()
    clf.fit(X, y)

    #=============================
    # RUN DATA AGAINST THE MODEL
    #=============================

    # Load the pandas dataframe from the DB using the experiment id
    pandas_id = id
    current_app.logger.debug('Preparing to make prediction for experiment: {}'.format(pandas_id))",app/science2.py,ChristopherGS/sensor_readings,1
"    # write_libsvm(pos_vec, neg_vec, write_filename)

    # Merge positive and negative feature vectors and generate their corresponding labels.
    vec = np.array(pos_vec + neg_vec)
    vec_label = np.array([1] * len(pos_vec) + [0] * len(neg_vec))

    # ##############################################################################
    # Classification and accurate analysis.

    # Use 5-fold cross-validation to evaluate the performance of the predictor.
    clf = svm.SVC(C=32768.0, gamma=0.001953125)
    scores = cross_validation.cross_val_score(clf, vec, y=vec_label, cv=5)
    print('Per accuracy in 5-fold CV:')
    print(scores)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

    # ###############################################################################
    # Classification and ROC analysis.

    # evaluate performance of the predictor by 5-fold cross-validation and plot the mean ROC curve.",repDNA/example/example2.py,liufule12/repDNA,1
"###############################################################################
def train_and_test_data( A, y, c ):
    ''' train model from given data (A) and labels (y)
        use cross validation to estimate power of model at FPR=0.05
     '''

    # normalize each sample A[i] s.t. it has unit norm
    A_norm = preprocessing.normalize( A )

    # classifier
    clf = svm.SVC( kernel=kernel, probability=True, C=c, cache_size=500 ) 

    # prep
    mean_tpr = 0.0
    mean_fpr = np.linspace( 0, 1, 100 )
    cv = StratifiedKFold( y, indices=False, n_folds=K ) # c.v. partition
    
    # mean ROC
    for i, (train, test) in enumerate(cv):
",CFPselect_train.py,rronen/HAF-score,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half5.py,mirjalil/ml-visual-recognition,1
"
# Our key parameters here are the penalty term, and the best k features from the univariate analysis

# In[84]:

score=0

for x in range(10,43):
    for y in np.linspace(.1,.5,5):
        var_filter=SelectKBest(f_classif)
        clf=svm.SVC(kernel='rbf')
        pipe_svm = Pipeline([('anova', var_filter), ('svc', clf)])
        pipe_svm.set_params(anova__k=x, svc__C=y)
        score_test = cross_validation.cross_val_score(pipe_svm, titanic_features, titanic_target, n_jobs=1,                                                        cv=StratifiedKFold(titanic_target, n_folds=10, shuffle=True, random_state=7132016))
        if score_test.mean()>score:
            score=score_test.mean()
            k_out=x
            C_out=y
            
print k_out",Final_setup_SVM.py,aaschroeder/Titanic_example,1
"    Example
    -------
    >>> from sklearn.svm import SVC
    >>> from sklearn.lda import LDA
    >>> from sklearn.feature_selection import SelectKBest
    >>> from epac.workflow.factory import NodeFactory
    >>> from epac.workflow.pipeline import __insert_node_at_leaf
    >>>
    >>> parent_node = NodeFactory.build(SelectKBest())
    >>> child_node = NodeFactory.build(LDA())
    >>> child_of_child_node = NodeFactory.build(SVC())
    >>>
    >>> __insert_node_at_leaf(child_node, parent_node)
    >>> __insert_node_at_leaf(child_of_child_node, parent_node)
    >>>
    >>> for node in parent_node.walk_nodes():
    ...     print node
    ...
    SelectKBest
    SelectKBest/LDA",epac/workflow/pipeline.py,neurospin/pylearn-epac,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIM.mim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_MIM.py,jundongl/scikit-feast,1
"    news_train_tf = tfidf.fit_transform(news_train)
    
    print dir(tfidf)
    feature_names = tfidf.get_feature_names()
    import numpy as np
    print np.shape(feature_names)
    print np.shape(news_train_tf)
    print np.shape(news_train)
    
    from sklearn.svm import LinearSVC
    clf = LinearSVC().fit(news_train_tf,y_train)
    trained_coefs = clf.coef_[0]
    
    print np.shape(trained_coefs)
    
    inds_max = trained_coefs.argsort()[-20:][::-1]
    inds_min = trained_coefs.argsort()[:20]
    print type(inds_max)
    feature_names = np.array(feature_names)
    for feature_name, coef in zip(feature_names[inds_min],trained_coefs[inds_min]):",machine_learning/picking_apart_learning.py,chrisjdavie/shares,1
"
# Predict can return an array. In this case EPAC will
# put the prediction in a Result (a dictionnary). with key = ""y/pred"". y being the
# difference between input agrument of fit and predict. The true y will also figure
# in the result with key ""y/true""
class MySVM:
    def __init__(self, C=1.0):
        self.C = C
    def fit(self, X, y):
        from sklearn.svm import SVC
        self.svc = SVC(C=self.C)
        self.svc.fit(X, y)
    def predict(self, X):
        return self.svc.predict(X)

svms = Methods(MySVM(C=1.0), MySVM(C=2.0))
cv = CV(svms, cv_key=""y"", cv_type=""stratified"", n_folds=2,
        reducer=None)
cv.run(X=X, y=y)  # top-down process to call transform
cv.reduce()       # buttom-up process",examples/design_new_plugin.py,neurospin/pylearn-epac,1
"            testN = testSizePositive + testSizeNegative
            testPos = choice(positive_examples,testSizePositive).tolist()
            testNeg = choice(negative_examples,testSizeNegative).tolist()
            test = vectors.loc[testPos+testNeg]
            test_Y = [term if x in testPos else ""not %s"" %(term) for x in test.index]
            trainPos = [x for x in positive_examples if x not in testPos]
            trainNeg = [x for x in negative_examples if x not in testNeg]
            trainN = len(trainPos) + len(trainNeg)
            train = vectors.loc[trainPos+trainNeg]
            train_Y = [term if x in trainPos else ""not %s"" %(term) for x in train.index]
            clf = svm.SVC(kernel=kernel)
            clf.fit(train, train_Y)
            # Make a single prediction based on vectors
            predictions=[]
            for t in test.index:
                predictions.append(clf.predict(test.loc[t])[0])
            correct = float(len([x for x in range(len(predictions)) if predictions[x]==test_Y[x]]))
            accuracy = correct / len(test_Y)
            positive_predictions = [x for x in range(len(predictions)) if predictions[x] == term]
            negative_predictions = [x for x in range(len(predictions)) if predictions[x] == ""not %s"" %term]",wordfish/models.py,word-fish/wordfish-python,1
"### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()




#########################################################
### your code goes here ###

from sklearn import svm
clf = svm.SVC(kernel=""rbf"",C=10000)
t0=time()

#reduce size
#features_train = features_train[:len(features_train)/100]
#labels_train = labels_train[:len(labels_train)/100]

clf.fit(features_train,labels_train)
print ""training time:"", round(time()-t0,3),""s""
",ud421-projects/svm/svm_author_id.py,sinanh/udacity,1
"

class skLearn_svm(Strategy):
    def __init__(self):
        self.trainer = ""skLearn svm""
        print(""Using %s Classifier"" % (self.trainer))

    def train_model(self, train_file_path, model_path):
        train_X, train_y = self.load_file(train_file_path)

        clf = svm.LinearSVC()

        print(""==> Train the model ..."")
        clf.fit(train_X, train_y)

        print(""==> Save the model ..."")
        pickle.dump(clf, open(model_path, 'wb'))

        return clf
",stst/classifier.py,fssqawj/classification_task,1
"           and platform.architecture()[0] == '64bit' \
           and self.oBuild.sKind == 'development' \
           and os.getenv('VERSIONER_PYTHON_PREFER_32_BIT') != 'yes':
            print ""WARNING: 64-bit python on darwin, 32-bit VBox development build => crash""
            print ""WARNING:   bash-3.2$ /usr/bin/python2.5 ./testdriver""
            print ""WARNING: or""
            print ""WARNING:   bash-3.2$ VERSIONER_PYTHON_PREFER_32_BIT=yes ./testdriver""
            return False;

        # Start VBoxSVC and load the vboxapi bits.
        if self._startVBoxSVC() is True:
            assert(self.oVBoxSvcProcess is not None);

            sSavedSysPath = sys.path;
            self._setupVBoxApi();
            sys.path = sSavedSysPath;

            # Adjust the default machine folder.
            if self.fImportedVBoxApi and not self.fUseDefaultSvc and self.fpApiVer >= 4.0:
                sNewFolder = os.path.join(self.sScratchPath, 'VBoxUserHome', 'Machines');",src/VBox/ValidationKit/testdriver/vbox.py,carmark/vbox,1
"        """"""
        :param train_data:
        :type train_data: ModelTrainerInput
        """"""

        probability = self.params['svm_reg']['probability']

        if self.params['svm_reg']['balanced_class_weights']:
            svmc = SVC(kernel='rbf', probability=probability, class_weight='balanced')
        else:
            svmc = SVC(kernel='rbf', probability=probability)

        Cs = self.params['svm_reg']['Cs']
        gammas = self.params['svm_reg']['gammas']
        out_filename = self.params['general']['scratch_directory'] + ""/"" + self.params['model_training']['output_model']

        print ""training model with SVM and grid search (%d combinations)..."" % (len(Cs) * len(gammas))
        print ""using ANOVA feature selection""
        print ""training set size: %d, # of features: %d"" % (len(train_data.labels), train_data.features.shape[1])
        print ""gammas: "", gammas",scripts/acf/tr_svm_reg.py,pymir3/pymir3,1
"y = data['y'][0]
X = data['X']

# Replace class label -1 with 0
y[y < 0] = 0

# Use shape to see the dimensions of the data
print(X.shape, y.shape)

# Load Support Vector Classifier
clf = svm.SVC(kernel=""linear"")
# # Fit the training data
clf.fit(X.T, y)

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-1, 1)
yy = a * xx - (clf.intercept_[0]) / w[1]

",tutorial/python/ml.py,Bubblbu/ssip_2016_team_d,1
"from scipy.io import savemat


def return_best_svm(X, Y, N, C, penalties):
    """"""
    Returns the best model for X data and Y targets
    """"""
    skf = StratifiedKFold(Y, N)

    # We define the logistic regression
    lg = SVC(tol=0.0001)

    param_grid = [{'C': C, 'kernel': penalties}]

    rsearch = GridSearchCV(estimator=lg, param_grid=param_grid, cv=skf)
    rsearch.fit(X, Y)
    
    return rsearch.best_estimator_, rsearch.best_score_, rsearch

",4_day/svm_save.py,h-mayorquin/g_node_data_analysis_205,1
"    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.5, random_state=42
    )

    # Set the parameters by cross-validation
    tuned_parameters = [
        {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}
    ]

    # Perform the grid search on the tuned parameters
    model = GridSearchCV(SVC(C=1), tuned_parameters, cv=10)
    model.fit(X_train, y_train)

    print(""Optimised parameters found on training set:"")
    print(model.best_estimator_, ""\n"")
    
    print(""Grid scores calculated on training set:"")
    for params, mean_score, scores in model.grid_scores_:
        print(""%0.3f for %r"" % (mean_score, params))",Document/szse/Quantitative Trading/sat-ebook-and-full-source-20150618/algo-ebook-full-source-code-20150618/chapter16/grid_search.py,Funtimezzhou/TradeBuildTools,1
"        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, datasets
    >>> from sklearn.model_selection import GridSearchCV
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",mlp/model_selection/_search.py,dwettstein/pattern-recognition-2016,1
"    print (""Loading the genre classifier"")
    genre_clf = joblib.load(CLASSIFIERS['GENRE']) 
    print (""Loading complete!"")
else:
    print (""Preparing the genre classifier"")
    Xs = clean_texts(dataframe['TEXT'])
    Ys = dataframe['GENRE']

    selected_classifiers = [(""Multinomial Naive Bayes Classifier"", MultinomialNB(alpha=.1)),\
                   (""Ridge Classifier"", RidgeClassifier(tol=1e-1, solver=""sag"")),\
                   (""SVM"", LinearSVC())]

    goto_pipeline = Pipeline ([
        ('vect', CountVectorizer(max_df=0.5, ngram_range=(1,2))),
        ('tfidf', TfidfTransformer(norm='l2', use_idf = True)),
        ('chi2', SelectKBest(chi2, k=3000)),
        ('clf', VotingClassifier(estimators=selected_classifiers[:], voting='hard', n_jobs=-1))
    ])

    goto_pipeline.fit(Xs, Ys)",ghost_main.py,mab1290/GHOST,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
         verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS",CRF/CLFL_mdf_classification.py,henchc/MHG_Scansion,1
"        if classifier is not None:
            self.clf = classifier

            from sklearn.svm import LinearSVC
            import random
            if isinstance(self.clf, LinearSVC):
                self.clf.set_params().random_state = random.randint(0, 200)
        else:
            if clf_method == 'SVM':
                from sklearn import svm
                self.clf = svm.SVC()
            elif clf_method == 'ERF':
                from sklearn.ensemble import ExtraTreesClassifier
                self.clf = ExtraTreesClassifier(n_estimators=100,
                                                max_depth=None, min_samples_split=1,
                                                random_state=0)
            elif clf_method == 'GBC':
                from sklearn.ensemble import GradientBoostingClassifier
                self.clf = GradientBoostingClassifier(n_estimators=100,
                                                      max_depth=1)",neurosynth/analysis/classify.py,neurosynth/neurosynth,1
"            sk_data['data'].append(text.strip())
    # divide data in training and testing samples            
    X_train, X_test, y_train, y_test = train_test_split(
        sk_data['data'], sk_data['target'], test_size=0.2, random_state=0)    

    # tokenizing and creating sparse array from words of text
    cv = CountVectorizer()
    cv_arr_train = cv.fit_transform(X_train)
    cv_arr_test = cv.transform(X_test)
    # simple default classifier with no tuning
    clf = svm.SVC()
    # training classifier, or fitting the data
    clf.fit(cv_arr_train, y_train)
    print('Classifier performance with default parameters:')
    for index, sample in enumerate(X_test):
        # convert text into sparse array of same dimension as training
        # set
        sample_arr = cv.transform([sample])
        print(""%s | %s"" % (sample, clf.predict(sample_arr)))
    print('')",svmClassifier.py,baali/svms,1
"    data_dict['y_test'] = y_test
    data_dict['X'] = X
    data_dict['y'] = _labels

    return data_dict


def train_simple_svm_model(data_dict):
    # Higher C seems to help with the desired overfitting
    # PCA speeds up a lot and helps with false predictions at ~500
    clf = SVC(C=10000000, gamma=0.00001)
    clf.fit(data_dict['X_train'], data_dict['y_train'])

    _pred = clf.predict(data_dict['X_test'])
    print accuracy_score(_pred, data_dict['y_test'].values)
    print confusion_matrix(_pred, data_dict['y_test'])

    print ""FINISHED SVM""
    return clf
",patterns/model.py,jsmithAP/CharttingTwitterBot,1
"        `sklearn.neighbors.base.KNeighborsMixin` that will be used to find
        the k_neighbors.

    out_step : float, optional (default=0.5)
        Step size when extrapolating. Used with kind='svm'.

    kind : str, optional (default='regular')
        The type of SMOTE algorithm to use one of the following options:
        'regular', 'borderline1', 'borderline2', 'svm'.

    svm_estimator : object, optional (default=SVC())
        If `kind='svm'`, a parametrized `sklearn.svm.SVC` classifier can
        be passed.

    n_jobs : int, optional (default=1)
        The number of threads to open if possible.

    Attributes
    ----------
    min_c_ : str or int",imblearn/over_sampling/smote.py,chkoar/imbalanced-learn,1
"	test_feature = np.array(list(r)[1:]);
	""""""
	# Test Script 

	digits = datasets.load_digits();
	train_feature = digits.data;
	train_target = digits.target;

	test_feature = digits.data[:20];
	""""""
	svc = svm.SVC();
	svc.fit(train_feature,train_target);

	cPickle.dump(svc,open('svm.pkl','wb'));

	result = svc.predict(test_feature);
	cPickle.dump(result,open('result.pkl','wb'));

	print result;
",DigitRecognizer/recongizer.py,dz1984/Kaggle,1
"    
    training_data = data[training_indices]
    training_labels = labels[training_indices]
    testing_data = data[testing_indices]
    testing_labels = labels[testing_indices]

    print(""%i training samples, %i testing samples"" % 
          (len(training_labels), len(testing_labels)))
    print(""="" * 50)
    
    my_svm = LinearSVC(C=1e0)
    my_svm.fit(training_data, training_labels)

    train_predictions = my_svm.predict(training_data)
    test_predictions = my_svm.predict(testing_data)

    print(""%.2f accuracy during training (%i predictions)"" % 
          (np.mean(train_predictions == training_labels), len(training_labels)))
    print(""%.2f accuracy during testing (%i predictions)"" % 
          (np.mean(test_predictions == testing_labels), len(testing_labels)))",src/py/deprecated/3/svm_classifier.py,leonardbj/rubik,1
"def loadData():

    # load the digits dataset
    digits = load_digits()
    X = digits.images.reshape((len(digits.images), -1))
    y = digits.target
    
    return digits, X, y

def createRFE():
    svc = SVC(kernel='linear', C=1)
    rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
    return rfe

def rankPixels(rfe, digits, X, y):
    rfe.fit(X, y)
    ranking = rfe.ranking_.reshape(digits.images[0].shape)
    return ranking

def plotRankPixels(ranking):",examples/scikit-learn/examples/general/recursive_feature_elimination.py,KellyChan/python-examples,1
"    learner = []

    pipeline = []

    ############################################################################

    if clf_or_regr == ""clf"":
        learner = GaussianNB()
        learner = DecisionTreeClassifier()
        #learner = RandomForestClassifier()
        #learner = SVC()
        #learner = LogisticRegression()

    if clf_or_regr == ""regr"":
        learner = LinearRegression()
        #learner = Ridge()
        #learner = Lasso()
        #learner = BayesianRidge()
        #learner = SGDRegressor()
        #learner = SVR()",python/py/quant/clf_or_regr.py,austinjalexander/sandbox,1
"                
            #fill in missing values
            imputer = Imputer(missing_values=self.null_value, strategy='mean')

            #Uses all defaults including an radial basis function (rbf)
            #kernal. Refer to this site: http://scikit-learn.org/stable/modules/svm.html
            #Would also be worthwile to investigate sklearn.svm.LinearSVC and sklearn.linear_model.SGDClassifier
            
            
            #Tried class_weight=""auto"" but it produced a worse fit 38%
            clf = svm.SVC() #currently at 66%
            
            grid_search = GridSearchCV(clf, param_grid=self.params)
            
            #classifier = DecisionTreeClassifier()  #produced 30% accuracy    
            classifier = Pipeline([('imputer', imputer), ('clf', grid_search)])
            
            classifier.fit(X, y)
            
            #This is here to preserve the train_time from the initial ",crimebusters/build.py,georgetown-analytics/dc-crimebusters,1
"    print ""random forest classifier score: %f"" %score_rfc
    
    #logistic regression
    logreg = LogisticRegression()
    logreg.fit(X_train, y_train)
    score_logreg = logreg.score(X_train, y_train)
    out_logreg = logreg.predict(X_test)
    print ""logistic regression score: %f"" %score_logreg
        
    #SVM
    svc = SVC()
    svc.fit(X_train, y_train)
    score_svc = svc.score(X_train, y_train)
    out_svc = svc.predict(X_test)    
    print ""SVM score: %f"" %score_svc    
    
    #knn classifier
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, y_train)
    score_knn = knn.score(X_train, y_train)",titanic/titanic_kernel.py,vsmolyakov/kaggle,1
"
targets_tr = traindf['cuisine']

#clf = ExtraTreesClassifier(n_estimators=100, max_depth=None min_samples_split=1, random_state=0, verbose= 2,n_jobs = -1)
penalty = [""l1""]#, ""l2""]

for j in penalty:
    for i in range(1,100):
        p = i*0.1
        print p
        clf = LinearSVC(C=p, penalty=j, dual=False)
#clf = GaussianNB()
#clf = svm.SVC(verbose= 2)
        clf = clf.fit(predictor_tr, targets_tr)
        scores = cross_val_score(clf, predictor_tr, targets_tr)
        print scores.mean()
'''
corpusts = testdf['ingredients_string']
tfidfts=vectorizertr.transform(corpusts).todense()
",second.py,RomainGefffraye/kaggle_yummly,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0521_2015.py,magic2du/contact_matrix,1
"y = [1] * 10 + [-1] * 10
sample_weight_last_ten = abs(np.random.randn(len(X)))
sample_weight_constant = np.ones(len(X))
# and bigger weights to some outliers
sample_weight_last_ten[15:] *= 5
sample_weight_last_ten[9] *= 15

# for reference, first fit without class weights

# fit the model
clf_weights = svm.SVC()
clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)

clf_no_weights = svm.SVC()
clf_no_weights.fit(X, y)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))
plot_decision_function(clf_no_weights, sample_weight_constant, axes[0],
                       ""Constant weights"")
plot_decision_function(clf_weights, sample_weight_last_ten, axes[1],",projects/scikit-learn-master/examples/svm/plot_weighted_samples.py,DailyActie/Surrogate-Model,1
"    estimated_hgts = learner.predict(estmat)
    print estimated_hgts
    print learner
    return estimated_hgts


def run(ngenes=50, ntaxa=5):
    testdata = gen_test_data(ntaxa=ntaxa, ngenes=ngenes, nreps=1)

#    learner = ensemble.RandomForestClassifier(n_estimators=100)
    learner = svm.SVC()

    ehgt = pipeline(testdata[0][0], testdata[1][0], testdata[2][0], testdata[3][0], testdata[4], learner = learner)

    print ehgt

    print testdata[3]
    sources = [i[0] for i in testdata[3][0]]

    print sources",simhgt.py,pranjalv123/hgt-nn,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",examples/plot_classifier_comparison.py,B3AU/waveTree,1
"                     map(os.path.split,
                         map(os.path.dirname, labels)))  # Get the directory.
        fname = ""{}/reps.csv"".format(args.workDir)
        embeddings = pd.read_csv(fname, header=None).as_matrix()
        le = LabelEncoder().fit(labels)
        labelsNum = le.transform(labels)
        nClasses = len(le.classes_)
        print(""Training for {} classes."".format(nClasses))

        if clfChoice == 'LinearSvm':
            clf = SVC(C=1, kernel='linear', probability=True)
        elif clfChoice == 'GMM':  # Doesn't work best
            clf = GMM(n_components=nClasses)

        # ref:
        # http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#example-classification-plot-classifier-comparison-py
        elif clfChoice == 'RadialSvm':  # Radial Basis Function kernel
            # works better with C = 1 and gamma = 2
            clf = SVC(C=1, kernel='rbf', probability=True, gamma=2)
        elif clfChoice == 'DecisionTree':  # Doesn't work best",openface1/evaluation/lfw-classification-unknown.py,EliHar/Pattern_recognition,1
"
    def __buid_grid_search(self, lcat):

        # -- build set the parameters for grid search
        log2c = np.logspace(-5, 20, 16, base=2).tolist()
        log2g = np.logspace(-15, 5, 16, base=2).tolist()

        search_space = [{'kernel': ['rbf'], 'gamma': log2g, 'C': log2c, 'class_weight': ['auto']}]
        # search_space += [{'kernel':['linear'], 'C':log2c, 'class_weight':['auto']}]

        grid_search = GridSearchCV(SVC(random_state=self.__seed), search_space, cv=10, scoring='roc_auc', n_jobs=self.n_jobs)

        return grid_search

    def __one_svm(self, cat):

        lcat = np.zeros(self.train_set['labels'].size)

        lcat[self.train_set['labels'] != cat] = -1
        lcat[self.train_set['labels'] == cat] = +1",antispoofing/spectralcubes/classification/svm.py,allansp84/spectralcubes,1
"def inv_log10(x):
    return 10**x
    
    
    
class SVC(BaseLearner):
    
    def learn(self, ds ):
        self.param_dict['max_iter']  =1e7
        
        return svm.SVC( **self.param_dict ).fit(ds.x, ds.y)


svc_grid = [
    VariableMap('C', -2, 5, inv_log10 ),
    VariableMap('gamma', -8, 3, inv_log10 )
    ]


",spearmint_salad/old/learners.py,recursix/spearmint-salad,1
"        Y = dataset[:, -1]
        X_train = X[:num_train]
        Y_train = Y[:num_train]
        X_test = X[-num_test:]
        Y_test = Y[-num_test:]
        gammaSteps = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]
        CSteps = [1, 3, 10, 30, 100, 300, 1000, 3000]
        maxScore = 0
        optC = 0
        optGamma = 0
        optClf = svm.SVC()
        self.stdout.write('Tuning parameters...')
        for tempC in CSteps:
            for tempGamma in gammaSteps:
                clf = svm.SVC(C=tempC, gamma=tempGamma)
                clf.fit(X_train, Y_train)
                score = clf.score(X_test, Y_test)
                self.stdout.write(
                    ""C: {0}, gamma: {1}, score: {2}"".format(
                        tempC, tempGamma, score))",svm/management/commands/svm.py,jayArnel/nursery-application,1
"from sklearn.externals import joblib

''' loading the digits data set: '''
digits = datasets.load_digits()
print(digits.data)

''' conversion to input and output: '''
X, y = digits.data, digits.target

''' training a linear SVM model: '''
clf = svm.SVC(gamma=0.001, C=100.)
clf.fit(X[:-1], y[:-1])

''' prediction: '''
yp = clf.predict(X[-1:])
print(""prediction: "", yp, ""  actual:"", y[-1:])

''' saving the model to a file: '''
joblib.dump(clf, 'digits_model.pkl')
",scikit-learn/scikit-learn-intro/main.py,balazssimon/ml-playground,1
"        fd, hogImage = hog(pixels, orientations=8, pixels_per_cell=(4, 4),cells_per_block=(16, 8), visualise=True)
        result.append(np.ravel(hogImage))
    return result

#training SVM
positiveSamples=getNorPositive()
negativeSamples=getNorNegative()
mergedSamples=positiveSamples+negativeSamples
data=np.zeros(len(mergedSamples))
data[0:25]=1
LinearClf = svm.LinearSVC()
print 'fitting',len(mergedSamples)
LinearClf.fit(mergedSamples, data)
joblib.dump(LinearClf, '/home/richard/FemurDatabase/LinearClf1.pkl') 
print 'finished'




",femurDetection/src/extractRoi/training.py,Dearbigdog/python,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",bp-revok/python/lib/python2.7/email/test/test_email_renamed.py,fkolacek/FIT-VUT,1
"		self.min_max_scaler = preprocessing.MinMaxScaler()
		X_train = self.min_max_scaler.fit_transform(X_train)
		print(self.min_max_scaler.scale_, self.min_max_scaler.min_)
		# 得到label Y
		Y_train = self.LC.construct(dataset)

		#for (i,d) in enumerate(dataset):
		#	print(d, X_train_minmax[i], Y_train[i])

		# 用SVM拟合数据
		self.svm = SVC()
		self.svm.fit(X_train, Y_train)

		print(""Linear SVM Model:"", self.svm)



	def predict(self, dataset):
		k, d, j = self.KDJ.current(dataset[-1], True)
		y = self.svm.predict(self.min_max_scaler.transform([k,d,j]))[0]",strategy/model/kdj.py,zsffq999/helloworld,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",net-p3/lib/python3.5/site-packages/sklearn/metrics/tests/test_classification.py,uglyboxer/linear_neuron,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",python/src/Lib/email/test/test_email.py,matmutant/sl4a,1
"    test = pca.transform(test)
    return train, test

def normalize(train, test):
    norm = preprocessing.Normalizer()
    train = norm.fit_transform(train)
    test = norm.transform(test)
    return train, test

def createSVM():
    clf = SVC()
    return clf

def createKNN():
    clf = KNeighborsRegressor(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeRegressor(max_depth=None, min_samples_split=1, random_state=0)
    return clf",kaggle-burn-cpu-burn/src/predict.py,KellyChan/Kaggle,1
"	print testX.shape

	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)

	#fit the svm model and compute accuaracy measure
	clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	#clf = svm.SVC(kernel='linear')
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithTreeFS/mnistBackImage/mnistIMAGE1Layer.py,akhilpm/Masters-Project,1
"                    else :
                        train_indices, test_indices = train_test_split(indices,
                                                                       train_size=self.config.learning_split,
                                                                       random_state=seed)
                    train_labels = [self.max_labels[i] for i in train_indices]
                    train_segments = [self.segments[i] for i in train_indices]
                    C_range     = numpy.logspace(-10, 10, 20)
                    gamma_range = numpy.logspace(-10, 10, 20)
                    param_grid = dict([(""gamma"", gamma_range), (""C"", C_range)])
                    cv = StratifiedShuffleSplit(train_labels, n_iter=5, test_size=0.2, random_state=seed)
                    grid = GridSearchCV(svm.SVC(), param_grid=param_grid, cv=cv)
                    grid.fit(train_segments, train_labels)
                    print(""RBFKernel : The best parameters are %s with a score of %0.2f""
                          % (grid.best_params_, grid.best_score_))
                    self.config.learning_C = grid.best_params_['C']
                    self.config.kernel_gamma = grid.best_params_['gamma']
                    self.gamma = grid.best_params_['gamma']
                    self.kernel_fun = RBFEntryWrapper(self.gamma)
                except ValueError as e :
                    print ""ValueError (%s) in cv search, trying again (%x)"" % (e, seed)",python/persistence/RBFKernel.py,gpersistence/tstop,1
"	def train_tree(self):
		self.clf = tree.DecisionTreeClassifier(min_samples_split=len(self.x) / 4,criterion='entropy', class_weight='balanced')
		# with open(self.model_path, 'rb') as f:
		# 	self.clf = pickle.load(f)

		self.clf.fit(self.x, self.y)
		self.pred = self.clf.predict(self.x)

	def train_svm(self):
		from sklearn import svm
		self.clf = svm.SVC()
		self.clf.fit(self.x, self.y)
		self.pred = self.clf.predict(self.x)

	def train_forest(self):
		self.clf = RandomForestClassifier(min_samples_split=len(self.x) / 8, class_weight='balanced')
	
	def display_accuracy(self):
		from sklearn.metrics import classification_report
		print(classification_report(self.y, self.pred))",tigertrace/tasks/train.py,tartavull/tigertrace,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(1,101):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=496.6,gamma=0.00767,probability=True, cache_size=7000)
        #args=[str(dim)+ ""Dgauss_dt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        clf=""This shouldnt be used as we are in Keras mode""
	args=[str(dim)+""Dgauss_nn_4layers_100neurons_onehot"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0,100,4]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,1,args)",Dalitz_simplified/evaluation_of_optimised_classifiers/nn_gauss/nn_Gauss_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_15_2015_01.py,magic2du/contact_matrix,1
"    ""Min. Temp (C)"", ""Median Temp (C)"", ""Max. Temp (C)""]
bands = [""TreeCover"", ""Soil-Cover"", ""Veg-Cover"", ""Impervious-Cover"", ""Temp-Min"", 
    ""Temp-Median"", ""Temp-Max""]
output_files = []
for band in bands:
    output_files.append(""{base}CR-{band}-plot.png"".format(base = base, band = band))
cols = aei.color.color_blind(len(bands))

# set the models to apply
models = [""DecisionTree"", ""SVM"", ""RandomForest"", ""GradientBoosting""]
mods = [tree.DecisionTreeClassifier(), svm.SVC(), ensemble.RandomForestClassifier(),
    ensemble.AdaBoostClassifier()]
output_models = []
for model in models:
    output_models.append(""{base}CR-southern-prediction-{model}.tif"".format(base = base, model = model))

# grab the indices for the aedes aegyptii and field plot data
fs_cb_band = fs_cb_ref.ReadAsArray()
fs_cb = np.where(fs_cb_band == 1)
fs_cb_band = None",bin/mosquito-plot-histograms.py,christobal54/aei-grad-school,1
"
kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']
degrees = [1,2,3,4,5]
cost = [0.01,0.1,1,10,100]

for k in kernels:
    for c in cost:
        if k == 'poly':
            for d in degrees:
                print(""===== Kernel: %s, Cost: %f, Degree: %d =====""%(k, c, d))
                svc = svm.SVC(kernel=k, C=c, degree=d)
                fit = svc.fit(data, labels)
                score = svc.score(data, labels)
                print(""Training Score: %f""%(score))
                print(""Cross Validation Score: %f""%(cross_val(data, labels, 10, rounds, svc)))
        else:
                print(""===== Kernel: %s, Cost: %f =====""%(k, c))
            svc = svm.SVC(kernel=k, C=c)
            fit = svc.fit(data, labels)
            score = svc.score(data, labels)",project/code/analysis/reviews_svm.py,cycomachead/info290,1
"
	# Split the dataset in train and test sets
	X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(
		X, Y, test_size=0.25, random_state=0)

	# Do grid search CV to find the best parameters for c and gamma
	c_range = 10.0 ** np.arange(-2,9)
	gamma_range = 10.0 ** np.arange(-5,4)
	param_grid = dict(gamma=gamma_range, C=c_range)
	cv = cross_validation.StratifiedKFold(y=Y_train, n_folds=3)
	grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
	grid.fit(X_train,Y_train)
	print ""Test error after grid search CV: {0}"".format(grid.score(X_test, Y_test))
	
	# Do a simple SVM
	clf_rbf = SVC()
	clf_rbf.fit(X_train,Y_train)
	print ""Test error without proper parameter search: {0}"".format(clf_rbf.score(X_test,Y_test))

	# Compute Confusion Matrix",run_experiment.py,TobiasMR/emotion-recognition-from-posture,1
"elif folding == 'kfolding':
    cv = KFold(n=y.shape[0], k=n_folds)
elif folding == 'leaveoneout':
    n_folds[0] = y.shape[0]
    cv = LeaveOneOut(n=y.shape[0])
else:
    print(""unknown crossvalidation method!"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- Clustering
n_clusters = 100
cluster = WardAgglomeration(n_clusters=n_clusters, connectivity=None,
    compute_full_tree='auto')
",JR_toolbox/skl_king_parallel_ward.py,kingjr/natmeg_arhus,1
"    # The channels to be used while decoding
    picks = mne.pick_types(raw.info, meg='grad', eeg=False, eog=True,
                           stim=True, exclude=raw.info['bads'])

    rt_client = MockRtClient(raw)

    # Constructing the pipeline for classification
    filt = FilterEstimator(raw.info, 1, 40)
    scaler = preprocessing.StandardScaler()
    vectorizer = Vectorizer()
    clf = SVC(C=1, kernel='linear')

    concat_classifier = Pipeline([('filter', filt), ('vector', vectorizer),
                                  ('scaler', scaler), ('svm', clf)])

    stim_server.start(verbose=True)

    # Just some initially decided events to be simulated
    # Rest will decided on the fly
    ev_list = [4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4]",examples/realtime/rt_feedback_server.py,palday/mne-python,1
"    y_train, y_test = y[:60000], y[60000:]

    #images_and_labels = list(zip(digits.images, digits.target))
    #for index, (image, label) in enumerate(images_and_labels[:4]):
    #    plt.subplot(2, 4, index + 1)
    #    plt.axis('off')
    #    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    #    plt.title('Training: %i' % label)

    classifiers = [
        #(""SVM"", svm.SVC(gamma=0.001)), # TODO doesn't finish; needs downsampled version?
        (""NN"", MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
                             solver='sgd', verbose=10, tol=1e-4, random_state=1,
                             learning_rate_init=.1)),
    ]

    for name, classifier in classifiers:
        print(name)
        classifier.fit(X_train, y_train)
        predicted = classifier.predict(X_test)",src/sklearn/main.py,turiphro/deeplearning,1
"Y = iris.target

print Y.size
print X.size
'''
h = .02  # step size in the mesh

classifiers = dict(
    knn=neighbors.KNeighborsClassifier(),
    logistic=linear_model.LogisticRegression(C=1e5),
    svm=svm.LinearSVC(C=1e5, loss='l1'),
    )


fignum = 1
# we create an instance of Neighbours Classifier and fit the data.
for name, clf in classifiers.iteritems():
    clf.fit(X, Y)

    # Plot the decision boundary. For that, we will asign a color to each",4155/assignments/a4/plot_iris_classifiers.py,moriarty/csci-homework,1
"Xtrain, ytrain, Xtest, ytest = cub.get_train_test(feature_extractor.extract_one)
Xtrain_c, ytrain_c, Xtest_c, ytest_c = cub.get_train_test(feature_extractor_c.extract_one)

print Xtrain.shape, ytrain.shape
print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(numpy.concatenate((Xtrain, Xtrain_c), 1), ytrain)
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(numpy.concatenate((Xtest, Xtest_c), 1))
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_concat_with_flip.py,yassersouri/omgh,1
"    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline

    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)

    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])

    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svn
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
",python/sklearn/sklearn/pipeline.py,seckcoder/lang-learn,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_18_2015.py,magic2du/contact_matrix,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    svd = TruncatedSVD(n_components=2)
    X_reduced = svd.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_random_hasher_sparse_data():
    X, y = datasets.make_multilabel_classification(return_indicator=True,
                                                   random_state=0)
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    X_transformed = hasher.fit_transform(X)",summary/sumy/sklearn/ensemble/tests/test_forest.py,WangWenjun559/Weiss,1
"
from __future__ import absolute_import

from numpy import genfromtxt
from sklearn.externals import joblib
from sklearn.svm import LinearSVC


def init():
    """"""Inits classifier with optimal options.""""""
    return LinearSVC(C=10.0)


def train(classifier, train_data_filename, save_classifier_filename=None):
    """"""Trains and saves classifier so that it could be easily loaded later.""""""
    file_data = genfromtxt(train_data_filename, delimiter="","")
    train_data, labels = file_data[:, :-1], file_data[:, -1]
    classifier.fit(train_data, labels)

    if save_classifier_filename:",talon/signature/learning/classifier.py,mailgun/talon,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 5.0,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/constraints_cmaessvc/setup.py,jpzk/evopy,1
"##   SVM Training   ##
######################

'''
#1st svm fit
print(""Svm Classification with first 500 features of layer fc7"")
C=1.0

print(""\tfitting svc..."")
sttime= time.clock()
svc = svm.SVC(kernel='linear', C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting rbf_svc..."")
sttime= time.clock()
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting poly_svc..."")
sttime= time.clock()",Region-to-Image_Matching/example/extract_train_test_DBH_cifar-100.py,SelinaChe/Complex-Object-Detection-StackGAN,1
"# Score: 0.78469
#
# argv[1] - training set file
# argv[2] - testing set file
# argv[3] - output file

import sys
from prediction_model import predict
from sklearn import svm

predict(sys.argv[1], sys.argv[2], sys.argv[3], lambda: svm.SVC())",Assignment-1/2B/vu-dm-16-model-svm.py,Ignotus/DataMining,1
"    The data is of the format where the first column are the classes and the rest are features
    """""" 
    training = np.loadtxt(path)
    classes = training[:,0]
    features = training[:,1:]
   
    c_scores = []
    # grid search for hyper parameters
    for i in xrange(-5, 5):
        c = 10**i
        clf = svm.SVC(kernel=k_type, C=c)
        clf.fit(features, classes)
        scores = cross_validation.cross_val_score(clf, features, classes, cv=num_folds)
        
        average_score = scores.mean()
        c_scores.append((c, scores.mean(), clf))
        print(""Accuracy with c=%0.2f: %0.2f (+/- %0.2f)""%(c, scores.mean(), scores.std()*2))
    return max(c_scores, key=operator.itemgetter(1))[2]

def classify(svm, test_data):",feature_extractor.py,dolphyin/smile_classifier,1
"output = output.values

traindata = data[0:30, :]
y_train = output[0:30, :]
# testdata = 
# y_test = output[30:,:] 
# x=[[1 1 1],[1,1,1]]
# y=[0,1,0,1]
#########CLASSIFIER#############
# clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,50), random_state=1)
clf = svm.SVC(kernel=""rbf"")
# print clf.fit(traindata,y_train)
# print (""hi"")
# s = pickle.dumps(clf)
joblib.dump(clf, 'svm_model.pkl') 

# predictedlabels = clf.predict(testdata)
# print predictedlabels
# print y_test
# confusion_matrix = np.zeros((num_classes,num_classes))",backup_stuff/text_classification/svm.py,priyanshsaxena/techmeet,1
"                idx = depth-1
                trn, trn_lbl, tst, numfeature= blor.get_new_table(testing, testing,depth)
                feature_nums[count, d, idx]= numfeature
                
                for j,fraction in enumerate([0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]):
                    new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
  
                    from sklearn.svm import SVC
                    from sklearn.neighbors import KNeighborsClassifier
    
                    clf= SVC(kernel='linear', C=10)
                    clf.fit(new_trn, trn_lbl)
                    tst_predict= clf.predict(new_tst)
                    svm_accs[count, d, j, idx]= mean(tst_predict==tst_lbl)

                    clf= KNeighborsClassifier(n_neighbors=3)
                    clf.fit(new_trn, trn_lbl)
                    tst_predict= clf.predict(new_tst)
                    knn_accs[count, d, j, idx]= mean(tst_predict==tst_lbl)
",problems/techTCrun_depth.py,lioritan/Thesis,1
"    X, y, names = df_xyf(df, predictors=predictors, target=target)

    if verbosity > 1:
        print ""names:"", "","".join(names)

    # Create the RFE object and compute a cross-validated score.
    if est is not None:
        estimator = est
        cv = cross_validation.StratifiedKFold(y, 4)
    elif isclass:
        # estimator = svm.SVC(kernel=""linear"",C=1.0)
        # estimator = get_clf('svm')
        # estimator = get_clf('lg2',C=1.0,class_weight='auto')
        estimator = \
            linear_model.LogisticRegression(penalty='l2', C=.01,
                                            class_weight='auto')
        cv = cross_validation.StratifiedKFold(y, 4)
    else:
        if False:
            from sklearn.ensemble import RandomForestRegressor",kgml/predictive_analysis.py,orazaro/kgml,1
"
        random_state = kwargs.pop('random_state', None)
        self.random_state_ = seed_random_state(random_state)

        self.logreg_param = kwargs.pop('logreg_param',
                {'multi_class': 'multinomial', 'solver': 'newton-cg',
                 'random_state': random_state})
        self.logistic_regression_ = LogisticRegression(**self.logreg_param)

        self.br_base = kwargs.pop('br_base',
              SklearnProbaAdapter(SVC(kernel='linear', probability=True,
                                      random_state=random_state)))

    @inherit_docstring_from(QueryStrategy)
    def make_query(self):
        dataset = self.dataset
        labeled_pool, Y = zip(*dataset.get_labeled_entries())
        unlabeled_entry_ids, X_pool = zip(*dataset.get_unlabeled_entries())
        labeled_pool = np.array(labeled_pool)
        Y = np.array(Y)",libact/query_strategies/multilabel/maximum_margin_reduction.py,ntucllab/libact,1
"import numpy as np

def main():
    data = pd.read_csv('train.txt', header = 0, index_col = 0, sep = '\t')
    labels = pd.read_csv('labels_train.txt', header = None, sep = '\t')
    labels = labels.unstack().tolist()
    print(len(labels))
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.5, random_state=5)

    pipe_svc = Pipeline([('scl', StandardScaler()),
                ('clf', SVC(random_state=1))])

    param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
    gamma_range = [0.004, 0.04, 0.4]

    param_grid = [{'clf__C': param_range, 
                   'clf__kernel': ['linear']},
                  {'clf__C': param_range, 
                   'clf__gamma': gamma_range, 
                   'clf__kernel': ['rbf', 'poly', 'sigmoid']}]",exercise-scripts/SVM_grid_search.py,Karl-Marka/data-mining,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",flask/Lib/site-packages/nltk/parse/transitionparser.py,hrishioa/Aviato,1
"
    intercept_ : array, shape = [n_class * (n_class-1) / 2]
        Constants in decision function.

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
        gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
        random_state=None, shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    See also
    --------",net-p3/lib/python3.5/site-packages/sklearn/svm/classes.py,uglyboxer/linear_neuron,1
"X =X.drop(""status"",1)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=7)

bestscore = 0

for c in xrange(5,205, 5):
	for g in xrange(1, 101, 1):
		c = c/100.0
		g = g/1000.0
		model = SVC(kernel = ""rbf"",C = c, gamma = g)
		model.fit(X_train,y_train)
		score = model.score(X_test,y_test)
		
		if score > bestscore:
			bestscore = score
			print ""C = ""+str(c) + "" gamma = ""+str(g) + ""; score "" + str(score)
",MLandDS/MachineLearning/SVC.py,adewynter/Tools,1
"                                unweighted_accuracyBlock[li, pi, ri, soi, i] = \
                                        np.mean(pred_unwt == y_test_pool)
                            '''
                        elif(clfScheme == clfSchemes[2]):
                            # Train and test Support Vector Machine algorithm
                            # Scale inputs to [-1 1] as SVM is scale sensitive
                            scaler_unwt = preprocessing.data.StandardScaler().fit(train_pool)
                            train_pool_unwt = scaler_unwt.transform(train_pool)
                            test_pool_unwt = scaler_unwt.transform(test_pool)

                            clf_unwt = SVC(cache_size=2048)
                            '''
                            ###################################################
                            # Grid Search
                            param_grid = [{'kernel': ['linear'], 'C': C_range}]
                            #cv = StratifiedKFold(y=y_train_pool, n_folds=3)
                            gridSearch = GridSearchCV(clf_unwt, param_grid=param_grid,
                                                pre_dispatch=n_jobs)
                            gridSearch.fit(train_pool_unwt, y_train_pool)
                            #gridSearch.fit(train_pool, y_train_pool)",Wronkiewicz_JNE_2015/pooledBCI_nSubjectsGain.py,drammock/LABSN-pubs,1
"        test_x = self.rescale(self.do_feature_vector(test))
        if self.CONFIG_DATA['NUMBER_OF_CLASSES'] == 5:
            test_y = self.svm_classify5(test)
        else:
            test_y = self.svm_classify3(test)
        train_x = self.rescale(self.do_feature_vector(train))
        if self.CONFIG_DATA['NUMBER_OF_CLASSES'] == 5:
            train_y = self.svm_classify5(train)
        else:
            train_y = self.svm_classify3(train)
        svc = svm.SVC()
        svc.fit(train_x,train_y)
        return svc.predict(test_x), test_y

    def run(self):
        global CONFIG_DATA
        self.CONFIG_DATA = self.config(self.c)
        loop = self.loops
        if loop < 0:
            loop = 1",src/multi_class_linsqu.py,pemami4911/Numerical-Analysis-Semester-Project,1
"            # add two symptoms from the middle
            X.append(feat[int(random.random() * len(feat))])
            y.append(clsid)
            X.append(feat[int(random.random() * len(feat))])
            y.append(clsid)
            X.append(feat[int(random.random() * len(feat))])
            y.append(clsid)
            set_size += 3
        clsid += 1

    clf = svm.LinearSVC()

    param_grid = {'C': [0.5, 5, 50, 500], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}

    gse = grid_search.GridSearchCV(clf, param_grid, n_jobs=os.cpu_count())
    gse.fit(X, y)

    clsid=0
    cfile=0
    error=0",stats.py,qwertzdenek/mtagger,1
"labels = labels[select]

clf = linear_model.LogisticRegression()
clf.fit(fbanks, labels)
print clf.score(fbanks, labels)

select_strided = np.logical_or(labels_strided==""p"", labels_strided==""h"") # TODO remove
strided = strided[select_strided]
labels_strided = labels_strided[select_strided]

#clf = svm.SVC()
clf = linear_model.LogisticRegression()
clf.fit(strided, labels_strided)
print clf.score(strided, labels_strided)
",classify.py,bootphon/monkey_business,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the score of each feature on the training set
        score = reliefF.reliefF(X[train], y[train])

        # rank features in descending order according to score
        idx = reliefF.feature_ranking(score)
",PyFeaST/example/test_reliefF.py,jundongl/PyFeaST,1
"def KNeighbors(X_train, X_test, y_train, y_test):
    for i in range(0,len(X_train)):
        knc = neighbors.KNeighborsClassifier()
        knc.fit(X_train[i], y_train[i])
        X_train[i] = knc.score(X_train[i], y_train[i])
        X_test[i] = knc.score(X_test[i], y_test[i])
    return X_train, X_test

def CSupSvc(X_train, X_test, y_train, y_test):
    for i in range(len(X_train)):
        csvm = svm.SVC()
        csvm.fit(X_train[i], y_train[i])
        X_train[i] = csvm.score(X_train[i], y_train[i])
        X_test[i] = csvm.score(X_test[i], y_test[i])
    return X_train, X_test
    
def RandomForest(X_train, X_test, y_train, y_test):
    for i in range(len(X_train)):
        rf = ensemble.RandomForestClassifier()
        rf.fit(X_train[i], y_train[i])",mltools.py,jrabenoit/skellify,1
"        label_bin.append(1)
    else:
        label_bin.append(0)
label_bin = pd.DataFrame(label_bin)



# if sim_mat is being read in
#sim_mat = np.load(""sim_mat_183.npy"")

clf = LinearSVC(penalty=""l1"", dual=False)

scores_accuracy = cross_val_score(clf, X=sim_mat, y=label_bin, cv=5, n_jobs=3, scoring=""accuracy"")

scores_f1 = cross_val_score(clf, sim_mat, label_bin, cv=5, n_jobs=3, scoring=""f1"")

scores_rocauc = cross_val_score(clf, sim_mat, label_bin, cv=5, n_jobs=3, scoring=""roc_auc"")

np.save(""sim_mat_183.npy"", sim_mat)
",MILES/src/1normSVM.py,kjacks21/MILES-python,1
"selector = SelectFpr(f_classif, alpha=0.1)
selector.fit(x, y)
scores = -np.log10(selector._pvalues)
scores /= scores.max()
pl.bar(x_indices-.45, scores, width=.3,
        label=r'Univariate score ($-Log(p_{value})$)',
        color='g')

################################################################################
# Compare to the weights of an SVM
clf = svm.SVC(kernel='linear')
clf.fit(x, y)

svm_weights = (clf.coef_**2).sum(axis=0)
svm_weights /= svm_weights.max()
pl.bar(x_indices-.15, svm_weights, width=.3, label='SVM weight',
        color='r')


# ################################################################################",fs_test.py,cocoaaa/ml_gesture,1
"	#power_smooth = spline(T,power,xnew)
	#plt.plot(x[:,num],y[:,num])

	fig_name = os.path.join(subdir,""{}_{}.png"".format(""image"", num))
	plt.savefig(fig_name)
	plt.cla()
	plt.clf()


#Fit SVM on attention features
#clf = svm.SVC()
#clf.fit(tr_data, train_labels.ravel())  
#y_hat = clf.predict(te_data)
#acc = metrics.accuracy_score(test_labels.ravel(),y_hat)
#y_score = clf.fit(tr_data, train_labels.ravel()).decision_function(te_data)
acc = svm_trainer(tr_data,te_data,train_labels.ravel(),test_labels.ravel(),
	num_estimates,num_training,num_testing)

#Extract HOG features
from skimage.feature import hog",sample_x_y.py,drewlinsley/draw_classify,1
"

df = pd.read_csv(""c:/Users/kyungyong/Documents/Python Scripts/SML Project/news_articles/final_DJIA.csv"")
cols_to_keep = ['Class', 'Tension', 'Depression', 'Anger', 'Fatigue', 'Confusion', 'Vigour']
data = df[cols_to_keep]

train_cols = data.columns[1:7]
print(train_cols)


clf1 = svm.SVC(kernel='rbf')
clf2 = svm.LinearSVC()
clf3 = svm.SVC(kernel='linear')
clf4 = svm.SVC(kernel='poly', degree=2)
clf5 = GaussianNB()
clf6 = BernoulliNB()
clf7 = KNeighborsClassifier(n_neighbors = 5)
clf8 = LogisticRegression()

",cross_validation.py,pgaines937/news_articles,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf, language='php').export()
print(output)

""""""
<?php

class Brain {",examples/classifier/LinearSVC/php/basics.py,nok/sklearn-porter,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,aestrivex/mne-python,1
"    params: dict
        A dictionary of list of dictionaries of the corresponding
        params for each classifier.
    """"""

    if len(data_shape) != 2:
        raise ValueError(""Only 2-d data allowed (samples by dimension)."")

    classifiers = {
        ""Nearest Neighbors"": KNeighborsClassifier(3),
        ""Linear SVM"": SVC(kernel=""linear"", C=1, probability=True),
        ""RBF SVM"": SVC(gamma=2, C=1, probability=True),
        ""Decision Tree"": DecisionTreeClassifier(max_depth=None,
                                                max_features=""auto""),
        ""Random Forest"": RandomForestClassifier(max_depth=None,
                                                n_estimators=10,
                                                max_features=""auto"",
                                                n_jobs=PROCESSORS),
        ""Logistic Regression"": LogisticRegression(),
        ""Naive Bayes"": GaussianNB(),",polyssifier.py,pliz/polyssifier,1
"                                 'n_estimators': [50, 100, 50, 100]},
                                columns=['mean', 'std', 'max_depth', 'n_estimators'])
        self.assertIsInstance(result, pdml.ModelFrame)
        self.assert_frame_equal(result, expected)

    def test_plotting(self):

        iris = datasets.load_iris()
        df = pdml.ModelFrame(iris)

        df.fit(df.svm.SVC())

        # raises if df.estimator is not XGBModel
        with tm.assertRaises(ValueError):
            df.xgb.plot_importance()

        with tm.assertRaises(ValueError):
            df.xgb.to_graphviz()

        with tm.assertRaises(ValueError):",pandas_ml/xgboost/test/test_base.py,sinhrks/pandas-ml,1
"    X = iris.data[:, 0:2]  # only take the first two features for viz
    y = iris.target
    n_features = X.shape[1]
    return X, y, n_features

def createClassifiers():

    C = 1.0
    classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'), \
                   'L2 logistic': LogisticRegression(C=C, penalty='l2'), \
                   'Linear SVC': SVC(kernel='linear', C=C, probability=True, random_state=0) \
                     }    
    
    n_classifiers = len(classifiers)

    return classifiers, n_classifiers


def plotProb(n_classifiers, classifiers, X, y):
",examples/scikit-learn/examples/general/plot_classification_probability.py,KellyChan/Python,1
"                   learning_rate=0.01, verbose=3)
#clf.score(x_tr, y_tr, x_te, y_te)
clf.fit()
pd = clf.predict(x_te)
print('SVI error = {}'.format(np.sum(len(np.where(pd != y_te)[0])) / float(x_te.shape[0])))
plt.figure(2)
ax = plt.subplot(222)
ax.set_title('GP with Stochastic Variational Inference')
ax.scatter(x_te[:,0], x_te[:,1], c=[colors[y] for y in pd], s=40)

clf = SVC()
clf.fit(x_tr, y_tr)
pd = clf.predict(x_te)
print('SVM error = {}'.format(np.sum(len(np.where(pd != y_te)[0])) / float(x_te.shape[0])))
plt.figure(2)
ax = plt.subplot(223)
ax.set_title('SVM with RBF Kernel')
ax.scatter(x_te[:,0], x_te[:,1], c=[colors[y] for y in pd], s=40)

clf = LogisticRegression()",GPSVI/test/playtoymoon.py,AlchemicalChest/Gaussian-Process-with-Stochastic-Variational-Inference,1
"                     #update_learning_rate=0.01,
                     #update_momentum=theano.shared(float32(0.9)),
                     eval_size=0.001,
                     verbose=1,				#batch_iterator_train=FlipBatchIterator(batch_size=128),
                     max_epochs=180,
                     update_learning_rate=theano.shared(float32(0.01)),
                     on_epoch_finished=[
                     #AdjustVariable('update_momentum', start=0.9, stop=0.99),
                     AdjustVariable('update_learning_rate', start=0.01, stop=0.001),])
                     #
    #clf3 = OneVsRestClassifier(SVC(C=5), n_jobs=-1)
    #print ""ES:""+X
    #print y
    net0.fit(X, y)
    nombre=""../Output/NN/Sub_4H120-100-100-150-100-adagrad-0.005-50prueba""+str(i)+"".csv""
    make_submission(net0, X_test, ids, encoder,nombre)",Python/python_NNet0.45.py,BaCaIs-eth0/Kaggle_Otto_Group,1
"
search_space = [
    {
        'k': list(range(1, num_features + 1)),
    },
]

for scoring_name in SCORING_NAMES:
    for scoring_func in SCORING_FUNCS:

        estimator = KBestSVC(X, y, score_func=scoring_func)

        grid = GridSearchCV(
            estimator=estimator,
            param_grid=search_space,
            verbose=True,
            cv=my_cv_generator(groups, num_instances),
            scoring=scoring_name,
            refit=False,
            iid=False,",scripts/kbest_recursive_grid_feature_selection.py,Rostlab/LocText,1
"	#set the timer
	start = time.time()

	trainX = np.load('trainX.npy')
	testX = np.load('testX.npy')
	trainY = np.load('trainY.npy')
	testY = np.load('testY.npy')
	print('\n!!! Data Loading Completed !!!\n')
	
	clf = KNeighborsClassifier(n_neighbors=25, weights='distance')
	#clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithTreeFS/mnistBackRandom/kernel.py,akhilpm/Masters-Project,1
"            
        
        def fit_transform(self, X, y, **fit_params):
            self.fit(X,y)
            return self.transform(X)
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    svm_model = OneVsRestClassifier(SVC(C=10.))
    
    # Create the pipeline 
    model = pipeline.Pipeline([('UnionInput', FeatureUnion([('svd', svd), ('dense_features', FeatureInserter())])),('scl', scl),('svm', svm_model)])

    # Fit Model
    model.fit(X, y)

    preds = model.predict(X_test)
###################################Model performance ############    ",Search_result/srr7.py,tanayz/Kaggle,1
"        y_train = data['target'][:-N_test]

        x_test = data['data'][-N_test:]
        y_test = data['target'][-N_test:]

        np.savetxt(""csvs/x_train.csv"", x_train, delimiter="","")#@asdjkk
        np.savetxt(""csvs/y_train.csv"", y_train, delimiter="","", newline="","")
        np.savetxt(""csvs/x_test.csv"", x_test, delimiter="","")
        np.savetxt(""csvs/y_test.csv"", y_test, delimiter="","", newline="","")

        ml = svm.LinearSVC()
        ml = ml.fit(x_train, y_train)
        yhat_test = ml.predict(x_test)

        print argwhere(abs(yhat_test - y_test) < 0.5).shape[0] / y_test.shape[0]
    data = datasets.load_digits()
    N_test = int(1050)
    x_train = data['data'][:-N_test]
    y_train = data['target'][:-N_test]
",python_testing/testing.py,shyamalschandra/swix,1
"        self.pushButton_3.clicked.connect(self.startsvm)

        self.pushButton_2 = QtGui.QPushButton(Form)
        self.pushButton_2.setGeometry(QtCore.QRect(50, 500, 161, 23))
        self.pushButton_2.setObjectName(_fromUtf8(""pushButton_2""))
        self.pushButton_2.clicked.connect(self.taketest)
        self.retranslateUi(Form)
        QtCore.QMetaObject.connectSlotsByName(Form)

    def startsvm(self):
        clf=svm.SVC()
        clf.fit(self.tr,self.classlabels)
        for i in self.te:
            print ""test record:"",i,""classlabel:"",clf.predict(i)
            
    def takeinput(self):
        fname = QtGui.QFileDialog.getOpenFileName(None, 'Open file', 'C:')
        print type(fname)
        import pandas as pd
        df = pd.read_csv(str(fname), sep="","")",FRONTEND/svmfront.py,vishnumani2009/OpenSource-Open-Ended-Statistical-toolkit,1
"        if predict_probs:
            # well, *log* probs, anyway
            preds = [p[1] for p in clf.predict_log_proba(X_test)]
        else:
            preds = clf.predict(X_test)
        return preds, y_test
    

    @staticmethod 
    def _get_SVM():
        return SVC(probability=True, kernel='linear', C=3)

    def train(self):
        features, y = self.features_from_citations()
        self.vectorizer = DictVectorizer(sparse=True)
        X_fv = self.vectorizer.fit_transform(self.features)
        
        self.clf = _get_SVM()

        ##",joint_supervised_learner.py,ijmarshall/cochrane-nlp,1
"Xtrain, ytrain, Xtest, ytest = cub.get_train_test(feature_extractor.extract_one)
Xtrain_f, ytrain_f, Xtest_f, ytest_f = cub.get_train_test(feature_extractor_f.extract_one)

print Xtrain.shape, ytrain.shape
print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(numpy.concatenate((Xtrain, Xtrain_f)), numpy.concatenate((ytrain, ytrain_f)))
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(Xtest_f)
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_augment.py,yassersouri/omgh,1
"
def processTweets(X_train, X_test):
        X_train = [sentiment.stem(sentiment.preprocessTweets(tweet)) for tweet in X_train]
        X_test = [sentiment.stem(sentiment.preprocessTweets(tweet)) for tweet in X_test]
        return X_train,X_test
        
# SVM classifier

def classifier(X_train,y_train):
        vec = TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf = True,use_idf = True,ngram_range=(1, 2))
        svm_clf =svm.LinearSVC(C=0.1)
        vec_clf = Pipeline([('vectorizer', vec), ('pac', svm_clf)])
        vec_clf.fit(X_train,y_train)
        joblib.dump(vec_clf, 'svmClassifier.pkl', compress=3)
        return vec_clf

# Main function

def main():
        X_train, X_test, y_train, y_test = getTrainingAndTestData()",Twitter-Sentiment-Classifier/src/training.py,cga-harvard/hhypermap-bop,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf).export()
# model = Porter(clf, language='java').export()
print(output)

""""""
class Brain {
",examples/classifier/LinearSVC/java/basics.py,nok/sklearn-porter,1
"        If set to 'raise', the error is raised. If a numeric value is given,
        FitFailedWarning is raised. This parameter does not affect the refit
        step, which will always raise the error.


    Examples
    --------
    >>> from sklearn import svm, grid_search, datasets
    >>> iris = datasets.load_iris()
    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    >>> svr = svm.SVC()
    >>> clf = grid_search.GridSearchCV(svr, parameters)
    >>> clf.fit(iris.data, iris.target)
    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    GridSearchCV(cv=None, error_score=...,
           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
                         decision_function_shape=None, degree=..., gamma=...,
                         kernel='rbf', max_iter=-1, probability=False,
                         random_state=None, shrinking=True, tol=...,
                         verbose=False),",projects/scikit-learn-master/sklearn/grid_search.py,DailyActie/Surrogate-Model,1
"
import sys
from prediction_model import predict
from sklearn import svm
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn import neighbors

n_neighbors = 5
predict(sys.argv[1], sys.argv[2], sys.argv[3], lambda:  Pipeline([
  ('feature_selection', svm.LinearSVC()),
  ('classification', neighbors.KNeighborsClassifier(n_neighbors, weights='distance')) 
]))",Assignment-1/2B/vu-dm-16-model-svmknn.py,Ignotus/DataMining,1
"    """"""
    @return X, y
    """"""
    X = vec.transform(features)
    y = np.array(labels)
    return X, y
    

def create_model(X, y):
    try:
        clf = svm.SVC(probability=True, kernel='linear')
        clf.fit(X, y)
        return clf
    except:
        return False

def token_exp(token, exptype=False):
    """"""
    @return set of exp-line_ids from GATE
    """"""",opinionholder.py,trondth/master,1
"digits.images[0]


# In[5]:

from sklearn import svm


# In[6]:

clf = svm.SVC(gamma=0.001, C=100.)


# In[8]:

clf.fit(digits.data[:-1], digits.target[:-1])


# In[9]:
",src/models/firstcAnalyses.py,dddTESTxx/Gym-Final,1
"def lin_svc(dataset, DV, lower_limit=0, upper_limit=''):
	""""""Runs a linear for a given DV, using all remaining
	variables as features.
	Prohibitive run time for N > 20,000. Cross-validation even worse.
	Consider anova_svm instead."""""" 

	start = time.time()

	X, y = Build_Data_Set(dataset, DV, lower_limit, upper_limit)

	clf = SVC(kernel=""linear"", C= 1.0)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Linear SVC""
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 

# Unhash to test:
#lin_svc('data/cs-training#3B.csv', 'serious_dlqin2yrs', 0, 150000)",pipeline/__5_Classifier.py,BridgitD/school-dropout-predictions,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_forward.decision_tree_forward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",PyFeaST/example/test_decision_tree_forward.py,jundongl/PyFeaST,1
"    reporter = Reporter(report_dir='/tmp/reporter')

    cv = ShuffleSplit(len(target), n_iter=5)
    Cs = [1e-3, 1e-2, 1e-1, 1., 10, 1e2, 1e3]

    scaler = StandardScaler()
    n_x, n_y, n_z = mask.shape
    connectivity = grid_to_graph(n_x, n_y, n_z, mask=mask_array)
    ward = WardAgglomeration(n_clusters=2000,
                             connectivity=connectivity, memory=memory)
    svc = LinearSVC(penalty='l1', dual=False)
    # rand_svc = RandomizedWardClassifier(mask_array, n_iter=16,
    #                                     memory=memory, n_jobs=-1)

    pipe = Pipeline([('scaler', scaler), ('clf', svc)])
    grid = GridSearchCV(pipe, param_grid={'clf__C': Cs},
                        cv=cv, n_jobs=1)
    grid.best_estimator_ = grid.estimator
    ovr = OneVsRestClassifier(grid, n_jobs=1)
",tests/test_decoding.py,schwarty/nignore,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_05082015_server.py.py,magic2du/contact_matrix,1
"        X_digits = digits.data
        y_digits = digits.target
        n_samples = len(X_digits)
        X_train = X_digits[:int(.9 * n_samples)]
        y_train = y_digits[:int(.9 * n_samples)]
        X_test = X_digits[int(.9 * n_samples):]
        y_test = y_digits[int(.9 * n_samples):]
        svm = SVM(sparkSession, is_multi_class=True, tol=0.0001)
        mllearn_predicted = svm.fit(X_train, y_train).predict(X_test)
        from sklearn import linear_model, svm
        clf = svm.LinearSVC()
        sklearn_predicted = clf.fit(X_train, y_train).predict(X_test)
        accuracy = accuracy_score(sklearn_predicted, mllearn_predicted)
        evaluation = 'test_svm accuracy_score(sklearn_predicted, mllearn_predicted) was {}'.format(accuracy)
        self.failUnless(accuracy > 0.95, evaluation)

    def test_naive_bayes(self):
        digits = datasets.load_digits()
        X_digits = digits.data
        y_digits = digits.target",src/main/python/tests/test_mllearn_numpy.py,apache/incubator-systemml,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",tests/test_future/disabled/test_email/disabled_test_email.py,krischer/python-future,1
"            np_pos = np.where(data_idx==i)[0]
            bow_trn_mat[cnt,:] = np.asarray(fsa.bow(data_mat[np_pos,:],
                                                    codebook))

        # Cross-validate (5-fold) SVM classifier and parameters
        param_selection = [{'kernel': ['rbf'],
                            'gamma': np.logspace(-6,2,10),
                            'C': [1, 10, 100, 1000]},
                           {'kernel': ['linear'],
                            'C': [1, 10, 100, 1000]}]
        clf = GridSearchCV(svm.SVC(C=1), param_selection, cv=5)
        clf.fit(bow_trn_mat, np.asarray(class_info)[trn])

        # Compute BoW histograms for testing data
        bow_tst_mat = np.zeros((len(tst), options.codewords))
        for cnt,i in enumerate(tst):
            pos =  np.where(data_idx==i)[0]
            bow_tst_mat[cnt,:] = fsa.bow(data_mat[pos,:], codebook)

",dcl.py,rkwitt/pyfsa,1
"    clf = cluser_digits(X_train)   # tok
    # show_cluster_digits(clf)  # TOK
    
    y_pred = predict_labels(clf, X_test, y_test, X_train, y_train)   # tok
    show_prediction_confusion_matrix(y_test, y_pred)   # tok
   
    homogeneity_score(clf, X_test, y_test, X_train, y_train, y_pred)   # tok
   
    ##########################################
    # try a different model
    svc_model, X_train, X_test, y_train, y_test, images_train, images_test = model_SVC(digits)   # tok
    
    # grid_search - use this to tune parameters
    grid_search(digits)   # tok
    apply_grid_search(clf, X_test, y_test, X_train, y_train)
    predicted = classify_rbf(svc_model, X_test, y_test, images_test)
    check_model_performance(y_test, predicted)
    
    show_model2_results(svc_model, X_train, y_train)
    ",aikif/.z_prototype/ml_ex1-digits.py,acutesoftware/AIKIF,1
"    results = {}
    for classname in CLASSES:
        results[classname] = MODELS[classname].predict(data)
    return results


def train_class(x_train, y_train):
    """"""
    Trains a model for one class
    """"""
    svm = LinearSVC(C=0.3, max_iter=300, loss='hinge')
    pipeline = Pipeline([
        ('union', FeatureUnion([
            ('sentiment', sentiment_extractor),
            ('temp', temp_extractor),
            ('wind', wind_extractor),
            ('vect', vectorizer),
        ])),
        ('cls', svm),
    ])",classify/classifier.py,jramcast/nuublo-predict,1
"

# make a plotting grid
h = .02  # step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# svm
clf = SVC(kernel='linear').fit(X, y)

# predict all points in grid
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

# separating plane and margins
w = clf.coef_[0]
a = -w[0] / w[1]
xxx = np.linspace(x_min, x_max)
yyy = a * xxx - (clf.intercept_[0]) / w[1]",support-vector-machines-101/svm-example.py,KehaoWu/Data-Science-45min-Intros,1
"	y_regis_train = y_regis[:nTrain]
	y_total_train = y_total[:nTrain]
	Xtest = X[nTrain:,:]
	y_casual_test = y_casual[nTrain:]
	y_regis_test = y_regis[nTrain:]
	y_total_test = y_total[nTrain:]
	
	neighbors = 4
	#linear
	#param_grid = {'C': [1, 5, 10, 100],}
	#clf = GridSearchCV(SVC(kernel='linear'), param_grid,n_jobs=-1)


	clf_regis = KNeighborsRegressor(n_neighbors=neighbors,algorithm='kd_tree',leaf_size=70,p=1)
	clf_regis.fit(Xtrain,y_regis_train)
	pred_regis = clf_regis.predict(Xtest)
	
	clf_casual = KNeighborsRegressor(n_neighbors=neighbors,algorithm='kd_tree',leaf_size=70,p=1)
	clf_casual.fit(Xtrain,y_casual_train)
	pred_casual = clf_casual.predict(Xtest)",nearest_neighbor/nearest_neighbors.py,agadiraju/519finalproject,1
"        '''
        trainingSet = self.get_training_set()
        if len(set(trainingSet)) < 2: # Needs at least two classes
            return 0
        dataSet = list(self.set_data())
        if not temp:
            currentData = self.current_data()
        else:
            currentData = self.current_data(temp)
        X = np.array(dataSet)
        clf = svm.SVC(kernel='linear')
        clf.fit(X, trainingSet)
        response = clf.predict([currentData])
        return response


def get_all_users():
    '''
    Returns:
        list: UserID of every user in 'UserDetails'.",ServerSide/utils/users.py,ARowden/Python-Thermostat,1
"	print('[INFO]: Confusion matrix:')
	print(confusionMatrix)

	# return benchmark parameters
	return classifierName, score, trainTime, testTime

# selected classifiers for testing
classifiers = [
	        (KNeighborsClassifier(n_neighbors=10), ""K Nearest Neighbors""),
	        (MultinomialNB(alpha=1e-2), ""Multinomial Naive Bayes""),
	        (LinearSVC(penalty='l2', dual=False, tol=1e-4), ""Linear SVC""),
	        (SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=50), ""SGD Classifier-l2""),
	        (SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-3, n_iter=50), ""SGD Classifier-elasticnet""),
	        (MLPClassifier(alpha=1e-2), ""Multi-layer Perceptron"")]

# benchmark classifiers and store results
results = []
for classifier, classifierName in classifiers:
	print('=' * 80)
	print(classifierName)",sentiment.py,RohitAGattani/TwitterSentimentAnalysis,1
"    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    blah1=zeros(19)
    blah2=zeros(19)
    blah3=zeros(19)
    blah4=zeros(19)
    blah5=zeros(19)
    for i,fraction in enumerate(fractions):
        new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
        
        clf= SVC(kernel='linear', C=100)
        clf.fit(new_trn, trn_lbl)    
        blah1[i]= mean(clf.predict(new_tst)!=tst_lbl)
        
        clf= KNeighborsClassifier(n_neighbors=3)
        clf.fit(new_trn, trn_lbl)    
        blah2[i]= mean(clf.predict(new_tst)!=tst_lbl)
        
        clf=DecisionTreeClassifier(criterion='entropy', min_samples_split=2, random_state=0)
        clf.fit(new_trn, trn_lbl)    ",problems/ohsumedTitleOnlyMulticlassRun.py,lioritan/Thesis,1
"
### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier
from sklearn.pipeline import Pipeline

svc_ovo = OneVsOneClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

svc_ova = OneVsRestClassifier(Pipeline([
                ('anova', SelectKBest(f_classif, k=500)),
                ('svc', SVC(kernel='linear'))
                ]))

### Cross-validation scores ###################################################
from sklearn.cross_validation import cross_val_score",examples/decoding/plot_haxby_multiclass.py,salma1601/nilearn,1
"
  X = preprocessing.scale(X)

  return X,y

def Analysis():
  test_size = 1000
  X, y = Build_Data_Set()
  print(len(X))

  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size])

  correct_count = 0

  for x in range(1, test_size+1):
    if clf.predict(X[-x])[0] == y[-x]:
      correct_count += 1

  print(""correct_count=%s""%correct_count)",p14.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,Teekuningas/mne-python,1
"X_test_pca = pca.transform(X_test)
print(""done in {dur:.3f}s"".format(dur=(datetime.now() - start).total_seconds()))


# In[23]:

print('Fitting the classifier to the training set')
start = datetime.now()
param_grid = {}
clf1 = LogisticRegression(C=1000.0, solver='sag')
clf2 = SVC(C=1000.0, gamma=0.001)
clf3 = MLPClassifier(hidden_layer_sizes=(750,), solver='lbfgs', alpha=0.0001)
clf = VotingClassifier(estimators=[('lr', clf1), ('svm', clf2), ('mlp', clf3)], voting='hard')
clf = clf.fit(X_train_pca, y_train)
print('done in {dur:.3f}s'.format(dur=(datetime.now() - start).total_seconds()))
# print('Best estimator found by grid search:')
# print(clf.best_estimator_)


# In[24]:",Sklearn_Face_Recognition/VotingClassifier.py,mikelane/FaceRecognition,1
"
#tr_features = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54']
#tr_features = ['f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54']
tr_features = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10']
ts_features = 'class'

classifiers = [
    KNeighborsClassifier(1),                                                    #  0 - !
    KNeighborsClassifier(3),                                                    #  1
    KNeighborsClassifier(6),                                                    #  2
    SVC(kernel=""linear"", C=0.025),                                              #  3
    SVC(kernel=""linear"", C=1),                                                  #  4
    SVC(kernel=""linear"", C=100),                                                #  5
    SVC(gamma=0.5, C=0.1),                                                      #  6
    SVC(gamma=2, C=1),                                                          #  7
    SVC(gamma=50, C=100),                                                       #  8
    DecisionTreeClassifier(max_depth=5),                                        #  9
    DecisionTreeClassifier(max_depth=10),                                       # 10 - !
    SVC(gamma=2, C=1000),                                                       # 11
    SVC(gamma=2, C=100),                                                        # 12",KForest/KForest/KForest.py,Andrew414/KForest,1
"    
    # d2v_model.get_avg_feature_vecs(w2v_model)      # 77.3 c=1  word vec average scheme
    # d2v_model.get_tf_idf_feature_vecs(w2v_model)   # 77.2 c=1  word vec tf-idf scheme
    # d2v_model.get_tf_idf_feature_vecs(w2v_model, cre_adjust=True)  # 76.9 c=1  word vec cre tf-idf scheme

    # d2v_model.create_bag_of_centroids(w2v_model) # 75.3 c=0.1 word vec bag of centroids
    d2v_model.create_bag_of_centroids(w2v_model, cre_adjust=True) # 76.2 c=0.1 word vec cre tfidf bag of centroids

    # d2v_model.cre_sim_doc_vecs(w2v_model) 

    text_clf = LinearSVC(C=c)
    _ = text_clf.fit(d2v_model.train_doc_vecs, d2v_model.train_labels)
    perf = text_clf.score(d2v_model.test_doc_vecs, d2v_model.test_labels) 
    perf2 = text_clf.score(d2v_model.train_doc_vecs, d2v_model.train_labels)
    print "" Train accuracy:"" + str(perf2)
    print "" Test accuracy:"" + str(perf)

    # clf = SVC(kernel='rbf',C=c, gamma=g).fit(train, train_Label)
    # perf2 = clf.score(train, train_Label)
    # perf = clf.score(test, test_Label)",doc-classify-1.1.py,linbojin/dv4sa,1
"    # do a cross validation with 70% to train and 30% to test
    seventy_precent_of_sample = int(.7 * n_sample)
    X_train = X[:seventy_precent_of_sample]
    y_train = y[:seventy_precent_of_sample]
    X_test = X[seventy_precent_of_sample:]
    y_test = y[seventy_precent_of_sample:]

    # Here you can output which ever result you would like by changing the Kernel and clf.predict lines
    # Change kernel here to poly, rbf or linear
    # adjusting the gamma level also changes the degree to which the model is fitted
    clf = svm.SVC(kernel='rbf', gamma=3).fit(X_train, y_train)
    y,x = dmatrices(formula_ml, data=test_data, return_type='dataframe')

    # Change the interger values within x.ix[:,[6,3]].dropna() explore the relationships between other
    # features. the ints are column postions. ie. [6,3] 6th column and the third column are evaluated.
    res_svm = clf.predict(x.ix[:,[6,3]].dropna())

    res_svm = DataFrame(res_svm,columns=['Survived'])
    res_svm.to_csv(""../data/output/svm_linear_63_g10.csv"") # saves the results for you, change the name as you please.
",code/support_vector_machine.py,caynan/Titanic,1
"                model = LogisticRegression()
            elif trainMode == 'NaiveBayes':
                model = MultinomialNB()
            elif trainMode == 'RF':
                model = ExtraTreesClassifier(n_estimators=50, random_state=0)
            elif trainMode == 'Ada':
                model = AdaBoostClassifier()
            elif trainMode == 'MLP':
                model = MLPClassifier(activation='logistic', solver='sgd', learning_rate_init=0.02, learning_rate='constant', batch_size=100)
            else:
                model = svm.SVC()

            model.fit(feature_train, label_train)
            predictions = model.predict(feature_test)

            if len(predictions) != len(label_test):
                print 'inference error!'
                resultFile.write('inferece error!\n')

            accuracy = model.score(feature_test, label_test)",runModel.py,renhaocui/tweetInfluencer,1
"
    def train(self, documents, labels, identifiers):
        """"""
        Fits vectorizer and classifier
        :param documents: (list)
        :param labels: (list)
        :param identifiers: (list)
        """"""

        self.vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1, tokenizer=lemma_tokenizer)
        self.clf = OneVsRestClassifier(LinearSVC(random_state=0))

        self._get_label_dicts(labels)
        self.doc_ids = identifiers

        x = self.vectorizer.fit_transform(documents)
        y = self.label_vectorizer(labels)
        self.clf.fit(x, y)

    def predict(self, documents):",src/model/base/model.py,bklakew/OpenAgClassifier,1
"    verbose = kwargs.pop('verbose', 0)
    random_state = kwargs.pop('random_state', 0)
    max_iter = kwargs.pop('max_iter', 1000)
    gs_n_jobs = kwargs.pop('gs_n_jobs', multiprocessing.cpu_count())

    # Make a grid_search if the parameter C is not given
    if 'C' in kwargs:
        C = kwargs.pop('C', 1.0)

        # Call the constructor with the proper input arguments
        clsvm = LinearSVC(penalty=penalty, loss=loss, dual=dual, tol=tol, C=C,
                          multi_class=multi_class,
                          fit_intercept=fit_intercept,
                          intercept_scaling=intercept_scaling,
                          class_weight=class_weight,
                          verbose=verbose, random_state=random_state,
                          max_iter=max_iter)

    else:
        # Import the grid search module",protoclass/classification/classification.py,glemaitre/protoclass,1
"    print()
    

    '''
    -This is where we define the models
    -Here, I use SVM and Decision tree with pre-defined parameters
    -We can learn these parameters given our data
    '''
    print('Defining and fitting models ...')
    t0 = dt.datetime.utcnow()   
    clf0 = svm.LinearSVC(C=100.)
    clf1 = tree.DecisionTreeClassifier()

    clf0.fit(X_train_scaled, Y_train)
    clf1.fit(X_train_scaled, Y_train)

    joblib.dump(clf0, 'models/svc.pickle')
    joblib.dump(clf1, 'models/tree.pickle')

    print('... finished in {} secs.'.format(dt.datetime.utcnow() - t0))",public_talks/2016_02_26_columbia/do_ml_on_feature_tables (all.csv).py,kylepjohnson/ipython,1
"data_test_size_mb = size_mb(data_test)
print('data loaded - train: {0}Mb test: {1}Mb'.format(data_train_size_mb, data_test_size_mb))


for clf, clf_name in [ (RandomForestClassifier(), ""Random Forest""),
                      (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
                      (Perceptron(n_iter=50), ""Perceptron""),
                      (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
                      (KNeighborsClassifier(n_neighbors=10), ""kNN""),
                      (BernoulliNB(alpha=.01), 'BNB'),
                      (LinearSVC(penalty=""l1"", dual=False, tol=1e-3), 'LVC1'),
                      (LinearSVC(),'LVC2') ]:
    
    print ""CHECKING "",clf_name

    # Create the feature extractor
    text_clf = Pipeline([ ('extract_features', sklearn_util.TextFeatures()),
                                    ('vect',DictVectorizer(sparse=False)),
                                    ('clf', clf),
                                 ])",feature_selection.py,linucks/textclass,1
"##   SVM Training   ##
######################

'''
#1st svm fit
print(""Svm Classification with first 500 features of layer fc7"")
C=1.0

print(""\tfitting svc..."")
sttime= time.clock()
svc = svm.SVC(kernel='linear', C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting rbf_svc..."")
sttime= time.clock()
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(train_feats_fc7[:,:500], train_labels)
print(""\t\ttook ""+str(time.clock()- sttime)+ "" Secs."")

print(""\tfitting poly_svc..."")
sttime= time.clock()",Region-to-Image_Matching/example/extract_train_test_DBH_bird.py,SelinaChe/Complex-Object-Detection-StackGAN,1
"#     C: Penalty parameter C of the error term
#     kernel:  ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’
#     degree: Degree of the polynomial kernel function (‘poly’).
#     gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’
#     coef0: Independent term in kernel function
#     probability: Whether to enable probability estimates.
#                  This must be enabled prior to calling fit, and will slow down that method.
#     shrinking, tol, cache_size, class_weight
#     decision_function_shape : ‘ovo’, ‘ovr’ or None
#     random_state
clf = svm.SVC()
clf.fit(x_train, y_train)

# test accuracy
accuracy = clf.score(x_test, y_test)
print(accuracy)

# made up point to test the classifier
example_measures = np.array([[4, 2, 1, 1, 1, 2, 3, 2, 1], [4, 2, 1, 2, 2, 2, 3, 2, 1]])
example_measures = example_measures.reshape(len(example_measures), -1)",sentdex/support_vector_machine.py,FelixPM/Learning-Machine-Learning,1
"
    if axis == 1:
        ax.set_zlabel('roll value')
    elif axis == 2:
        ax.set_zlabel('pitch value')
    else:
        ax.set_zlabel('yaw value')
    plt.show()


def trainingSVC(data, test, gamma, C):
    x = data[:, 0:2]
    label = data[:, -1]
    
    print x.shape
    print label

    clf = svm.SVC(kernel='linear').fit(x, label)
    
    print clf",src/clf/dataLoader.py,changkun/MotionTouch,1
"x_n = np.random.multivariate_normal(np.ones(dim) * 1.5, np.eye(dim), num_n)
x = np.vstack([x_p, x_n])
y = np.array([1.] * num_p + [-1.] * num_n)

# Hyper parameters
cost = 1e0
gamma = 0.1
max_iter = 2500
tol = 1e-5

clf_mdsvm = csvc.SVC(C=cost, kernel='rbf', max_iter=max_iter, gamma=gamma, tol=tol)
clf_sklearn = svm.SVC(C=cost, kernel='rbf', max_iter=max_iter, gamma=gamma, tol=tol, shrinking=False, cache_size=4000)

t = time.time()
clf_mdsvm.fit(x, y)
print 'MDSVM:', time.time() - t, clf_mdsvm.score(x, y)

t = time.time()
clf_sklearn.fit(x, y)
print 'scikit-learn:', time.time() -t, clf_sklearn.score(x, y)",benchmarks/bench_rbf_svc.py,sfujiwara/mdsvm,1
"

# ### Support vector machine cross-validation
# The best performance (although probably not statistically significantly better) is from SVM

# In[62]:

# Cross validate SVM
from sklearn import svm

clf = svm.SVC()
prediction = cross_validation.cross_val_predict(clf,train_data_scaled, y, cv=kfold)

examine_prediction(y, prediction, train_data, features, show_misidentified=False)


# ### Choice of number of features and feature importance
# We can use [recursive feature elimination with cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) to see how many features to use, and then by fitting a random forest, rank their importance.

# In[63]:",detect_rap.py,drsaunders/RapDetector,1
"

X_train = np.array(X_list)

mlb = MultiLabelBinarizer()
Y = mlb.fit_transform(Y_train)
    
classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

classifier.fit(X_train, Y)
X_test = []

files = [f for f in listdir('articles')]
print(""\n\nPerforming Classification\n"")
for f in files:
    filepath = join('articles', f)
    with open(filepath, 'r') as json_file:",skclassifier.py,Kronosfear/inkbot,1
"    Y = np.r_[Y_n, Y_s]

    # normalize data
    # A = preprocessing.normalize( A )
    # A = preprocessing.scale( A )

    # fit classifier
    # Note: in LinearSVC use dual=False when n_samples > n_features
    #       'l2' for squared hinge, 'l1' for regular hinge
    clf = GradientBoostingClassifier(n_estimators=200, max_depth=3)
    # clf = svm.SVC(kernel='rbf', cache_size=500)  # rbf
    # clf = svm.LinearSVC(dual=False, loss='l2')

    clf.fit(A, Y)

    return clf",learn.py,rronen/HAF-score,1
"    # dt = DictVectorizer()
    # ten_percent = len(raw_routes_data) / 10
    # print raw_routes_data[0]
    #
    # # Training
    # training_label = all_labels[ten_percent:]
    # training_raw_data = raw_routes_data[ten_percent:]
    # training_data = dt.fit_transform(training_raw_data).toarray()
    #
    #
    # learner = svm.LinearSVC()
    # learner.fit(training_data, training_label)
    #
    # # Predicting
    # testing_label = all_labels[:ten_percent]
    # testing_raw_data = raw_routes_data[:ten_percent]
    # testing_data = dt.transform(testing_raw_data).toarray()
    #
    # testing_predictions = learner.predict(testing_data)
    #",analysis/training_and_predict.py,lorenanicole/pytennessee,1
"        kwargs['mapping'] = 'ZeroMeanUnitVarianceMapper'
        super(TrainLinearResolver, self).__init__(**kwargs)

    def init_model(self):
        hparams = {
            'kernel': 'rbf',
            'C': 1000.,
            'probability': True
        }

        return SVC(**hparams)

    def iter_instances(self, docs):
        toggle = True

        for doc in docs:
            for chain in doc.chains:
                if not chain.candidates:
                    # skip mentions without candidates
                    continue",nel/learn/resolving.py,wikilinks/nel,1
"chunk = 10000

##plt.scatter(scores[:chunk], stars[:chunk], color='r')
####plt.scatter(range(len(all_stars))[:chunk], normalizeList(scores[:chunk]), color='b')
##plt.grid()
##plt.show()

from sklearn import svm
X = [[s] for s in scores[:chunk]]
Y = stars[:chunk]
clf = svm.SVC()
clf.fit(X, Y)

# bid_topics = dict()
# bid_sentiment_scores = dict()

# for bid, rids in br.iteritems():
# 	temp_topic = []
# 	temp_score = []
# 	for rid in rids:",Python Files/b_topics_score.py,apanimesh061/YDC,1
"    mat_content = sio.loadmat(data_file)
    
    X = mat_content['X']
    y = mat_content['y']
    y = y.ravel()

    print('Training Linear SVM (Spam Classification)')
    
    C = 0.1

    model = svm.SVC(C=C, kernel='linear', max_iter=200)
    model.fit(X, y)

    print('Training Accuracy: %f' % model.score(X, y))
    
    raw_input('Program paused. Press enter to continue')

    # =================== Part 4: Test Spam Classification ===================
    
    data_file = '../../data/ex6/spamTest.mat'",solutions/ex6/ex6_spam.py,cameronlai/ml-class-python,1
"# from sklearn.naive_bayes import GaussianNB
# naivebayes_model = GaussianNB()
# naivebayes_model.fit(X_cropped, y_cropped)
# y_validation_predicted = naivebayes_model.predict(X_validation)
# print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
#
#
# # Start SVM Classification
# print ""Performing SVM Classification:""
# from sklearn.svm import SVC
# svm_model = SVC(kernel='rbf' ,probability=True, max_iter=1000)
# svm_model.fit(X_cropped, y_cropped)
# y_train_predicted = svm_model.predict(X_train)
# print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# # ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# # plt.show()
#
# y_validation_predicted = svm_model.predict(X_validation)
# print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
#",Code/Machine_Learning_Algos/training_t5.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"c=1
r = range(0, 10)
for i in r:
    print ""cv = %d"" % i
    d2v_model.train_test_split(i)  
    d2v_model.count_data()
    d2v_model.get_bag_of_words_sklearn()           # 77.1 c=1  tf-idf weight scheme in sklearn
    # d2v_model.get_bag_of_words(cre_adjust=False)   # 77.2 c=1  custom tf-idf 
    # d2v_model.get_bag_of_words(cre_adjust=True)    # 77.5 c=1  custom cre tf-idf weight
    
    text_clf = LinearSVC(C=c)
    _ = text_clf.fit(d2v_model.train_doc_vecs, d2v_model.train_labels)
    perf = text_clf.score(d2v_model.test_doc_vecs, d2v_model.test_labels) 
    perf2 = text_clf.score(d2v_model.train_doc_vecs, d2v_model.train_labels)
    print "" Train accuracy:"" + str(perf2)
    print "" Test accuracy:"" + str(perf)

    print 
    train_results.append(perf2)
    test_results.append(perf)",sentiment-analysis-bow.py,linbojin/dv4sa,1
"            with open(svmTrainedData):
                print('  Opening SVM training model...\n')
                clf = joblib.load(svmTrainedData)
        else:
            raise ValueError('  Force retraining SVM model')
    except:
        #**********************************************
        ''' Retrain training model if not available'''
        #**********************************************
        print('  Retraining SVM data...')
        clf = svm.SVC(C = svmDef.Cfactor, decision_function_shape = 'ovr', probability=True)
        
        print(""  Training on the full training dataset\n"")
        clf.fit(A,Cl)
        accur = clf.score(A_test,Cl_test)
        print('  Mean accuracy: ',100*accur,'%')

        Z = clf.decision_function(A)
        print('\n  Number of classes = ' + str(Z.shape[1]))
        joblib.dump(clf, svmTrainedData)",SpectraLearnPredict.py,feranick/SpectralMachine,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    svd = TruncatedSVD(n_components=2)
    X_reduced = svd.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/ensemble/tests/test_forest.py,RPGOne/Skynet,1
"def lin_svc(dataset, DV, lower_limit=0, upper_limit=''):
	""""""Runs a linear for a given DV, using all remaining
	variables as features.
	Prohibitive run time for N > 20,000. Cross-validation even worse.
	Consider anova_svm instead."""""" 

	start = time.time()

	X, y = Build_Data_Set(dataset, DV, lower_limit, upper_limit)

	clf = SVC(kernel=""linear"", C= 1.0)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Linear SVC""
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 

# Unhash to test:
#lin_svc('data/cs-training#3B.csv', 'serious_dlqin2yrs', 0, 150000)",pipeline/__5_Classifier.py,jmausolf/HIV_Status,1
"print '\nPredict Takes: ', t1-t0
#print pred2
acc = (0.0+sum(Y==pred2))/len(Y)

print 'acc=',acc


#libsvm from sklearn
from sklearn import svm

clf = svm.SVC(C=C,kernel='linear',verbose=True)
clf = svm.SVC(C=C,kernel='rbf',gamma=gamma,verbose=True)
t0=time.clock()
svm_m= clf.fit(X,Y)
t1=time.clock()
#
print '\nTrains Takes: ', t1-t0
#print 'alpha\n',clf.dual_coef_.toarray()

#print 'nSV=',clf.n_support_",examples/svm_gpu_example.py,ksirg/pyKMLib,1
"        targett = targett[0:N]
    
    print n, l, d, rho, N
    Gp, Hp = learner(n, l, d, rho, Y)
    print 'Learned the network'

    Yp = decoder(Gp, Hp)
    print 'NN training error: %.4f' % l1_loss(Y, Yp)
    
    if 1:
        clf = svm.LinearSVC(loss='l2', penalty='l1', dual=False)
        clf.fit(Hp, target)
        targetp = clf.predict(Hp)
        print 'training error: %.4f' % (zero_one_loss(target, targetp))

        Htp = encoder(d, Gp, Yt)
        targettp = clf.predict(Htp)
        print 'test error: %.4f' % (zero_one_loss(targett, targettp))

",nn/sparsenet/mnist.py,pratikac/python,1
"    features = np.array(dataset.data, 'int16') 
    labels = np.array(dataset.target, 'int')

    # Extract the hog features
    list_hog_fd = []
    for feature in features:
        fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
        list_hog_fd.append(fd)
    hog_features = np.array(list_hog_fd, 'float64')

    clf = LinearSVC()

    # On realise l'entrainement pour l'idendification
    clf.fit(hog_features, labels)

    # On enregistre le classifier
    joblib.dump(clf, ""digits_cls.pkl"", compress=3)


def find_lim(img, func, lim_min, lim_max):",digitRecognition/performRecognition.py,TeKrop/PyDigR,1
"import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 2, 4], 'gamma':[0.125, 0.25, 0.5 ,1, 2, 4]}
svr = svm.SVC()
clf = GridSearchCV(svr, parameters, n_jobs=-1)
clf.fit(iris.data, iris.target)
cv_result = pd.DataFrame.from_dict(clf.cv_results_)
with open('cv_result.csv','w') as f:
    cv_result.to_csv(f)
    
print('The parameters of the best model are: ')
print(clf.best_params_)
",svm_grid_search/svm_grid_search.py,JiJingYu/tensorflow-exercise,1
"# X is the input features by row.
X = np.zeros((200,3))
X[:n_samples/2] = rs.multivariate_normal( np.ones(3), np.eye(3), size=n_samples/2)
X[n_samples/2:] = rs.multivariate_normal(-np.ones(3), np.eye(3), size=n_samples/2)
# Y is the class labels for each row of X.
Y = np.zeros(n_samples); Y[n_samples/2:] = 1

print(X)

# Fit the data with an svm
svc = SVC(kernel='linear')
svc.fit(X,Y)

# The equation of the separating plane is given by all x in R^3 such that:
# np.dot(svc.coef_[0], x) + b = 0. We should for the last coordinate to plot
# the plane in terms of x and y.

z = lambda x,y: (-svc.intercept_[0]-svc.coef_[0][0]*x-svc.coef_[0][1]) / svc.coef_[0][2]

tmp = np.linspace(-2,2,51)",3d.py,lucasalexsorensen/invo,1
"
Comparison of different linear SVM classifiers on a 2D projection of the iris
dataset. We only consider the first 2 features of this dataset:

- Sepal length
- Sepal width

This example shows how to plot the decision surface for four SVM classifiers
with different kernels.

The linear models ``LinearSVC()`` and ``SVC(kernel='linear')`` yield slightly
different decision boundaries. This can be a consequence of the following
differences:

- ``LinearSVC`` minimizes the squared hinge loss while ``SVC`` minimizes the
  regular hinge loss.

- ``LinearSVC`` uses the One-vs-All (also known as One-vs-Rest) multiclass
  reduction while ``SVC`` uses the One-vs-One multiclass reduction.
",projects/scikit-learn-master/examples/svm/plot_iris.py,DailyActie/Surrogate-Model,1
"#print l_train
#print Y

if (X_train.size != len(l_train)):
    print ""Training sample size %d, Exercise Labels %d Do not match"" % ( X_train.size, len(l_train))
    sys.exit()    
   

ex_classifier = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

ex_classifier.fit(X_train, Y)
    
# save the classifier
_ = joblib.dump(ex_classifier, 'cf_ml_exercise.pkl', compress=9)
_ = joblib.dump(lb, 'cf_ml_binerizer.pkl', compress=9)

print ""Successfully generated the classfier and binarizer""",multilabel/online_training/cf_ml_trainer.py,praveen049/text_classification,1
"        test_label[test_label == -1] = 0
        pred[pred == -1] = 0

        score.append(accuracy_score(test_label, pred))
        score.append(precision_score(test_label, pred))
        score.append(recall_score(test_label, pred))
        score.append(f1_score(test_label, pred))
        scores.append(score)
    avg = np.average(scores, axis=0)
    return avg
print validate(SVC(C=10000, gamma=0.75), feature, 500)
print validate(LinearSVC(C=100), feature, 500)
print validate(LogisticRegression(C=100), feature, 500)",learning/final4.py,fcchou/CS229-project,1
"    parse_data=parse_csv(Train_File_name)
    
    #input Feature & output label
    X,Y = create_dataset(parse_data)
    #for i in range(len(X)):
    #    print X[i],Y[i]

    #Draw(Y,X)
    
    #Now Create & Train Our Classifier
    clsf=SVC(kernel='rbf',gamma=0.1,C=1) #SVM Classier
    print 'Training Started ..'
    a = datetime.datetime.now()
    clsf.fit(X, Y)
    b=datetime.datetime.now()
    print 'Training Is completed, Time taken for training :',b-a
    
    #Now Load Testing dataset
    parse_data=parse_csv(Test_file_name,)
    P,Q= create_dataset(parse_data,Train=False)",Kaggle/Titanic/MySVMPredicter.py,sudhanshuptl/Machine-Learning,1
"                #(""jj_preprocess"", POSTransformer(
                    #tokens_to_replace=[""JJ"", ""JJR"", ""JJS""])),
                #(""jj_vect"", TfidfVectorizer(   preprocessor=pass_input,
                                               #tokenizer=pass_input,
                                               #use_idf=True,
                                               #sublinear_tf=True,
                                               #smooth_idf=True))
            #])),
        #])),
        #(""selection"", SelectKBest(f_classif, k=2000)),
        ##(""clf"", svm.LinearSVC())
        #(""clf"", svm.SVC())
        ##(""clf"", MultinomialNB())
    #])

    #linear_svc = svm.LinearSVC(loss=""l2"")

    #pipe = Pipeline([
        #(""trigrams"", Pipeline([
            #(""preprocess"", TrigramPOSTransformer()),",ps3.py,dropofwill/nlp-predicting-turn-types,1
"	y = np.ravel(np.array(tr_c))
	X_test = np.array(ts_s)
	y_test = np.ravel(np.array(ts_c))

	# KNN
	clf = neighbors.KNeighborsClassifier()
	clf.fit(X, y)
	knn_score = clf.score(X_test, y_test)
	
	# SVM
	clf = svm.SVC()
	clf.fit(X, y)
	svm_score = clf.score(X_test, y_test)
	
	# output results
	print(extname)
	print(""KNN: "" + str(knn_score * 100) + ""%"")
	print(""SVM: "" + str(svm_score * 100) + ""%"")",classification/classification.py,keizerzilla/char-ext,1
"            else:
                print('Label {0} exists with score {1}. Retraining'.format(label, score))
        self.fit_new_node(X, Y, label)
        return True

    def fit_new_node(self, X, Y, label):
        sample_count = X.shape[0]
        input_and_features = np.zeros(shape=[sample_count, self.input_dimension + len(self.labels)])
        input_and_features[:, :self.input_dimension] = X
        input_and_features[:, self.input_dimension:] = self.activate_all(X)
        linear_svc = svm.LinearSVC(dual=False, penalty='l1')
        linear_svc.fit(input_and_features, Y)
        score = linear_svc.score(input_and_features, Y)
        print('Trained new Linear SVC with score ' + str(score))
        learned_transform_function = transform_function.SVCTransformFunction(
            input_dimension=input_and_features.shape[1],
            svm=linear_svc)
        node_name = self.get_new_node_name(label)
        input_names = self.node_manager.get_input_names() + self.latest_node_names()
        new_node = node.Node(name=node_name,",layeredneuralnetwork/layered_neural_network.py,abhishekraok/LayeredNeuralNetwork,1
"    n = len(t)
    true_num = 0
    for i in range(n):
        X_train = list(X_vectors)
        del X_train[i]
        t_train = list(t)
        del t_train[i]
        X_test = X_vectors[i]
        t_test = t[i]

        clf = SVC()
        clf.fit(X_train, t_train)
        y = clf.predict(X_test)
        if y == t_test:
            true_num += 1
    accuracy = 1.0 * true_num / n

    # 8/2 split
    X = np.array(X_vectors)
    tt = list(t)",test_algorithm/test.py,enirinth/free-food-calendar,1
"df.max()
df.min()
type(predictors)

#==============================================================================
# Model fitting part 2?
#==============================================================================

#==============================================================================
# # fit the model
# clf = svm.NuSVC()
# clf.fit(predictors, outcomes)
#
# # plot the decision function for each datapoint on the grid
# Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
# Z = Z.reshape(xx.shape)
#
# plt.imshow(Z, interpolation='nearest',
#            extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
#            origin='lower', cmap=plt.cm.PuOr_r)",Visual_Game/CreatingBaseDF.py,kizzen/Baller-Shot-Caller,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def processFrame(self, dataURL, identity):
        head = ""data:image/jpeg;base64,""
        assert(dataURL.startswith(head))
        imgdata = base64.b64decode(dataURL[len(head):])
        imgF = StringIO.StringIO()
        imgF.write(imgdata)
        imgF.seek(0)
        img = Image.open(imgF)",demos/web/websocket-server.py,cmusatyalab/openface,1
"                X_valid,y_valid = valid_set
                X_test,y_test = test_set
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_0519_2.py,magic2du/contact_matrix,1
"predictors_tr = tfidftr

targets_tr = traindf['cuisine']

predictors_ts = tfidfts


# classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
classifier = RandomForestClassifier(n_estimators=200)

classifier=classifier.fit(predictors_tr,targets_tr)
",deep_learn/whatiscooking/ReadCookingPlus.py,zhDai/CToFun,1
"        from sklearn.model_selection import StratifiedKFold
        from sklearn.svm import SVC
        from sklearn.pipeline import Pipeline
        from nilearn.decoding import SearchLight

        nimgs = self.write_4D(return_nimg=True)
        cv = StratifiedKFold(self.y, n_folds=n_folds)

        if estimator is None:
            estimator = Pipeline([('scaler', StandardScaler()),
                                  ('svm', SVC(kernel='linear', C=1))])

        if isinstance(radius, (int, float)):
            radius = [radius]

        for i, nimg in enumerate(nimgs):

            if mask is None:
                mask_tmp = self.fs_masks[i]
                if mask_tmp is not None:",skbold/core/mvp_between.py,lukassnoek/skbold,1
"import matplotlib.pyplot as plt
import numpy as np
from sklearn import svm

# we create 40 separable points
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# fit the model
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)

# get the separating hyperplane
w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-5, 5)
yy = a * xx - (clf.intercept_[0]) / w[1]

# plot the parallels to the separating hyperplane that pass through the",projects/scikit-learn-master/examples/svm/plot_separating_hyperplane.py,DailyActie/Surrogate-Model,1
"	kf_n_c = model_selection.KFold( n_splits=n_splits, shuffle=shuffle)
	kf_n = kf5_ext_c.split( xM)
	yV_pred = model_selection.cross_val_predict( clf, xM, yV, cv = kf_n, n_jobs = n_jobs)

	if graph:
		print('The prediction output using cross-validation is given by:')
		jutil.cv_show( yV, yV_pred, grid_std = grid_std)

	return yV_pred

def gs_SVC( X, y, params, n_splits = 5):
	return gs_classfier( svm.SVC(), X, y, params, n_splits=n_splits)

def gs_classfier( classifier, xM, yVc, params, n_splits=5, n_jobs=-1):
	""""""
	gs = gs_classfier( classifier, xM, yVc, params, n_splits=5, n_jobs=-1)

	Inputs
	======
	classifier = svm.SVC(), for example",kgrid_r0.py,jskDr/jamespy_py3,1
"    cache = os.path.join(args.workDir, 'fisherFacesExp.pkl')
    fishFacesDf = cacheToFile(cache)(opencvExp)(lfwPpl, cls)

    print(""LBPH Experiment"")
    cls = cv2.createLBPHFaceRecognizer()
    cache = os.path.join(args.workDir, 'lbphExp.pkl')
    lbphFacesDf = cacheToFile(cache)(opencvExp)(lfwPpl, cls)

    print(""OpenFace CPU/SVM Experiment"")
    net = openface.TorchNeuralNet(args.networkModel, 96, cuda=False)
    cls = SVC(kernel='linear', C=1)
    cache = os.path.join(args.workDir, 'openface.cpu.svm.pkl')
    openfaceCPUsvmDf = cacheToFile(cache)(openfaceExp)(lfwPpl, net, cls)

    print(""OpenFace GPU/SVM Experiment"")
    net = openface.TorchNeuralNet(args.networkModel, 96, cuda=True)
    cache = os.path.join(args.workDir, 'openface.gpu.svm.pkl')
    openfaceGPUsvmDf = cacheToFile(cache)(openfaceExp)(lfwPpl, net, cls)

    plotAccuracy(args.workDir, args.largeFont,",openface/evaluation/lfw-classification.py,SeonghoBaek/RealtimeCamera,1
"    smote = SMOTE(random_state=RND_SEED, kind=kind, k_neighbors=nn_k)

    assert_raises_regex(ValueError, ""has to be one of"",
                        smote.fit_sample, X, Y)


def test_sample_regular_with_nn_svm():
    # Create the object
    kind = 'svm'
    nn_k = NearestNeighbors(n_neighbors=6)
    svm = SVC(random_state=RND_SEED)
    smote = SMOTE(
        random_state=RND_SEED, kind=kind, k_neighbors=nn_k, svm_estimator=svm)

    X_resampled, y_resampled = smote.fit_sample(X, Y)

    X_gt = np.array([[0.11622591, -0.0317206], [0.77481731, 0.60935141],
                     [1.25192108, -0.22367336], [0.53366841, -0.30312976],
                     [1.52091956, -0.49283504], [-0.28162401, -2.10400981],
                     [0.83680821, 1.72827342], [0.3084254, 0.33299982],",imblearn/over_sampling/tests/test_smote.py,chkoar/imbalanced-learn,1
"        self.assert_frame_equal(tr, df.loc[['g', 'a', 'e', 'f', 'd', 'h']].reset_index(drop=True))
        self.assert_numpy_array_equal(tr.target.values, np.array([7, 1, 5, 6, 4, 8]))
        self.assert_frame_equal(te, df.loc[['c', 'b']].reset_index(drop=True))
        self.assert_numpy_array_equal(te.target.values, np.array([3, 2]))

    def test_cross_val_score(self):
        import sklearn.svm as svm
        digits = datasets.load_digits()

        df = expd.ModelFrame(digits)
        clf = svm.SVC(kernel=str('linear'), C=1)
        result = df.cross_validation.cross_val_score(clf, cv=5)
        expected = cv.cross_val_score(clf, X=digits.data, y=digits.target, cv=5)
        self.assert_numpy_array_almost_equal(result, expected)

    def test_permutation_test_score(self):
        import sklearn.svm as svm
        iris = datasets.load_iris()

        df = expd.ModelFrame(iris)",expandas/skaccessors/test/test_cross_validation.py,sinhrks/expandas,1
"
#svm = SVC(kernel='linear', C=best_params['C1'], class_weight=class_weights)
#svm.fit(data[:,1:3], data[:,0])
#print svm.coef_
#print svm.intercept_
#svm = SVC(kernel='linear', C=best_params['C1'], class_weight=class_weights)
#svm.fit(data[:,3:5], data[:,0])
#print svm.coef_
#print svm.intercept_

#svm = SVC()
#search = GridSearchCV(svm, {
#    'kernel': ('rbf',), 'C': [1,10,100,1000],
#    'gamma': [0.05, 0.1, 0.25, 0.128, 0.5, 1.0, 1.5],
#    'class_weight': ({1:1,-1:1},),
#    }, cv=cv)
svm = LinearSVC()
search = GridSearchCV(svm, {
    'C': np.logspace(0,4,15).tolist(),
    'class_weight': (",scripts/pw_analyze/svmdelme.py,mop/LTPTextDetector,1
"X =X.drop(""status"",1)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=7)

bestscore = 0

for c in xrange(5,205, 5):
	for g in xrange(1, 101, 1):
		c = c/100.0
		g = g/1000.0
		model = SVC(kernel = ""rbf"",C = c, gamma = g)
		model.fit(X_train,y_train)
		score = model.score(X_test,y_test)
		
		if score > bestscore:
			bestscore = score
			print ""C = ""+str(c) + "" gamma = ""+str(g) + ""; score "" + str(score)
",MachineLearning/SVC.py,adrianjg/Tools,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_04_22_2015_parallel_for_final.py,magic2du/contact_matrix,1
"
    print ""Training...""

    # commented out to delete .toarray() option, because len(data_X[0]) is not defined
    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.2.py,totuta/deep-supertagging,1
"    
	####################################################################
	# Gaussian samples operation
	####################################################################

	for i in range(1,101):
		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        clf = SVC(C=496.6,gamma=0.00767,probability=True, cache_size=7000)
        args=[str(dim)+ ""Dgauss_svm_again"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
        #For nn:
        #args=[str(dim)+""Dgauss_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),200,6]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,0,args)
",Dalitz_simplified/evaluation_of_optimised_classifiers/svm_gauss/svm_Gauss_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"				 map(os.path.split,
					 map(os.path.dirname, labels))) # Get the directory.
	fname = ""{}/reps.csv"".format(workDir)
	embeddings = pd.read_csv(fname, header=None).as_matrix()
	le = LabelEncoder().fit(labels)
	labelsNum = le.transform(labels)
	nClasses = len(le.classes_)
	print(""Training for {} classes."".format(nClasses))

	# Linear SVM classifier
	clf = SVC(C=1, kernel='linear', probability=True)

	if ldaDim > 0:
		clf_final = clf
		clf = Pipeline([('lda', LDA(n_components=ldaDim)),
						('clf', clf_final)])

	clf.fit(embeddings, labelsNum)

	# save classifer in package",testing_files/infer.py,xamgeis/project_vulcan,1
"
def sgrna_from_doench_on_fold(feature_sets, train, test, y, y_all, X, dim, dimsum, learn_options):
    assert len(feature_sets.keys()) == 1, ""should only use sgRNA Score here""
    assert feature_sets.keys()[0] == ""sgRNA Score""
    y_pred = X[test][:, 0]
    return y_pred, None


def SVC_on_fold(feature_sets, train, test, y, y_all, X, dim, dimsum, learn_options):
    y_bin = y_all[learn_options['binary target name']].values[:, None]
    clf = LinearSVC(penalty='l2', dual=False)
    clf.fit(X[train], y_bin[train].flatten())
    y_pred = clf.predict(X[test])[:, None]
    return y_pred, clf


    ",azimuth/models/baselines.py,benchling/Azimuth,1
"from sklearn.ensemble import RandomForestClassifier


def My_random_forest(train_data_features,labels,test_data_features):
	forest = RandomForestClassifier(n_estimators=100);
	forest = forest.fit(train_data_features,labels);
	return forest.predict(test_data_features);

def My_svm(train_data_features,labels,test_data_features):

	#clf = svm.SVC(kernel='linear');
	clf = svm.SVC()
	clf.fit(train_data_features,labels);
	return clf.predict(test_data_features);

def My_svm2(train_data_features,labels,test_data_features):

	clf = svm.SVC(kernel='linear');
	clf.fit(train_data_features,labels);
	return clf.predict(test_data_features);",Classifiers.py,yxun/SAL,1
"

class ErrorGenBadHost(ErrorGenBase):
    CLASS = SCMPClass.ROUTING
    TYPE = SCMPRoutingClass.BAD_HOST
    DESC = ""bad host""

    def _build_pkt(self):
        pkt = super()._build_pkt()
        pkt.set_payload(IFIDPayload.from_values(77))
        pkt.addrs.dst.host = HostAddrSVC(99, raw=False)
        return pkt


class ErrorGenBadVersion(ErrorGenBase):
    DESC = ""bad version""

    def _send_pkt(self, spkt, first_hop):
        next_hop, port = self._get_next_hop(spkt)
        raw = bytearray(spkt.pack())",python/integration/scmp_error_test.py,netsec-ethz/scion,1
"            vector[word_dict[word][0]] += 1
        for word in x:
            vector[word_dict[word][0]] *= math.log10(1.0 * doc_num / word_dict[word][1])
        X_vectors.append(vector)

    prior, likelihood, num = train_naive_bayes(X, t)
    nb_clf = [prior, likelihood, num]
    joblib.dump(nb_clf, 'nb.pkl')
    clf = joblib.load('nb.pkl')

    svm_clf = SVC()
    svm_clf.fit(X_vectors, t)
    joblib.dump(svm_clf, 'svm.pkl')

    knn_clf = KNeighborsClassifier()
    knn_clf.fit(X_vectors, t)
    joblib.dump(knn_clf, 'knn.pkl')

    dt_clf = DecisionTreeClassifier()
    dt_clf.fit(X_vectors, t)",test_algorithm/train.py,enirinth/free-food-calendar,1
"


#########################################################
### your code goes here ###

from sklearn.svm import SVC
from sklearn.externals import joblib

# C = 10000 is optimized following training course. 10,100,1000 were tried as well.
clf = SVC(kernel=""rbf"", C=10000)
retrain = False
try:
    clf = joblib.load('clf.pki')
except:
    print('Need to retrain');
    retrain = True

# slice the array to 1% since RBF is so slow
#features_train = features_train[:len(features_train)/100] ",proj/ud120-projects/svm/svm_author_id.py,askldjd/udacity-machine-learning,1
"    #generated train and test lists, incuding indices of the examples in training/test
    #for the specific fold. Indices starts from 0 now
    total_index=copy(train_index)
    total_index.extend(test_index)
    #print total_index
    #print y_train.tolist(),y_test.tolist()
    temp=copy(y_train.tolist())
    temp.extend(y_test.tolist())
    y_total=np.array(temp)
    #y_total.extend(y_test)
    clf = svm.SVC(C=c, kernel='precomputed',max_iter=10000000)
    clf1 = svm.SVC(C=c, kernel='precomputed',max_iter=10000000)
    
    train_gram = [] #[[] for x in xrange(0,len(train))]
    test_gram = []# [[] for x in xrange(0,len(test))]
    total_gram = []
    #compute training and test sub-matrices
    index=-1    
    for row in gram:
        index+=1",scripts/cross_validation_NEUROCOMPUTING16.py,nickgentoo/scikit-learn-graph,1
"    X_testtarget = array_testtarget
    data_X_test = X_test[:count_test]
    data_Y_test = X_testtarget[:count_test]
    # data_X_test = X_test[:int(DATA_SIZE*0.8)]
    # data_Y_test = X_testtarget[:int(DATA_SIZE*0.8)]


    # create linear regression object
    # regr = linear_model.LinearRegression()
    # regr = linear_model.Perceptron()
    # regr = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))
    regr = OneVsRestClassifier(linear_model.LinearRegression())

    # train the model using the training data
    # regr.fit(X_train, X_target)
    regr.fit(data_X_train, data_Y_train)

    # predict to test
    print ""Predicting...""
    test_result = regr.predict(data_X_test)",DST_v1.0.py,totuta/deep-supertagging,1
"def main():
   tweets = read_tsvs(args.tsv_files)
   tweets = CountVectorizer(min_df=0.).fit_transform(tweets)
   l.info('read tweets with dimension %s', tweets.shape)
   tweets = resize(tweets, args.test_size)
   l.info('resized to %s', tweets.shape)
   l.info('training on %d random instances.' % args.train_size)
   classifiers = [
      LogisticRegression(),
      MultinomialNB(),
#      SVC(kernel='linear'),  # too slow! (orders of magnitude)
      ]
   y = rand.random_integers(0, 1, args.train_size)
   for c in classifiers:
      c.fit(tweets[:args.train_size], y)
   l.info('beginning testing. all times in seconds.')
   slices = [int(x) for x in args.slices.split(':')]
   sizes = range(args.test_size + 1)[slices[0]:slices[1]:slices[2]]
   l.info('    model name   \t' + '\t'.join(str(s) for s in sizes))
   times = []",experiments/2014_CSCW_Tweetlocating/code/classify_benchmark.py,joh12041/quac,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the gini_index score of each feature
        score = gini_index.gini_index(X[train], y[train])

        # rank features in descending order according to score
        idx = gini_index.feature_ranking(score)
",PyFeaST/example/test_gini_index.py,jundongl/PyFeaST,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = RSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 5.0,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/constraints_cmaesrsvc/setup.py,jpzk/evopy,1
"fit_data = instance_a[0] + instance_n[0]
fit_classes = instance_a[1] + instance_n[1]

print(""Training the model...."")

##################################################
##############Train the Support Vector############
######################Machine#####################
##################################################

clf = svm.SVC()
clf.fit(fit_data,fit_classes)

print(""Model has been trained, building test dataset..."")

##################################################
#############Create the validation data###########
##################################################
##################################################
",format_py/n_gram_svm_cv.py,doylew/detectionsc,1
"
high_C = 1
for pre in np.arange(5) * 0.05 + 0.55:
    low_C = 0
    while(True):
        
        learner = LogisticRegression(penalty = 'l1',
                                     dual = False,
                                     C = high_C,
                                     fit_intercept = True)
        learner = sklearn.svm.LinearSVC(C = high_C,
            penalty = 'l1',
            dual = False)
        learner.fit(tmpX[cvs[0][0],],y[cvs[0][0]])
        
        feature_count = len(learner.coef_[learner.coef_ != 0])
        score = roc_auc_score(y[cvs[0][0]], learner.decision_function(
            tmpX[cvs[0][0],]))
        print(feature_count, score, low_C, high_C)
        if (pre - score > 0.005):",test_ratboost.py,adrinjalali/Network-Classifier,1
"
# import data
train, labels, test, _, _ = utils.load_data()

# transform counts to TFIDF features
tfidf = feature_extraction.text.TfidfTransformer(smooth_idf=False)
train = np.append(train, tfidf.fit_transform(train).toarray(), axis=1)
test = np.append(test, tfidf.transform(test).toarray(), axis=1)

# feature selection
feat_selector = LinearSVC(C=0.095, penalty='l1', dual=False)
train = feat_selector.fit_transform(train, labels)
test = feat_selector.transform(test)

print train.shape

# encode labels
lbl_enc = preprocessing.LabelEncoder()
labels = lbl_enc.fit_transform(labels)
",otto/model/model_16_random_forest_calibrated_feature_selection/random_forest_calibrated_feature_selection.py,ahara/kaggle_otto,1
"                     ('nbclf', MultinomialNB()),
                    ])


def svc_pipeline():
    if PRODUCTION:
        return Pipeline([('vect', CountVectorizer(ngram_range=(1,2),
                                  analyzer='word', tokenizer=LemmaTokenizer(),
                                  max_features=1000, binary=True)),
                         ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),
                         ('svcclf', SVC(kernel='rbf', gamma='0.001', C='1000')),
                        ])

    return Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('svcclf', SVC()),
                    ])


def pipeline_classify(training_data, training_classes, test_data, test_data_classes):",src/data_set_builder/bag_of_words.py,piatra/ssl-project,1
"        self.assert_frame_equal(tr, df.loc[['g', 'a', 'e', 'f', 'd', 'h']].reset_index(drop=True))
        self.assert_numpy_array_equal(tr.target.values, np.array([7, 1, 5, 6, 4, 8]))
        self.assert_frame_equal(te, df.loc[['c', 'b']].reset_index(drop=True))
        self.assert_numpy_array_equal(te.target.values, np.array([3, 2]))

    def test_cross_val_score(self):
        import sklearn.svm as svm
        digits = datasets.load_digits()

        df = pdml.ModelFrame(digits)
        clf = svm.SVC(kernel=str('linear'), C=1)
        result = df.model_selection.cross_val_score(clf, cv=5)
        expected = ms.cross_val_score(clf, X=digits.data, y=digits.target, cv=5)
        self.assert_numpy_array_almost_equal(result, expected)

    def test_permutation_test_score(self):
        import sklearn.svm as svm
        iris = datasets.load_iris()

        df = pdml.ModelFrame(iris)",pandas_ml/skaccessors/test/test_model_selection.py,sinhrks/pandas-ml,1
"            # print(""grammar: ""+str(grammar)
            #         +""\n\tfirst: ""+str(first)
            #         +""\n\tfollow: ""+str(follow)
            #         +""\n\tfeature: ""+str(feature)
            #         +""\n"")
        if not (n == len(fNLines)+1):
            raise Exception('Julien messed up the training data format')
        fN.close()

        #use the training data to build the model
        self.clf = svm.SVC()
        self.clf.fit(self.tData, self.labels)


    def predictGrammar(self, grammar, first, follow):
        # A helper function that predict whether a single grammar is interesting 
        # Input: the feature array of a single grammar
        # Return: [0] indicating not interesting or [1] indicating interesting
        if not (isinstance(grammar,dict) and isinstance(first,dict) and isinstance(follow,dict)):
            raise Exception('Wrong format of grammar input')",LL1_Academy/tools/SvmLearn.py,H-Huang/LL1-Academy,1
"      y = np.asarray(self.data)[:, 2]
      if np.unique(y).size == 1:
         # only one-class
         self.clf = isvc(nu=params['nu'],
                          gamma=params['gamma'],
                          degree=params['degree'],
                          coef0=params['coef0'],
                          kernel=params['kernel'])
         self.clf.fit(X)
      else:
         self.clf = SVC(C=params['C'],
                        gamma=params['gamma'],
                        degree=params['degree'],
                        coef0=params['coef0'],
                        kernel=params['kernel'])
         self.clf.fit(X, y)
      self.is_fitted = True
      self.changed(""model_fitted"")

   def changed(self, status):",mlcore/views/classifier_demo.py,feuerchop/h3lib,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)


def test_ajd():
    """"""Test if Approximate joint diagonalization implementation obtains same
    results as the Matlab implementation by Pham Dinh-Tuan.
    """"""",mne/decoding/tests/test_csp.py,jniediek/mne-python,1
"    transformer = TfidfTransformer()
    tfidf_train = transformer.fit_transform(countMatrix_train)
    tfidf_test = transformer.transform(countMatrix_test)
    
    tfidf_train2 = transformer.fit_transform(countMatrix_train2)
    tfidf_test2 = transformer.transform(countMatrix_test2)
    
    print tfidf_train.shape
    print tfidf_test.shape
    
    clf_svm = svm.SVC(kernel='linear')
    clf_svm.fit(tfidf_train, y_train)
    
    clf_mNB=MultinomialNB()
    clf_mNB.fit(tfidf_train, y_train)
    
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(tfidf_train, y_train)
    
    clf_ada=RandomForestClassifier(n_estimators=25)",code_python27/tfidfANDclasification/k10foldClassify.py,rcln/tag.suggestion,1
"from sklearn.neighbors import KNeighborsClassifier

class train(object):
	
	def __init__(self,features,labels):
		self.features = features
		self.labels = labels


	def getSVM(self):
		clf = svm.SVC(kernel = ""rbf"",gamma = 0.001,C = 100)
		clf.fit(np.array(self.features),np.array(self.labels))
		return clf

	def getGaussianNB(self):
		clf  = GaussianNB()
		clf.fit(np.array(self.features),np.array(self.labels))
		return clf

	def getDecisionTree(self):",train.py,CyrisCodev/Custom-3D-Gesture-Recognition-Python-,1
"    }

    return classifiers[clfmethod], clgrid[clfmethod]


#-------------------------------------------------------------------------------
def get_fsmethod (fsmethod, n_feats, n_subjs, n_jobs=1):

    #Feature selection procedures
                                #http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html
    fsmethods = { 'rfe'       : RFE(estimator=SVC(kernel=""linear""), step=0.05, n_features_to_select=2),
                                #http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html
                  'rfecv'     : RFECV(estimator=SVC(kernel=""linear""), step=0.05, loss_func=auc_score), #cv=3, default; cv=StratifiedKFold(n_subjs, 3)
                                #Univariate Feature selection: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html
                  'univariate': SelectPercentile(f_classif, percentile=5),
                                #http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html
                  'fpr'       : SelectFpr (f_classif, alpha=0.05),
                                #http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html
                  'fdr'       : SelectFdr (f_classif, alpha=0.05),
                                #http://scikit-learn.org/stable/modules/feature_selection.html",aizkolari_classification.py,alexsavio/aizkolari,1
"    c=1
    return 1. / (c+dist)


features, class_outputs = get_training('../joined_matrix_2.txt') 
# 0:27 category  28:38= supervisor district, 39 = count 40= output
class_outputs = preprocessing.binarize(class_outputs)
features_10k, class_10k = features[:10000, 28:39], class_outputs[:10000]
test_features_10k, test_class_10k = features[10000:20000, 28:39], class_outputs[10000:20000]

#all_models = [svm.LinearSVC(), svm.SVC(), tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
#all_models = [tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
all_models = [ensemble.AdaBoostClassifier()]
""""""
param_grids = [[{'min_samples_leaf':[500], 'splitter':['best']}],
		[{'min_samples_leaf':[5], 'n_estimators':[300]}],
		[{'learning_rate':[0.7], 'n_estimators':[100]}],
		[{'weights':[better_inv_dist], 'n_neighbors':[500]}]]
""""""
opt_models = dict()",classificationAggregate/train_classification.py,JamesWo/cs194-16-data_manatees,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-5, 5, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = ORIDSESAlignedSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 70,
        initial_sigma = matrix([[4.5, 4.5]]),
        delta = 4.5,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/problems/TR/ORIDSESAlignedSVC.py,jpzk/evopy,1
"from math import sqrt
import numpy as np


iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target
C = 1.0

# kernels
rbf_ker = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_ker = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)

# kfold cv
score_rbf = cross_val_score(rbf_ker, iris.data, iris.target, cv=6,
                            scoring='f1_macro')
score_poly = cross_val_score(poly_ker, iris.data, iris.target, cv=6,
                             scoring='f1_macro')
# mitchell ttest
k = len(score_rbf)",machine-learning/ml#2/svm.py,JakubNvk/school,1
"
class _NotFittedError(ValueError, AttributeError):
    """"""Exception class to raise if estimator is used before fitting.
    This class inherits from both ValueError and AttributeError to help with
    exception handling and backward compatibility.
    Examples
    --------
    >>> from sklearn.svm import LinearSVC
    >>> from sklearn.exceptions import NotFittedError
    >>> try:
    ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
    ... except NotFittedError as e:
    ...     print(repr(e))
    ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    NotFittedError('This LinearSVC instance is not fitted yet',)

    Copied from https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
    """"""

def _accuracy_score(y_true, y_pred):",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,shiyemin/tensorflow,1
"	#X_train = np.load('X_train.npy')
	#print X_train.shape
	#X_test = np.load('X_test.npy')
	X_train, X_test = compute_tSNE_embedding(trainX, testX)

	#plot_embedding(X_train, trainY, ""tsne embedding of the digits (time %.2fs)"" %(time.time() - start))
	#plt.show()

	parameters = {'n_neighbors' : list(np.arange(20)+1)}
	clf = GridSearchCV(KNeighborsClassifier(weights='distance', n_jobs=-1), parameters)
	#clf = svm.SVC(kernel=kernel.arc_cosine, cache_size=2048)
	clf.fit(X_train, trainY)


	pred = clf.predict(X_test)
	print accuracy_score(testY, pred)
	print confusion_matrix(testY, pred)
	#print(clf.best_params_)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))
",visual_analysis/tSNE_Embedding/visualize.py,akhilpm/Masters-Project,1
"        print(""+ Training SVM on {} labeled images."".format(len(self.images)))
        d = self.getData()
        if d is None:
            self.svm = None
            return
        else:
            (X, y) = d
            numIdentities = len(set(y + [-1]))
            if numIdentities <= 1:
                return
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)
            globalSvm = self.svm
            globalImages = self.images
            globalPeople = self.people
            
            # save the trained data set
            
            pickle.dump(self.people, open(""../RTNiFiOpenFace/ofpeople.ini"", ""wb""))
            pickle.dump(self.images, open(""../RTNiFiOpenFace/ofimages.ini"", ""wb""))
            ",websocket-server.py,richards-tech/RTNiFiOpenFace,1
"                    for otherVector in matrix:
                        otherVector.append(default)
            matrix.append(dense)
        return matrix

## SupportVectorMachine #################################################

class SupportVectorMachine(SKLearnClassifier):
    
    def __init__(self, C=1.0):
        self.clf = sklearn.svm.SVC(C=C)
    
    def classify(self, object):
        return self.clf.predict(self.featurize(object))[0]

## LogisticRegressor ####################################################

class LogisticRegressor(Classifier):

    def __init__(self):",ashlib/ml/clf.py,ashkonf/python-ashlib,1
"class SVMIrisTestCase(unittest.TestCase):

    def setUp(self):
        iris = datasets.load_iris()
        X = iris.data
        y = iris.target
        self.X_train, self.X_test, self.y_train, self.y_test = \
            train_test_split(X, y, test_size=0.3, random_state=1126)

    def test_svm(self):
        svc_clf = SVC()
        svc_clf.fit(self.X_train, self.y_train)
        svm = SVM()
        svm.train(Dataset(self.X_train, self.y_train))

        assert_array_equal(
            svc_clf.predict(self.X_train), svm.predict(self.X_train))
        assert_array_equal(
            svc_clf.predict(self.X_test), svm.predict(self.X_test))
        self.assertEqual(",libact/models/tests/test_svm.py,ntucllab/libact,1
"# Split the data into a training set and a test set.
train_x, test_x, train_y, test_y = train_test_split(
    iris.data, iris.target, test_size=0.10, random_state=20)

# These are the training features.
# Only use Petal length and Petal width features
train_x = train_x[:, :2]
test_x = test_x[:, :2]

# Generate the classifier model. Fit the data.
lin_svc = LinearSVC().fit(train_x, train_y)

# Get the model's predictions for the test data.
predictions = lin_svc.predict(test_x)

# Evaluate the accuracy of the predictions.
score = metrics.accuracy_score(test_y, predictions)
print ""accuracy:  %0.3f"" % score

# Generate a more detailed classification report.",tutorial_3/3_svm.py,chene5/Big-Data-in-Psychology,1
"                        y.shape=(y.size,)    
                        cv = StratifiedKFold(y, n_folds=3)
                        grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_rf, cv=cv,n_jobs=n_jobs)
                        grid.fit(x, y)
                        model = grid.best_estimator_
                        model.fit(x,y)        
                    elif inClassifier == 'SVM':
                        param_grid_svm = dict(gamma=2.0**sp.arange(-4,4), C=10.0**sp.arange(-2,5))
                        y.shape=(y.size,)    
                        cv = StratifiedKFold(y, n_folds=5)
                        grid = GridSearchCV(SVC(), param_grid=param_grid_svm, cv=cv,n_jobs=n_jobs)
                        grid.fit(x, y)
                        model = grid.best_estimator_
                        model.fit(x,y)
                    elif inClassifier == 'KNN':
                        param_grid_knn = dict(n_neighbors = sp.arange(1,20,4))
                        y.shape=(y.size,)    
                        cv = StratifiedKFold(y, n_folds=3)
                        grid = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid=param_grid_knn, cv=cv,n_jobs=n_jobs)
                        grid.fit(x, y)",function_historical_map.py,lennepkade/HistoricalMap,1
"    def evaluate_ind(self, ind):
        no_of_inputs = ind.count(1)
        # Uses fold=0 for feature selection
        expected_outputs = get_training_data().Survived.values
        train_data = self.massage_data_with_outputs(get_training_data(), ind)

        # Uses fold=0 for feature selection
        expected_eval_outputs = get_evaluation_data().Survived.values
        eval_data = self.massage_data_with_outputs(get_evaluation_data(), ind)

        clf = svm.SVC()

        clf.fit(train_data, expected_outputs)

        evaluation = clf.predict(eval_data)

        em = EvaluationMetrics(evaluation, expected_eval_outputs)
        f1 = em.calculate_f1()

        # Optional",src/ga_for_svm.py,JakeCowton/titanic,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = DISR.disr(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_DISR.py,jundongl/scikit-feature,1
"import matplotlib.pyplot as plt

from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.learning_curve import validation_curve


cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]
training_scores, test_scores = validation_curve(LinearSVC(), X, y,
                                                param_name=""C"", param_range=cs)
plt.figure()
plot_validation_curve(range(7), training_scores, test_scores)


ks = range(10)
training_scores, test_scores = validation_curve(KNeighborsClassifier(), X, y,
                                                param_name=""n_neighbors"", param_range=ks)
plt.figure()",day3-machine-learning/solutions/validation_curve.py,Nschanche/AstroHackWeek2015,1
"dim = 30
x_p = np.random.multivariate_normal(np.ones(dim) * 1, np.eye(dim), num_p)
x_n = np.random.multivariate_normal(np.ones(dim) * 1.5, np.eye(dim), num_n)
x = np.vstack([x_p, x_n])
y = np.array([1.] * num_p + [-1.] * num_n)

# Hyper parameters
cost = 1e0
max_iter = 10000000

clf_mdsvm = csvc.SVC(C=cost, kernel='linear', max_iter=max_iter)
clf_sklearn = svm.SVC(C=cost, kernel='linear', max_iter=max_iter, shrinking=False, cache_size=1000)

t = time.time()
clf_mdsvm.fit(x, y)
print 'MDSVM:', time.time() - t, clf_mdsvm.score(x, y)

t = time.time()
clf_sklearn.fit(x, y)
print 'scikit-learn:', time.time() -t, clf_sklearn.score(x, y)",benchmarks/bench_linear_svc.py,sfujiwara/mdsvm,1
"class NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,TakayukiSakai/tensorflow,1
"    Yt = data[:,-1]
    return X, Y, Xt, Yt

 

def plotting(X, Y, Xt, Yt, labelx, labely, outputfile):
    h = .02  # step size in the mesh
    classifiers = dict(
    knn=neighbors.KNeighborsClassifier(4),
    logistic=linear_model.LogisticRegression(C=1e5),
    svm=svm.SVC(C=1e5),
    adaboost=ensemble.AdaBoostClassifier(),
    naivebay=naive_bayes.GaussianNB())

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    fignum = 1
    # we create an instance of Neighbours Classifier and fit the data.
    for name, clf in classifiers.iteritems():",MLNet-2.0/plotting/lr-knn-svm-comparison/plot_no_out_with_zero.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"from sklearn.naive_bayes import GaussianNB
naivebayes_model = GaussianNB()
naivebayes_model.fit(X_cropped, y_cropped)
y_validation_predicted = naivebayes_model.predict(X_validation)
print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)


# Start SVM Classification
print ""Performing SVM Classification:""
from sklearn.svm import SVC
svm_model = SVC(kernel='rbf' ,probability=True, max_iter=100000)
svm_model.fit(X_cropped, y_cropped)
y_train_predicted = svm_model.predict(X_train)
print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# plt.show()

y_validation_predicted = svm_model.predict(X_validation)
print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
",Code/Machine_Learning_Algos/training_t3.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"
#training tools
from sklearn import svm
from trainer import DemystegofierTrainer

class Demystegofier:
    def __init__(self):
        """"""Call before every test case.""""""

        #we are testing trainin
        self.trainer = DemystegofierTrainer( svm.SVC(kernel='linear'))
        self.feature_db = []

    def add_packet_path(self, packet_path, target_mark):
        """"""
        read all files in steg_packet_path and compute all features against
        the packet inside them. Then mark them as the target and add them
        to the feature_db

        INPUT::",src/demystegofier.py,vmon/demystegofier,1
"
mvp = MvpBetween(source=source, subject_idf='sub???', mask=bmask)
mvp.create()
fpath = op.join(testdata_path, 'sample_behav.tsv')
mvp.add_y(fpath, col_name='var_categorical', index_col=0, remove=999)


@pytest.mark.parametrize(""method"", ['fwm', 'forward', 'ufs'])
def test_mvp_results(method):

    clf = SVC(kernel='linear')
    ufs = SelectKBest(score_func=f_classif, k=100)
    pipe = Pipeline([('ufs', ufs), ('clf', clf)])

    folds = StratifiedKFold(n_splits=2)
    mvpr = MvpResultsClassification(mvp=mvp, n_iter=2,
                                    feature_scoring=method,
                                    out_path=testdata_path)

    for train_idx, test_idx in folds.split(mvp.X, mvp.y):",skbold/postproc/tests/test_mvp_results.py,lukassnoek/skbold,1
"    training_fids, test_fids = split_list(sample_fids, percent=percent)
    training_samples = vectorizer.fit_transform(training_fids)
    test_samples = vectorizer.fit_transform(test_fids)
    training_predictions, test_predictions = split_list(predictions, percent=percent)
    return training_samples, training_predictions, test_samples, test_predictions

ALL_WORDS = get_all_words()
TRAINING_SAMPLES, TRAINING_PREDICTIONS, TEST_SAMPLES, TEST_PREDICTIONS = get_samples_predictions(ALL_WORDS, 0.8)

print('Training...')
CLASSIFIER = SVC(kernel='rbf')
CLASSIFIER.fit(TRAINING_SAMPLES, TRAINING_PREDICTIONS)
print('Calculating mean accuracy')
print(CLASSIFIER.score(TEST_SAMPLES, TEST_PREDICTIONS))

################################################################################

profiler.disable()
stream = io.StringIO()
ps = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')",classification/classifier.py,sortizm/TextMiningPrototypes,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_08_2015_03.py,magic2du/contact_matrix,1
"import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt

sigma = np.dot(normalized.T, normalized) / (normalized.shape[0])
U, S, V = np.linalg.svd(sigma)

Features = np.arange(7, normalizedTrain.shape[1] + 1, 2)
clf = svm.SVC(C=3.4, gamma=0.0285, cache_size=1000)
probMatrix = []
errMatrix = []

for iii in Features:
    ZTrain = np.dot(normalizedTrain, U[:, 0:iii])
    ZCV = np.dot(normalizedCV, U[:, 0:iii])
    clf.fit(ZTrain, yTrain)
    err = []
    for jjj in np.arange(yTrain.size):",Evaluation/testPca.py,andimarafioti/AIAMI,1
"mset['Age'].dropna().hist(bins=32, range=(0,80), alpha = .5)
mset1 = train_df[train_df.Survived == 0]
mset1['Age'].dropna().hist(bins=32, range=(0,80), alpha = .5)
P.show()
print(train_df.columns.values)

#exit(0)
print 'Training...'
end_train = len(train_data)*0.8

model = SVC(kernel='linear')
model.fit(train_normalize_data[0:end_train:,3::], train_data[0:end_train:,0])
#model.fit(train_normalize_data[0::,0::], train_data[0::,0])

print 'Cross validation'
expected = train_data[end_train::,0]
predicted = model.predict(train_normalize_data[end_train::,3::])
#
# # summarize the fit of the model
print(metrics.classification_report(expected, predicted))",problems/Kaggle/TitanicMachineLearningfromDisaster/myfirstsvm.py,nesterione/problem-solving-and-algorithms,1
"    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
                     ' instance' % seed)


def has_fit_parameter(estimator, parameter):
    """"""Checks whether the estimator's fit method supports the given parameter.

    Examples
    --------
    >>> from sklearn.svm import SVC
    >>> has_fit_parameter(SVC(), ""sample_weight"")
    True

    """"""
    return parameter in signature(estimator.fit).parameters


def check_symmetric(array, tol=1E-10, raise_warning=True,
                    raise_exception=False):
    """"""Make sure that array is 2D, square and symmetric.",projects/scikit-learn-master/sklearn/utils/validation.py,DailyActie/Surrogate-Model,1
"from sklearn import datasets
from sklearn import svm
import pickle
#from sklearn.externals import joblib

iris = datasets.load_iris()
# sklearn.datasets.base.Bunch
#print iris.data
print iris.target

clf = svm.SVC(gamma=0.001,C=100.)

#clf.fit(iris.data[:-1],iris.target[:-1])
clf.fit(iris.data,iris.target)

predictions = clf.predict(iris.data)

print ""predictions:"",predictions

",python/scikitlearn/survivaltest/scripts/iris.py,jdurbin/sandbox,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_estimator_)
    print()
    print(""Grid scores on development set:"")
    print()
    for params, mean_score, scores in clf.grid_scores_:",project_code/Example Scripts/grid_search_digits.py,e-koch/Phys-595,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def saveDB(self, jsImages, jsPeople, training):
        print(""save images into DATABASE"")
        conn = sqlite3.connect('faces.db')
        c = conn.cursor()

        for jsImage in jsImages:
            face = (sqlite3.Binary(cPickle.dumps(jsImage)), )
            c.execute(""INSERT INTO faces VALUES (?) "", face)",demos/web/websocket-server.py,nhzandi/openface,1
"	X.append(pixels.flatten())
	Y.append(label)
X = np.array(make_binary(X))
Y = np.array(Y)
print X 
print Y
print ""Done.""

# classify
print ""Classifying...""
clf = SVC(C=2., kernel='poly', degree=11, gamma=1./2048., coef0=1) 
preds = clf.fit(X, Y).predict(X)
print ""Done.""
print ""Computing E_in...""
errors = [preds[j] == Y[j] for j in xrange(len(Y))].count(False)
E_in = float(errors)/float(len(Y))
print ""E_in"", E_in
print ""Done.""

# predict for test set ",svm.py,ielashi/chinese-zodiac-classifier,1
"
dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

ab=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1, n_estimators=10, random_state=1)
dt=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')
gb=GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=25, presort='auto', random_state=1, subsample=1.0, verbose=0, warm_start=False)
rf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False)
svcl=LinearSVC(C=0.9, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=1, tol=0.0001, verbose=0)
lr=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=1, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)

",scripts/histograms/normalised-ml.py,jmrozanec/white-bkg-classification,1
"
if __name__ == '__main__':
    n_folds = 5

    skf = sklearn.cross_validation.StratifiedKFold(data.target, n_folds)
    clfs = [sklearn.ensemble.AdaBoostClassifier(n_estimators=100, random_state=process.seed),
            sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, random_state=process.seed),
            sklearn.ensemble.RandomForestClassifier(n_estimators=100, random_state=process.seed),
            sklearn.linear_model.LogisticRegression(random_state=process.seed),
            sklearn.naive_bayes.MultinomialNB(),
            sklearn.svm.LinearSVC(random_state=process.seed)]

    blend_train = numpy.zeros((process.bag_of_words.train.shape[0], len(clfs)))
    blend_test = numpy.zeros((process.bag_of_words.test.shape[0], len(clfs)))

    for i, clf in enumerate(clfs):
        blend_test_i = numpy.zeros((process.bag_of_words.test.shape[0], len(skf)))
        for j, (train_index, test_index) in enumerate(skf):
            fold_train = process.bag_of_words.train[train_index]
            fold_target = data.target[train_index]",word2vec_nlp_tutorial/blend.py,wjfwzzc/Kaggle_Script,1
"
    elif params['feat_pre'] == str(d_feat_pre['RBFSampler']) or params['feat_pre'] == 'RBFSampler':
        gamma = float(params['fp4:gamma'])
        n_components = int(float(params['fp4:n_components']))
        fp = RBFSampler(gamma=gamma,
                        n_components=n_components)

    elif params['feat_pre'] == str(d_feat_pre['LinearSVC']) or params['feat_pre'] == 'LinearSVC':
        tol = float(params['fp5:tol'])
        C = float(params['fp5:C'])
        fp = svm.LinearSVC(penalty='l1',
                           loss='squared_hinge',
                           dual=False,
                           tol=tol,
                           C=C,
                           multi_class='ovr',
                           fit_intercept=True,
                           intercept_scaling=1,
                           class_weight=params['class_weight'])
",benchmarks/sklearn/ml_framework.py,yuyuz/FLASH,1
"MODELS = {
    MLPRegressor(): nnparams,
    SVR(): {
            'C': (Real(-4.0, 4.0), pow10map),
            'epsilon': (Real(-4.0, 1.0), pow10map),
            'gamma': (Real(-4.0, 1.0), pow10map)},
    DecisionTreeRegressor(): {
            'max_depth': (Real(1.0, 4.0), pow2intmap),
            'min_samples_split': (Real(1.0, 8.0), pow2intmap)},
    MLPClassifier(): nnparams,
    SVC(): {
            'C': (Real(-4.0, 4.0), pow10map),
            'gamma': (Real(-4.0, 1.0), pow10map)},
    DecisionTreeClassifier(): {
            'max_depth': (Real(1.0, 4.0), pow2intmap),
            'min_samples_split': (Real(1.0, 8.0), pow2intmap)}
}

# every dataset should have have a mapping to the mixin that can handle it.
DATASETS = {",benchmarks/bench_ml.py,iaroslav-ai/scikit-optimize,1
"import sklearn.naive_bayes as nb
import sklearn.ensemble as em
from sklearn import svm
from sklearn import tree
from sklearn.metrics import f1_score

needsScaling = True
needsNoScaling = False

classifiers = [(""svc[ovo]"",svm.SVC(decision_function_shape='ovo'), needsScaling),
               (""svc"",svm.SVC(), needsScaling),
               (""lin svc"",svm.LinearSVC(), needsScaling),
               (""lr"",lm.LogisticRegression(), needsScaling),
               (""nn"",nc.NearestCentroid(), needsScaling),
               (""lr(l1)"",lm.LogisticRegression(penalty='l1'), needsScaling),
               (""sgd[hinge]"",lm.SGDClassifier(loss=""hinge"", penalty=""l2""), needsScaling),
               (""navie bayse"",nb.MultinomialNB(), needsScaling),
               (""decision tree"",tree.DecisionTreeClassifier(), needsNoScaling),
               (""random forrest"",em.RandomForestClassifier(n_estimators=10), needsNoScaling),
               (""ada boost"",em.AdaBoostClassifier(n_estimators=100), needsScaling),",analysis/Classification.py,joergsimon/gesture-analysis,1
"
# ----------------------------------------------------------------------------------------------------

annotator, X, y, groups = get_model_and_data(sentence_distance, use_pred)
# X = X.toarray()
print(""SVC after preprocessing, #features: {} && max value: {}"".format(X.shape[1], max(sklearn.utils.sparsefuncs.min_max_axis(X, axis=0)[1])))

print(""Shape X, before: "", X.shape)

feature_selections = [
    # (""LinearSVC_C=4.0"", SelectFromModel(LinearSVC(C=4.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    (""LinearSVC_C=2.0"", SelectFromModel(LinearSVC(C=2.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    (""LinearSVC_C=1.0"", SelectFromModel(LinearSVC(C=1.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    # (""LinearSVC_C=0.5"", SelectFromModel(LinearSVC(C=0.5, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    # (""LinearSVC_C=0.25"", SelectFromModel(LinearSVC(C=0.25, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),

    # (""RandomizedLogisticRegression_C=1"", SelectFromModel(RandomizedLogisticRegression(C=1))),
    # (""RandomizedLogisticRegression_C=0.5"", SelectFromModel(RandomizedLogisticRegression(C=0.5))),

    # (""PCA_2"", PCA(2)),",scripts/quick_feature_selection_and_pipeline_evaluation.py,juanmirocks/LocText,1
"# Train classifiers
#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.

C_range = np.logspace(-2, 10, 13)
gamma_range = np.logspace(-9, 3, 13)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(n_iter=5, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
grid.fit(X, y)

print(""The best parameters are %s with a score of %0.2f""
      % (grid.best_params_, grid.best_score_))

# Now we need to fit a classifier for all parameters in the 2d version
# (we use a smaller set of parameters here because it takes a while to train)

C_2d_range = [1e-2, 1, 1e2]",projects/scikit-learn-master/examples/svm/plot_rbf_parameters.py,DailyActie/Surrogate-Model,1
"# Maybe some original features where good, too?
selection = SelectKBest(k=1)

# Build estimator from PCA and Univariate selection:

combined_features = FeatureUnion([(""pca"", pca), (""univ_select"", selection)])

# Use combined features to transform dataset:
X_features = combined_features.fit(X, y).transform(X)

svm = SVC(kernel=""linear"")

# Do grid search over k, n_components and C:

pipeline = Pipeline([(""features"", combined_features), (""svm"", svm)])

param_grid = dict(features__pca__n_components=[1, 2, 3],
                  features__univ_select__k=[1, 2],
                  svm__C=[0.1, 1, 10])
",projects/scikit-learn-master/examples/feature_stacker.py,DailyActie/Surrogate-Model,1
"  parser = argparse.ArgumentParser(description='Perform cross validation on the dataset')
  parser.add_argument('-source', help='subdirectory for the source data')
  parser.add_argument('-prefix', help='prefix for file names')
  args = parser.parse_args()

  x, y = loadData( args.source, args.prefix )

  # X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y[:,1], test_size=0.4, random_state=0)

  
  clf = svm.SVC(kernel='linear', C=1)
  scores = cross_validation.cross_val_score(clf, x, y[:,0], cv=5)

  print clf
  print ""coef"", clf.coef_
  print ""intercept"", clf.intercept_
  print scores

  print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
",project/simple-rain.py,n7jti/machine_learning,1
"
        cls = int(stats.mode(res)[0])
        callback(row, Classifier.CLASSIFIED, cls)

    def new_training(X, y):
        """"""reads training vector and fits new hypothesis
        :param X: training vector
        :param y: target vector (classes)
        :returns: True on success and False afterwards
        """"""
        clf = svm.SVC()
        param_grid = {'C': [0.5, 5, 50, 500], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}
        Classifier.clf = grid_search.GridSearchCV(clf, param_grid, n_jobs=os.cpu_count())
        try:
            Classifier.clf.fit(X, y)
        except ValueError as e:
            return False;
        Classifier.learned = True
        return True
",classifier.py,qwertzdenek/mtagger,1
"    使用LR进行测试
    :param train_file: 
    :param test_file: 
    :param rate_feature:    特征一共300个，选择比率 
    :param rate_example:    样本选择比率
    :return: 
    """"""

    trainX, trainY, testX, testY = read_data()

    # clf = svm.SVC(kernel='rbf', gamma=0.7, C=1.0).fit(trainX, trainY)
    # y_predicted = clf.predict(testX)
    W = np.ones(trainX.shape[0]) / trainX.shape[0]

    clf = DecisionTreeClassifier(max_depth=3).fit(trainX, trainY, sample_weight=W)

    # clf = LogisticRegression().fit(trainX, trainY, sample_weight=W)
    y_predicted = clf.predict(testX)
    print(""fit finish!"")
",3CNN/ensemble_svd.py,songjs1993/DeepLearning,1
"    self.modelType = modelType
    if self.modelType == ""GB2"":     self.fitGB2 ( X, y )
    if self.modelType == ""GB1"":     self.fitGB1 ( X, y )
    if self.modelType == ""LR1"":     self.fitLR1 ( X, y )
    if self.modelType == ""LR2"":     self.fitLR2 ( X, y )
    if self.modelType == ""SV1"":     self.fitSV1 ( X, y )
    if self.modelType == ""SV2"":     self.fitSV2 ( X, y )

  def fitSV1( self, X, y ):
    #Fit Support Vector Machine when the possible scores may be 0 or 1
    self.model01 = SVC(kernel = 'rbf', C = self.param1, gamma = self.param2, probability = True, cache_size = 2048, random_state = 2512 )    
    self.model01.fit( X , y )

  def fitSV2( self, X, y ):
    #Fit Support Vector Machine when the possible scores may vary from 0 to 2
    self.model01 = SVR(kernel = 'rbf', C = self.param1, gamma = self.param2, probability = True, cache_size = 2048, random_state = 2512 )
    self.model01.fit( X , y )

  def fitGB2( self, X, y ):
    #Fit Gradient Boosting Machine when the possible scores may vary from 0 to 2, 3 or 4",scripts/mainModel.py,luistp001/AI-LT_engine,1
"        vScores = -numpy.log10(oSelector.pvalues_)
        """"""This indicates the relative importance of the metrics in
        where the noisier terms occur first""""""        
        self.vFScoreRanks = vScores.argsort()
                
        """"""SVC fit""""""
        """"""Training subset""""""
        mXTrain = mXRemoved[0:800,:]
        vYTrain = vY[0:800]
        """"""The default C=1 here yields 99% and 96% success for training and cross-validation prediction""""""
        oSVCModel = svm.SVC()
        oSVCModel.fit(mXTrain, vYTrain) 

        vbPredTrain = oSVCModel.predict(mXTrain)
        trainSuccess = 1 - abs(vbPredTrain - vYTrain).mean()

        """"""Cross-validation subset""""""
        mXCross = mXRemoved[800:1000,:]
        vYCross = vY[800:1000]
",mywork/task4.py,DoctorKhan/deals,1
"                                   test_times=test_times)
    gat.fit(epochs)
    with warnings.catch_warnings(record=True):  # not vectorizing
        gat.score(epochs)
    assert_array_equal(np.shape(gat.y_pred_[0]), [1, len(epochs), 1])
    assert_array_equal(np.shape(gat.y_pred_[1]), [2, len(epochs), 1])
    # check cannot Automatically infer testing times for adhoc training times
    gat.test_times = None
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,wronk/mne-python,1
"    testY = np.array(testY)
    
    print (trainY.shape , testY.shape)
    np.savetxt('20news_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+ '_train_output_labels.txt', trainY, fmt = '%s')
    np.savetxt('20news_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+ '_test_output_labels.txt', testY, fmt = '%s')
    return trainY, testY


def evaluate_with_SVM(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting SVM"")
    clf = svm.SVC(kernel='linear')
    clf.fit(train_X, train_Y)
    predict_Y = clf.predict(test_X) 
    s=metrics.accuracy_score(test_Y, predict_Y) 
    print (""SVM Testing Acc: "", s)
    return s


def evaluate_with_KNN(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting KNN"")",Model/20News/20news_LSTM-SVM_CV.py,irisliu0616/Short-text-Classification,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Lib/email/test/test_email.py,OS2World/APP-INTERNET-torpak_2,1
"import numpy as np
from sklearn import svm

train = pd.read_csv(train_fn,header=0)

#Separate out label data and convert to numpy array
train_predictor_data = train.iloc[ :num_fit, 1:].values
train_target_data = train.iloc[ :num_fit, 0].values

#Estimator constructor 
clf = svm.SVC(C=3.1622776601683795,gamma=1.7782794100389229e-05,verbose=True)
#Training montage
clf.fit(train_predictor_data, train_target_data)

#Save model
barrel = open(model_fn, 'wb')
pickle.dump(clf, barrel)
barrel.close()
",svm.py,josh314/squinty,1
"
predictors_tr = tfidftr

targets_tr = traindf['cuisine']

predictors_ts = tfidfts


classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# clf = LinearSVC()
# clf = LogisticRegression()

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)

classifier=classifier.fit(predictors_tr,targets_tr)

predictions=classifier.predict(predictors_ts)
testdf['cuisine'] = predictions",deep_learn/whatiscooking/test.py,zhDai/CToFun,1
"        #(KNeighborsClassifier(n_neighbors=10, n_jobs=-1, leaf_size=120), ""kNN""),
        (RandomForestClassifier(n_estimators=100, n_jobs=-1), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",classifier_university_comments.py,denimalpaca/293n,1
"print ""Class Label Vector Y of size "", len(YLabels), "" extracted"";

#Setting up scaler for standardisation
from sklearn import preprocessing
scaler = preprocessing.StandardScaler();

# Training SVM
from sklearn import svm
from sklearn import linear_model
print ""Declaring SVM""
#clf = svm.LinearSVC(); # linearsvc1
clf = svm.LinearSVC(C=1000.0, class_weight='auto', penalty='l1', dual=0); # linearsvc2
#clf = svm.SVC(cache_size = 1000, class_weight='auto', kernel = 'poly'); # Predicts all as POSITIVE :((
#clf = linear_model.SGDClassifier();  # not tried yet
print ""standardising training data""
XFeatures = scaler.fit_transform(XFeatures, YLabels);
print ""Fitting Data To SVM""
clf.fit(XFeatures, YLabels);
print ""SVM trained""
",Preliminary experimentation/code/UnigramBigramSVMIgnoreUNK.py,prernaa/NLPCourseProj,1
"                     map(os.path.split,
                         map(os.path.dirname, labels)))  # Get the directory.
        fname = ""{}/reps.csv"".format(args.workDir)
        embeddings = pd.read_csv(fname, header=None).as_matrix()
        le = LabelEncoder().fit(labels)
        labelsNum = le.transform(labels)
        nClasses = len(le.classes_)
        print(""Training for {} classes."".format(nClasses))

        if clfChoice == 'LinearSvm':
            clf = SVC(C=1, kernel='linear', probability=True)
        elif clfChoice == 'GMM':  # Doesn't work best
            clf = GMM(n_components=nClasses)

        # ref:
        # http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#example-classification-plot-classifier-comparison-py
        elif clfChoice == 'RadialSvm':  # Radial Basis Function kernel
            # works better with C = 1 and gamma = 2
            clf = SVC(C=1, kernel='rbf', probability=True, gamma=2)
        elif clfChoice == 'DecisionTree':  # Doesn't work best",evaluation/lfw-classification-unknown.py,xamgeis/project_vulcan,1
"LIWC = getLIWCFeatures(file)
X = vectorizer.fit_transform(corpus)
print(X.toarray())
X_tfidf = transformer.fit_transform(X.toarray())
print(X_tfidf.toarray())#spare to intense
X_pca = pca.fit_transform(X_tfidf.toarray())
print(X_pca)

X_tfidf = np.concatenate((X_tfidf.toarray(), np.transpose(LDA).reshape(len(LDA),1)),axis=1)
X_tfidf = np.concatenate((X_tfidf,LIWC),axis=1)
#clf=svm.SVC()#rbf kernel
clf=svm.LinearSVC(C=1)#linear kernel
#clf.fit(X.toarray(),np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X.toarray(),np.transpose(y),scoring=""mean_squared_error"",cv=10)
#clf.fit(X_tfidf.toarray(),np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X_tfidf.toarray(),np.transpose(y),scoring=""mean_squared_error"",cv=10)
clf.fit(X_tfidf,np.transpose(y))
cv=cross_validation.cross_val_score(clf,X_tfidf,np.transpose(y),scoring=""accuracy"",cv=10)
#clf.fit(X_pca,np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X_pca,np.transpose(y),scoring=""mean_squared_error"",cv=10)",svmcv-with-liwc-and-other-features.py,evazyin/Capstoneproject,1
"        x_n = np.random.multivariate_normal(np.ones(dim) * 2, np.eye(dim), num_n)
        x = np.vstack([x_p, x_n])
        y = np.array([1.] * num_p + [-1.] * num_n)
        # Set parameters
        max_iter = 500000
        C = 1e1
        gamma = 0.01
        tol = 1e-7
        kernel = 'poly'
        # Training
        model_md = csvc.SVC(C=C, kernel=kernel, max_iter=max_iter, tol=tol, gamma=gamma)
        model_sk = svm.SVC(C=C, kernel=kernel, max_iter=max_iter, tol=tol, gamma=gamma)
        model_md.fit(x, y)
        model_sk.fit(x, y)
        # Compute objective value
        obj_sk = np.dot(
            model_sk.dual_coef_[0],
            np.dot(
                model_md._kernel_function(x[model_sk.support_], x[model_sk.support_]),
                model_sk.dual_coef_[0]",tests/test_csvc.py,sfujiwara/mdsvm,1
"arguments               = train_test.py -id $(Process) -params = .tmp/australian_scale/params_file.pkl
should_transfer_files   = IF_NEEDED
when_to_transfer_output = ON_EXIT
log                     = australian_scale.log


queue 1""""""

class TestHTClusterTrainTest(TestCase):
    def test_fit(self):
        clf = SVC()
        dataset = 'australian_scale'
        param_grid = {'C': [1, 100]}
        grid = HTClusterTrainTest(clf, 'data_home', dataset, param_grid=param_grid)
        try:
            grid.fit()
        except RuntimeError:
            pass
        with open(os.path.join('.tmp', dataset, 'submit.sub')) as f:
            submission = f.read()",test.py,blauigris/htcluster,1
"'''

from sklearn import datasets, svm
import cPickle as pkl


__author__ = 'noahsark'


train = datasets.fetch_20newsgroups_vectorized(subset='train')
clf = svm.LinearSVC()
clf.fit(train.data, train.target)
with open('storm-starter/multilang/resources/svm_model.pkl', 'wb') as fp_:
    pkl.dump(clf, fp_)",storm_demo/train_model.py,jimmylai/slideshare,1
"scaler = StandardScaler()
scaler.fit(features)
features = scaler.transform(features)


# # Create range of classifiers
# et = ExtraTreesClassifier(
#     n_estimators=100, max_depth=None, min_samples_split=1, random_state=0)
# knn = KNeighborsClassifier()
# log = linear_model.LogisticRegression(C=1e5)
# svc = svm.SVC(kernel='linear')
# svc3 = svm.SVC(kernel='poly', degree=3)
# svc4 = svm.SVC(kernel='poly', degree=4)
# svcrbf = svm.SVC(kernel='rbf')
# lasso = linear_model.LassoCV()
# nn = MLPClassifier(
#     solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
# gnb = GaussianNB()
# dt = tree.DecisionTreeClassifier()
",classify.py,robjordan/gpx-stops,1
"
    f.close()

clfs = []

# Through cv testing, I found the optimal number of estimators to be 15
clfs.append(ensemble.RandomForestClassifier(n_estimators=150))
clfs.append(ensemble.GradientBoostingClassifier(n_estimators=200))
clfs.append(ensemble.AdaBoostClassifier(n_estimators=135))
#clfs.append(neighbors.KNeighborsClassifier(n_neighbors=10))
#clfs.append(svm.SVC())

#predictificate(data, target, test, clfs)

clf = ensemble.RandomForestRegressor()
clf.fit(train_data, target)
probabilities = clf.predict(test_data)
print probabilities
#f = open('preds.csv', 'w')
",Honors/code.py,bcspragu/Machine-Learning-Projects,1
"        assert(np.sum(ysub < 157) == 0)
        ysub[np.where(ysub <  162)[0]] = -1
        ysub[np.where(ysub >= 162)[0]] =  1

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)
        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=0.8, kernel='rbf', gamma=0.0050)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]
        Xcv = Xcv.iloc[np.where(ytrue_cv >= 157)[0],:]
        print('CrossVal Shape= %d,%d' %Xcv.shape)
        ytrue_cv = ytrue_cv[np.where(ytrue_cv >= 157)[0]].values
        ytrue_cv[np.where(ytrue_cv <  162)[0]] = -1
        ytrue_cv[np.where(ytrue_cv >= 162)[0]] =  1",codes/classify_half2.py,mirjalil/ml-visual-recognition,1
"import numpy
import logging
from sandbox.util.Evaluator import Evaluator
from exp.metabolomics.leafrank.AbstractFunctionalPredictor import AbstractFunctionalPredictor
from exp.metabolomics.leafrank.SVC import SVC
from apgl.data.Standardiser import Standardiser

class SvcFGs(AbstractFunctionalPredictor):
    def __init__(self):
        super(SvcFGs, self).__init__()
        self.SVC = SVC()

    def learnModel(self, X, y, folds=3):
        """"""
        Train using the given examples and labels, however first conduct grid
        search in conjunction with cross validation to find the best parameters.
        We also conduct filtering with a variety of values.
        """"""
        #Hard coding this is bad
        Cs = 2**numpy.arange(-2, 6, dtype=numpy.float)",sandbox/ranking/leafrank/SvcFGs.py,charanpald/sandbox,1
"


####################### MAIN!!! ########################
if __name__ == '__main__':
    train, target, test, passanger_id = load_data()
    train, test = preprocessing(train, test)

    clf = tree.DecisionTreeClassifier(min_samples_leaf=10)      # DECISION TREE
    # clf = ensemble.RandomForestClassifier(n_estimators=100)     # RANDOM FOREST
    # clf = svm.SVC(kernel='poly', degree=4)                        # SVM
    # clf = clf = ensemble.AdaBoostClassifier(n_estimators=10, base_estimator=ensemble.RandomForestClassifier())       # ADA BOOST
    clf.fit(train, target)
    performance(clf, train, target)
    my_prediction = clf.predict(test)
    submit(my_prediction, passanger_id, submit_csv='submission.csv')



",Kagle_Titanic_Python/Decision_tree.py,Skobnikoff/Datasets,1
"from sklearn.cross_validation import train_test_split
from sklearn import svm
from sklearn import mixture
from sklearn.metrics.classification import accuracy_score, classification_report

digits = load_digits()
print(digits.data.shape)
data_train, data_test, label_train, label_test = train_test_split(digits.data, digits.target)

#学習1 - 線形SVC
lin_svc = svm.LinearSVC()
lin_svc.fit(data_train, label_train)

#予測
predict = lin_svc.predict(data_test)

#正答率
print (accuracy_score(label_test,predict))

#正答率レポート",src/samples/scikit_test.py,000ubird/HighResolution,1
"# {% endif %}
# {% if c is re_match(""^ipv4_"") %}
# {% set ipname = c | re_sub(""^ipv4_(.*)"", ""ip4-\\1"") | re_sub(""_"", ""."")
# %}
# {% set ip_list = ip_list + [ipname] %}
# {% endif %}
# {% endfor %}
# 
# Enabled services:
# {% for s in svc_list %}
# SVC({{loop.index0}})={{s}}
# {% endfor %}
# 
# Interfaces:
# {% for s in if_list %}
# IF({{loop.index0}})={{s}}
# {% endfor %}
# 
# IP addresses:
# {% for s in ip_list %}",tools/ext_re.py,huawenyu/Design-Patterns-in-C,1
"rootdir = os.getcwd()
if not os.path.exists('sklearnTry'):
        os.makedirs('sklearnTry')
newdir = os.path.join(rootdir,'sklearnTry')
fout = open(os.path.join(newdir,'SVMOut.txt'),'w+')

train_features, train_labels, test_features, test_labels, test_keys = GetData() 
train_features, train_labels = ShuffleTrainFeatures(train_features, train_labels)


#model1 = svm.SVC(decision_function_shape ='ovo')

model = svm.SVC(decision_function_shape ='ovr', probability=True)

#c = runkFoldCrossValidation(train_features, train_labels, model)
#model, score = runkFoldCrossValidationModel(train_features, train_labels, model)
#c =0.8
#model.set_params( C = c)

gs = GridSearchCV(model, param_grid={",SVM.py,vidhyal/WitchMusic,1
"        self.projection = None
        self.geotrans = None
        self.imgOriginal = None
        self.shpOriginal = None
        self.Threads = Threads
        self.model = model

        if self.model == None:

            if modeltype == 1:
                self.model = SVC(C = 1.0, \
                       cache_size = 200, \
                       class_weight = None, \
                       coef0 = 0.0, \
                       decision_function_shape = None, \
                       degree = 3, \
                       gamma = 1 / 0.2, \
                       kernel = 'rbf', \
                       max_iter = -1, \
                       probability = False, \",mlh.py,madi/DeadTrees-BDEOSS,1
"    for i in range(0,len(tweets)-1):
        tweets[i].vector = ft[i]
    return (fit_vectorizer,ft)

def split(tweets):
    x=getY(tweets)
    return vectorize(tweets),x

def gs(X,Y,folds,parameters):
    cv=cross_validation.KFold(len(X), n_folds=folds,shuffle=True,random_state=None)
    svr = SVC()
    clf = grid_search.GridSearchCV(svr, parameters,cv=cv)
    print(""About to fit..."")
    clf.fit(X,Y)
    pprint.pprint(clf.grid_scores_)
    pprint.pprint(clf.best_params_)

def regularSVM(X,Y,c,pctTest,shouldReturnMetrics):
    #svm = LinearSVC(C=c);
    svm=linear_model.LogisticRegression(C=c);",calculations.py,josbys1/twitter-svm,1
"
train_samples = samples[0:index_split]
train_labels = labels[0:index_split]

perc_test = 100-perc_train
test_samples = samples[index_split+1:n_samples]
test_labels = labels[index_split+1:n_samples]

text_clf = Pipeline([
	('vect', TfidfVectorizer(max_df = 0.8,sublinear_tf=True,use_idf=True)),
	('clf', svm.LinearSVC(C=1.0)),
])


print ""Starting fit process...""
text_clf.fit(train_samples, train_labels)

print ""Starting prediction...""
predicted = text_clf.predict(test_samples)
",machine.py,ecairol/sklearn-amazon-reviews,1
"	choise='sigmoid'
elif choise==""5"":
	choise='precomputed'
	
###Data preparation
train_results = train_set[""Hottness""]
del train_set[""Hottness""]
###End data preparation

##Selftest
machine=svm.SVC(kernel=choise)
machine.fit(train_set.astype(int), train_results.astype(int))
predicted_hotness=machine.predict(train_set.values)

predictionSuccess=(1-np.mean(predicted_hotness != train_results.values))*100
print(""Test against training set(self test): ""+str(predictionSuccess)+""% correctness"")
###End selftest


###Predict survival of test.csv",Cloudera/Code/million_song_dataset/test_scripts/SVM_predict.py,cybercomgroup/Big_Data,1
"
    Xtrain = np.concatenate((Xtrain_r, Xtrain_c, Xtrain_p_h), axis=1)
    Xtest = np.concatenate((Xtest_r, Xtest_c, Xtest_p_h), axis=1)
    ytrain = ytrain_r
    ytest = ytest_r

    print Xtrain.shape, Xtest.shape

    # do classification
    tic = time()
    model = sklearn.svm.LinearSVC(C=C)
    model.fit(Xtrain, ytrain)
    predictions = model.predict(Xtest)
    toc = time() - tic

    print 'classification in', toc
    print '--------------------'
    print 'C:', C
    print '--------------------'
    print 'accuracy', sklearn.metrics.accuracy_score(ytest, predictions), 'mean accuracy', utils.mean_accuracy(ytest, predictions)",src/scripts/rf_experiment.py,yassersouri/omgh,1
"    model.fit(X_train, y_train_decoded)
    predict = model.predict(X_test)
    return metrics.f1_score(y_test_decoded, predict, average=average)

models_to_compare = [
                     ('log_reg_sgd', linear_model.SGDClassifier(loss='log')),
                     ('log_reg_liblinear', linear_model.LogisticRegression()),
                     ('log_reg_cv', linear_model.LogisticRegressionCV()),
#                     ('bernoulli nb',  BernoulliNB()),
#                     ('gaussian nb',  GaussianNB()),
#                     ('svm_linear', svm.LinearSVC()),
#                     ('svm_rbf', svm.SVC(kernel='rbf')),
#                     ('K=5nn', KNeighborsClassifier(n_neighbors=5)),
                    ]
ntm_models_to_compare = [
                     ('srntm model 1 32*32', ntm1),
                    ]                        

for name, model in ntm_models_to_compare:
    f1_scores = [f1_score(s[0], s[1], 'binary', model) for s in sequences]",evaluate2.py,tombosc/ntm_experiments,1
"
hog = HOG(orientations=18, pixelsPerCell=(10, 10), cellsPerBlock=(1, 1), transform=True)

for image in digits:
    image = dataset.deskew(image, 20)
    image = dataset.center_extent(image, (20, 20))

    hist = hog.describe(image)
    data.append(hist)

model = LinearSVC(random_state=42)
model.fit(data, target)
",hoshiwomiru/029_train.py,guybrush007/hoshiwomiru,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Applications/Python/Python-2.7.2/Lib/email/test/test_email.py,hwu25/AppPkg,1
"            self._crossvalidation.crossvalidate(\
                scaled_cv_feasibles, scaled_cv_infeasibles)

        # @todo WARNING maybe rescale training feasibles/infeasibles (!) 
        fvalues = [f.getA1() for f in self._selected_feasibles]
        ivalues = [i.getA1() for i in self._selected_infeasibles]

        points = ivalues + fvalues
        labels = [-1] * len(ivalues) + [1] * len(fvalues) 

        self._clf = svm.SVC(kernel = 'linear', C = self._best_parameter_C, tol = 1.0)
        self._clf.fit(points, labels)  
        self.logger.log()

        return True

    def get_normal(self):
        # VERY IMPORTANT
        w = self._clf.coef_[0]
        nw = w / sqrt(sum(w ** 2))",evopy/metamodel/rsvc_linear_meta_model.py,jpzk/evopy,1
"    assert_true('a' in est.get_params())
    assert_true('a' in est.get_params(deep=True))
    assert_true('a' in est.get_params(deep=False))

    assert_true('b' not in est.get_params())
    assert_true('b' not in est.get_params(deep=True))
    assert_true('b' not in est.get_params(deep=False))


def test_is_classifier():
    svc = SVC()
    assert_true(is_classifier(svc))
    assert_true(is_classifier(GridSearchCV(svc, {'C': [0.1, 1]})))
    assert_true(is_classifier(Pipeline([('svc', svc)])))
    assert_true(is_classifier(Pipeline([('svc_cv',
                                         GridSearchCV(svc, {'C': [0.1, 1]}))])))


def test_set_params():
    # test nested estimator parameter setting",projects/scikit-learn-master/sklearn/tests/test_base.py,DailyActie/Surrogate-Model,1
"    For best results, this accepts a matrix in csr format
    (scipy.sparse.csr), but should be able to convert from any array-like
    object (including other sparse representations).

    Examples
    --------
    >>> import numpy as np
    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    >>> y = np.array([1, 1, 2, 2])
    >>> from sklearn.svm.sparse import SVC
    >>> clf = SVC()
    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE
    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
            gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
            shrinking=True, tol=0.001, verbose=False)
    >>> print(clf.predict([[-0.8, -1]]))
    [ 1.]
    """"""

    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma=0.0,",python/sklearn/sklearn/svm/sparse/classes.py,seckcoder/lang-learn,1
"for i in train_y:
    if i >  train_rating_average:
        binary_train_y[iteration] = 1.0
    else:
        binary_train_y[iteration] = 0.0
    iteration += 1
##

X_train, X_test, y_train, y_test = train_test_split(train_x, binary_train_y, test_size=0.01, random_state=0)

cls = svm.SVC()
cls.fit(X_train, y_train)
p=cls.predict(X_test)
s = cls.score(X_test,y_test)
print(s)

r = p - y_test
r = np.power(r,2)
print(""wrong guesses: "",np.sum(r))
",analisys/svm_classifier.py,chavdim/amazon_comments,1
"peekData(X_train)


#
# TODO: Create an SVC classifier. Leave C=1, but set gamma to 0.001
# and set the kernel to linear. Then train the model on the training
# data / labels:
print(""Training SVC Classifier..."")
#
from sklearn.svm import SVC
model = SVC(C=1, gamma=0.001, kernel='rbf')
model.fit(X_train, y_train)



# TODO: Calculate the score of your SVC against the testing data
print(""Scoring SVC Classifier..."")
#
score = model.score(X_test, y_test)
print(""Score:\n"", score)",Module6/assignment2.py,szigyi/DAT210x,1
"    """"""

    # define helper function
    # use close price predict the trend of the next day
    def predict(CLOSE, gap):
        lookback = CLOSE.shape[0]
        X = np.concatenate([CLOSE[i:i + gap] for i in range(lookback - gap)], axis=1).T
        y = np.sign((CLOSE[gap:lookback] - CLOSE[gap - 1:lookback - 1]).T[0])
        y[y==0] = 1

        clf = svm.SVC()
        clf.fit(X, y)

        return clf.predict(CLOSE[-gap:].T)

    nMarkets = len(settings['markets'])
    gap = settings['gap']

    pos = np.zeros((1, nMarkets), dtype='float')
    for i in range(nMarkets):",sampleSystems/svm.py,Quantiacs/quantiacs-python,1
"from sklearn import svm
from sklearn.model_selection import train_test_split

clf = svm.SVC(gamma=0.001, C=100.)
data = []
target = []

test_data = []
test_target = []

print(""generating random test data..."")
for p in range(1,4):
    if p == 1:",predict.py,MoreIsTheNewLess/backend,1
"<<<<<<< HEAD
		return [{'hash': float(fea[0]),'url': float(fea[1]),'neg':float(fea[9]),'neu': float(fea[10][:4]),'pos': float(fea[11][:4]),'exla':float(fea[13]),'quest':float(fea[14])}for fea in data]


	


if __name__=='__main__':
	data=build_data_frame('data.csv')
	#clf=SGDClassifier(n_jobs = -1, n_iter = 100, eta0=0.1)
	#clf=OneVsRestClassifier(svm.SVC(kernel='rbf',gamma=0.001,C=100,max_iter=-1))
	clf=MultinomialNB()

	pipeline = Pipeline([	
			('features', FeatureUnion(
				transformer_list=[
					('sentiment',Pipeline([
						('selector',ItemSelector(key='features')),#Select numerical values
						('sentiment',Sentiment()),
						('vect',DictVectorizer())		  #Transforms lists of feature-value mapping to vectors",hybrid_classifier.py,Yiangos01/ADE2017,1
"clf = linear_model.RidgeCV(alphas=[1e-4,1e-3, 0.1, 1.0, 10.0, 100])
clf.fit(tr_data,train_labels.ravel())
clf.coef_
clf.intercept_

in_labels = train_labels.ravel()



#Fit SVM on attention features
#clf = svm.SVC()
#clf.fit(tr_data, train_labels.ravel())  
#y_hat = clf.predict(te_data)
#acc = metrics.accuracy_score(test_labels.ravel(),y_hat)
#y_score = clf.fit(tr_data, train_labels.ravel()).decision_function(te_data)
acc = svm_trainer(tr_data,te_data,train_labels.ravel(),test_labels.ravel(),
	num_estimates,num_training,num_testing)

#Extract HOG features
from skimage.feature import hog",vis_classifier.py,drewlinsley/draw_classify,1
"from sklearn import ensemble, linear_model, preprocessing, svm
from sklearn import decomposition, metrics, cross_validation
from data_io import *
np.set_printoptions(suppress=True)

train_data = data_binary_sparse('dorothea/dorothea_train.data', nbr_features=100000)
test_data = data_binary_sparse('dorothea/dorothea_test.data', nbr_features=100000)
valid_data = data_binary_sparse('dorothea/dorothea_valid.data', nbr_features=100000)
labels = np.loadtxt('dorothea/dorothea_train.solution')

clf1 = svm.SVC(verbose = 2, probability = True, C = 100.)
clf2 = linear_model.LogisticRegression(C=100.)
clf3 = linear_model.LogisticRegression(C=1.)

clf1.fit(train_data, labels)
clf2.fit(train_data, labels)
clf3.fit(train_data, labels)
test_preds = (clf1.predict_proba(test_data)[:,1] + clf2.predict_proba(test_data)[:,1] + clf3.predict_proba(test_data)[:,1])/3.
valid_preds = (clf1.predict_proba(valid_data)[:,1]+ clf2.predict_proba(valid_data)[:,1] + clf3.predict_proba(valid_data)[:,1])/3.
",Phase0/dorothea_main.py,abhishekkrthakur/AutoML,1
"    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact be computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.96) than than the non
    # shuffling variant (around 0.86).

    digits = load_digits()
    X, y = digits.data[:800], digits.target[:800]
    model = SVC(C=10, gamma=0.005)
    n = len(y)

    cv = cval.KFold(n, 5, shuffle=False)
    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.88, mean_score)
    assert_greater(mean_score, 0.85)

    # Shuffling the data artificially breaks the dependency and hides the
    # overfitting of the model with regards to the writing style of the authors",venv/lib/python2.7/site-packages/sklearn/tests/test_cross_validation.py,chaluemwut/fbserver,1
"        solver = ['lbfgs']
        max_iter = [1000]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(mlp, dict(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter, early_stopping=[False]), X, y)
        f = open('output/ti.mlp.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates SVM classifier
        svm = SVC()
        kernel = ['linear', 'rbf', 'poly', 'sigmoid']
        Cs = np.logspace(-3, 4, 8) # C = [0.001, 0.01, .., 1000, 10000]
        gamma = np.logspace(-3, 4, 8) # gamma = [0.001, 0.01, .., 1000, 10000]
        degree = [2, 3]
        coef0 = [0.0]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(svm, dict(kernel=kernel, C=Cs, gamma=gamma, degree=degree, coef0=coef0), X, y)
        f = open('output/ti.svm.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):",tests/test_ti.py,fberanizo/author-profiling,1
"    )

    images = io.load_images_from_dir(data_dir, extension)
    mask = io.load_boolean_mask(mask_file)
    conditions = io.load_labels(epoch_file)
    processed_data, labels = prepare_mvpa_data(images, conditions, mask)

    # transpose data to facilitate training and prediction
    processed_data = processed_data.T

    clf = svm.SVC(kernel='linear', shrinking=False, C=1)
    # doing leave-one-subject-out cross validation
    for i in range(num_subjects):
        leave_start = i * num_epochs_per_subj
        leave_end = (i+1) * num_epochs_per_subj
        training_data = np.concatenate((processed_data[0:leave_start], processed_data[leave_end:]), axis=0)
        test_data = processed_data[leave_start:leave_end]
        training_labels = np.concatenate((labels[0:leave_start], labels[leave_end:]), axis=0)
        test_labels = labels[leave_start:leave_end]
        clf.fit(training_data, training_labels)",examples/fcma/mvpa_classification.py,lcnature/brainiak,1
"print('Processando Documentos...')
train_corpus, train_y, train_filenames = zip(*montar_corpus(train_docs))
test_corpus, test_y, test_filenames = zip(*montar_corpus(train_docs))

pipeline = make_pipeline(
 CountVectorizer(min_df=30, max_df=0.99, ngram_range=(1, 4), encoding='utf-8'),
 TfdcfTransformer(use_product=False, norm='l2', sublinear_tf=True, binary=True, relative=False),
 # TfidfTransformer(norm='l2', use_idf=True, sublinear_tf=True),
 TruncatedSVD(100),
 # LogisticRegression(C=20.0, multi_class='multinomial', solver='lbfgs'))
 # SVC(C=150, gamma=2e-2, probability=True))
 GradientBoostingClassifier(n_estimators=50000, learning_rate=2**(-9.5), max_features='log2', max_depth=7, random_state=1, verbose=1))


print('Produzindo o Modelo...')
pipeline.fit(train_corpus, train_y)
print(pipeline.score(test_corpus, test_y))

print('Persistindo o Modelo...')
joblib.dump(pipeline, os.path.join(lib.MODELS_DIR, lib.CLASSIFICADOR_INICIAL_MODELO))",src/classifier/train_classificacao.py,pcastanha/rest-api,1
"from sklearn.svm import SVC
from .plot_2d_separator import plot_2d_separator
from .tools import make_handcrafted_dataset
from .plot_helpers import discrete_scatter


def plot_svm(log_C, log_gamma, ax=None):
    X, y = make_handcrafted_dataset()
    C = 10. ** log_C
    gamma = 10. ** log_gamma
    svm = SVC(kernel='rbf', C=C, gamma=gamma).fit(X, y)
    if ax is None:
        ax = plt.gca()
    plot_2d_separator(svm, X, ax=ax, eps=.5)
    # plot data
    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    # plot support vectors
    sv = svm.support_vectors_
    # class labels of support vectors are given by the sign of the dual coefficients
    sv_labels = svm.dual_coef_.ravel() > 0",mglearn/plot_rbf_svm_parameters.py,JoostVisser/ml-assignment2,1
"        basic.JSONParser(),
        basic.IgnoreMetadata(),
        basic.TweetStandardizer(),
        similar.TextCounter(),
        similar.FuzzyTextCounter(),
        nlp.POSTagger(),
        nlp.SequenceTagger(),
        nlp.DBpediaSpotter(),

        ml.CorpusClassifier(a126730_datasource(), naive_bayes.MultinomialNB()),
        ml.CorpusClassifier(a121571_datasource(), svm.SVC(gamma=2, C=1)),
        ml.CorpusClassifier(a126728_datasource(), neighbors.KNeighborsClassifier(3)),
        ml.CorpusClassifier(a122047_datasource(), linear_model.LogisticRegression()),

        basic.LineStream(sys.stdout),
    )

    logger.debug('Pipeline created')

    try:",tweedr/cli/pipeline.py,dssg/tweedr,1
"
labels1=N.loadtxt(os.path.join(basedir,'data_prep/data_key_run1.txt'))[:,1]
labels2=N.loadtxt(os.path.join(basedir,'data_prep/data_key_run2.txt'))[:,1]
N.random.shuffle(labels1)

print 'loading data...'
data1=N.load(os.path.join(basedir,'data_prep/zstat_run1_allgood.npy')).T


print 'training...'
clf=OneVsRestClassifier(LinearSVC()).fit(data1,labels1)

del data1

print 'loading test data...'
data2=N.load(os.path.join(basedir,'data_prep/zstat_run2_allgood.npy')).T

print 'predicting...'
pred=clf.predict(data2)
print 'Mean accuracy:',N.mean(pred==labels2)",openfmri_paper/9.1_classify_subjects_rand.py,poldrack/openfmri,1
"makeVectorsResultsTest = makeVectors(tokenizeResults,wordvecModel,selectedTokens)

# run binary svm experiments: one for each target class
for targetClass in targetClasses:
    # read the training and test file again to get the right class distribution for this target class
    readDataResultsTrain = naiveBayes.readData(trainFile,targetClass)
    readDataResultsTest = naiveBayes.readData(testFile,targetClass)
    # get binary version of train classes
    binTrainClasses = makeBinary(readDataResultsTrain[""classes""])
    # perform svm experiment: http://scikit-learn.org/stable/modules/svm.html (1.4.1.1)
    clf = svm.SVC(decision_function_shape='ovo')     # definition
    clf.fit(makeVectorsResultsTrain,binTrainClasses) # training
    outFile = open(testFile+"".out.""+targetClass,""w"") # output file for test results
    scores = clf.decision_function(makeVectorsResultsTest) # process all test items
    for i in range(0,len(makeVectorsResultsTest)):
        guess = ""O""
        if scores[i] >= 0: guess = targetClass
        print >>outFile, ""# %d: %s %s %0.3f"" % (i,readDataResultsTest[""classes""][i],guess,scores[i])
    outFile.close()
",word2vec.py,online-behaviour/machine-learning,1
"    Yt = data[:,-1]
    return X, Y, Xt, Yt

 

def plotting(X, Y, Xt, Yt, labelx, labely, outputfile):
    h = .02  # step size in the mesh
    classifiers = dict(
    knn=neighbors.KNeighborsClassifier(4),
    logistic=linear_model.LogisticRegression(C=1e5),
    svm=svm.SVC(C=1e5),
    adaboost=ensemble.AdaBoostClassifier(),
    naivebay=naive_bayes.GaussianNB())

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    fignum = 1

    for name, clf in classifiers.iteritems():",MLNet-2.0/plotting/lr-knn-svm-comparison/plot_with_out_with_zero.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"
        # Split up data into randomized training and test sets
        rand_state = np.random.randint(0, 100)
        X_train, X_test, y_train, y_test = train_test_split(
            scaled_X, y, test_size=0.2, random_state=rand_state)

        print('Using:', orient, 'orientations', pix_per_cell,
              'pixels per cell and', cell_per_block, 'cells per block')
        print('Feature vector length:', len(X_train[0]))
        # Use a linear SVC
        svc = SVC()
        # Check the training time for the SVC
        t = time.time()
        svc.fit(X_train, y_train)
        t2 = time.time()
        print(round(t2 - t, 2), 'Seconds to train SVC...')
        # Check the score of the SVC
        print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
        # Check the prediction time for a single sample
        t = time.time()",Term01-Computer-Vision-and-Deep-Learning/P5-Vehicle-Detection/detection_functions/train_classifier.py,Raag079/self-driving-car,1
"def load_datasets(fname):
	return list(csv.reader(open(fname,'rb')));

if __name__ == '__main__':
	train_feature = np.array(load_datasets('Data/train.csv')
				,dtype=np.float);
	train_target = np.array(load_datasets('Data/trainLabels.csv'),dtype=np.int);

	test_feature = np.array(load_datasets('Data/test.csv'),dtype=np.float);
	""""""
	svm = svm.SVC(C=1e-10,kernel='linear');
	svm.fit(train_feature,train_target);

	print svm.score(train_feature,train_target);
	""""""
	#knn = neighbors.KNeighborsClassifier(weights='uniform');
	rfc = ensemble.RandomForestClassifier(n_estimators=10);
	#for i in xrange(1,21):
	#knn.fit(train_feature,train_target);
		#knn.n_neighbors = i;",DataScience/classifier.py,dz1984/Kaggle,1
"    from sklearn.cross_validation import train_test_split
    news_train, news_test, y_train, y_test = train_test_split(
        dataset.data[:N_samp], dataset.target[:N_samp], test_size=0.25, random_state=5)
    
    from sklearn.feature_extraction.text import TfidfVectorizer
    vect = TfidfVectorizer(stop_words='english')
    
    from sklearn.pipeline import Pipeline
    from sklearn.svm import LinearSVC
    from sklearn.feature_extraction.text import CountVectorizer
    clf0 = LinearSVC()
    clf = Pipeline([('vect', vect),
                    ('clf',clf0)
                   ]) 
    
    parameters = {'vect__ngram_range': [(1, 1), (1, 2)]
                 }    

    from sklearn.grid_search import GridSearchCV    
    gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)",machine_learning/teaching_try1.py,chrisjdavie/shares,1
"import string
import sklearn
import numpy as np
from titleParse import *
import os
from sklearn import svm
from sklearn.preprocessing import StandardScaler

#testStringList =  getTitles(""test_data"" + os.sep + ""merged.txt"")
#testStringList = getTitles(""test.txt"")
clf = svm.SVC(gamma = .001)

def extractVectorsFromListOfPosts(postList):
    """"""This is a function designed to extract an attribute vector out of the text of
    a Craigslist posting. These attribute vectors will be fed to the SciKit Learn
    module to determine the quality of the posting itself.""""""

    
    def extractVectorFromPost(postText):
        upperCaseText = string.upper(postText)",vE2.py,bharadwajramachandran/TartanRoof,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)


def test_ajd():
    """"""Test if Approximate joint diagonalization implementation obtains same
    results as the Matlab implementation by Pham Dinh-Tuan.
    """"""",mne/decoding/tests/test_csp.py,Teekuningas/mne-python,1
"
    labels = np.unique(Y_train)

    ## # Scale Data
    scaler = MinMaxScaler()
    X_test = scaler.fit_transform(X_test)
    X_train = scaler.fit_transform(X_train)

    ## # Run svm
    # Define classifier
    clf = SVC(C=C,
              kernel=kernel,
              degree=degree,
              gamma=gamma,
              probability=True,
              tol=tol,
              decision_function_shape=decision_function_shape)
    # Fit
    clf.fit(X_train, Y_train)
    # Predict",lobpredictrst/svm.py,doutib/lobpredict,1
"        stats.N[n] = len(Food_df[cTestF])

        forest2 = ExtraTreesClassifier(n_estimators=50, max_depth=None, min_samples_split=1, random_state=0)
        forest2.fit(TrainX,TrainY)
        forestOut2 = forest2.predict(TestX)                              
        stats.ET.TP[n] = sum(forestOut2[0:stats.P[n]] == TestY[0:stats.P[n]])
        stats.ET.TN[n] = sum(forestOut2[stats.P[n]+1:] == TestY[stats.P[n]+1:])
        stats.ET.FP[n] = stats.N[n] - stats.ET.TN[n]
        stats.ET.FN[n] = stats.P[n] - stats.ET.TP[n]

        clf2 = svm.LinearSVC()
        clf2.fit(TrainX,TrainY)
        clfOut2 = clf2.predict(TestX)
        stats.SVC.TP[n] = sum(clfOut2[0:stats.P[n]] == TestY[0:stats.P[n]])
        stats.SVC.TN[n] = sum(clfOut2[stats.P[n]+1:] == TestY[stats.P[n]+1:])
        stats.SVC.FP[n] = stats.N[n] - stats.SVC.TN[n]
        stats.SVC.FN[n] = stats.P[n] - stats.SVC.TP[n]
    return stats

## Testing",machine_learn/HOG/calc_stats_for_HogMethod.py,kaylanb/SkinApp,1
"			x = points[:,0][classes == c]
			y = points[:,1][classes == c]
			plt.plot(x, y, 'x', ms=2)
			#print c, len(x), x[:10], y[:10]
		#plt.savefig('svm_classifier.pdf', bbox_inches='tight')
		#print 'svm_classify --'
	u = points.mean(axis=0)
	s = points.std(axis=0)
	transform = lambda p: (p - u)/s
	points = transform(points)
	clf = sklearn.svm.NuSVC(nu=0.05, probability=True, kernel='rbf')
	clf.fit(points, classes)

	if plot:
		x = numpy.linspace(0, 1, 100)
		y = numpy.linspace(0, 1, 100)
		grid = numpy.array([[[xi, yi] for xi in x] for yi in y])
		dists = numpy.array([[clf.predict_proba(transform([[xi, yi]]))[0][0] for xi in x] for yi in y])
		print 'levels:', dists.max(), dists.min()
		plt.contour(x, y, dists, [0.99, 0.9, 0.1], colors=['red', 'red', 'red'], linestyles=['-', '--', ':'])",nested_sampling/samplers/svm/svmnest.py,JohannesBuchner/UltraNest,1
"
# fit scaler
scaler = StandardScaler()
scaler.fit(X_train)

# scale data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# fit model
#model = SVC(kernel=""linear"", C=0.025)
#model = SVC(gamma=2, C=1)
model = KNeighborsClassifier(10)
#model = GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True)
#model = DecisionTreeClassifier(max_depth=5)
#model = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)
#model = MLPClassifier(alpha=1)
#model = AdaBoostClassifier()
#model = GaussianNB()
#model = QuadraticDiscriminantAnalysis()",src/model.py,jokruger/python-ml-basics,1
"                                                                                                  column_subsample=params['column_subsample'],
                                                                                                  calibration_method=calibration_method,
                                                                                                  n_folds=n_folds)
    clf = gl_wrapper.BoostedTreesClassifier(**params)

elif model == 'svm':
    params = {'C': 5, 'cache_size': 2048}
    method = 'svm_{C}_nfolds_{n_folds}_calibration_{calibration_method}'.format(n_folds=n_folds,
                                                                                C=params['C'],
                                                                                calibration_method=calibration_method)
    clf = OneVsRestClassifier(SVC(**params), n_jobs=-1)


skf = cross_validation.StratifiedKFold(target, n_folds=n_folds, random_state=random_state)

ccv = CalibratedClassifierCV(base_estimator=clf, method=calibration_method, cv=skf)

print 'fit the data'

fit = ccv.fit(X, target)",src/sklearn_callibratedCV.py,ternaus/kaggle_otto,1
"		else:
			Y += [3]*20

		# Create training data
		for row in coords:
			X.append(create_features(p, row[0], row[1], box_size))

	# ========== Train model =========
	print(""Training Random Forest"")
	clf = RandomForestClassifier(n_estimators=500)
	# clf = SVC()
	clf = clf.fit(X, Y)

	# ========== Test model =========
	print(""Predicting"")

	# Initilize dict for prediction results
	preds = {'white':np.empty((1,2)),
		   	   'brown':np.empty((1,2)),
		       'gray':np.empty((1,2)),",slum_kibera/ml_playground.py,Bubblbu/ssip_2016_team_d,1
"            {'model':RandomForestClassifier(),
             'params':{'n_estimators':[500,1000],
                       'max_depth':[40,80,160,500,1000],
                       'min_samples_split':[2,5,10],
                       'probability':[True]}},
            'ada_boost':
            {'model':AdaBoostClassifier(),
             'params':{'n_estimators':[500,1000],
                       'learning_rate':[0.1,0.5,0.75,1.0]}},
            'svc':
            {'model':SVC(),
             'params':{'C':[0.1,0.5,1.0],
                       'kernel':['linear','rbf'],
                       'probability':[True]}},
##            'decision_tree':
#            {'model':DecisionTreeClassifier(),
#             'params':{'n_estimators':[100,500,1000],
#                       'max_depth':[10,40,80,160,320],
 #                      'min_samples_split':[2,5,10],
 #                      'probability':[True]}},",WorldBank2015/Code/modeling/model_pipeline_script.py,eredmiles/Fraud-Corruption-Detection-Data-Science-Pipeline-DSSG2015,1
"        """""" Fit classifier to car and not car data
        """"""
        if os.path.isfile('model.p'):
            with open('model.p', 'rb') as data_file:
                data = pickle.load(data_file)
                self.model = data['model']
                self.scaler = data['scaler']
            return self.model

        x_train, y_train, x_test, y_test = self.get_data()
        svc = LinearSVC(max_iter=20000)
        svc.fit(x_train, y_train)
        data = {
            'model' : svc,
            'scaler' : self.scaler
        }
        with open('model.p', ""wb"") as data_file:
            pickle.dump(data, data_file)
        self.model = svc
        return self.model",car_classifier.py,shawpan/vehicle-detector,1
"                np.any([np.sum(y == class_) < n_folds for class_ in
                        self.classes_]):
            raise ValueError(""Requesting %d-fold cross-validation but provided""
                             "" less than %d examples for at least one class.""
                             % (n_folds, n_folds))

        self.calibrated_classifiers_ = []
        if self.base_estimator is None:
            # we want all classifiers that don't expose a random_state
            # to be deterministic (and we don't want to expose this one).
            base_estimator = LinearSVC(random_state=0)
        else:
            base_estimator = self.base_estimator

        if self.cv == ""prefit"":
            calibrated_classifier = _CalibratedClassifier(
                base_estimator, method=self.method)
            if sample_weight is not None:
                calibrated_classifier.fit(X, y, sample_weight)
            else:",site/lib/python2.7/site-packages/sklearn/calibration.py,asnorkin/sentiment_analysis,1
"    return X, y


def plot_rbf_svm_parameters():
    X, y = make_handcrafted_dataset()

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, C in zip(axes, [1e0, 5, 10, 100]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='rbf', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)

    fig, axes = plt.subplots(1, 4, figsize=(15, 3))
    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])
        svm = SVC(gamma=gamma, kernel='rbf', C=1).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""gamma = %f"" % gamma)",notebooks/figures/plot_rbf_svm_parameters.py,PepSalehi/scipy_2015_sklearn_tutorial,1
"		yint = cl_model.predict( Xpart_ct)

		X_train, X_test, y_train, y_test = \
			model_selection.train_test_split( Xpart_cf, yint, test_size = 0.2)

		model = tree.DecisionTreeClassifier()
		model.fit( X_train, y_train)
		dt_score = model.score( X_test, y_test)
		print( ""DT-C:"", dt_score)

		model = svm.SVC( kernel = 'linear')
		model.fit( X_train, y_train)
		sv_score = model.score( X_test, y_test)
		print( ""SVC:"", sv_score)

		model = kkeras.MLPC( [Xpart_cf.shape[1], 30, 10, nb_classes])
		model.fit( X_train, y_train, X_test, y_test, nb_classes)
		mlp_score = model.score( X_test, y_test)
		print( ""MLP:"", mlp_score)
",kcell.py,jskDr/jamespy_py3,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,annapasca/mne-python,1
"



y = np.array(y)

X = np.array(X)

clf = KNeighborsClassifier(n_neighbors=11)
# clf = RandomForestClassifier(n_estimators=20)
# clf = SVC(gamma=0.001, kernel='rbf', C=100)

skf = StratifiedKFold(y, n_folds=2)
for train_index, test_index in skf:
    print(""Detailed classification report:"")
    print()
    print(""The model is trained on the full development set."")
    print(""The scores are computed on the full evaluation set."")
    print()
    X_train, X_test = X[train_index], X[test_index]",tests/ensemble_gradient.py,schae234/gingivere,1
"    sel = subsample(sel, numsub)

    norm_trn_data = normdata.loc[sel['trn'], sel['feats']]
    norm_tst_data = normdata.loc[sel['tst'], sel['feats']]
    tst_data = rawdata.loc[sel['tst'], sel['feats']]

    t1 = time()
    #################### CLASSIFICATION ################
    sklda = LDA()
    skknn = KNN(3, warn_on_equidistant=False)
    sksvm = SVC()
    sklda.fit(norm_trn_data, sel['trnl'])
    skknn.fit(norm_trn_data, sel['trnl'])
    sksvm.fit(norm_trn_data, sel['trnl'])
    errors['lda'] = (1-sklda.score(norm_tst_data, sel['tstl']))
    errors['knn'] = (1-skknn.score(norm_tst_data, sel['tstl']))
    errors['svm'] = (1-sksvm.score(norm_tst_data, sel['tstl']))
    print(""skLDA error: %f"" % errors['lda'])
    print(""skKNN error: %f"" % errors['knn'])
    print(""skSVM error: %f"" % errors['svm'])",exps/tcga.py,binarybana/samcnet,1
"

def get_classifier(classifier_str):
    '''
    This functions maps the classifier string classifier_str to the
    corresponding classifier object with the default paramers set.
    '''

    # SVC
    if(classifier_str == 'linearsvc'):
        cl = svm.LinearSVC(**svm_default_param)
    elif(classifier_str == 'svc_linear'):
        libsvm_default_param['kernel'] = 'linear'
        cl = svm.SVC(**libsvm_default_param)
    elif(classifier_str == 'svc_rbf'):
        libsvm_default_param['kernel'] = 'rbf'
        cl = svm.SVC(**libsvm_default_param)
    # polynomial, sigmoid kernel
    # nuSVC
    # Nearest Neighbors (euclidian distance used by default)",spice/classification.py,basvandenberg/spice,1
"        for cls in classes:
            models.extend(task_manager.get_best_models(cls, n))

        return list(reversed(sorted(models, key=lambda x: x[0])))

    def _get_models_with_scores(self, models, scores):
        return np.array(list(reversed(sorted(zip(scores, models)))))


class BlendClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, estimators=(SVC(),), coefs=(1,)):
        """"""
        Estimator which result is blending of many models.

        Parameters
        ----------
        estimators : tuple of classifiers, optional, default: (SVC(),)
            The tuple of classifiers to blend.
        coefs : tuple of numbers (float, int), optional, default: (1,)
            The tuple of coefficients for classifiers blending.",ensemble/_ensemble.py,asnorkin/parapapapam,1
"import template
import numpy as np
from sklearn import svm

if __name__ == ""__main__"":
	print ""Receiving Data ...""
	trainX, trainY, m = template.ReceiveData()
	
	limit = 1000
	print ""Creating Model""
	clf = svm.SVC()
	print ""Fitting Data""
	clf.fit(trainX[:-limit], trainY[:-limit])
	print ""Predicting Test Data""
	predictions = clf.predict(trainX[limit:])
	print ""Prediction Error:"", len(np.where(predictions - trainY[limit:] != 0))/limit
",machine_learning/Generalized Linear Models/support_vector_machine.py,sihrc/Personal-Code-Bin,1
"# use models to predict relevance on test dataset
def train_and_classify(train, test):
    prods_train = group_products_by_queries(train)
    vocab = get_product_vocab(prods_train)

    f = build_features(prods_train, vocab)
    clfs = {}
    for query,products in f.items():
        (X, Y, weights) = products
        print ""query: %s. number of products: %d"" % (query, X.shape[0])
        clf = svm.SVC(C=SVM_C, random_state=SEED, kernel='linear')
        clf.fit(X,Y)
        #clf.fit(X,Y,weights)

        clfs[query] = clf

    prods_test = group_products_by_queries(test,True)
    f_t = build_features(prods_test,vocab,True)

    res = {}",src/multi_svm_model.py,gbakie/kaggle-cf-search,1
"    gat.predict(epochs)
    assert_array_equal(np.shape(gat.y_pred_), (15, 5, 14, 1))
    # --- silly value
    gat.test_times = 'foo'
    assert_raises(ValueError, gat.predict, epochs)
    assert_raises(RuntimeError, gat.score)
    # --- unmatched length between training and testing time
    gat.test_times = dict(length=.150)
    assert_raises(ValueError, gat.predict, epochs)

    svc = SVC(C=1, kernel='linear', probability=True)
    gat = GeneralizationAcrossTime(clf=svc, predict_mode='mean-prediction')
    with warnings.catch_warnings(record=True):
        gat.fit(epochs)

    # sklearn needs it: c.f.
    # https://github.com/scikit-learn/scikit-learn/issues/2723
    # and http://bit.ly/1u7t8UT
    assert_raises(ValueError, gat.score, epochs2)
    gat.score(epochs)",mne/decoding/tests/test_time_gen.py,jona-sassenhagen/mne-python,1
"    :param param_grid: grid of parameters to try
    :return: clf.best_estimator_ the best classifier

    '''
    #X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=0)

    #X_train,X_test,y_train,y_test = split_data(feature_matrix,labels,params['split_percentage'])

    score = 'f1_weighted'

    clf = GridSearchCV(SVC(C=1), param_grid, cv=5, scoring=score, n_jobs=10)

    clf.fit(X_train, y_train)

    print ""Best score during training: "", clf.best_score_
    print ""Best estimator"", clf.best_estimator_

    print ""Classification report for validation set:""
    print classification_report(y_test,clf.predict(X_test))
",scripts/python/train_svm.py,imatge-upc/trecvid-2015,1
"__author__ = 'Jiarui Xu'

from sklearn import svm
import pandas as pd

data_path = ""../../examples/data/iris.csv""

data = pd.read_csv(data_path)
clf = svm.SVC()
clf.fit(data.iloc[:, 0:4], data['Name'])
",learnpy/tests/comparisons/SVM_scikit.py,LargePanda/LearnPy,1
"    print(""n_samples: %d"" % len(dataset.data))

    # split the dataset in training and test set:
    docs_train, docs_test, y_train, y_test = train_test_split(
        dataset.data, dataset.target, test_size=0.25, random_state=None)

    # TASK: Build a vectorizer / classifier pipeline that filters out tokens
    # that are too rare or too frequent
    pipeline = Pipeline([
        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),
        ('clf', LinearSVC(C=1000)),
    ])
    ##TfidfVectorizer params:
    #min_df: ignore terms that have doc frequency < threshold (cut-off). 
    #float in [0,1] represents proportion of docs, integer represents absolute counts
    #max_df: similar

    # TASK: Build a grid search to find out whether unigrams or bigrams are
    # more useful.
    # Fit the pipeline on the training set using grid search for the parameters",sklearn/exercise_02_sentiment.py,gnublet/py_explorations,1
"
class SvmAlgorithm(IClassificationAlgorithm):
    """"""
    Class for building model using Support Vector Machine method
    """"""
    def __init__(self, sentence_embedding, **kwargs):
        IClassificationAlgorithm.__init__(self)
        self.sentence_embedding = sentence_embedding
        if ""n_jobs"" in kwargs:
            del kwargs[""n_jobs""] # multi-threading not available here
        self.clf = svm.SVC(**kwargs)

    def fit(self, features, labels):
        self.clf.fit(features, labels)

    def predict(self, sentence):
        return int(self.clf.predict([self.sentence_embedding[sentence]])[0])

    def predict_proba(self, sentence):
        return self.clf.predict_proba([self.sentence_embedding[sentence]])[0]",src/models/algorithms/svm_algorithm.py,mikolajsacha/tweetsclassification,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",python/mlAlgorithms/demo_classifier.py,veksev/cydi,1
"classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))

###############################################################################
print(""PREPARE CLASSIFICATION"")


#-- classifier

clf = GridSearchCV(svm.SVC(kernel='linear', probability=True),
    {'C': svm_C}, score_func=precision_score)

#-- normalizer
scaler = Scaler()

#-- feature selection
if fs_n > 1:
    fs = SelectKBest(f_classif, k=fs_n)
elif fs_n == -1:",JR_toolbox/skl_king_parallel_gs2.py,kingjr/natmeg_arhus,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,",playground/clustering/landsat.py,geobricks/Playground,1
"    k(X, Y) = X  (    ) Y.T
                 (0  1)
    """"""
    M = np.array([[2, 0], [0, 1.0]])
    return np.dot(np.dot(X, M), Y.T)


h = .02  # step size in the mesh

# we create an instance of SVM and fit out data.
clf = svm.SVC(kernel=my_kernel)
clf.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
",examples/svm/plot_custom_kernel.py,ngoix/OCRF,1
"for i in svm_linear_grid:
    for j in svm_linear_grid:
         svm_rbf_grid.append([i,j])
randforest_grid = [2,3,5,10,20,40,60]

kneighbors_rates = rates(data, kneighbors_grid, lambda K: NeighborsClassifier(n_neighbors=K))
print_result(kneighbors_rates)
kneighbors_pca_rates = rates(data_pca, kneighbors_grid, lambda K: NeighborsClassifier(n_neighbors=K))
print_result(kneighbors_pca_rates)

svm_linear_rates = rates(data, svm_linear_grid, lambda C: SVC(C=C, kernel='linear'))
print_result(svm_linear_rates)
svm_linear_pca_rates = rates(data_pca, svm_linear_grid, lambda C: SVC(C=C, kernel='linear'))
print_result(svm_linear_pca_rates)

svm_rbf_rates = rates(data, svm_rbf_grid, lambda C: SVC(C=C[0], gamma=C[1], kernel='rbf'))
print_result(svm_rbf_rates)
svm_rbf_pca_rates = rates(data_pca, svm_rbf_grid, lambda C: SVC(C=C[0], gamma=C[1], kernel='rbf'))
print_result(svm_rbf_pca_rates)
",proj03/proj03.py,rerthal/mc886,1
"
    ndcgs = compute_ndcgs_without_ws(data);
    print('Current NDCG: {}, std: {}'.format(np.mean(ndcgs), np.std(ndcgs)))
    print()

    xs, ys = transform_data(data)

    if args.plot:
        plot_diagrams(xs, ys, FEATURES)

    clf = svm.LinearSVC(random_state=args.seed)
    cv = cross_validation.KFold(len(ys), n_folds=5, shuffle=True, random_state=args.seed)

    # ""C"" stands for the regularizer constant.
    grid = {'C': np.power(10.0, np.arange(-5, 6))}
    gs = grid_search.GridSearchCV(clf, grid, scoring='accuracy', cv=cv)
    gs.fit(xs, ys)

    ws = gs.best_estimator_.coef_[0]
    max_w = max(abs(w) for w in ws)",search/search_quality/scoring_model.py,sdesimeur/omim,1
"
classifier_list = [
    # ['GradientBoosting', GradientBoostingClassifier()],
    ['RandomForest', RandomForestClassifier()],
    # ['GaussianNB', GaussianNB()],
    # ['DecisionTree', DecisionTreeClassifier()],
    # ['LogisticRegression', LogisticRegression()],
    # ['MLP', MLPClassifier()],
    # ['AdaBoost', AdaBoostClassifier()],
    # ['KNN', KNeighborsClassifier()]
    #    ['SVC', SVC(probability=True)],
    #    ['QDA', QuadraticDiscriminantAnalysis()],
]

samplers_list = [
    ['RandomOverSampler', RandomOverSampler()],
    # ['SMOTE', SMOTE()],
    # ['DummySampler', DummySampler()],
    # ['SMOTEENN', SMOTEENN()],
    # ['SMOTETomek', SMOTETomek()],",ml_files/perform_sklearn.py,patemotter/trilinos-prediction,1
"        bestFeatureID = np.argmax(classifierScore)
        featureAccuracy[(nFeaturesIter-1),:] = results[np.sum(featOnListArray, axis=1)==nFeaturesIter][bestFeatureID,:]
        print('For a classifier using %d features,' %(nFeaturesIter))
        print('the most informative features are:')
        print(bestFeatures.astype(list), 'with a score of %f' %(np.max(classifierScore)))
        scoreList = []
        for iter in range(results.shape[1]):
            Xrand = np.random.randn(XCull.shape[0], nFeaturesIter)
            X_train, X_test, y_train, y_test = cross_validation.train_test_split(
              Xrand, yCull, test_size=0.2)
            svc = SVC(kernel='rbf', probability=True)
            svc.fit(X_train,y_train)
            scoreList.append(svc.score(X_test, y_test))

        print('%d random features have a score of: %f' %(nFeaturesIter, np.mean(scoreList)))
        print()

    pl.figure()
    pl.errorbar((np.arange(nTrueFeatures)+1).astype(int), np.mean(featureAccuracy,axis=1), np.std(featureAccuracy,axis=1))
    pl.title('Area Under ROC Curve vs Number of Features')",ipython/seafloor/sea_floor_feature.py,GPlates/Portal,1
"           and platform.architecture()[0] == '64bit' \
           and self.oBuild.sKind == 'development' \
           and os.getenv('VERSIONER_PYTHON_PREFER_32_BIT') != 'yes':
            print ""WARNING: 64-bit python on darwin, 32-bit VBox development build => crash""
            print ""WARNING:   bash-3.2$ /usr/bin/python2.5 ./testdriver""
            print ""WARNING: or""
            print ""WARNING:   bash-3.2$ VERSIONER_PYTHON_PREFER_32_BIT=yes ./testdriver""
            return False;

        # Start VBoxSVC and load the vboxapi bits.
        if self._startVBoxSVC() is True:
            assert(self.oVBoxSvcProcess is not None);

            sSavedSysPath = sys.path;
            self._setupVBoxApi();
            sys.path = sSavedSysPath;

            # Adjust the default machine folder.
            if self.fImportedVBoxApi and not self.fUseDefaultSvc and self.fpApiVer >= 4.0:
                sNewFolder = os.path.join(self.sScratchPath, 'VBoxUserHome', 'Machines');",src/VBox/ValidationKit/testdriver/vbox.py,Chilledheart/vbox,1
"
from bayes_opt import BayesianOptimization

# Load data set and target values
data, target = make_classification(n_samples=2500,
                                   n_features=45,
                                   n_informative=12,
                                   n_redundant=7)

def svccv(C, gamma):
    return cross_val_score(SVC(C=C, gamma=gamma, random_state=2),
                           data, target, 'f1', cv=5).mean()

def rfccv(n_estimators, min_samples_split, max_features):
    return cross_val_score(RFC(n_estimators=int(n_estimators),
                               min_samples_split=int(min_samples_split),
                               max_features=min(max_features, 0.999),
                               random_state=2),
                           data, target, 'f1', cv=5).mean()
",examples/sklearn_example.py,scr4t/BayesianOptimization,1
"    train_means, train_covs = compute_means_covs(ts_train, t_ref, gp_parms)
    test_means, test_covs = compute_means_covs(ts_test, t_ref, gp_parms)

    # We use 500 random features with low-rank approximation, rank 10 in this
    # case, and normalize the random feature vector to have unit length.
    # By dropping the rank argument or set rank to 0 turns off the low rank
    # approximation.
    # The parameters gamma and C can be chosen using cross validation.
    rp = FastfoodEGK(gamma=20, n_sample=500, rank=10,
                     normalize=True)
    clf = LinearSVC(C=100)
    X_train = rp.fit_transform(train_means, train_covs)
    clf.fit(X_train, l_train)
    X_test = rp.transform(test_means, test_covs)
    l_predict = clf.predict(X_test)
    accuracy = np.mean(l_predict == l_test)
    print accuracy


if __name__ == '__main__':",demo.py,mlds-lab/egk,1
"# convert all strings to ints
X, y = map(lambda x:list(map(int, x)), X), map(int, y)
X, y = np.array(list(X)), np.array(list(y))
n_samples, n_features = X.shape

# create a stratified 10-fold cross-validation iterator that generates the
# indices for us
cv = StratifiedKFold(y=y, n_folds=10, shuffle=True)

# create a support vector machine classifier
clf_svm = svm.SVC(kernel='linear', probability=True)

# create a Gaussian naive Bayes classifier
clf_gauss_nb = GaussianNB()

# create a multivariate Bernoulli naive Bayes classifier
clf_binom_nb = BernoulliNB()

# create a logistic regression classifier
clf_logistic = linear_model.LogisticRegression()",code/sklearn/different_clfs.py,tianyupu/hons-thesis,1
"          (.2, -2.3),
          (0, -2.7),
          (1.3, 2.1)].T
Y = [0] * 8 + [1] * 8

# figure number
fignum = 1

# fit the model
for kernel in ('linear', 'poly', 'rbf'):
    clf = svm.SVC(kernel=kernel, gamma=2)
    clf.fit(X, Y)

    # plot the line, the points, and the nearest vectors to the plane
    plt.figure(fignum, figsize=(4, 3))
    plt.clf()

    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
                facecolors='none', zorder=10)
    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)",projects/scikit-learn-master/examples/svm/plot_svm_kernels.py,DailyActie/Surrogate-Model,1
"from utils import *
from sklearn import svm
import time

X_train, y_train = load_data('data/poker-hand-training-true.data', True)
t1 = time.time()
clf = svm.SVC()
clf.fit(X_train, y_train)
t2 = time.time()
print 'Training: %f s used.' % (t2-t1)

X_test, y_test = load_data('data/poker-hand-testing.data', True)
t1 = time.time()
y_pred = clf.predict(X_test)
t2 = time.time()
print 'Testing: %f s used.' % (t2-t1)",ml/poker/src/svm.py,huangshenno1/algo,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_11_2015.py,magic2du/contact_matrix,1
"            #            acc.append(float(cm[i, i])/float(total[i]))

            #normalized_score += sum(acc)/float(len(acc))

	    normalized_score += numpy.trace(cm)/cm.shape[0]

            print ""score : "" + str(score)
            # print metrics.classification_report(y_true, y_pred)
            print ""score norm: "", numpy.trace(cm)/cm.shape[0]
            print ""----------------------------""
            #clf = sklearn.svm.SVC(kernel=""linear"", C=1)

        print ""total score = "" + str(total_score / len(classes))
        print ""total norm = "" + str(normalized_score / len(classes))
        print parametros
            #clf.fit(data['Xtrain'], ytrain)
            #predict = clf.predict(data['Xval'])
        '''
            yVal = data['Yval'].T.reshape(data['Yval'].shape[1])
            print ""Acuracia: "", sklearn.metrics.accuracy_score(yVal, predict)",svm_grid.py,alan-mnix/MLFinalProject,1
"print(sentence_distance, use_pred)

# ----------------------------------------------------------------------------------------------------

SCORING_NAMES = [
    F05_SCORER,
    'f1',
]

FEATURE_SELECTIONS = [
    # (""LinearSVC_C=4.0"", SelectFromModel(LinearSVC(C=2.0, penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    (""LinearSVC_C=2.0"", SelectFromModel(LinearSVC(C=2.0, penalty=""l1"", dual=False, tol=1e-5))),
    (""LinearSVC_C=1.0"", SelectFromModel(LinearSVC(C=1.0, penalty=""l1"", dual=False, tol=1e-5))),
]

PIPELINE = Pipeline([
    # (""feat_sel"", SelectFromModel(LinearSVC(penalty=""l1"", dual=False, random_state=2727, tol=1e-5))),
    ('classify', SVC(kernel=""linear""))
])
",scripts/grid_search_hyperparameters.py,juanmirocks/LocText,1
"from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from xgb import XGBoostClassifier


BasicSVM = Pipeline([(""SVM (linear)"", LinearSVC())])
NB = Pipeline([(""Gaussian NB Bayes"", GaussianNB())])
SGD = Pipeline([(""Stochastic Gradient Descent"", SGDClassifier())])
DTC = Pipeline([(""Decision Tree"", DecisionTreeClassifier())])
AdaBoost = Pipeline([(""Ada Boost"", AdaBoostClassifier())])
GradientBoosting = Pipeline([(""Gradient Boosting"", GradientBoostingClassifier())])
XGB = Pipeline([(""XGBoost"", XGBoostClassifier(num_class=2, silent=1))])
RandomForest = Pipeline([(""Random Forest"", RandomForestClassifier())])",titanic/classifier.py,lbn/kaggle,1
"    >>> from sklearn import svm
    >>> from sklearn.datasets import samples_generator
    >>> from sklearn.feature_selection import SelectKBest
    >>> from sklearn.feature_selection import f_regression
    >>> from sklearn.pipeline import Pipeline
    >>> # generate some data to play with
    >>> X, y = samples_generator.make_classification(
    ...     n_informative=5, n_redundant=0, random_state=42)
    >>> # ANOVA SVM-C
    >>> anova_filter = SelectKBest(f_regression, k=5)
    >>> clf = svm.SVC(kernel='linear')
    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
    >>> # You can set the parameters using the names issued
    >>> # For instance, fit using a k of 10 in the SelectKBest
    >>> # and a parameter 'C' of the svm
    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
    ...                                              # doctest: +ELLIPSIS
    Pipeline(steps=[...])
    >>> prediction = anova_svm.predict(X)
    >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS",sklearn/pipeline.py,mjudsp/Tsallis,1
"import matplotlib.pyplot as plt
import networkx as nx
G = {}
np.seterr(divide='ignore',invalid='ignore')

trainArticles= open('data/singleShort.txt','r').readlines()#=importArticles.getData('train')
testArticles = open('data/singleShortTest.txt','r').readlines()#= importArticles.getData('test')
print len(trainArticles)
print len(testArticles)
listOfYears = []
clf = svm.SVC(probability=True)
probs = []
titles = []
#A
def getArticle(article):
    singleSets = []
    try:
        chunks = gc.getChunks(article[1])
        tags =  tag.getTags(article[1],chunks)
        #if tags == []:",classifier-title.py,JFriel/honours_project,1
"
    def get_classifier(self, train=True, test=True):

        all_output = """"
        h = .02  # step size in the mesh
        self.names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
                      ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
                      ""Quadratic Discriminant Analysis""]
        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LinearDiscriminantAnalysis(),
            QuadraticDiscriminantAnalysis()]

        for i in range(0, len(self.names)):",history/models.py,owocki/pytrader,1
"import numpy as np

class Classifier(object):
    MODELS = OrderedDict([
        ('LR', LogisticRegression()),
        ('LDA', LinearDiscriminantAnalysis()),
        ('CART', DecisionTreeClassifier()),
        ('KNN', KNeighborsClassifier()),
        ('NB', GaussianNB()),
        ('K Neighbors', KNeighborsClassifier(3)),
        ('SVM Linear', SVC(kernel=""linear"", C=0.025)),
        ('SVM Gamma', SVC(gamma=2, C=1)),
        ('GaussianProcessClassifier', GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True)),
        ('Decision Tree Classifier', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest Classifier', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),
        ('MLP Classifier', MLPClassifier(alpha=1)),
    ])

    @classmethod
    def score(cls, filename):",bricklayer/doctor/classifier.py,cbrentharris/bricklayer,1
"#in_file_name = '../data/train_text.txt'
in_file_name = '../data/train_text2.txt' #去除标签,处理A_B
#in_file_name = '../data/train_text3.txt' #换stop
bin_vectorizer, term_occurence = feature_extraction(in_file_name)

#提取属性
#term occurence
#'''
#clf, f = model_cross_validation(svm.SVC, term_occurence, class_label, \
#    feature_name = 'term_occurence')
clf = svm.SVC(kernel='linear', C=0.025)
#训练模型
print 'training model...'
clf.fit(term_occurence, class_label)
#预测
print 'making prediction...'
#test_file_name = '../data/test_text2.txt'
#out_file_name = '../data/test_label2.txt'
test_file_name = '../data/test_text3.txt'
out_file_name = '../data/test_label3.txt'",source/linear_svm_classifier.py,y3ah/Sentiment_Categorization,1
"                 map(os.path.split,
                     map(os.path.dirname, labels)))  # Get the directory.
    fname = ""{}/reps.csv"".format(args.workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)
    labelsNum = le.transform(labels)
    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))

    if args.classifier == 'LinearSvm':
        clf = SVC(C=1, kernel='linear', probability=True)
    elif args.classifier == 'GridSearchSvm':
        print(""""""
        Warning: In our experiences, using a grid search over SVM hyper-parameters only
        gives marginally better performance than a linear SVM with C=1 and
        is not worth the extra computations of performing a grid search.
        """""")
        param_grid = [
            {'C': [1, 10, 100, 1000],
             'kernel': ['linear']},",openface1/demos/classifier_mod.py,EliHar/Pattern_recognition,1
"    print(""Loaded %d sessions and %d truthy values for %s"" % (len(X), len(y), filename))

    leave_one_out = cross_validation.LeaveOneOut(len(y))

    act = []
    pred = []
    for train_index, test_index in leave_one_out:
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        clf = SVC()
        clf.fit(X_train, y_train)
        p = clf.predict(X_test)

        act.append(y_test[0])
        pred.append(p[0])
        #print ""%s, predicted %s"" % (y_test[0], y_pred[0])

    return act, pred
",source/classification/reference/cross_validate.py,jschavem/facial-expression-classification,1
"from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.grid_search import GridSearchCV


def make_clf():
    estimators = [('reduce_dim', PCA()), ('svm', SVC())]
    return Pipeline(estimators)


def make_gs():
    params = dict(reduce_dim__n_components=[2, 4, 8, 16, 32, 64], svm__C=[0.1, 10, 100])
    return GridSearchCV(make_clf, param_grid=params, scoring='f1-score', cv=5)


def print_best_parameters(gs_clf):",tests/clf_creators.py,bhillmann/gingivere,1
"
precision, recall, f1, accuracy, support, fn = 0, 0, 0, 0, 0, 0

loo = LeaveOneOut()

start = timer()
for train, test in loo.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	
	clf1 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
	y_prob1 = clf1.predict_proba(X_test)[:,1]
	y_pred1 = clf1.predict(X_test)
	y_acc1 = accuracy_score(y_test, y_pred1)

	clf2 = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)
	y_prob2 = clf2.predict_proba(X_test)[:,1]
	y_pred2 = clf2.predict(X_test)
	y_acc2 = accuracy_score(y_test, y_pred2)
",LeaveOneOut/WeightedAveraging.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"        # Set the parameters by cross-validation
        tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                             'C': [1, 10, 100, 1000]},
                            {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

        scores = ['precision', 'recall']

        for score in scores:
            print(""# Tuning hyper-parameters for %s \n"" % score)           

            clf = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=5, scoring=score)
            clf.fit(to_train, to_target)

            print(""Best parameters set found on development set:\n"")
            print(clf.best_estimator_)            
            print(""Grid scores on development set:\n"")
            
            for params, mean_score, scores in clf.grid_scores_:
                print(""%0.3f (+/-%0.03f) for %r""
                      % (mean_score, scores.std() / 2, params))",Model.py,A-Malone/DMC-2015,1
"y = data[label_name]
X = data.drop(label_name, axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(),
    LinearSVC(),
    SVC(),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True,
        copy_X_train=False),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    MLPClassifier(),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]",data-scripts/mongo-all-classifiers.py,JohnStarich/github-code-recommendations,1
"# Created by svenko11 on 10/30/15 6:28 PM
__author__ = 'Sven Vidak'

from sklearn.svm import SVC as skSVC

class SVC:
	def __init__(self, C, kernel, gamma=0.0, verbose=True):
		self.clf = skSVC(C=C, kernel=kernel, gamma=gamma, verbose=verbose)

	def fit(self, X, y):
		self.clf.fit(X, y)

	def predict(self, X):",classic/classifiers/SVC.py,mister11/otd_master,1
"        return results

    def SVC_learn(self, ndays, c = 1, kernel='rbf', p={'meanfractal', 'loc_vol'},y_colums=None,forward_look=0):
        pred = self.pred
        if y_colums is None:
            pred[""switch""] = np.where((pred.make_splits(5, inplace=False).shift(1) / pred.make_splits(5, inplace=False))
                                  > (1.0025 / 0.9975) + self.min_shift, 1, 0)
        else:
            pred[""switch""] = pred[y_colums]
        self.indicators = p
        clf = svm.SVC(kernel=kernel, gamma=0.7, C=c,probability=True)
        results = pd.DataFrame()
        accuracy = []
        for i in range(ndays, len(pred.index)-forward_look):
            # We perform a 80/20 split on the data
            ind = int(np.round(ndays*0.8))
            X_TRAIN = pred.ix[(i - ndays):(i - ndays + ind),p]
            if forward_look > 0:
                idx = pred.ix[(i - ndays):(i - ndays + ind+forward_look)].index
                Y_TRAIN = pred.switch.ix[idx].shift(-1*forward_look)[:(-1*forward_look)]",Methods/ML.py,simonward86/MySJcLqwwx,1
"#
# naivebayes_model = GaussianNB()
# naivebayes_model.fit(X_cropped, y_cropped)
# y_validation_predicted = naivebayes_model.predict(X_validation)
# print ""Naive Bayes Error Rate on Validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)


# # Start SVM Classification
# print ""Performing SVM Classification:""
# from sklearn.svm import SVC
# svm_model = SVC(kernel='rbf' ,probability=True, max_iter=100000)
# svm_model.fit(X_cropped, y_cropped)
# y_train_predicted = svm_model.predict(X_train)
# print ""SVM Error rate on training data (t1): "", ml_aux.get_error_rate(y_train, y_train_predicted)
# ml_aux.plot_confusion_matrix(y_train, y_train_predicted, ""CM SVM Training (t1)"")
# plt.show()
#
# y_validation_predicted = svm_model.predict(X_validation)
# print ""SVM Error rate on validation (t1): "", ml_aux.get_error_rate(y_validation, y_validation_predicted)
",Code/Machine_Learning_Algos/training_t1.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"                X_valid,y_valid = valid_set
                X_test,y_test = test_set
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = (X_train,y_train),(X_train,y_train), (X_test,y_test)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                fisher_mode = settings['fisher_mode']
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append(( subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Mnist_classification_0519.py,magic2du/contact_matrix,1
"    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    X = np.c_[dataset_Monte[:,0], dataset_Monte[:,1]]
    y = dataset_Monte[:,2]
    #print dataset_Monte
    return 1-pclf.score(X,y), 1-clf.score(X,y)
    

if __name__ == ""__main__"":
    clf = SVC(C = 1000, kernel = 'linear')  
    pclf = Perceptron()
    avgpla = list()
    avglf = list()
    perc = 0
    for k in range(1000):
        nopoints = 100
        line = getRandomLine()
        sample = getSample(line[0], line[1], nopoints)
            #print(sample)",Week 7/plavssvm.py,pramodh-bn/learn-data-edx,1
"
letters = load_letters()
X, y, folds = letters['data'], letters['labels'], letters['folds']
# we convert the lists to object arrays, as that makes slicing much more
# convenient
X, y = np.array(X), np.array(y)
X_train, X_test = X[folds == 1], X[folds != 1]
y_train, y_test = y[folds == 1], y[folds != 1]

# Train linear SVM
svm = LinearSVC(dual=False, C=.1)
# flatten input
svm.fit(np.vstack(X_train), np.hstack(y_train))

# Train linear chain CRF
model = ChainCRF()
ssvm = FrankWolfeSSVM(model=model, C=.1, max_iter=11)
ssvm.fit(X_train, y_train)

print(""Test score with chain CRF: %f"" % ssvm.score(X_test, y_test))",examples/plot_letters.py,pystruct/pystruct,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",socialite/jython/Lib/email/test/test_email.py,ofermend/medicare-demo,1
"       print ""Exceptions:"",name    
    j=-1
    for nuc in pentanucleotides:
        j+=1
        referencedata[j,i]=dnaseq.count(nuc) # Calculating tetranucleotide frequency
referencedata=np.transpose(referencedata)
print ""Reference data is:"",referencedata

# Validating datasets

clf = svm.SVC(gamma=0.0070, C=5)
clf.fit(querydata, querynames)
referencepredict=clf.predict(referencedata)
print ""Confusion matrix for DNA/LTR retrotransposon is:"",confusion_matrix(referencenames, referencepredict)
print ""Accuracy for DNA/LTR retrotransposon is:"", accuracy_score(referencenames, referencepredict)
repmods=0  #Use this variable for adding ML classifications to 'repeatclassification' list

unkretro=0
for name in fastanamesrandom2:
  if ""Retro"" in name: ",repeatpipeline.py,rop14009/Repeat-analysis-pipeline,1
"nuts = ImageSet(data_path + '/data/supervised/nuts')
nut_blobs = [n.invert().findBlobs()[0] for n in nuts]
for n in nut_blobs:
    tmp_data.append([n.area(), n.height(), n.width()])
    tmp_target.append(1)

dataset = np.array(tmp_data)
targets = np.array(tmp_target)

print 'Training Machine Learning'
clf = LinearSVC()
clf = clf.fit(dataset, targets)
clf2 = LogisticRegression().fit(dataset, targets)

print 'Running prediction on bolts now'
untrained_bolts = ImageSet(data_path + '/data/unsupervised/bolts')
unbolt_blobs = [b.findBlobs()[0] for b in untrained_bolts]
for b in unbolt_blobs:
    ary = [b.area(), b.height(), b.width()]
    name = target_names[clf.predict(ary)[0]]",SimpleCV/examples/machine-learning/machine-learning_nuts-vs-bolts.py,beni55/SimpleCV,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_01_08_2015_parallel.py,magic2du/contact_matrix,1
"#
#
##train,test=StratifiedKFold(Y,2,indices=False)
##
##h=0.01
##x_min, x_max = X[:, 0].min() - 1, X[9:, 0].max() + 1
##y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
##xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
##                     np.arange(y_min, y_max, h))
##
#rbf_svc = sk.svm.SVC(kernel='rbf', gamma=0.8, C=1000.0).fit(X_train, y_train)
##
#print 'Score precision',sk.metrics.accuracy_score(y_test,rbf_svc.predict(X_test))
##
#print 'Report:', sk.metrics.classification_report(y_test,rbf_svc.predict(X_test))
#print sk.metrics.confusion_matrix(y_test,rbf_svc.predict(X_test))
#--------------------------------------------------------------------------------------------
#
#
#",leaf/leaf.py,scidam/leafs,1
"                    assert(name in colnames)
                except AssertionError:
                    raise AssertionError('ERROR: \'%s\' not found in %s' % (name, ', '.join(colnames)))

            assert(np.all(TEST_X == x))

            assert(np.all(TEST_Y == y))

            # generate and test the mRMR portion
            mrmr = MRMR(
                estimator=SVC(kernel='linear'),
                n_features_to_select=ARGS.NUM_FEATURES,
                method=ARGS.MRMR_METHOD,
                normalize=ARGS.MRMR_NORMALIZE,
                similar=ARGS.SIMILAR
                )

            mrmr.fit(x, y)

    finally:",idepi/test/_discrete.py,veg/idepi,1
"    print(len(neg_vec))

    # Merge positive and negative feature vectors and generate their corresponding labels.
    vec = np.array(pos_vec + neg_vec)
    vec_label = np.array([1] * len(pos_vec) + [0] * len(neg_vec))

    # ##############################################################################
    # Classification and accurate analysis.

    # evaluate performance of the predictor by 5-fold cross-validation and plot the mean ROC curve.
    clf = svm.SVC(C=32, gamma=0.5)
    scores = cross_validation.cross_val_score(clf, vec, y=vec_label, cv=5)
    print('Per accuracy in 5-fold CV:')
    print(scores)
    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

    # ###############################################################################
    # Classification and ROC analysis.

    # evaluate performance of the predictor by 5-fold cross-validation and plot the mean ROC curve.",repDNA/example/example3.py,liufule12/repDNA,1
"del train_set[""Cabin""]
del train_set[""Ticket""]
del train_set[""PassengerId""]

train_set.fillna(0, None, None, True)		
titanic_results = train_set[""Survived""]
del train_set[""Survived""]
###End data preparation

##Selftest
machine=svm.SVC(kernel='linear')
machine.fit(train_set.values, titanic_results.values)
predicted_survival=machine.predict(train_set.values)

predictionSuccess=(1-np.mean(predicted_survival != titanic_results.values))*100
print(""Test against training set(self test): ""+str(predictionSuccess)+""% correctness"")
###End selftest


###Predict survival of test.csv",Cloudera/Code/SVM/titanic_linear_predict.py,cybercomgroup/Big_Data,1
"            fds.append(fd)
            labels.append(1)
    for root, dirs, files in neg_list_dirs:
        for f in files:
            path = os.path.join(root, f)
            fd = joblib.load(path)
            fds.append(fd)
            labels.append(0)

    # train classifier as LIN_SVM
    # clf = LinearSVC()
    clf = svm.SVC()
    clf.fit(fds, labels)
    date = 20170623
    joblib.dump(clf, model_path + date.__str__() + '.clf')


def predict_image():
    ii = 1
    src_image = eva.load_image(Const.image + ii.__str__() + '.jpg')",Train.py,Esmidth/DIP,1
"            'mask size: %d' %
            np.sum(mask)
        )
        data, labels = io.prepare_searchlight_mvpa_data(data_dir, extension, epoch_file)
        # the following line is an example to leaving a subject out
        #epoch_info = [x for x in epoch_info if x[1] != 0]
    num_subjs = int(sys.argv[5])
    # create a Searchlight object
    sl = Searchlight(sl_rad=1)
    mvs = MVPAVoxelSelector(data, mask, labels, num_subjs, sl)
    clf = svm.SVC(kernel='linear', shrinking=False, C=1)
    # only rank 0 has meaningful return values
    score_volume, results = mvs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        score_volume = np.nan_to_num(score_volume.astype(np.float))
        io.write_nifti_file(score_volume, mask_img.affine, 'result_score.nii.gz')
        seq_volume = np.zeros(mask.shape, dtype=np.int)
        seq = np.zeros(len(results), dtype=np.int)
        with open('result_list.txt', 'w') as fp:",examples/fcma/mvpa_voxel_selection.py,TuKo/brainiak,1
"        self.scores_val = np.zeros(len(self.data._y_val), dtype=Cfg.floatX)
        self.auc_train = np.zeros(1, dtype=Cfg.floatX)
        self.auc_val = np.zeros(1, dtype=Cfg.floatX)
        self.rho = None

    def initialize_svm(self, loss):

        assert loss in ('SVC', 'OneClassSVM')

        if loss == 'SVC':
            self.svm = svm.SVC(kernel='precomputed', C=Cfg.svm_C)
        if loss == 'OneClassSVM':
            self.svm = svm.OneClassSVM(kernel='precomputed', nu=Cfg.svm_nu)

    def load_data(self, data_loader=None):

        self.data = data_loader()

    def flush_data(self):
",src/svm.py,LRuff/pl-cnn,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-1, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = SVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'mirror')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[100.0, 100.0]]),
        sigma = 1.0,
        beta = 1.0,
        meta_model = meta_model)

    return method
",evopy/examples/problems/SchwefelsProblem26/CMAESRSVC.py,jpzk/evopy,1
"            param_grid = dict(gamma=g_range, C=c_range)
        per = self._percentile(threshold)
        dataset = self.points[:]
        mce = []
        for obj in range(self.f_dim):
            y = zeros(self.npoints)  # classification of data
            for i in range(self.npoints):
                if self.f[i][obj] > per[obj]:
                    y[i] = 1

            grid = GridSearchCV(estimator=SVC(
                kernel=kernel, degree=2), param_grid=param_grid, cv=StratifiedKFold(y, k_tune))
            grid.fit(dataset, y)
            test_score = cross_val_score(
                estimator=grid.best_estimator_, X=dataset, y=y, scoring=None, cv=StratifiedKFold(y, k_test))
            mce.append((ones(k_test) - test_score).tolist())
        return mce  # mce[n_obj][k_test]

    def _svm_p_values(self, threshold=50, k_tune=3, k_test=10, l=True, q=True, n=True):
        """"""",PyGMO/util/_analysis.py,esa/pagmo,1
"from utilfile import *
from config import Config
from sklearn.svm import SVC
import numpy as np

class FBInit(object):
    
    _instance = None
    dict = []
    clf = SVC()
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            print 'fb init instance'
            cls._instance = super(FBInit, cls).__new__(
                                cls, *args, **kwargs)
            fileUtil = FileUtil()
            cls.dict = [a.encode('utf-8')[:-1] for a in fileUtil.read_file(Config.base_dir+'dict/tdict.txt')]
            cls.dict.extend([a.encode('utf-8')[:-1] for a in fileUtil.read_file(Config.base_dir+'dict/eng.txt')])",initial.py,chaluemwut/fbserver,1
"
	X = df[['sepal_length','sepal_width','petal_length','petal_width']]
	Y = df[['class']]
	X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validationsize, random_state=seed)

	print(""Separado"")
	model = {
	'LogisticRegression':LogisticRegression(),
	'KNeighborsClassifier':KNeighborsClassifier(),
	'DecisionTreeClassifier':DecisionTreeClassifier(),
	'SVC':SVC()}[modelname]

	json1_file = open(inputparams)
	json1_str = json1_file.read()
	modelParams = json.loads(json1_str)
	print(""model params: {}"".format(modelParams))
	# print(type(modelParams['C']))

	#print(""X shape {}"".format(X_train.shape))
	#print(""Y shape {}"".format(Y_train.values().ravel().shape))",magicloop/tasks/test-python/script.py,eduardomtz/magicloop,1
"import numpy as np

METHODS_DIRECTORY = 'new_methods/methods/*.txt'
LABELS_DIRECTORY = 'new_annot/*.txt'

def main():

	dimensions = labels_matrices() # List of WordMatrix objects
	methods = methods_matrix() # WordsMatrix object

	result_LSVC = [OVRC(LSVC()).fit(methods.get_matrix(),
		dimension.get_matrix()) for dimension in dimensions]

	result_MNB = [OVRC(MNB()).fit(methods.get_matrix(),
		dimension.get_matrix()) for dimension in dimensions]

	pickle.dump(result_LSVC, open('pickles/result_LSVC.pkl', 'wb'), -1)
	pickle.dump(result_MNB, open('pickles/result_MNB.pkl', 'wb'), -1)

	pickle.dump(methods, open('pickles/methods.pkl', 'wb'), -1)",Scikit/Summer/matrix.py,AthenaSTM/ExperimentalMLScripts,1
"    """"""
    returns the trained text classifier 

    :param source:      list of comments
    :param targets:     list of comment categories related to the comments in source
    :param stop_words:  list of stop words 
    """"""

    text_clf = Pipeline([('vect', CountVectorizer(stop_words=stop_words, max_df=1.0, min_df=2)), 
                        ('tfidf', TfidfTransformer()),
                        ('clf', SVC(decision_function_shape='ovr', kernel='linear'))])

    text_clf.fit(source, targets)

    return text_clf


def runExperiment(source, stop_words, groups, n_validation=10):
    """"""
    Split the samples parsed from source file into train set and test set randomly.",project/classification.py,postalC/python-basic,1
"        # in danger (near the boundary). The level of extrapolation is
        # controled by the out_step.#
        if kind == 'svm':
            # As usual, use scikit-learn object#
            from sklearn.svm import SVC

            # Store extrapolation size#
            self.out_step = out_step

            # Store SVM object with any parameters#
            self.svm_ = SVC(**kwargs)

    def resample(self):
        """"""
        Main method of all children classes.

        :return: Over-sampled data set.
        """"""

        # Start by separating minority class features and target values.",unbalanced_dataset/over_sampling.py,yangspeaking/UnbalancedDataset,1
"@param.float(""Gamma-Exp"", interval=[-6, 0], step=0.1)
def f(C_exp, gamma_exp):
    iris = datasets.load_iris()

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        iris.data, iris.target, test_size=0.4, random_state=0)

    C = 10 ** C_exp
    gamma = 10 ** gamma_exp

    clf = svm.SVC(C=C, gamma=gamma)

    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)


def main():
    from metaopt.core.optimize.optimize import optimize
    from metaopt.optimizer.rechenberg import RechenbergOptimizer",examples/showcase/svm_rechenberg_global_timeout.py,cigroup-ol/metaopt,1
"    features.append(arr)

features = np.array(features)
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer()
features = transformer.fit_transform(features).toarray()

labels, features, names = shuffle(labels, features, names)
#clf = LogisticRegression()
#clf = MultinomialNB()
#clf = sklearn.svm.SVC(C=0.001, gamma =10000)
clf = sklearn.svm.LinearSVC(C=10)

from sklearn.metrics import precision_score, accuracy_score, recall_score


accu = []
prec = []
reca = []
cv = cross_validation.StratifiedKFold(labels, n_folds=10)",learning/old/cp_analysis.py,fcchou/CS229-project,1
"mon_con = MongoClient('localhost', 27017) # NOTE: change to localhost when running on server
mon_col = mon_con['hillary']['emails']

### load in training data
movie_reviews_data_folder = 'txt_sentoken'
rev_dataset = load_files(movie_reviews_data_folder, shuffle=False)

# split the movie reviews dataset into a train and test set
rev_X_train, rev_X_test, rev_y_train, rev_y_test = train_test_split(rev_dataset.data, rev_dataset.target, test_size=0.25, random_state=None)

pipeline = Pipeline( [ ('vect', StemmedTfidfVectorizer(min_df=1, stop_words = 'english')), ('clf', LinearSVC(C=1000)) ] )

# TASK: Build a grid search to find out whether unigrams or bigrams are
# more useful.
# Fit the pipeline on the training set using grid search for the parameters
parameters = { 'vect__ngram_range': [(1, 1), (1, 2)] }
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)
grid_search.fit(rev_X_train, rev_y_train)

# TASK: print the cross-validated scores for the each parameters set",classification.py,RakshakTalwar/HillaryEmails,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_15_2014_server3.py,magic2du/contact_matrix,1
"            semanticFeatures.append(numpy.array(temp))
            classes.append(labels[index])

        if featureMode == 0:
            features = csr_matrix(numpy.array(semanticFeatures))
        elif featureMode == 1:
            features = vectorMatrix
        else:
            features = hstack((vectorMatrix, csr_matrix(numpy.array(semanticFeatures))), format='csr')

        model = svm.SVC(probability=True)
        model.fit(features, classes)

        joblib.dump(model, 'adData/analysis/groups/' + groupTitle + '/model/svm' + str(group) + '_'+str(vectorMode)+'_'+str(featureMode)+'.pkl')

def infer(modelTitle, modelGroup, vectorMode, featureMode, inferData, inferParseLength, inferHeadCount):
    print 'loading...'
    # load model files
    mentionMapper = mapMention('adData/analysis/ranked/mention.json')
    model = joblib.load('adData/analysis/groups/' + modelTitle + '/model/svm' + str(modelGroup) + '_'+str(vectorMode)+'_'+str(featureMode)+'.pkl')",modelUtility.py,renhaocui/adPlatform,1
"    values = values.split("","")
    vec = [float(qt_rmvd(x)) for x in values[:-1]]
    c = int(qt_rmvd(values[-1])) 
    vecList.append(vec)
    classList.append(c)
#print c,'\n',vecList
vecList = np.asarray(vecList)
classList = np.asarray(classList)
min_max_scaler = preprocessing.MinMaxScaler()
vecList = min_max_scaler.fit_transform(vecList)
clf = svm.SVC(kernel='rbf', C = 1)
clf.fit(vecList,classList)
l = joblib.dump(clf, 'Schlumberger-SVM.pkl')
'''s = p.dumps(clf)
#print s
f = open(""svm.model"",""w"")
f.write(s)
f.close()'''
#print commands.getoutput(""ls"")
print commands.getoutput(""hadoop fs -rm  /user/dropuser/schlumberger-model/*"")",enu/real_time_event_detection/hadoopstream/reducer_train.py,WiproOpenSourcePractice/bdreappstore,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC
from sklearn.cross_validation import ShuffleSplit
from mne.decoding import CSP

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,christianbrodbeck/mne-python,1
"



def main():

    # Supported classifier models
    n_neighbors = 3
    models = {
        'nb' : naive_bayes.GaussianNB(),
        'svm-l' : svm.SVC(),
        'svm-nl' : svm.NuSVC(),
        'tree' : tree.DecisionTreeClassifier(),
        'forest': AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"",n_estimators=200),
        'knn-uniform' : neighbors.KNeighborsClassifier(n_neighbors, weights='uniform'),
        'knn-distance' : neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
    }

    userInput = getUserInput(models)
    appDf = loadAppData(userInput['file'])",classifier.py,seekshreyas/classifier-scaffold,1
"        from sklearn import cross_validation
        from sklearn import datasets
        from sklearn import svm
        
    #load dataset only once
    global iris
    if not iris:
        iris = datasets.load_iris()
        
    #run experiment
    clf = svm.SVC(kernel='rbf', C=float(parameters))
    scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)
    s = ""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() / 2)
    tmp_file = ""/tmp/"" + parameters + "".txt""
    write_to_file(tmp_file,s)    
    cp(""file://"" + tmp_file, ""s3n://dl-stl-ml-awsdev/accuracies/"" + parameters + "".txt"")",examples/scikit_script.py,adobe-research/mississippi,1
"    Yt = data[:,-1]
    return X, Y, Xt, Yt

 

def plotting(X, Y, Xt, Yt, labelx, labely, outputfile):
    h = .02  # step size in the mesh
    classifiers = dict(
    knn=neighbors.KNeighborsClassifier(4),
    logistic=linear_model.LogisticRegression(C=1e5),
    svm=svm.SVC(C=1e5),
    adaboost=ensemble.AdaBoostClassifier(),
    naivebay=naive_bayes.GaussianNB())

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
    
    fignum = 1
    # we create an instance of Neighbours Classifier and fit the data.
    for name, clf in classifiers.iteritems():",MLNet-2.0/plotting/lr-knn-svm-comparison/plot_with_out_no_zero.py,bt3gl/MLNet-Classifying-Complex-Networks,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",nltk-3.0.3/nltk/parse/transitionparser.py,zimmermegan/MARDA,1
"        if isinstance(X, spmatrix):
            indices = np.arange(len(r))
            r_sparse = coo_matrix(
                (r, (indices, indices)),
                shape=(len(r), len(r))
            )
            X_scaled = X * r_sparse
        else:
            X_scaled = X * r

        lsvc = LinearSVC(
            C=self.C,
            fit_intercept=self.fit_intercept,
            max_iter=10000
        ).fit(X_scaled, y)

        mean_mag =  np.abs(lsvc.coef_).mean()
        coef_ = (1 - self.beta) * mean_mag * r + self.beta * (r * lsvc.coef_)
        intercept_ = (1 - self.beta) * mean_mag * b + self.beta * lsvc.intercept_
",fsa/dev/optimizer.py,bobflagg/sentiment-analysis,1
"    """"""
    Custom kernel to pass to sklearn svm.SVC via its kernel parameter
    
    x1 : matrix, a collection of data points, shape = [n_samples, n_features]
    x2 : matrix, a collection of data points, shape = [n_samples, n_features]
    """"""
    return np.dot(map_func(x1), map_func(x2).T)


# fit SVM with custom non-linear kernel
model = svm.SVC(kernel=my_kernel, C=1000, tol=1e-3)
model.fit(X,Y)

# coef : dual coefficients alpha_i*y_i per support vector in decision function
coef = model.dual_coef_[0]
# intercept in decision function
b = model.intercept_

# bug in sklearn versions before 0.16: https://github.com/scikit-learn/scikit-learn/issues/4262
# The first class to appear in the dataset, i.e. Y[0], is mapped to the class +1, even if it is labeled -1",svm.py,EFavDB/svm-classification,1
"# cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100,
#                                    test_size=0.2, random_state=0)

# estimator = GaussianNB()
# plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

# title = ""Learning Curves (SVM, RBF kernel, $\gamma=0.001$)""
# # SVC is more expensive so we do a lower number of CV iterations:
# cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,
#                                    test_size=0.2, random_state=0)
# estimator = SVC(gamma=0.001)
# plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

# plt.show()",v1/plot_learning_curve.py,JVP3122/Python-Machine-Learning-NFL-Game-Predictor,1
"                                    n_features=200000,
                                    n_informative=2,
                                    random_state=1)

Xy = dict(X=X, y=y)
## 2) Building workflow
## =======================================================
print "" -> Pt2 : X and y created, building workflow""
from sklearn import svm, cross_validation
#kfold = cross_validation.KFold(n=len(X), n_folds=3)
#svc = svm.SVC(C=1, kernel='linear')
#print [svc.fit(X[train], y[train]).score(X[test], y[test]) for train, test in kfold]
from epac import CV, Methods
cv_svm_local = CV(Methods(*[svm.SVC(kernel=""linear""),
                            svm.SVC(kernel=""rbf"")]),
                  n_folds=3)
print "" -> Pt3 : Workflow built, defining local engine""
cv_svm = None
n_proc = 2
# Running on the local machine",test/bug_joblib/test_bug_200000ft.py,neurospin/pylearn-epac,1
"	
	#classification
	#train_x,test_x,train_y,test_y=train_test_split(features,categories,test_size=0.25,random_state=42)
	#print train_x
	#test_x=np.array(test_x)
	#test_y=np.array(test_y)
	#print train_x[0]
	#print (train_y6
	#print(str(len(x_train))+'\t'+str(len(x_test))+'\t'+str(len(y_train))+'\t'+str(len(y_test)))
	#clf=OneVsRestClassifier(svm.SVC(kernel='rbf',gamma=0.01,C=100,max_iter=-1,class_weight='balanced'))
	clf=OneVsRestClassifier(svm.SVC(kernel='linear',gamma=0.001,C=100,max_iter=-1))
	#clf=svm.SVR(
	#clf=svm.LinearSVC(max_iter=-1,class_weight='balanced')
	#clf=OneVsRestClassifier(RandomForestClassifier(n_estimators=450,class_weight='balanced',min_samples_split=4))
	#clf=ExtraTreesClassifier(n_estimators=30)
	#clf.fit(train_x,train_y)
	scores=cross_val_score(clf,features,categories,cv=5)
	#clf.decision_function(train_x)
	#prediction=clf.predict(test_x)
	#print prediction",SVM.py,Yiangos01/ADE2017,1
"match_func = lambda user : int(user.match >= MATCH_THRESHOLD)
gender_func = lambda user : user.gender



class L1LinearSVC(LinearSVC):

    def fit(self, X, y):
        # The smaller C, the stronger the regularization.
        # The more regularization, the more sparsity.
        self.transformer_ = LinearSVC(penalty=""l1"",
                                      dual=False, tol=1e-3)
        X = self.transformer_.fit_transform(X, y)
        return LinearSVC.fit(self, X, y)

    def predict(self, X):
        X = self.transformer_.transform(X)
        return LinearSVC.predict(self, X)

",okclassify.py,ned2/okdata,1
"


class Classifier:
	
    results = []

    def getLearn(self, option):

	if option == 'svm':
	    results.append(evaluation(SVC(C=1)))

	elif option == 'multinomial':
	    results.append(evaluation(MultinomialNB(alpha=.01)))

	elif option == 'bernouli':
	    results.append(evaluation(BernoulliNB(alpha=.01)))

    def evaluation(clf):
    	print('_' * 80)",src/Classifier.py,yr3var14/ata,1
"bins_5 = np.array([0,0.2,0.4,0.6,0.8,1])
bins_4 = np.array([0,0.25,0.5,0.75,1])
bins_3 = np.array([0,0.33,0.66,1])

# here we use bins_5
y = np.digitize(y, bins_5)

n = len(y)

# SVM/SVC model
clf = SVC()

# use 5-fold cross-validation to test the model accuracy
kf = KFold(n, n_folds=5, shuffle=True)
scores = []
for train, test in kf:
	X_train = [X[i] for i in train]
	y_train = [y[i] for i in train]

	clf.fit(X_train,y_train)",train.py,jwheatp/eiffelometre,1
"        Callable object that returns a scalar score; greater is better.

    Examples
    --------
    >>> from sklearn.metrics import fbeta_score, make_scorer
    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
    >>> ftwo_scorer
    make_scorer(fbeta_score, beta=2)
    >>> from sklearn.grid_search import GridSearchCV
    >>> from sklearn.svm import LinearSVC
    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
    ...                     scoring=ftwo_scorer)
    """"""
    sign = 1 if greater_is_better else -1
    if needs_proba and needs_threshold:
        raise ValueError(""Set either needs_proba or needs_threshold to True,""
                         "" but not both."")
    if needs_proba:
        cls = _ProbaScorer
    elif needs_threshold:",sklearn/metrics/scorer.py,B3AU/waveTree,1
"
                # calculate ttest for each of the continuous varaibles you would like to control for
                
               # if all of the control variables are insignificant p > 0.5 and haven't previously generated that split:
                #    c=c+1
                 
                
        #    return splits
        
        # create the classifier that we intend to use
        svcClassifier = svm.LinearSVC(C=100.0)
        splits = splitHalf(imgLabels,10)
        
        # determine
        nprs=NPAIRS(dataAry, imgLabels,svcClassifier,splits)
        (pred,rep)=nprs.run()
        
        pass

",pyNPAIRS/tests/test_npairs.py,ccraddock/pyNPAIRS,1
"    return np.median(minDists)

median = calcMedian(data, classes)
b = 10
C = np.array([b**(-3), b**(-2), b**(-1), 1, b, b**2, b**3], float)
jaak = 1/(2*median**2)
print ""Gamma value suggested by the Jaakkola Heuristic:""
print jaak
gamma = jaak * C
parameters = [{'gamma': gamma, 'C': C, 'kernel': ['rbf']}]
svr = svm.SVC(kernel='rbf')
clf = grid_search.GridSearchCV(svr, parameters, cv=5)
clf.fit(data, classes)
print ""Best parameters:""
print clf.best_params_
print ""Success rate for training data:""
print clf.score(data, classes)
print ""Success rate for test data:""
print clf.score(testData, testClasses)",ml/exam/q2.py,Rathcke/uni,1
"
def sk_hinge_losses(clf, X, Y):
    decs = clf.decision_function(X)
    margin = Y * decs[:, 0]
    hlosses = 1 - margin
    hlosses[hlosses <= 0] = 0
    return hlosses


def sk_weightedCSVMrbf(Xain, Yain, Xst, Yst, alpha, C, gamma, n_init=10):
    clf = SVC(C=C, kernel='rbf', gamma=gamma)
    n_train_samples, n_dims = Xain.shape
    w = ones(n_train_samples) / n_train_samples
    #clf.fit(Xain, Yain)
    clf.fit(Xain, Yain, sample_weight=w * C + 0.0000000001)
    for _ in range(n_init):
        test_hlosses = sk_hinge_losses(clf, Xst, Yst)
        mistakeRatio = zero_one_score(Yst, clf.predict(Xst))
        print ""mean loss on test data: %s %% correct on test data: %s"" % (test_hlosses.mean(), mistakeRatio)
        train_hlosses = sk_hinge_losses(clf, Xain, Yain)",src/simpleInterface.py,daniel-vainsencher/regularized_weighting,1
"import numpy as np
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from scorer import score


def svc(X_train, y_train):
    #clf = GridSearchCV(SVC(), scoring=make_scorer(score))
    clf = SVC()
    clf.fit(X_train, y_train)
    print ""The best classifier is: "", clf.best_estimator_
    return clf.best_estimator_


def gnb(X_train, y_train):
    clf = GaussianNB()
    clf.fit(X_train, y_train)",anaphora/learning/learn.py,OpenCorpora/opencorpora,1
"y_test = y[nighty_precent_of_sample:]

# create a list of the types of kerneks we will use for your analysis
types_of_kernels = ['linear', 'rbf', 'poly']

# specify our color map for plotting the results
color_map = plt.cm.RdBu_r

# fit the model
for fig_num, kernel in enumerate(types_of_kernels):
    clf = svm.SVC(kernel=kernel, gamma=3)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=color_map)

    # circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
    
    plt.axis('tight')",SVM.py,Sapphirine/Predict-Survival-on-the-Titanic,1
"#single_classifier = svm.SVC(kernel='linear', cache_size = 2048)

#single_classifier = neural_network.MLPClassifier(hidden_layer_sizes=10 * (100,))

single_classifier = svm.LinearSVC()

#single_classifier = naive_bayes.GaussianNB()
#single_classifier = naive_bayes.MultinomialNB()
#single_classifier = naive_bayes.BernoulliNB()

#single_svc = svm.SVC(kernel='rbf', cache_size = 2048)

single_classifier = make_pipeline(TfidfTransformer(), single_classifier)
#single_classifier = make_pipeline(preprocessing.StandardScaler(), single_classifier)
#single_classifier = make_pipeline(SelectFromModel(ExtraTreesClassifier(), prefit = False), TfidfTransformer(), single_classifier)

#classifier = OneVsRestClassifier(make_pipeline(preprocessing.StandardScaler(),single_classifier))

classifier = OneVsRestClassifier(single_classifier)
",laws/tags/tags_autolearn_play.py,otadmor/Open-Knesset,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = decision_tree_backward.decision_tree_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeast/example/test_decision_tree_backward.py,jundongl/scikit-feast,1
"                #self.clf = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True, priors=self.priors)
            elif self.classifier == 'qda':
                self.clf = QuadraticDiscriminantAnalysis(priors=self.priors)
            elif self.classifier == 'rf':
                # the gala parameters
                #self.clf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=20,
                #    bootstrap=False, random_state=None)
                #self.clf = RandomForestClassifier(n_estimators=5*nfeatures,n_jobs=self.nthreads,max_depth=10)
                self.clf = RandomForestClassifier(n_estimators=256,n_jobs=self.nthreads,max_depth=16)
            elif self.classifier == 'svm':
                self.clf = SVC(kernel='rbf',probability=True,cache_size=2000)
            elif self.classifier == 'nb':
                self.clf = GaussianNB()
            elif self.classifier == 'kn':
                self.clf = KNeighborsClassifier(n_neighbors=10,n_jobs=self.nthreads)
            elif self.classifier == 'dc':
                self.clf = DecisionTreeClassifier(max_depth=10)
            elif self.classifier == 'ada':
                self.clf = AdaBoostClassifier()
            elif self.classifier == 'lr':",recon/python/dpSupervoxelClassifier.py,elhuhdron/emdrp,1
"
train1_labs, train1_times = sn.make_label_by_time(train_samp1)
test1_labs, test1_times = sn.make_label_by_time(test_samp1)

on_off1_train = sn.on_off_course(GUMP_SCENES_IDS, train1_labs)
on_off1_test = sn.on_off_course(GUMP_SCENES_IDS, test1_labs)

subarr1_train = combined_runs[:,train1_times].T #rows correspond to images, colums to voxels 
subarr1_test = combined_runs[:,test1_times].T #data we feed into our classifier 

clf = svm.SVC(C=100, kernel='linear') #Paramters obtained through cross-validation
clf.fit(subarr1_train, on_off1_train)
pred_svm1 = clf.predict(subarr1_test)
accuracy_score(on_off1_test, pred_svm1) #52%

knn = KNeighborsClassifier()
knn.fit(subarr1_train, on_off1_train)
pred_knn1 = knn.predict(subarr1_test)
accuracy_score(on_off1_test, pred_knn1) #69%
",code/scenes_pred.py,rishizsinha/project-beta,1
"model10 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=6, earlystopping=True, patience=20, filtersize=3, epochs=200)
model11 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=128, earlystopping=True, patience=20, filtersize=3, epochs=200)
model12 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=150, earlystopping=True, patience=20, filtersize=3, epochs=200)

model_list = [model1, model2, model3, model4,
              model5, model6, model7, model8,    
              model9, model10, model11, model12,]
                 
#==== Second Level: Blending Models ====================
rf = RandomForestClassifier(n_estimators = 200)
svm0 = svm.SVC(decision_function_shape='ovo', probability=True)
trees = ExtraTreesClassifier(max_depth=3, n_estimators=200, random_state=0)
sgd = SGDClassifier(loss=""modified_huber"", penalty=""l2"")
booster = AdaBoostClassifier(n_estimators=200)

meta_classifiers = [rf, svm0, trees, sgd,  booster]
#======================

X_train, y_train, X_test, y_test = utils.train_test_splitter(X, y, test_size=.25)
",Examples/EnsembleDemo.py,jacobzweig/RCNN_Toolbox,1
"
### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()
# features_train = features_train[:len(features_train)/100]
#labels_train = labels_train[:len(labels_train)/100]

from sklearn.svm import SVC

clf = SVC(kernel=""rbf"", C=10000.0)
t0 = time.time()
clf.fit(features_train, labels_train)
print ""training time:"", round(time.time() - t0, 3), ""s""
#training time: 178.676 s


t0 = time.time()
pred = clf.predict(features_test)
print ""predict time:"", round(time.time() - t0, 3), ""s""",svm/svm_author_id.py,filipenevola/learning-ml,1
"from sklearn import svm
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
import numpy as np

def train(x_train, y_train, C):
  clf = LogisticRegression(C=C)
  #clf = svm.LinearSVC(C=C)
  clf.fit(x_train, y_train)
  return clf


def scoring_fxn(classifier, x, actual_y, cv = False):
  if cv:
    pred_y = cross_val_predict(classifier, x, actual_y, cv=5)
  else:
    pred_y = classifier.predict(x)",scripts/run_prediction_analysis.py,jsedoc/social-media-s-v-o-clusters,1
"#        for l in f.readlines():
#            data.append(l.strip().split())
#            target.append(i)
#
#    return np.array(data), np.array(target)


def train_network(data, target):
    #print ""Training started""
    #data, target = sample2train()
    #clf = svm.SVC()

    # dimension reduction
    pca = decomposition.RandomizedPCA(n_components=21, whiten=True)
    pca.fit(data)
    data_pca = pca.transform(data)

    clf = svm.SVC(kernel=""rbf"", C=1000000.0, gamma=0.00000001)
    #clf = svm.NuSVC(kernel=""rbf"")
    #clf = svm.SVR(C=1.0, epsilon=0.2)",sample2train.py,crosvera/arboris,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the gini_index score of each feature
        score = gini_index.gini_index(X[train], y[train])

        # rank features in descending order according to score
        idx = gini_index.feature_ranking(score)
",skfeature/example/test_gini_index.py,jundongl/scikit-feature,1
"
dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})
	]",scripts/05-histogram-normalised-techniques-search.py,jmrozanec/white-bkg-classification,1
"        img = cv2.resize(img, spatial_size)
        feature_vector, _ = extract_feature_from_image(img, spatial_size, hist_bins, hist_range, hog_feature_vec=True)
        features.append(feature_vector)
        labels.append(label)
    return np.array(features), np.array(labels)

def train_model(features, labels):
    logger.info(""Training Model"")
    features, labels = shuffle(features, labels)
    X_train, X_test, y_train, y_test = train_test_split(features, labels, )
    clf = svm.SVC(verbose=True)
    decision_tree_clf = tree.DecisionTreeClassifier()
    t = time.time()
    model = clf.fit(X_train, y_train)
    t2 = time.time()
    print(round(t2 - t, 2), 'Seconds to train SVC...')
    # Check the score of the SVC
    print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))

    t = time.time()",vehicle_detection/vehicle_detection.py,ssarangi/self_driving_cars,1
"                training_X = np.column_stack( tuple( np.ravel( j[ np.logical_and(prior.data>0 , trainmask.data>0 ) ] ) for j in images  ) )
                training_Y = np.ravel( prior.data[ np.logical_and(prior.data>0 , trainmask.data>0 ) ] )
            else:
                training_X = np.column_stack( tuple( np.ravel( j[ prior.data>0 ] ) for j in images  ) )
                training_Y = np.ravel( prior.data[ prior.data>0 ] )
        
        
            if options.debug: print(""Fitting..."")
        
            if options.method==""SVM"":
                clf = svm.SVC()
            elif options.method==""nuSVM"":
                clf = svm.NuSVC()
            elif options.method=='NN':
                clf = neighbors.KNeighborsClassifier(options.n)
            elif options.method=='RanForest':
                clf = ensemble.RandomForestClassifier(n_estimators=options.n,random_state=options.random)
            elif options.method=='AdaBoost':
                clf = ensemble.AdaBoostClassifier(n_estimators=options.n,random_state=options.random)
            elif options.method=='tree':",examples/image_classify.py,BIC-MNI/pyezminc,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Trog Build Dependencies/Python26/Lib/email/test/test_email.py,DecipherOne/Troglodyte,1
"                LogisticRegression():
            { 
                'penalty': ['l2'],
                'C' : [.01, 0.1, 1, 10],
                'random_state' : [1]
            },
                KNeighborsClassifier():
            {
                'n_neighbors' : [2, 5, 10, 50]
            },
                SVC():
            {
                'C': [.01, 0.1, 1, 10],
                'kernel': ['linear', 'rbf', 'poly'],
                'degree' : [2,3],
                'gamma' : [0, 0.01, 0.1, 0.5],
                'random_state' : [1]
            },
                GradientBoostingClassifier():
            {",models.py,joseburaschi/QEDA,1
")
X = newsgroups.data
y = newsgroups.target

vectorized = TfidfVectorizer(stop_words='english')
my_features = vectorized.fit_transform(X, y)


grid = {'C': np.power(10.0, np.arange(-5, 6))}
cv = KFold(y.size, n_folds=5, shuffle=True, random_state=241)
clf = SVC(kernel='linear', random_state=241)
gs = grid_search.GridSearchCV(clf, grid, scoring='accuracy', cv=cv)
gs.fit(my_features, y)",data science/Assessment 6/2.py,sergiy-evision/math-algorithms,1
"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB

import collections

import asl

training_data, test_data, training_target, test_target = train_test_split(asl.data, asl.target, test_size=0.5, random_state=0)

classifiers = {
        'SVCP': svm.SVC(gamma=0.001, C=10),
        'SVCR': svm.SVC(gamma=0.0001, C=50),
        'NB ': GaussianNB(),
        'BNB': BernoulliNB(),
        'NBU': neighbors.KNeighborsClassifier(5, weights='uniform'),
        'NBD': neighbors.KNeighborsClassifier(5, weights='distance'),
        'TRE': tree.DecisionTreeClassifier(),
        'GBC': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),
        'RFC': RandomForestClassifier()
    }",score_classifiers.py,ssaamm/sign-language-translator,1
"    evaluate_and_print_scores(X_train_featsel, y_train, X_test_featsel, y_test, goal_ind, C=l2_c)
    #del X_train_featsel, X_test_featsel

    print(""Remove features useful for telling domains apart"")
    ## Create dataset for training a classifier to distinguish between domains:
    X_all = np.zeros((num_instances+num_test_instances, num_feats))
    X_all[:num_instances,:] += X_train
    X_all[num_instances:,:] += X_test
    y_dataset_discrim = np.zeros(num_instances+num_test_instances)
    y_dataset_discrim[:num_instances] = 1
    svc = svm.LinearSVC()
    svc.fit(X_all, y_dataset_discrim)
    train_error = 1 - svc.score(X_all, y_dataset_discrim)
    print(""Train error in trying to differentiate these datasets is %f"" % train_error)

    (chi2_dd, pval_dd) = chi2(X_all, y_dataset_discrim)
    dd_feats_inds = np.where(pval_dd > 0.05)[0]
    X_train_domains = zero_nonpivot_columns(X_train, dd_feats_inds)
    X_test_domains = zero_nonpivot_columns(X_test, dd_feats_inds)
    (l2_c, l2_f1) = find_best_c(X_train_domains, y_train, goal_ind)",scripts/transform_features.py,tmills/uda,1
"		""""""
		self.C = param

	def tuneParameter(self, parameters):
		""""""
		This function is used for tuning the parameter of SVM.

		example:
		parameters = {'kernel':('linear', 'poly','rbf'), 'C':[1, 10]}
		""""""
		self.clf = grid_search.GridSearchCV(svm.SVC(), parameters)

	def buildModel(self):
		""""""
		This builds the model of the classifier
		""""""
		self.clf =  svm.SVC()

	def trainSVM(self,X, Y):
		""""""",classifiers/supportVectorMachine.py,USCDataScience/NN-fileTypeDetection,1
"y_test=np.array(Y_test)

##==============================================================================
## testing cycle
##==============================================================================

for c in [1, 10, 100, 1000]:
    for g in [1, 10, 100]:
        s = 'SVC linear c: '+str(c)+' gamma: '+str(g)
        # computing kernel
        svc = svm.SVC(kernel='linear', C=c, gamma=g).fit(X, y)
        acc = TestAccuracy(svc, X_test, y_test)
        s = s + ' accuracy: '+str(acc)
        if f:
            print(s, end=""\n"", file=f) 
            f.flush()
            os.fsync(f.fileno())
        print (s)
        CreateFigure(svc, X,Y, s, mapcolors, folder)
",SupportVectorMachines_classifiers.py,patriziobellan86/tf_pos_tagging,1
"
	clf = svm.SVR()
	#parmas = {'alpha': np.logspace(1, -1, 9)}
	kf5 = cross_validation.KFold( xM.shape[0], n_folds=n_folds, shuffle=True)
	gs = grid_search.GridSearchCV( clf, svr_params, scoring = 'r2', cv = kf5, n_jobs = -1)

	gs.fit( xM, yV.A1)

	return gs

def gs_SVC( xM, yVc, params):
	""""""
	Since classification is considered, we use yVc which includes digital values 
	whereas yV can include float point values.
	""""""

	print xM.shape, yVc.shape

	clf = svm.SVC()
	#parmas = {'alpha': np.logspace(1, -1, 9)}",jgrid.py,jskDr/jamespy,1
"		'nsynapses': 392,
		'seg_th': 10,
		
		'syn_th': 0.5,
		'pinc': 0.001,
		'pdec': 0.002,
		'pwindow': 0.01,
		'random_permanence': True,
		
		'nepochs': 10,
		'clf': LinearSVC(random_state=seed),
		'log_dir': os.path.join('simple_mnist', '1-1')
	}
	
	# Seed numpy
	np.random.seed(seed)
	
	# Get the data
	(tr_x, tr_y), (te_x, te_y) = load_mnist()
	x, y = np.vstack((tr_x, te_x)), np.hstack((tr_y, te_y))",src/examples/mnist_simple.py,tehtechguy/mHTM,1
"class EvalKnn(object):
    """"""docstring for EvalTree""""""
    b_preict = []
    def __init__(self):
        super(EvalKnn, self).__init__()


  

    def init_classifier(self):
        # clf = svm.SVC(kernel = 'rbf', gamma=self.gamma_value, C=self.c_value)
        # print ""SVM configuration... \n\n"", clf
        clf = KNeighborsClassifier(n_neighbors=3)
        return clf



    def fit_train_data(self, clf, a_train, b_train):
        # clf = svm.SVC(kernel = 'rbf', gamma=gamma_value, C=c_value)
        # print ""SVM configuration... \n\n"", clf",implementation/evaluation/knn.py,imink/UCL_COMPIG15_Project,1
"def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)
    clf.fit(X, y_regr)

    clf = AdaBoostRegressor(SVR(), random_state=0)",projects/scikit-learn-master/sklearn/ensemble/tests/test_weight_boosting.py,DailyActie/Surrogate-Model,1
"
n_samples = len(asl.data)
X = asl.data
y = asl.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 0)

tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.001, 0.00055, 0.0001], 'C': [10, 50, 100]}]

for score in ['precision', 'recall']:
	clf = GridSearchCV(SVC(C = 1), tuned_parameters, cv = 5, scoring = '%s_weighted' %score)
	clf.fit(X_train, y_train)",find_params.py,paolo-torres/Sign-Language-Translator,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.lda import LDA
from sklearn.qda import QDA

np.set_printoptions(threshold=sys.maxint)


names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
	KNeighborsClassifier(3),
	SVC(kernel=""linear"", C=0.025),
	SVC(gamma=2, C=1),
	DecisionTreeClassifier(max_depth=5),
	RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
	AdaBoostClassifier(),
	GaussianNB(),
	LDA(),
	QDA()
	]
",scikit_code/classifier_with_multi_components.py,chakpongchung/RIPS_2014_BGI_source_code,1
"

def KNN(X_train, y_train, X_test, y_test):
    clf = neighbors.KNeighborsClassifier()
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test, y_test)
    return accuracy


def SVMClass(X_train, y_train, X_test, y_test):
    clf = LinearSVC()
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test, y_test)
    return accuracy


def QuadDA(X_train, y_train, X_test, y_test):
    clf = QDA()
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test, y_test)",ML_based/machineLearning.py,Tingguo/stock,1
"rng = check_random_state(42)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]


def test_libsvm_parameters():
    """"""
    Test parameters on classes that make use of libsvm.
    """"""
    clf = svm.SVC(kernel='linear').fit(X, Y)
    assert_array_equal(clf.dual_coef_, [[0.25, -.25]])
    assert_array_equal(clf.support_, [1, 3])
    assert_array_equal(clf.support_vectors_, (X[1], X[3]))
    assert_array_equal(clf.intercept_, [0.])
    assert_array_equal(clf.predict(X), Y)


def test_libsvm_iris():
    """"""",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/svm/tests/test_svm.py,RPGOne/Skynet,1
"
		print 'adding training vectors'
		for p in train_keys:
			X.append(MatrixUtils.get_reduced_vector(p_ppr[p], center))
			if p.find('.male.') > 0:
				y.append(1)
			else:
				y.append(0)
			
		### classifiersV
		#clf = svm.SVC(kernel='linear')
		#clf = svm.SVC(kernel='rbf', C=10000, gamma=0.1)
		#clf = svm.SVC(kernel='rbf', C=10, gamma=10)    
		clf = AdaBoostClassifier(DecisionTreeClassifier(criterion='gini', max_depth=conf['classification']['dtree_depth'], max_features=None, min_density=None, min_samples_leaf=1, min_samples_split=2), algorithm=""SAMME"", n_estimators=conf['classification']['ada_n_estimators'])
		
		print 'training classifier'
		clf.fit(X, y)  

		print 'testing'
		counter = 0.",train_and_test_binary_dim.py,jimbotonic/df_nlp,1
"    # test fit and transform:
    hasher = RandomTreesEmbedding(n_estimators=30, random_state=1)
    assert_array_equal(hasher.fit(X).transform(X).toarray(),
                       X_transformed.toarray())

    # one leaf active per data point per forest
    assert_equal(X_transformed.shape[0], X.shape[0])
    assert_array_equal(X_transformed.sum(axis=1), hasher.n_estimators)
    svd = TruncatedSVD(n_components=2)
    X_reduced = svd.fit_transform(X_transformed)
    linear_clf = LinearSVC()
    linear_clf.fit(X_reduced, y)
    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)",venv/lib/python2.7/site-packages/sklearn/ensemble/tests/test_forest.py,chaluemwut/fbserver,1
"	cross_edge_len, cross_nonedge_len = len(edges) - train_edge_len, len(nonedges) - train_nonedge_len

	X_train = normalize(nonedges[:train_nonedge_len] + 
						edges[:train_edge_len])
	y_train = [0] * train_nonedge_len + [1] * train_edge_len

	X_cross = normalize(nonedges[train_nonedge_len:] + 
						edges[train_edge_len:])
	y_cross = [0] * cross_nonedge_len + [1] * cross_edge_len

	clf = svm.SVC(gamma=.001, C=100.)
	clf.fit(X_train, y_train)
	print(""prediction: {}"".format(list(clf.predict(X_cross))))
	print(""actuallity: {}"".format(y_cross))
	print(clf.score(X_cross, y_cross))

def get_column(img, i):
	w, h = img.size
	column = []
	for j in range(h):",comic_classify/comic_detect.py,docileninja/Calvin-and-Hobbes-Viewer,1
"    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    
    sc=[]
    for train_index, test_index in kf:
        #print(""TRAIN:"", train_index, ""TEST:"", test_index)
    
        #generated train and test lists, incuding indices of the examples in training/test
        #for the specific fold. Indices starts from 0 now
        
        clf = svm.SVC(C=c, kernel='precomputed')
        train_gram = [] #[[] for x in xrange(0,len(train))]
        test_gram = []# [[] for x in xrange(0,len(test))]
        #compute training and test sub-matrices
        index=-1    
        for row in gram:
            index+=1
            if index in train_index:
                train_gram.append([gram[index,i] for i in train_index])
            else:",scripts/cross_validation_ICML16.py,nickgentoo/scikit-learn-graph,1
"perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
# sparsify
iris.data = sparse.csr_matrix(iris.data)


def test_svc():
    """"""Check that sparse SVC gives the same result as SVC""""""

    clf = svm.SVC(kernel='linear').fit(X, Y)
    sp_clf = svm.SVC(kernel='linear').fit(X_sp, Y)

    assert_array_equal(sp_clf.predict(T), true_result)

    assert_true(sparse.issparse(sp_clf.support_vectors_))
    assert_array_almost_equal(clf.support_vectors_,
            sp_clf.support_vectors_.todense())

    assert_true(sparse.issparse(sp_clf.dual_coef_))",venv/lib/python2.7/site-packages/sklearn/svm/tests/test_sparse.py,devs1991/test_edx_docmode,1
"
        # --- SVM smote
        # Unlike the borderline variations, the SVM variation uses the support
        # vectors to decide which samples are in danger (near the boundary).
        # Additionally it also introduces extrapolation for samples that are
        # considered safe (far from boundary) and interpolation for samples
        # in danger (near the boundary). The level of extrapolation is
        # controled by the out_step.
        if self.kind == 'svm':
            # Store SVM object with any parameters
            self.svm = SVC(random_state=self.random_state, **self.kwargs)",imblearn/over_sampling/smote.py,fmfn/UnbalancedDataset,1
"        
        img = Image.open(""data/zero.bmp"")
        img = img.resize(STANDARD_SIZE)
        img = list(img.getdata())
        img = map(list, img)
        x = array(img)
        X.append(x.flatten().tolist())
        y.append(0)
        
        
        self.__model = svm.SVC()
        
        # Regress y on X
        self.__model.fit(X,y)
        
        # Save out model 
        fp = open(self.__dataFile, ""w"")
        pickle.dump(self.__model, fp)
        fp.close()
        ",src/tmachine/DigitClassification.py,jlepird/turing,1
"    XXX -> is there a better way to do this?
    why is there pandas.crosstab() but not pandas.tabulate()?
    """"""
    return pandas.Series(s).groupby(by = lambda x: s[x]).count()

# --- script ---
import pandas
import sklearn.svm

def correct(X,Y,labels):
    G = sklearn.svm.SVC(probability=True)
    G.fit(X, Y)
    Pp = G.predict_proba(X) #a matrix as tall as the dataset as as wide as the number of categories
    Pp = Pp.argmax(axis=1)  #pick a prediction by choosing the largest probability for each datapoint;
                            # this does 'arg'max so it gets the *index into the libsvm category labels*, 
    P = [G.classes_[i] for i in Pp] #known in C as svm_model->labels[] and in python as G.classes_[]),
                            # not necessarily the actual inputted labels
    P = [labels[e] for e in P]
    
    P = pandas.Series(P)",bugs/libsvm_classification/classfail.py,kousu/statasvm,1
"
def SVM_train(x_train, y_train):
    '''
    Training of SVM
    INPUT 
    x_train : Training file with feature Vector
    y_train : Labelled Classes 
    OUTPUT
    Trained SVM Classifier 
    '''
    clf = svm.SVC(C = 5., gamma =0.001)
    clf.fit(x_train, y_train)
    return clf
    
def resizeImages(path, totalNo, size):
    '''
    Code for resizing images so as to have constant length of HOG feature
    vector
    INPUT 
    path : Default directory path where images are there",main.py,tusharmakkar08/elephant_detection,1
"
        print f1
        return {'loss': -f1, 'status': STATUS_OK}    

    def get_model(self,args):
        if args['model']['model'] == 'LR':
            model = lr(penalty=args['model']['regularizer_lr'], C=args['model']['C_lr'],n_jobs=self.cjobs)
        elif args['model']['model'] == 'SVM':
            if args['model']['regularizer_svm'] == 'l1':
                #squared hinge loss not available when penalty is l1. 
                model = svm.LinearSVC(C=args['model']['C_svm'], penalty=args['model']['regularizer_svm'],dual=False,n_jobs=self.cjobs)#loss='hinge')
            else:
                model = svm.LinearSVC(C=args['model']['C_svm'], penalty=args['model']['regularizer_svm'],n_jobs=self.cjobs)
        return model

    def run(self,max_evals=100):
        trials = Trials()
        best = fmin(self.call_experiment,
                    space=space,
                    algo=tpe.suggest,",learn_classifier.py,benbo/QPR_CP1,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = CMAESSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 4.5,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/generations_cmaes_cmaessvc/setup.py,jpzk/evopy,1
"### you'll need to use Pipelines. For more info:
### http://scikit-learn.org/stable/modules/pipeline.html

# Provided to give you a starting point. Try a variety of classifiers.
#from sklearn.naive_bayes import GaussianNB
#clf = GaussianNB()

# Algorithm pipeline
selectkbest = SelectKBest()
dtc = DecisionTreeClassifier()
svc = SVC()

steps = [('feature_selection', selectkbest), ('dtc', dtc)]

pipeline = Pipeline(steps)

parameters = dict(
                  feature_selection__k=[2, 3, 5, 6], 
                  dtc__criterion=['gini', 'entropy'],
                  dtc__max_depth=[None, 1, 2, 3, 4],",udacity/enron/ud120-projects-master/final_project/poi_id_final.py,harish-garg/Machine-Learning,1
"            param_grid = dict(gamma=g_range, C=c_range)
        per = self._percentile(threshold)
        dataset = self.points[:]
        mce = []
        for obj in range(self.f_dim):
            y = zeros(self.npoints)  # classification of data
            for i in range(self.npoints):
                if self.f[i][obj] > per[obj]:
                    y[i] = 1

            grid = GridSearchCV(estimator=SVC(
                kernel=kernel, degree=2), param_grid=param_grid, cv=StratifiedKFold(y, k_tune))
            grid.fit(dataset, y)
            test_score = cross_val_score(
                estimator=grid.best_estimator_, X=dataset, y=y, scoring=None, cv=StratifiedKFold(y, k_test))
            mce.append((ones(k_test) - test_score).tolist())
        return mce  # mce[n_obj][k_test]

    def _svm_p_values(self, threshold=50, k_tune=3, k_test=10, l=True, q=True, n=True):
        """"""",PyGMO/util/_analysis.py,kartikkumar/pagmo,1
"
print(""Kernel and parameters to be tested:"")
for entry in tuned_parameters:
    print(entry)
print("""")

# We try with kernels and parameters defined in 'tunned_parameters'
# To do it, we split the trainning data if 5 parts and execute 5 times
# In each time one part will be the validation data and the rest the
# trainning data
clf = GridSearchCV(SVC(C=1.), tuned_parameters, n_jobs=2, cv=5,
                   scoring='%s' % score)
clf.fit(X_train, y_train)

# The best kernel and parameters is printed
print(""Best parameters set found on development set:"")
print("""")
print(clf.best_params_)
print("""")
print(""Grid scores on development set:"")",scripts/sklearn_sift.py,davidglt/Attractive,1
"from sklearn import svm


def call_sklearn(Ce=1.8333, Gamma=0.1366, cache=1000):
    clf = svm.SVC(C=Ce, gamma=Gamma, cache_size=cache)
    return clf


def trainSVM(features, classes, call=False, clf=None):
    if call:
        clf = call_sklearn()

    clf.fit(features, classes)
",Identification/svm.py,andimarafioti/AIAMI,1
"                                                   maxEpochs=10)

        print ""[Trainer] -> training done.""
        print errors
        save_to_file(net, model_path)
        print ""[Trainer] -> model save to model.xml file.""


class SVMTrainer(AbstractTrainer):
    def train(self, summarizer, dataset, testset, model_path):
        clf = svm.SVC(cache_size=5000, verbose=True, C=10, gamma=0.2)
        print clf.fit(dataset['input'], dataset['target'])
        save_to_file(clf, model_path)

def save_dataset_as_csv(dataset):
    file = codecs.open(""dataset.csv"", ""w"", ""utf-8"")
    for input, target in zip(dataset['input'], dataset['target']):
        file.write("";"".join([str(i) for i in input]) + "";"" + str(target[0]) + ""\n"")

",Summarizer/trainer.py,lpawluczuk/summar.pl,1
"    Ypred_libsvmffi = libsvm.svmutil.svm_predict(Y,X , model)
    print map(int,Ypred_libsvmffi[0])
    
    ### sklearn
    print 'sklean -------------------'
    import numpy as np

    X = np.array(X)
    Y = np.array(Y)
    from sklearn.svm import SVC
    clf=SVC(C=3.0, gamma=0.7, kernel='rbf')
    clf.fit(X, Y)

    Ypred_sklearnSVC=clf.predict(X)
    print  Ypred_sklearnSVC.tolist()
    from sklearn.metrics import accuracy_score
    print accuracy_score(Y, Ypred_sklearnSVC)
    
    print 'cmdline wrap-------------------'
    clfcmd=ClassifySVM2ClassCuda(C=3.0, gamma=0.7, kernel='rbf')    ",python/test.py,niitsuma/gpusvm,1
"
start=0
i=11000
tsize=1100

trainData=allData[start:i+start,:]
trainLabel=allLabel[start:i+start]
testData=allData[i+1:i+tsize,:]
testLabel=allLabel[i+1:i+tsize]

clf=svm.SVC(kernel='linear')
clf.fit(preprocessing.scale(trainData),trainLabel)
pred=clf.predict(testData)
confusion_matrix(allLabel[7000:8100],pred)
",svmClassification.py,TheVenu/3D-Gesture-Recognition,1
"            image_array = cv2.imread(path + ""/"" + image)
            X.append(extract_feature(image_array))
            Y.append(i)

    X,Y = data_shuffle(X,Y)

    return (np.asarray(X),np.asarray(Y))

#http://scikit-learn.org/stable/modules/svm.html
def trainSVM(X,y):
    clf = svm.SVC(kernel='linear')
    #try different kernel
    clf.fit(X, y)
    return clf
   
def testSVM(testDataSetpath,clf):
    for image in testDataSetpath:
        if label(image)==clf.predict(extract_feature(image)):
            ok+=1
        else:",train_SVM.py,stelat/GoRec,1
"def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)
    clf.fit(X, y_regr)

    clf = AdaBoostRegressor(SVR(), random_state=0)",net-p3/lib/python3.5/site-packages/sklearn/ensemble/tests/test_weight_boosting.py,uglyboxer/linear_neuron,1
"          ""subsample"": 0.67,
          ""colsample_bytree"": 0.9,
          ""eval_metric"": ""logloss"",
          ""n_estimators"": 100,
          ""silent"": 1,
          ""seed"": 93425
          }
          
    #Defining the classifiers
    clfs = {'LRC'  : LogisticRegression(n_jobs=-1, random_state=random_state), 
            'SVM' : SVC(probability=True, max_iter=100, random_state=random_state), 
            'RFC'  : RandomForestClassifier(n_estimators=100, n_jobs=-1, 
                                       random_state=random_state), 
            'GBM' : GradientBoostingClassifier(n_estimators=50, 
                                           random_state=random_state), 
            'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1, 
                                     random_state=random_state),
            'KNN' : KNeighborsClassifier(n_neighbors=30, n_jobs=-1),
            'ABC' : AdaBoostClassifier(DecisionTreeClassifier(max_depth=30),
                                       algorithm=""SAMME"", n_estimators=350,",scripts/two_layer_training.py,HighEnergyDataScientests/bnpcompetition,1
"
	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)

	#fit the svm model and compute accuaracy measure
	parameters = {'n_neighbors' : list(np.arange(20)+1)}
	clf = GridSearchCV(KNeighborsClassifier(weights='distance', n_jobs=-1), parameters)
	#clf = svm.SVC(kernel=arc_cosine, cache_size=2048)
	clf.fit(trainX, trainY)


	pred = clf.predict(testX)
	print(accuracy_score(testY, pred))
	print(confusion_matrix(testY, pred))
	#print(clf.best_params_)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))
",UMKL/exploratory/add_kernels.py,akhilpm/Masters-Project,1
"def Analysis():
  test_size = 1
  invest_amount = 10000 # dollars
  total_invests = 0
  if_market = 0
  if_strat = 0

  X, y, Z = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    invest_return = 0
    market_return = 0
    if clf.predict(X[-x])[0] == y[-x]: # test data
      correct_count += 1
",p23.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"#create 3 tf-idf dtms 
X_dtm1 = vectTFidf1.fit_transform(X)

X_dtm2 = vectTFidf2.fit_transform(X)

X_dtm3 = vectTFidf3.fit_transform(X)


from sklearn.svm import LinearSVC #nice
from sklearn import cross_validation
svm = LinearSVC(C=1, penalty='l2', loss='hinge')



from sklearn.naive_bayes import MultinomialNB #nice
from sklearn.linear_model import LogisticRegression
#unigrams
scores = cross_validation.cross_val_score(svm, X_dtm1, y, scoring='recall', cv=10)
print(scores.mean())
print vectTFidf1.get_feature_names()[-50:]",project2/convert files with doc ext to text.py,jdweaver/ds_sandbox,1
"gamma_range = gamma_range.flatten()

# generate matrix with all C
#C_range = np.outer(np.logspace(-3, 3, 7),np.array([1,2, 5]))
C_range = np.outer(np.logspace(-1, 1, 3),np.array([1,5]))
# flatten matrix, change to 1D numpy array
C_range = C_range.flatten()

parameters = {'kernel':['rbf'], 'C':C_range, 'gamma': gamma_range}

svm_clsf = svm.SVC()
grid_clsf = GridSearchCV(estimator=svm_clsf,param_grid=parameters,n_jobs=1, verbose=2)


start_time = dt.datetime.now()
print('Start param searching at {}'.format(str(start_time)))

grid_clsf.fit(X_train, y_train)

elapsed_time= dt.datetime.now() - start_time",modeling/svm/svm_mnist_classification.py,jwilliamn/handwritten,1
"
    cpu_count = 10

    # cvs = list(cv.StratifiedShuffleSplit(y, n_iter = 100, test_size = 0.2))

    all_scores = defaultdict(list)

    print('svms')
    for nu in np.arange(7) * 0.1 + 0.05:
        try:
            machine = svm.NuSVC(nu=nu,
                                kernel='linear',
                                verbose=False,
                                probability=False)
            machine.fit(Xtrain, ytrain)
            scores = roc_auc_score(ytest, machine.decision_function(Xtest))
            print(scores)
            this_method = 'SVM, linear kernel'
            add_to_scores([this_method, ('nu', nu)])
            all_scores[this_method][('nu', nu)] = [scores]",run_synthesized.py,adrinjalali/Network-Classifier,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",python3-alpha/python3-src/Lib/email/test/test_email.py,837468220/python-for-android,1
"    
####################################################################
# Dalitz operaton
####################################################################

for i in range(100):
	comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.{0}.0.txt"".format(i), os.environ['MLToolsDir']+""/Dalitz/dpmodel/data/data.2{0}.1.txt"".format(str(i).zfill(2))))
    
#clf = tree.DecisionTreeClassifier('gini','best',46, 100, 1, 0.0, None)
#clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.95,n_estimators=440)
clf = SVC(C=1.0,gamma=0.0955,probability=True, cache_size=7000)
args=[""dalitz_svm"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13)]
#For nn:
#args=[""dalitz_nn"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),71,1]

classifier_eval_simplified.classifier_eval(0,0,args)

",Dalitz_simplified/evaluation_of_optimised_classifiers/svm_Dalitz/svm_Dalitz_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"    test = pca.transform(test)
    return train, test

def normalize(train, test):
    norm = preprocessing.Normalizer()
    train = norm.fit_transform(train)
    test = norm.transform(test)
    return train, test

def createSVM():
    clf = SVC()
    return clf

def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf",libs/classifiers.py,KellyChan/Kaggle,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Python27/Lib/email/test/test_email.py,Jeff-Tian/mybnb,1
"elif folding == 'kfolding':
    cv = KFold(n=y.shape[0], k=n_folds)
elif folding == 'leaveoneout':
    n_folds[0] = y.shape[0]
    cv = LeaveOneOut(n=y.shape[0])
else:
    print(""unknown crossvalidation method!"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
fs = SelectPercentile(f_classif, percentile=fs_n)

print(""INITIALIZE RESULTS"")
if compute_predict:",JR_toolbox/skl_king.py,cjayb/kingjr_natmeg_arhus,1
"            X_test = tfv.transform(testdata)
            
            # Initialize SVD
            svd = TruncatedSVD(n_components=175,random_state=None)
            X=svd.fit_transform(X)+trssv.sensimvar()
            X_test=svd.fit_transform(X_test)+tsssv.sensimvar()
            # Initialize the standard scaler 
            scl = StandardScaler()
            
            # We will use SVM here..
            svm_model = SVC()
#            rf=RandomForestClassifier(n_estimators=400,max_depth=15,n_jobs=4,verbose=True)
            
            # Create the pipeline 
            clf = pipeline.Pipeline([
            						 ('scl', scl),
#                            	     ('rf', rf)])
                                     ('svm', svm_model)])
            
            # Create a parameter grid to search for best parameters for everything in the pipeline",Search_result/srr6.py,tanayz/Kaggle,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half3.py,mirjalil/ml-visual-recognition,1
"X_1 = X_train.todense().tolist()  # samples 72 features above 7129
y_1 = map(int,y_train)   # classes 2
'''

#print(sub_data.shape)
X_1 = sub_data.todense().tolist()
y_1 = map(int,sub_sample)
#print(len(y_1))

#L2 SVM trained on the features selected by the L1 SVM subSampling
clf = LinearSVC(penalty='l1', dual=False,C=c).fit(X_1, y_1)
model = SelectFromModel(clf, prefit=True)
X = model.transform(X_1)

print(""number of featuer selected %d"", X.shape[1])
clf = LinearSVC(penalty='l2',dual=False,C=c)
scores = cross_validation.cross_val_score(clf, X, y_1, cv=10)

print(scores)
print(""L2 SVM trained on the features selected by the L1 SVM subsampling. \n  Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))",comparisionL2SvmArcene.py,narendrameena/featuerSelectionAssignment,1
"#
# For an initial search, a logarithmic grid with basis
# 10 is often helpful. Using a basis of 2, a finer
# tuning can be achieved but at a much higher cost.


C_range = np.logspace(-2, 10, 13)
gamma_range = np.logspace(-9, 3, 13)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)
grid = GridSearchCV(SVC(probability=True), scoring=p_value_scoring_object.p_value_scoring_object ,param_grid=param_grid, cv=cv)
grid.fit(X, y)

print(""The best parameters are %s with a score of %0.2f""
      % (grid.best_params_, grid.best_score_))

# Now we need to fit a classifier for all parameters in the 2d version
# (we use a smaller set of parameters here because it takes a while to train)

C_2d_range = [1e-2, 1, 1e2]",Dalitz_simplified/classifier_eval_simplified_example.py,weissercn/MLTools,1
"	for i in range(1):

		comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_5__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_legendre_contrib0__1_0__""+contrib_string0+""contrib1__0_0__""+contrib_string1+""contrib2__2_0__""+contrib_string2+""contrib3__0_7__""+contrib_string3+""sample_{0}.txt"".format(i)))

		#comp_file_list.append((os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high"" +str(dim)+""Dgauss_10000_0.5_0.1_0.0_{0}.txt"".format(i),os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/higher_dimensional_gauss/gauss_data/data_high""+str(dim)+""Dgauss_10000_0.5_0.1_0.01_{0}.txt"".format(i))) 

	#comp_file_list=[(os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_sin_100_periods_1D_sample_0.txt"",os.environ['MLToolsDir']+""/Dalitz/gaussian_samples/legendre/legendre_data/data_sin_99_periods_1D_sample_0.txt"")]

        #clf = tree.DecisionTreeClassifier('gini','best',37, 89, 1, 0.0, None)
        #clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2), learning_rate=0.01,n_estimators=983)
        #clf = SVC(C=params['aC'],gamma=params['agamma'],probability=True, cache_size=7000)
        #args=[str(dim)+ ""Dlegendre4contrib_bdt"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0]
        #For nn:
	clf=""This shouldnt be used as we are in Keras mode""
        args=[str(dim)+""Dlegendre4contrib_nn_4layers_100neurons_onehot"",""particle"",""antiparticle"",100,comp_file_list,1,clf,np.logspace(-2, 10, 13),np.logspace(-9, 3, 13),0,100,4]

        ####################################################################


	classifier_eval_simplified.classifier_eval(0,1,args)",Dalitz_simplified/evaluation_of_optimised_classifiers/nn_legendre/nn_Legendre_evaluation_of_optimised_classifiers.py,weissercn/MLTools,1
"def Analysis():
  test_size = 1
  invest_amount = 10000 # dollars
  total_invests = 0
  if_market = 0
  if_strat = 0

  X, y, Z = Build_Data_Set()
  print(len(X))
  
  clf = svm.SVC(kernel=""linear"", C=1.0)
  clf.fit(X[:-test_size],y[:-test_size]) # train data

  correct_count = 0
  for x in range(1, test_size+1):
    invest_return = 0
    market_return = 0
    if clf.predict(X[-x])[0] == y[-x]: # test data
      correct_count += 1
",p25.py,PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project,1
"n_permutations = 200
train_ratios = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
ntm_models = [
             ('srntm model 1 32*32', ntm1),
             ]                        
models = [
         ('log_reg_sgd', linear_model.SGDClassifier(loss='log')),
         ('log_reg_liblinear', linear_model.LogisticRegression()),
         ('log_reg_cv', linear_model.LogisticRegressionCV()),
#        ('gaussian nb',  GaussianNB()),
#        ('svm_linear', svm.LinearSVC()),
#        ('svm_rbf', svm.SVC(kernel='rbf')),
#        ('K=5nn', KNeighborsClassifier(n_neighbors=5)),
         ]

total_scores = []
for ratio in train_ratios:
    samples = sample_from_dataset(X_data, y_data, n_permutations, ratio)
    sequences = create_learning_sequences(*samples) 
    scores = []",evaluate_iris.py,tombosc/ntm_experiments,1
"from sklearn.linear_model import LogisticRegression
from settings import *


names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(25),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]


def main():
",kpcaWithTreeFS/mnistBackImage/classifiers.py,akhilpm/Masters-Project,1
"classifier_liblinear = svm.LinearSVC()
t0 = time.time()
classifier_liblinear.fit(train_vectors, train_labels)
t1 = time.time()
prediction_liblinear = classifier_liblinear.predict(test_vectors)
t2 = time.time()
time_liblinear_train = t1-t0
time_liblinear_predict = t2-t1

# Print results in a nice table
print(""Results for SVC(kernel=rbf)"")
print(""Training time: %fs; Prediction time: %fs"" % (time_rbf_train, time_rbf_predict))
print(classification_report(test_labels, prediction_rbf))
print(""Results for SVC(kernel=linear)"")
print(""Training time: %fs; Prediction time: %fs"" % (time_linear_train, time_linear_predict))
print(classification_report(test_labels, prediction_linear))
print(""Results for LinearSVC()"")
print(""Training time: %fs; Prediction time: %fs"" % (time_liblinear_train, time_liblinear_predict))
print(classification_report(test_labels, prediction_liblinear))",train_bow.py,eamosse/word_embedding,1
"  y = np.concatenate((lw, lp))
  N = len(y)

  shuffler = cross_validation.ShuffleSplit(N, 1, 0.6)
  for train_idx, test_idx in shuffler:
    X_train = X[train_idx,:]
    X_test = X[test_idx,:]
    y_train = y[train_idx]
    y_test = y[test_idx]

    clf = svm.SVC(C=100000000.0, kernel='rbf', degree=3, gamma=0.1, verbose=True)
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    print 'Correct rate:', or_ml.correctRate(pred, y_test)
    print 'Support vectors:', len(clf.support_)

    joblib.dump(clf, opt.outfile)

    classnames = ['wooden', 'plastic']
    joblib.dump(classnames, or_util.appendToName(opt.outfile, '_classes'))",or_lib/scripts/material_train.py,Beautiful-Flowers/object_recognizer,1
"# transform counts to TFIDF features
tfidf = feature_extraction.text.TfidfTransformer(smooth_idf=False)
train = tfidf.fit_transform(train).toarray()
test = tfidf.transform(test).toarray()

# encode labels
lbl_enc = preprocessing.LabelEncoder()
labels = lbl_enc.fit_transform(labels)

# train classifier
clf = OneVsRestClassifier(svm.SVC(C=4.919646+2., kernel='rbf', tol=.001,
                                  verbose=True, probability=True, gamma=0.646508+.3, random_state=23))

if MODE == 'cv':
    scores, predictions = utils.make_blender_cv(clf, train, labels, calibrate=True)
    print 'CV:', scores, 'Mean log loss:', np.mean(scores)
    utils.write_blender_data(consts.BLEND_PATH, MODEL_NAME + '.csv', predictions)
elif MODE == 'submission':
    calibrated_classifier = CalibratedClassifierCV(clf, method='isotonic', cv=utils.get_cv(labels))
    fitted_classifier = calibrated_classifier.fit(train, labels)",otto/model/model_03_svm/svm.py,ahara/kaggle_otto,1
"    testdata  = open(""testnew.txt"")
    traindata.readline() # 跳过第一行
    testdata.readline()
    train = np.loadtxt(traindata)
    test = np.loadtxt(testdata)
    X = train[0:4628,0:27]
    y = train[0:4628,27]
    test_x = test[0:1437,0:27]
    test_y = test[0:1437,27]

    model1 = LinearSVC()
    model2 = LogisticRegression()
    model3 = GaussianNB()
    model4 = RandomForestClassifier()
    model5 = KNeighborsClassifier()
    model1.fit(X,y)
    model2.fit(X,y)
    model3.fit(X,y)
    model4.fit(X,y)
    model5.fit(X,y)",分类和回归/newcla.py,vimilimiv/weibo-popularity_judge-and-content_optimization,1
"    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    'GBRT': GradientBoostingClassifier(n_estimators=250),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),
    'RandomForest': RandomForestClassifier(n_estimators=20),
    'CART': DecisionTreeClassifier(min_samples_split=5),
    'SGD': SGDClassifier(alpha=0.001, n_iter=2),
    'GaussianNB': GaussianNB(),
    'liblinear': LinearSVC(loss=""l2"", penalty=""l2"", C=1000, dual=False,
                           tol=1e-3),
    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)
}

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument('--classifiers', nargs=""+"",
                        choices=ESTIMATORS, type=str,
                        default=['liblinear', 'GaussianNB', 'SGD', 'CART'],",projects/scikit-learn-master/benchmarks/bench_covertype.py,DailyActie/Surrogate-Model,1
"    #model = CalibratedClassifierCV(model, method='isotonic', cv=3)
    #model = KNeighborsClassifier(n_neighbors=5)
    #model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=20,oob_score=False,class_weight='auto')
    #model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=20,oob_score=False,class_weight=None)
    #model = CalibratedClassifierCV(model, method='isotonic', cv=3)
    
    #model =  ExtraTreesClassifier(bootstrap=False,n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='entropy', max_features=20,oob_score=False)
    
    #basemodel = GradientBoostingClassifier(loss='deviance',n_estimators=4, learning_rate=0.03, max_depth=10,subsample=.5,verbose=1)
    
    #model = SVC(kernel='rbf',C=10.0, gamma=0.0, verbose = 0, probability=True)
    
    #model = XgboostClassifier(booster='gblinear',n_estimators=50,alpha_L1=0.1,lambda_L2=0.1,n_jobs=2,objective='multi:softprob',eval_metric='mlogloss',silent=1)#0.63
    #model = XgboostClassifier(n_estimators=400,learning_rate=0.05,max_depth=10,subsample=.5,n_jobs=1,objective='multi:softprob',eval_metric='mlogloss',booster='gbtree',silent=1,eval_size=-1)#0.45
    #basemodel1 = XgboostClassifier(n_estimators=120,learning_rate=0.1,max_depth=6,subsample=.5,n_jobs=1,objective='multi:softprob',eval_metric='mlogloss',booster='gbtree',silent=1,eval_size=0.0)#0.46
    #basemodel1 = LogisticRegression(C=1.0,penalty='l2')
    #model = OneVsOneClassifier(basemodel1)
    
    #hyper_opt1 0.443
    #basemodel1 = XgboostClassifier(n_estimators=200,learning_rate=0.13,max_depth=10,subsample=.82,colsample_bytree=0.56,n_jobs=4,objective='multi:softprob',eval_metric='mlogloss',booster='gbtree',silent=1)",competition_scripts/otto.py,chrissly31415/amimanera,1
"            testDocVector = docVectors[testIndex[fold]]

            # training
            features = docAppend(trainDocVector, trainProbData, labelCorpus, modelList, trainSize, useDocVector)
            #trainLabels, validList = trainLabeler(trainProbData, trainLabels, modelList, trainSize)
            #trainFeature = features[validList]
            '''
            if classifier == 'MaxEnt':
                model = LogisticRegression()
            elif classifier == 'SVM':
                model = svm.SVC()
            '''
            if classifier == 'MLP':
                model = MLPClassifier(algorithm='sgd', activation='logistic', learning_rate_init=0.02, learning_rate='constant', batch_size=10)

            model.fit(features, trainLabels)

            # testing
            flag, testSize = eu.checkSize(testProbData, modelList)
            if not flag:",docFeatureEnsemble.py,renhaocui/ensembleTopic,1
"


#
# TODO: Create an SVC classifier named svc
# Use a linear kernel, and set the C value to C
#
# .. your code here ..

from sklearn.svm import SVC
svc = SVC(C = C, kernel = kernel) #C=1, kernel = 'linear' defined in the beginning of this code


#
# TODO: Create an KNeighbors classifier named knn
# Set the neighbor count to 5
#
# .. your code here ..

",Module 6/assignment1.py,LamaHamadeh/Microsoft-DAT210x,1
"
    def predict(self, X):
        return self._clf.predict(X)

    def get_params(self, deep=True):
        return dict(clf=self._clf)


def LinearSVC_Proba(probability=False, method='sigmoid', cv=5, **kwargs):
    if probability is True:
        base_estimator = LinearSVC(**kwargs)
        return CalibratedClassifierCV(base_estimator=base_estimator,
                                      method=method, cv=cv)
    else:
        return LinearSVC(**kwargs)


def SVC_Light(probability=False, method='sigmoid', cv=5, **kwargs):
    """"""
    Similar to SVC(kernel='linear') without having to store 'support_vectors_'",gat/classifiers.py,kingjr/gat,1
"    logging.info(""Beginning K-Nearest Neighbor classification."")
    data = data.toarray()
    run_classification( kn, data, labels )
# }}}



# Perform svm {{{
def do_svm( data, labels, config ):

    sv = svm.LinearSVC()

    logging.info(""Beginning SVM classification."")
    data = data.toarray()
    run_classification( sv, data, labels )
# }}}



# Executable (Main) {{{",code/classifier.py,jrouly/stackmining,1
"    data = sklearn.datasets.load_digits()
    X = data[""data""]
    y = data[""target""]

    paramgrid = {""kernel"": [""rbf""],
                 ""C"": np.logspace(-9, 9, num=25, base=10),
                 ""gamma"": np.logspace(-9, 9, num=25, base=10)}

    random.seed(1)

    cv = EvolutionaryAlgorithmSearchCV(estimator=SVC(),
                                       params=paramgrid,
                                       scoring=""accuracy"",
                                       cv=StratifiedKFold(n_splits=4),
                                       verbose=1,
                                       population_size=10,
                                       gene_mutation_prob=0.10,
                                       gene_crossover_prob=0.5,
                                       tournament_size=3,
                                       generations_number=5,",test.py,rsteca/sklearn-deap,1
"fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

for i in range(64):
    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    
    ax.text(0, 7, str(digits.target[i]))
  
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)  
for kernel in ['rbf', 'linear']:
    clf = SVC(kernel=kernel).fit(Xtrain, ytrain)
    ypred = clf.predict(Xtest)
    print(""SVC: kernel = {0}"".format(kernel))
    print(metrics.accuracy_score(ytest, ypred))
    plt.figure()
    plt.imshow(metrics.confusion_matrix(ypred, ytest),
               interpolation='nearest', cmap=plt.cm.binary)
    plt.colorbar()
    plt.xlabel(""true label"")
    plt.ylabel(""predicted label"")",codes/Support Vector Machines/digit_svm.py,GaryLv/GaryLv.github.io,1
"		target_file	= open( 'bases/targets/' + target_files[ i ] )

		data, target = load_data( data_file, target_file )
		data = numpy.array( data )
		target = numpy.array( target )
	
		st = random.randint( 0, 99999 ); print st
		cv = cross_validation.KFold( len( data ), 5, random_state = st, shuffle = True )
	
		print ' ' + str( data_files[ i ].split( '_' )[ 2 ].split( '.' )[ 0 ] ) + ' :: linear '
		clf = svm.SVC( kernel = 'linear', C = 1000 )
		scores.append( cross_validation.cross_val_score( clf, data, target, cv = cv, n_jobs = -1 ) )
	
		print ' ' + str( data_files[ i ].split( '_' )[ 2 ].split( '.' )[ 0 ] ) + ' :: poly '
		clf = svm.SVC( kernel = 'poly', degree = 3, C = 1000 )
		scores.append( cross_validation.cross_val_score( clf, data, target, cv = cv, n_jobs = -1 ) )
		
		print ' ' + str( data_files[ i ].split( '_' )[ 2 ].split( '.' )[ 0 ] ) + ' :: rbf '
		clf = svm.SVC( kernel = 'rbf', C = 1000 )
		scores.append( cross_validation.cross_val_score( clf, data, target, cv = cv, n_jobs = -1 ) )",Exemplo_svms.py,alessandro-sena/slearning-stackoverflow,1
"                                  Y=dataset['sequences'],
                                  amino_acid_property_file='../amino_acids_matrix/AA.blosum50.dat',
                                  sigma_position=1.0,
                                  sigma_amino_acid=1.0,
                                  substring_length=2,
                                  normalize_matrix=True)
    
    print
    print 'Fitting an SVM classifier...'
    print len(dataset['sequences'])
    estimator = SVC(kernel='precomputed')
    estimator.fit(X=train_matrix, y=dataset['classes'])
    
    print 'Done!'
    
    ",exercises/code/GSkernel_source/examples/mhc_binding_prediction.py,aldro61/microbiome-summer-school-2017,1
"    mat = np.nan_to_num(mat)
    mat = mat / math.sqrt(mat.shape[0])
    return mat

def test_classification():
    fake_raw_data = [create_epoch(i, 5) for i in range(20)]
    labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
    # 5 subjects, 4 epochs per subject
    epochs_per_subj = 4
    # svm
    svm_clf = svm.SVC(kernel='precomputed', shrinking=False, C=1)
    training_data = fake_raw_data[0:12]
    clf = Classifier(svm_clf, epochs_per_subj=epochs_per_subj)
    clf.fit(list(zip(training_data, training_data)), labels[0:12])
    expected_confidence = np.array([-1.18234421, 0.97403604, -1.04005679, 
                                    0.92403019, -0.95567738, 1.11746593,
                                    -0.83275891, 0.9486868])
    recomputed_confidence = clf.decision_function(list(zip(fake_raw_data[12:],
                                                           fake_raw_data[12:])))
    hamming_distance = hamming(np.sign(expected_confidence),",tests/fcma/test_classification.py,IntelPNI/brainiak,1
"from sklearn.datasets import load_digits
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

digits = load_digits()
X = digits.data
y = digits.target
param_range = np.logspace(-6, -2.3, 5)
train_loss, test_loss = validation_curve(
        SVC(), X, y, param_name='gamma', param_range=param_range, cv=10,
        scoring='mean_squared_error')
train_loss_mean = -np.mean(train_loss, axis=1)
test_loss_mean = -np.mean(test_loss, axis=1)

plt.plot(param_range, train_loss_mean, 'o-', color=""r"",
             label=""Training"")
plt.plot(param_range, test_loss_mean, 'o-', color=""g"",
             label=""Cross-validation"")
",DeepLearnMaterials/tutorials/sklearnTUT/sk10_cross_validation3.py,MediffRobotics/DeepRobotics,1
"    return x


if __name__ == ""__main__"":
    def list_split(l):
        center = len(l) / 2
        return l[:center], l[center:]

    MAX_LINES = -1

    #clf = LinearSVC(dual=False, C=1.1, loss='l2', penalty='l2', tol=1e-4)
    clf = LinearSVC(dual=False, C=1.1, loss='l2', penalty='l1', tol=1e-4)
    #clf = LinearSVC(dual=True, C=1.1, loss='l1', penalty='l2')
    #clf = linear_model.SGDClassifier()
    X = []
    Y = []

    reader = sys.stdin
    read_from_file = ""--read_from_file=1"" in sys.argv
    print_svm_score = ""--print_svm_score=1"" in sys.argv",gold-digger/svm/mapper.py,lukaselmer/ethz-data-mining,1
"
if __name__=='__main__':
    parse_data=parse_csv(Train_File_name)
    
    #input Feature & output label
    X,Y = create_dataset(parse_data)

    #Draw(Y,X)
    
    #Now Create & Train Our Classifier
    clsf=SVC(kernel='rbf',gamma=1,C=1) #SVM Classier
    print 'Training Started ..'
    a = datetime.datetime.now()
    clsf.fit(X, Y)
    b=datetime.datetime.now()
    print 'Training Is completed, Time taken for training :',b-a
    
    #Now Load Testing dataset
    parse_data=parse_csv(Test_file_name)
    P,Q= create_dataset(parse_data)",SVM Classifier/SVM Cancer dataset/SVM_Breast_Cancer_Dataset.py,sudhanshuptl/Machine-Learning,1
"
# Split up data into randomized training and test sets
rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(
    scaled_X, y, test_size=0.2, random_state=rand_state)

print('Using:', orient, 'orientations', pix_per_cell,
      'pixels per cell and', cell_per_block, 'cells per block')
print('Feature vector length:', len(X_train[0]))

# svc = LinearSVC()  # Use a linear SVC
MLP = MLPClassifier()

t = time.time()  # Check the training time

#svc.fit(X_train, y_train)
MLP.fit(X_train, y_train)

t2 = time.time()
print(round(t2 - t, 2), 'Seconds to train...')",p5-car-detection/project/methods/processImage.py,swirlingsand/self-driving-car-nanodegree-nd013,1
"        reversed_horizontal_silhouette,
        vertical_silhouette,
        reversed_vertical_silhouette,
        middle_silhouette,
        vertical_symmetry,
        horizontal_symmetry,
]

SVM_EXTRACTORS = [scale_image_down(positions)]
def svm_engine():
    return svm.SVC(kernel='poly', degree=2)

FOREST_EXTRACTORS = ALL_EXTRACTORS
def forest_engine():
    return ensemble.RandomForestClassifier(n_estimators=50, n_jobs=2)

class CaptchaDecoder(object):
    def __init__(self, *args, **kwargs):
        self.engine = svm_engine()
        self.feature_extractor = compose_extractors(SVM_EXTRACTORS)",models.py,aflag/captcha-study,1
"from sklearn.pipeline import Pipeline
from sklearn.cross_validation import cross_val_score, ShuffleSplit

from mne.decoding import ConcatenateChannels, FilterEstimator

scores_x, scores, std_scores = [], [], []

filt = FilterEstimator(rt_epochs.info, 1, 40)
scaler = preprocessing.StandardScaler()
concatenator = ConcatenateChannels()
clf = SVC(C=1, kernel='linear')

concat_classifier = Pipeline([('filter', filt), ('concat', concatenator),
                              ('scaler', scaler), ('svm', clf)])

for ev_num, ev in enumerate(rt_epochs.iter_evoked()):

    print(""Just got epoch %d"" % (ev_num + 1))

    if ev_num == 0:",examples/realtime/plot_compute_rt_decoder.py,christianbrodbeck/mne-python,1
"                                                                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0,
                                                                 min_density=None, compute_importances=None)
    Scikit_SVM_Model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)
    
    Scikit_GradientBoostingClassifier_Model = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=1, random_state=0)

    # ANOVA SVM-C
    anova_filter = SelectKBest(f_regression, k=5)

    clf = Pipeline([
        #('feature_selection', svm.LinearSVC(C=0.01, penalty=""l1"", dual=False)),
        ('anova', anova_filter),
        ('classification', Scikit_SVM_Model)
    ])
    numberOfSamples = trainingSamples + testingSamples
    #accuracy, testing_Labels, predict_Labels =  sc.Classification_CrossValidation(Scikit_RandomForest_Model, features, labels, numberOfSamples, 10)    
    accuracy, testing_Labels, predict_Labels =  sc.Classification(Scikit_GradientBoostingClassifier_Model, features, labels, trainingSamples, testingSamples)
    sc.Result_Evaluation('data/evaluation_result/evaluation_RF.txt', accuracy, testing_Labels, predict_Labels)

    endtime = strftime(""%Y-%m-%d %H:%M:%S"",gmtime())",RandomForest_test.py,DistributedSystemsGroup/YELP-DS,1
"def train_classifier(train_data, train_resp, classifier='SVM'):
    """"""
    :param train_data: training data array
    :param train_resp: training data labels
    :param classifier: Classifier model ('SVM' or 'KNN')
    :return: trained classifier object
    """"""
    if classifier == 'KNN':
        model = KNeighborsClassifier(weights='distance', n_jobs=-1)
    else:
        model = LinearSVC()
    model.fit(train_data, train_resp)
    return model


def test_classifier(model, test_data, test_resp):
    """"""
    :param model: trained kNN classifier object
    :param test_data: test data array
    :param test_resp: test data labels",experiments/image-analysis/mp-assignment-2/image_classification.py,DrigerG/IIITB-ML,1
"    #print(features)
    
def test_cross_validations(df):
    features = df.columns[2:]
    classifiers = [
        {'label': 'Decision Tree', 'algorithm': tree.DecisionTreeClassifier()},
        {'label': 'Gradient Boost', 'algorithm': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)},
        {'label': 'Random Forest', 'algorithm': RandomForestClassifier(n_jobs=-1)},
        {'label': 'Gaussian Naive Bayes', 'algorithm': GaussianNB()},
        {'label': 'AdaBoost', 'algorithm': AdaBoostClassifier(n_estimators=100)},
        {'label': 'Linear SVC', 'algorithm': LinearSVC()}
    ]
    
    for classifier in classifiers: 
        print('\n' + classifier['label'])
        cross_validate(classifier['algorithm'], df, features, 'Survived')        
        print('\n' + classifier['label'] + ' Pipelined')
        pipeline = Pipeline([
          ('first', tree.DecisionTreeClassifier()),
          #('second', RandomForestClassifier(n_jobs=-1)),",predict.py,orangepips/kaggle-titantic,1
"

def generate_model(data, classes, args):

    # Define the parameters
    tuned_parameters = {'C': C_RANGE,
                        'class_weight': CLASS_WEIGHTS}

    # Define the classifier
    if args.kernel == 'rbf':
        clf = svm.SVC(cache_size=CACHE_SIZE)
        tuned_parameters['gamma'] = GAMMA_RANGE
    else:
        clf = svm.LinearSVC(dual=False)

    print_verbose(""Classifier: %s"" % str(clf), 5)
    print_verbose(""Parameters: %s"" % str(tuned_parameters), 5)

    # Generate the K-fold development
    skf = cross_validation.StratifiedKFold(classes, n_folds=K_FOLD, shuffle=True)",src/analysis/generate_model.py,gfolego/vangogh,1
"
sub_data = data[0][i.tolist()]
sub_sample = data[1][i.tolist()] # check for this step


X_1 = sub_data.todense().tolist()
y_1 = map(int,sub_sample)


#L1 SVM
l1svc = LinearSVC(penalty='l1', dual=False).fit(X_1, y_1)

#print(len(l1svc.coef_[0]))





coef = l1svc.coef_.tolist()[0]
#print(coef[0])",subSamplingL1Svm.py,narendrameena/featuerSelectionAssignment,1
"import plistlib
import argparse
from sklearn import linear_model
from sklearn import svm
from numpy import genfromtxt
import numpy.lib.recfunctions as nlr
from scipy.optimize import curve_fit
import pickle
from sklearn.externals import joblib

clf = svm.LinearSVC(random_state=10)

def FitFunction(App_number):
    y=np.genfromtxt('my_file' + str(App_number)  + '.csv',dtype={'names': ('kId','First_Hit_Time_period','Avg_Time_Between_Hits','Time_Between_Last_Hits','Max_Time_between_Hits','Amount_Of_Hits_Till_Arrival','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(6)).astype(np.float)
    x=np.genfromtxt('my_file' + str(App_number) + '.csv',dtype={'names': ('kId','a','Avg_Time_Between_Hits','Time_Between_Last_Hits','Max_Time_between_Hits','Amount_Of_Hits_Till_Arrival','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(1)).astype(np.float)
    t=np.genfromtxt('my_file' + str(App_number) + '.csv',dtype={'names': ('kId','First_Hit_Time_period','a','Max_Time_between_Hits','Amount_Of_Hits_Till_Arrival','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(2)).astype(np.float)
    s=np.genfromtxt('my_file' + str(App_number) + '.csv',dtype={'names': ('kId','First_Hit_Time_period','Avg_Time_Between_Hits','Time_Between_Last_Hits','a','Amount_Of_Hits_Till_Arrival','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(3)).astype(np.float)
    r=np.genfromtxt('my_file' + str(App_number) + '.csv',dtype={'names': ('kId','First_Hit_Time_period','Avg_Time_Between_Hits','Time_Between_Last_Hits','a','Amount_Of_Hits_Till_Arrival','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(4)).astype(np.float)
    z=np.genfromtxt('my_file' + str(App_number) + '.csv',dtype={'names': ('kId','First_Hit_Time_period','Avg_Time_Between_Hits','Time_Between_Last_Hits','Max_Time_between_Hits','a','target'),'formats': ('i10','f8','f8','f8','f8','f8','f8')},delimiter=',',skiprows=1,usecols=(5)).astype(np.float)
    X=np.hstack((x.reshape(len(x), 1),t.reshape(len(t), 1),s.reshape(len(s), 1),r.reshape(len(r), 1),z.reshape(len(z), 1)))",machine_learning_flashiness.py,utah-scs/lsm-sim,1
"    #plot_samples_3d(samples, labels)
    #plot_samples_3d(samples_train, labels_train)
    #plot_samples_3d(samples_test, labels_test)


    from sklearn.tree import DecisionTreeClassifier
    clf = DecisionTreeClassifier()
    run_classifier(clf, samples_train, labels_train, samples_test, labels_test)

    from sklearn import svm
    clf = svm.SVC(kernel='linear')
    run_classifier(clf, samples_train, labels_train, samples_test, labels_test)

    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=10)
    run_classifier(knn, samples_train, labels_train, samples_test, labels_test)

    from sklearn.neighbors.nearest_centroid import NearestCentroid
    nc = NearestCentroid()
    run_classifier(nc, samples_train, labels_train, samples_test, labels_test)",tools/dsp/extractFeatures.py,williampierce/cc,1
"        n_workers = 1
        #try:
            #from multiprocessing import cpu_count
            #n_workers = cpu_count()
        #except:
            #n_workers = 1

        # Define the parameter ranges for C and gamma and perform a grid search for the optimal setting
        parameters = {'C': 2**np.arange(-5,11,2, dtype=float),
                      'gamma': 2**np.arange(3,-11,-2, dtype=float)}                
        clf = GridSearchCV(SVC(kernel='rbf'), parameters, n_jobs=n_workers, score_func=precision_score)
        clf.fit(self.svm_train_values, self.svm_train_labels, 
                cv=StratifiedKFold(self.svm_train_labels, nValidation))

        # Pick the best parameters as the ones with the maximum cross-validation rate
        bestParameters = max(clf.grid_scores_, key=lambda a: a[1])
        bestC = bestParameters[0]['C']
        bestGamma = bestParameters[0]['gamma']
        logging.info('Optimal values: C=%s g=%s rate=%s'%
                     (bestC, bestGamma, bestParameters[1]))",cpa/supportvectormachines.py,afraser/CellProfiler-Analyst,1
"                  (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
                  (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
    print 80 * '='
    print name
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print 80 * '='
    print ""%s penalty"" % penalty.upper()
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                          penalty=penalty)))

# Train SGD with Elastic Net penalty
print 80 * '='
print ""Elastic-Net penalty""",python/sklearn/examples/document_classification_20newsgroups.py,seckcoder/lang-learn,1
"    #parse input and determin its type
    try:
        featureValue= float(input_dict[""featureIn""]) if '.' in input_dict[""featureIn""] else int(input_dict[""featureIn""]) #return int or float
    except ValueError:
        featureValue= input_dict[""featureIn""] #return string
    clf = tree.DecisionTreeClassifier(max_features=featureValue, max_depth=int(input_dict[""depthIn""]))
    output_dict={}
    output_dict['treeOut'] = clf
    return output_dict

def scikit_linearSVC(input_dict):
    from sklearn.svm import LinearSVC
    clf = LinearSVC(C=float(input_dict[""penaltyIn""]),loss=input_dict[""lossIn""],penalty=input_dict[""normIn""], multi_class=input_dict[""classIn""])
    output_dict={}
    output_dict['SVCout'] = clf
    return output_dict

def scikit_SVC(input_dict):
    from sklearn.svm import SVC
    clf = SVC(C=float(input_dict[""penaltyIn""]), kernel=str(input_dict[""kernelIn""]), degree=int(input_dict[""degIn""]))",workflows/scikit/library.py,xflows/textflows,1
"    ----------
    .. [1] `Wikipedia entry on the Hinge loss
            <http://en.wikipedia.org/wiki/Hinge_loss>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
         random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",sklearn/metrics/metrics.py,johnowhitaker/bobibabber,1
"                for r in range(new_h.shape[0]):
                    row_label = np.zeros(label_count)
                    row_label[next_label] = 1
                    labels = np.vstack((labels,row_label))
                next_label += 1

        # From the documentation:
        #SVC is quadratic with the number of samples and a dataset of 10k+ is hard
        #I used this because an example I found did. We should find out if there is
                # a better one
        clf = OneVsRestClassifier(SVC(probability=True), n_jobs = -1)
        scaler = preprocessing.MaxAbsScaler().fit(all_h)
        all_m_s = scaler.transform(all_h)

        observed_m_s = scaler.transform(observed)

        clf.fit(all_m_s, labels)

        # now test the new anomaly against the classifier
        # this seems to work but with this quirk: if two training classes have similar data",AnomalyDetection/python/anomalydetection/svm_calc.py,dpinney/essence,1
"                            grid.fit(x, y)
                            model = grid.best_estimator_
                            model.fit(x,y)        
                        elif inClassifier == 'SVM':
                            param_grid_svm = dict(gamma=2.0**sp.arange(-4,4), C=10.0**sp.arange(-2,5))
                            y.shape=(y.size,)    
                            if model_selection : 
                                cv = StratifiedKFold(n_splits=5).split(x,y)
                            else:
                                cv = StratifiedKFold(y, n_folds=5)
                            grid = GridSearchCV(SVC(), param_grid=param_grid_svm, cv=cv,n_jobs=n_jobs)
                            grid.fit(x, y)
                            model = grid.best_estimator_
                            model.fit(x,y)
                        elif inClassifier == 'KNN':
                            param_grid_knn = dict(n_neighbors = sp.arange(1,20,4))
                            y.shape=(y.size,)    
                            if model_selection : 
                                cv = StratifiedKFold(n_splits=3).split(x,y)
                            else:",scripts/mainfunction.py,lennepkade/dzetsaka,1
"""""""
K-fold Cross-validation
""""""

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

K = 10

svc = svm.SVC(C=1, kernel='linear')

# KFoldは訓練集合とテスト集合の分割結果のインデックスを返す
kfold = cross_validation.KFold(len(X_digits), n_folds=K)
for i, (train_indices, test_indices) in enumerate(kfold):
    print ('%d : Train: %s | test: %s' % (i, len(train_indices), len(test_indices)))

# 分類精度を評価
scores = cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)
",sklearn/cross_validation_easy.py,TenninYan/Perceptron,1
"        """"""
        self.C = C
        self.penalty = penalty
        self.loss = loss
        self.tol = 1e-7
        self.clf = None

    def train(self, data, labels, devdata=None, devlabels=None):
        """""" Training
        """"""
        self.clf = LinearSVC(C=self.C, penalty=self.penalty,
                             loss=self.loss)
        self.clf.fit(data, labels)
        predlabels = self.clf.predict(data)
        acc = accuracy_score(labels, predlabels)
        print 'Training Accuracy: {}'.format(acc)
        if devdata is not None:
            devpredlabels = self.clf.predict(devdata)
            devacc = accuracy_score(devlabels, devpredlabels)
            print 'Dev Accuracy: {}'.format(devacc)",discoseg/model/classifier.py,jiyfeng/DPLP,1
"data = scipy.io.loadmat('ex6data1.mat')
X = data['X']
y = data['y'].flatten()

print 'Training Linear SVM ...'

# You should try to change the C value below and see how the decision
# boundary varies (e.g., try C = 1000)

C = 1
clf = svm.SVC(C=C, kernel='linear', tol=1e-3, max_iter=20)
model = clf.fit(X, y)
visualizeBoundaryLinear(X, y, model)

raw_input(""Program paused. Press Enter to continue..."")

## =============== Part 3: Implementing Gaussian Kernel ===============
#  You will now implement the Gaussian kernel to use
#  with the SVM. You should complete the code in gaussianKernel.m
#",ex6/ex6.py,JediKoder/coursera-ML,1
"X_train = center_scale(X)
y_train = y

gamma = 1 / np.median(pdist(X_train, 'euclidean'))
C = compute_crange(rbf_kernel(X_train, gamma=gamma))

tuned_parameters = [{'kernel': ['rbf'], 'gamma': list(2 ** np.arange(-3, 4.) * gamma), 'C': list(C),
                    'class_weight': [{1: int((y_train == -1).sum() / (y_train == 1).sum())}]},
                    ]

clf = GridSearchCV(SVC(), tuned_parameters, cv=5,
                   scoring='accuracy', n_jobs=10, verbose=True)
clf.fit(X_train, y_train)

best = clf.best_estimator_

print(""Best estimator has training accuracy of %.4g"" % clf.best_score_)
svmfile = 'svm_version3/svm'
print(""Writing to svm file: "",svmfile)
joblib.dump(best, svmfile)",train_SVM_preprocess.py,cajal/pupil-tracking,1
"###############################################################################
def train_and_test_data( A, y, c ):
    ''' train model from given data (A) and labels (y)
        use cross validation to estimate power of model at FPR=0.05
     '''

    # normalize each sample (A[i]) s.t. it has unit norm
    A_norm = preprocessing.normalize( A )

    # classifier
    clf = svm.SVC( kernel=kernel, probability=True, C=c, cache_size=500 ) 

    # prep
    mean_tpr = 0.0
    mean_fpr = np.linspace( 0, 1, 100 )
    cv = StratifiedKFold( y, indices=False, n_folds=K ) # c.v. partition
    
    # mean ROC
    for i, (train, test) in enumerate(cv):
",HFselect_train.py,rronen/HAF-score,1
"    print 'Accuracy: {0}% Train({1}):Test({2}) - Model: {3}'.format(
        int(accuracy*1000)/10.0,
        len(train_Y),
        len(test_Y),
        str(sklnr).replace('\n',' '))

# choose different learners
learner = [
        naive_bayes.GaussianNB(),
        linear_model.SGDClassifier(),
        svm.SVC(),
        tree.DecisionTreeClassifier(),
        ensemble.RandomForestClassifier(),
        # neural_network.MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10, 2), random_state=1)
    ]

# run
for l in learner:
    fit_and_predict(l, train_X, train_Y, test_X)",ml_test.py,james-jz-zheng/jjzz,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_11_20_2014.py,magic2du/contact_matrix,1
"		X.append(value)
		y.append(1)
	rand_smpl = [nonAllStar[pos][i] for i in sorted(random.sample(xrange(len(nonAllStar[pos])),len(allStar[pos])))]

	for value in rand_smpl:
		X.append(value)
		y.append(0)
	# print(X)
	# X=np.array(X)
	# y=np.array(y)
	clf=svm.SVC(probability=True)
	clf.fit(X,y)
	allStarModels[pos]=clf

for pos in mvpModels:
	X=[]
	y=[]
	for value in mvp[pos]:
		X.append(value)
		y.append(1)",rahulCode_redo/project1Part3/nba/final/playerProbabilityGenerator.py,bam2332g/proj1part3,1
"trainidx,testidx = cvpartition(np.arange(1,n_samples),10)


v.train(train_data, max_iter=MAX_ITER, max_epochs=MAX_EPOCHS, cross_validate=False,
        verbose=False, save=True, outdir=METAGRAPH_DIR, plots_outdir=PLOTS_DIR,
        plot_latent_over_time=False)
print(""Trained!"")



#clf = svm.SVC(decision_function_shape='ovr')
#clf.fit(z, TDl) 
#p=clf.predict(z)

# Visualize decoded

fig, axs = plt.subplots(5,10, figsize=(15, 6))
fig.subplots_adjust(hspace = .5, wspace=.001)
axs = axs.ravel()
idx=np.random.randint(test_labels.shape[0],size=50)",challenge/challenge3_mnist.py,juanka1331/VAN-applied-to-Nifti-images,1
"            dual = bool(operator[5])

            if penalty_selection == 'l1':
                dual = False

            if result_name != operator[2]:
                operator_text += ""\n{OUTPUT_DF} = {INPUT_DF}.copy()"".format(OUTPUT_DF=result_name, INPUT_DF=operator[2])

            operator_text += """"""
# Perform classification with a LinearSVC classifier
lsvc{OPERATOR_NUM} = LinearSVC(C={C}, penalty=""{PENALTY}"", dual={DUAL}, random_state=42)
lsvc{OPERATOR_NUM}.fit({OUTPUT_DF}.loc[training_indices].drop('class', axis=1).values, {OUTPUT_DF}.loc[training_indices, 'class'].values)

{OUTPUT_DF}['lsvc{OPERATOR_NUM}-classification'] = lsvc{OPERATOR_NUM}.predict({OUTPUT_DF}.drop('class', axis=1).values)
"""""".format(OUTPUT_DF=result_name, OPERATOR_NUM=operator_num, C=C, PENALTY=penalty_selection, DUAL=dual)

        elif operator_name == '_passive_aggressive':
            C = min(1., max(0.0001, float(operator[3])))
            loss = int(operator[4])
",tpot/export_utils.py,bartleyn/tpot,1
"import numpy as np
from sklearn.svm import SVC

rng = np.random.RandomState(0)
X = rng.rand(100, 10)
y = rng.binomial(1, 0.5, 100)
X_test = rng.rand(5, 10)

clf = SVC()
models = clf.set_params(kernel='linear').fit(X, y)
result = clf.predict(X_test)
print(result)

models = clf.set_params(kernel='rbf').fit(X, y)
result = clf.predict(X_test)",sklearnLearning/quickStart/refittingUpdatingParameters.py,zhuango/python,1
"norm_tst_data = norm(tst_data)

norm_trn_data0, norm_trn_data1 = split(norm_trn_data)
norm_tst_data0, norm_tst_data1 = split(norm_tst_data)
trn_data0, trn_data1 = split(trn_data)
tst_data0, tst_data1 = split(tst_data)

#################### CLASSIFICATION ################
sklda = LDA()
skknn = KNN(3)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)
errors['lda'] = (1-sklda.score(norm_tst_data, tst_labels))
errors['knn'] = (1-skknn.score(norm_tst_data, tst_labels))
errors['svm'] = (1-sksvm.score(norm_tst_data, tst_labels))

bayes0 = GaussianBayes(np.zeros(num_feat), 1, 8, np.eye(num_feat)*3, norm_trn_data0)
bayes1 = GaussianBayes(np.zeros(num_feat), 1, 8, np.eye(num_feat)*3, norm_trn_data1)",exps/mpm.py,binarybana/samcnet,1
"data_train_pos = load_data(new_train_data_path, type=""pos/"")
labels_train_pos = np.ones((data_train_pos.shape[0], 1))
data_train_neg = load_data(new_train_data_path, type=""neg/"")
labels_train_neg = np.zeros((data_train_neg.shape[0], 1))
x_train = np.concatenate((data_train_pos, data_train_neg), axis=0)
y_train = np.concatenate((labels_train_pos, labels_train_neg), axis=0)

x_train_features = hog_extraction(x_train)

print(""Apprentissage."")
# error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.05), N=5)
# error, clf = cross_validation(x_train_features, y_train, AdaBoostClassifier(n_estimators=50))
# error, clf = cross_validation(x_train_features, y_train, RandomForestClassifier(), N=0)
error, clf = cross_validation_svm(x_train_features, y_train, N=3)

window_w = SIZE_TRAIN_IMAGE[0]
window_h = window_w

print(""Predictions sur les données de test."")
# The images in which a face is to detect.",toto.py,jjerphan/SY32FacialRecognition,1
"	y_regis_train = y_regis[:nTrain]
	y_total_train = y_total[:nTrain]
	Xtest = X[nTrain:,:]
	y_casual_test = y_casual[nTrain:]
	y_regis_test = y_regis[nTrain:]
	y_total_test = y_total[nTrain:]

	'''
	#rbf
	param_grid = {'C': [1, 5, 10, 100],'gamma': [0.00001,0.0001, 0.001, 0.01, 0.1],}
	#clf = GridSearchCV(SVC(kernel='rbf'), param_grid,n_jobs=-1)
	clf = SVC(kernel='rbf',C=5.0,gamma=0.0001)
	clf.fit(Xtrain,ytrain)
	pred = clf.predict(Xtest)
	
	print ""best estimator = "",clf.best_estimator_
	print ""RMSE rbf = "", rmsle(ytest, pred)
	#print classification_report(ytest, pred)
	'''
	#new stuff",svm/test_rbf_svm.py,agadiraju/519finalproject,1
"stacking_create_training_set('ensemble_duke_output_raw_T2_n%d.txt' %N,'training_set_T2_n%d.csv' %N, gold_standard_name, N)

#read it and make machine learning on it

data = pd.read_csv('training_set_T2_n%d.csv' %N)

#turn data into arrays
X = data.values[:,2:(N+2)] #x variables
y = np.array(data['y']) #class variables

clf = SVC( kernel = 'rbf',cache_size = 1000)
#parameters = [{'kernel' : ['rbf'],'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}, {'kernel' : ['linear'], 'C': np.logspace(-2,10,30)}]
parameters = {'gamma' : np.logspace(-9,3,30),'C': np.logspace(-2,10,30)}

gs_rbf = grid_search.GridSearchCV(clf,param_grid=parameters,cv = 4)
gs_rbf.fit(X,y)

clf = gs_rbf.best_estimator_

precision_cross_scores = cross_validation.cross_val_score(clf, X, y, cv = 4, scoring = 'precision')",validation/FEIII2016/precision_recall_threshold_curve/ensemble_duke_T2_stacking_prfoutput_cv.py,enricopal/STEM,1
"            maxWeights[wx > maxWeights] = wx
          returnDict[target] = predictions
      elif self.partitionPredictor == 'svm':
        partitions = self.__amsc[index].Partitions(self.simplification)
        labels = np.zeros(self.X.shape[0])
        for idx,(key,indices) in enumerate(partitions.iteritems()):
          labels[np.array(indices)] = idx
        # In order to make this deterministic for testing purposes, let's fix
        # the random state of the SVM object. Maybe, this could be exposed to the
        # user, but it shouldn't matter too much what the seed is for this.
        svc = svm.SVC(probability=True,random_state=np.random.RandomState(8),tol=1e-15)
        svc.fit(self.X,labels)
        probabilities = svc.predict_proba(featureVals)

        classIdxs = list(svc.classes_)
        if self.blending:
          weightedPredictions = np.zeros(len(featureVals))
          sumW = 0
          for idx,key in enumerate(partitions.keys()):
            fx = self.__amsc[index].Predict(featureVals,key)",framework/SupervisedLearning.py,idaholab/raven,1
"
    print ""Writing results""
    f = open('predictions-' + name + '.csv', 'w')
    f.write(""ID,Category\n"")

    for i, res in enumerate(pred):
        f.write(""%d,%d\n"" % (i+1,res))

    f.close()

bench('SVM', svm.SVC())
bench('DTr', tree.DecisionTreeClassifier())
bench('KNN', neighbors.KNeighborsClassifier())",Black-Box/default.py,bcspragu/Machine-Learning-Projects,1
"
# Convert Train Dataset
# Apply transformation, from labels to you know what I mean
converted_train_data = ([[ d[f].value for f in trainingSet.domain if f != trainingSet.domain.class_var] for d in trainingSet])
converted_train_data = [dict(enumerate(d)) for d in converted_train_data]
vector = DictVectorizer(sparse=False)
converted_train_data = vector.fit_transform(converted_train_data)

converted_train_targets = ([ 0 if d[trainingSet.domain.class_var].value == 'ALL' else 1 for d in trainingSet ])

clf = svm.SVC(kernel='linear')
clf.fit(converted_train_data, converted_train_targets)
logmessage(""Model learnt"", success)

# Performances

# Apply Discretization and feature selection to validation set
validationSet = ds.discretizeDataset(validationSet)
validationSet = fs.select(validationSet)
logmessage(""Validation set length is %s"" % len(validationSet), info)",main.py,Sh1n/AML-ALL-classifier,1
"    weights.append( class_weights[ Y[i] ] )

# Normalize data
h = .01  # step size in the mesh
X = preprocessing.scale(np.array(X))
Y = np.array(Y)

# Fit the models
clf = DecisionTreeClassifier()
#clf = neighbors.KNeighborsClassifier()
#clf = svm.SVC(C=1.0, kernel='linear')
clf.fit(X, Y)

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

# Prediction
for input_set in args:
    print '> predicting', input_set
    with open(input_set) as fit_data:",quadclass/quadlearn.py,vavrusa/seqalpha,1
"
    # a carefully hand-designed dataset lol
    y[7] = 0
    y[27] = 0
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

    for ax, C in zip(axes, [1e-2, 1, 1e2]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='linear', C=C, tol=0.00001).fit(X, y)
        w = svm.coef_[0]
        a = -w[0] / w[1]
        xx = np.linspace(6, 13)
        yy = a * xx - (svm.intercept_[0]) / w[1]
        ax.plot(xx, yy, label=""C = %.e"" % C, c='k')
        ax.set_xlim(x_min, x_max)
        ax.set_ylim(y_min, y_max)
        ax.set_xticks(())
        ax.set_yticks(())",day3-machine-learning/plots/plot_linear_svc_regularization.py,aphearin/AstroHackWeek2015,1
"
Xtrain, ytrain, Xtest, ytest = cub.get_train_test(feature_extractor.extract_one)

print Xtrain.shape, ytrain.shape
print Xtest.shape, ytest.shape

from sklearn import svm
from sklearn.metrics import accuracy_score

a = dt.now()
model = svm.LinearSVC(C=0.0001)
model.fit(Xtrain, ytrain)
b = dt.now()
print 'fitted in: %s' % (b - a)

a = dt.now()
predictions = model.predict(Xtest)
b = dt.now()
print 'predicted in: %s' % (b - a)
",src/scripts/classify_simple.py,yassersouri/omgh,1
"#### Libraries
# My libraries
import mnist_loader 

# Third-party libraries
from sklearn import svm

def svm_baseline():
    training_data, validation_data, test_data = mnist_loader.load_data()
    # train
    clf = svm.SVC()
    clf.fit(training_data[0], training_data[1])
    # test
    predictions = [int(a) for a in clf.predict(test_data[0])]
    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))
    print ""Baseline classifier using an SVM.""
    print ""%s of %s values correct."" % (num_correct, len(test_data[1]))

if __name__ == ""__main__"":
    svm_baseline()",neural-networks-and-deep-learning/src/mnist_svm.py,the-deep-learners/study-group,1
"	print testX.shape

	#save the new featurset for further exploration
	np.save('trainX_feat', trainX)
	np.save('testX_feat', testX)
	np.save('trainY_feat', trainY)
	np.save('testY_feat', testY)	

	#fit the svm model and compute accuaracy measure
	clf = KNeighborsClassifier(n_neighbors=15, weights='distance')
	#clf = svm.SVC(kernel=arc_cosine)
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))

",kpcaWithTreeFS/mnistBackImage/mnistIMAGEwithFS.py,akhilpm/Masters-Project,1
"# Load the Spam Email dataset
# You will have X, y in your environment
data = scio.loadmat('spamTrain.mat')
X = data['X']
y = data['y'].flatten()

print('Training Linear SVM (Spam Classification)')
print('(this may take 1 to 2 minutes)')

c = 0.1
clf = svm.SVC(c, kernel='linear')
clf.fit(X, y)

p = clf.predict(X)

print('Training Accuracy: {}'.format(np.mean(p == y) * 100))

# ===================== Part 4: Test Spam Classification =====================
# After training the classifier, we can evaluate it on a test set. We have
# included a test set in spamTest.mat",machine-learning-ex6/ex6/ex6_spam.py,nsoojin/coursera-ml-py,1
"			clf = svm.SVR()
		elif (model_prefered==""Gaussian""):
			clf = GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
		elif (model_prefered==""Gradient""):
			clf = GradientBoostingRegressor(n_estimators=100)
		else:
			clf = ensemble.RandomForestRegressor(n_estimators=100)
	else:
		#print ""classifier""
		if (model_prefered == ""SVM""):
			clf = svm.SVC()
		elif (model_prefered==""Gaussian""):
			exit() #cannot use gaussian for classification
			clf = GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
		elif (model_prefered==""Gradient""):
			clf = GradientBoostingClassifier(n_estimators=100)
		else:
			clf = ensemble.RandomForestClassifier(n_estimators=100)
	clf.fit(training_x, training_y)
	#print ""Done training""",test.py,Basvanstein/IARI,1
"	try:
		clf = joblib.load(filePath)
		print ""using trained model""		
	except:
		print ""building svm model""
		X = loadDesc(setting)
		X = X.astype('float')
		timer = Timer()	

		timer.tic()
		clf = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y)
		timer.toc()
		print timer.total_time

		joblib.dump(clf, filePath)

	# TEST
	# print clf.decision_function(X[0])
	# print clf.predict(X[5000])
	return clf",train_model.py,bubae/gazeAssistRecognize,1
"from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.svm import LinearSVC

from modules.tokenizer import ngrams_tokenizer
from modules.cleaner import clean
from modules.similarity import *

print('PROGRESS: Initializing...')

count_vect = CountVectorizer(preprocessor=clean)
clf = LinearSVC(max_iter=10000)
splitBy = 20
calculations = [
    Cosine(),
    Dice(),
    Jaccard(),
    Overlap(),
]
thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
ngrams = 1",birch_svm_10folds_cv.py,dwiajik/twit-macet-mining-v3,1
"tstdata = tst[:,0:65]
trnlabs = trn[:,-2]
tstlabs = tst[:,-2]

model = pca.PCA(n_components=2)
model.fit(trndata)
Ptrndata = model.transform(trndata)
Ptstdata = model.transform(tstdata)

nn = 2        
#clf = svm.LinearSVC()
clf = neighbors.KNeighborsClassifier(nn, weights='uniform')
clf.fit(Ptrndata,trnlabs)
pred = clf.predict(Ptstdata)
pcm = np.zeros((2,2))
for p,t in zip(pred,tstlabs):
    pcm[t,p] += 1
uwr = np.sum(np.diag(pcm)/np.sum(pcm,axis=1))/2
print uwr
        ",src/ex_phonemes.py,shahmohit/pynca,1
"    except:
        continue
    x = [0]*F_SIZE
    for item in entry.split(' '):
        index,level = item.split(':')
        x[int(index)] = float(level)
    data.append(x)
    cls.append(y)

#train model
clf = svm.SVC()
clf.fit(data,cls)

#save model
fout = open('model.dat','w')
pickle.dump(clf,fout)
fout.close()

",alg/SVC_train.py,zchgeek/wlan_positioning,1
"from nose.tools import *
from sklearn import svm
from sklearn import decomposition
from book_classification.tests.books import *


def test_ClassificationModel():
    tokenizer = bc.BasicTokenizer()
    extractor = bc.FrequenciesExtractor(tokenizer)
    classification_model = bc.ClassificationModel(
        extractor, decomposition.TruncatedSVD(10), svm.SVC())
    classification_model.fit(trainingCollection)
    classification_results = classification_model.predict(testingCollection)
    eq_(classification_results._expected, classification_results._predicted)


def test_ClassificationResults():
    pass",book_classification/tests/classification_test.py,alepulver/tp-final-incc,1
"
    def apply_logistic_regressor(self, X_train, y_train, X_test, y_test, C=1):
        clf = LogisticRegression(C=C)
        clf.fit(X_train, y_train)
        accuracy = clf.score(X_test, y_test)
        print(""Accuracy for Logistic Classifier %s"" % accuracy)
        return accuracy


    def apply_svc(self, X_train, y_train, X_test, y_test, kernel='linear', C=1):
        clf = SVC(kernel=kernel, C=C)
        clf.fit(X_train, y_train)
        accuracy = clf.score(X_test, y_test)
        print(""Accuracy for SVM Classifier %s"" % accuracy)
        return accuracy

    def apply_knn(self, X_train, y_train, X_test, y_test):
        clf = neighbors.KNeighborsClassifier()
        clf.fit(X_train, y_train)
        accuracy = clf.score(X_test, y_test)",Capstone/DataProcessor.py,abhipr1/DATA_SCIENCE_INTENSIVE,1
"from sklearn import model_selection
import validate_dataset as validate

# models structure
models = [
    ('LR',   'Logistic Regression',                 LogisticRegression()),
    ('LDA',  'Linear Discriminant Analysis',        LinearDiscriminantAnalysis()),
    ('KNN',  'K-Nearest Neighbors',                 KNeighborsClassifier()),
    ('CART', 'Classification and Regression Tree',  DecisionTreeClassifier()),
    ('NB',   'Gaussian Naive Bayes',                GaussianNB()),
    ('SVM',  'Support Vector Machines',             SVC()),
]

# number of splits dataset and times to evaluated
NUM_FOLDS = 10
# strategy to scoring
SCORING = 'accuracy'


def ranking():",app/select_model.py,lucasb/iris-machine-learning,1
"            self.grid_dictionary = {'C':0, 'gamma':1, 'fe_params':2}
            
        return param_grid, scores
    
    def set_params_list(self, learner_params, i):
        
        c = learner_params[0]
        gamma = learner_params[1]
        
        if self.method == 'classification':
            self.learner = SVC(kernel = self.kernel, cache_size=2000, 
                               C = c, gamma = gamma, probability = True)
        
        elif self.method == 'regression':
            self.learner = SVR(kernel = self.kernel, cache_size=2000, 
                               C = c, gamma = gamma, probability = True)
           
    def set_params_dict(self, learner_params):
        
        default_tolerance = 0.001",BCI_Framework/SVM_Learner.py,lol/BCI-BO-old,1
"    trainX = np.concatenate([trainXPos[ordersPos[len(trainXPos)/5:len(trainXPos)]], trainXNeg[ordersNeg[len(trainXNeg)/5:len(trainXNeg)]]])
    trainY = np.asarray([1] * (len(trainXPos)*4/5) + [-1] * (len(trainXNeg)*4/5))
    
    optimal_C = []
    optimal_cv_accuracy = 0

    C = 0.001
    MAX_C = 1000
    while C < MAX_C:

        clf = mysvm.SVC(C=C)
        clf.fit(trainX, trainY)

        train_Y = clf.predict(trainX)
        train_score = score(train_Y, trainY)
        cv_Y = clf.predict(cvX)
        cv_score = score(cv_Y, cvY)

        if cv_score > optimal_cv_accuracy:
            optimal_cv_accuracy = cv_score",solution/main.py,Shuailong/SVM,1
"
parser = argparse.ArgumentParser()
parser.add_argument(""train"", help=""training data filename"")
parser.add_argument(""test"", help=""testing data filename"")

args = parser.parse_args()

X_train, y_train = load_svmlight_file(args.train)
X_test, y_test = load_svmlight_file(args.test)

clf = LinearSVC()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))

y_pred = clf.predict(X_test)

print(unique(y_test))
print(unique(y_pred))
print(precision_recall_fscore_support(y_test, y_pred, labels=[-1, 0, +1], average='macro'))
print(precision_recall_fscore_support(y_test, y_pred, labels=[-1, 0, +1], average='micro'))",tools/helix-svm/scripts/svm.py,jvirtanen/helix,1
"                                                 scoring='roc_auc',
                                                 n_jobs=FLAGS.n_jobs,
                                                 max_iter=10000, verbose=1)
    elif FLAGS.model == 'random_forest':
        return ensemble.RandomForestClassifier(n_estimators=100,
                                               n_jobs=FLAGS.n_jobs,
                                               class_weight='balanced',
                                               verbose=1)
    elif FLAGS.model == 'svm':
        return grid_search.GridSearchCV(
            estimator=svm.SVC(kernel='rbf', gamma='auto',
                              class_weight='balanced'),
            param_grid={'C': np.logspace(-4, 4, 10)}, scoring='roc_auc',
            n_jobs=FLAGS.n_jobs, verbose=1)
    else:
        raise ValueError('Unrecognized model %s' % FLAGS.model)


def roc_enrichment(fpr, tpr, target_fpr):
    """"""Get ROC enrichment.""""""",paper/code/models.py,skearnes/color-features,1
"print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
#Y_train = np_utils.to_categorical(y_train, nb_classes)
#Y_test = np_utils.to_categorical(y_test, nb_classes)
Y_train = [int(y) for y in y_train]
Y_test = [int(y) for y in y_test]

clf = SVC(kernel='rbf', probability=True, tol=0.1)
clf = clf.fit(X_train, y_train)

yt_predict = clf.predict_proba(X_train)
y_predict = clf.predict_proba(X_test)
print(log_loss(Y_train, yt_predict))
print(log_loss(Y_test, y_predict))
predict=[]
for yy in y_predict: predict.append(np.argmax(yy))
#print(predict)",mnist/sklearn/svm.py,choupi/NDHUDLWorkshop,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_12_2015_01.py,magic2du/contact_matrix,1
"        See SVC.__init__ for details.
    coef0 : float
        Optional parameter of kernel.
        See SVC.__init__ for details.
    degree : int
        Degree of kernel, if kernel is polynomial.
        See SVC.__init__ for details.
    """"""

    def __init__(self, C, kernel='rbf', gamma = 1.0, coef0 = 1.0, degree = 3):
        estimator = SVC(C=C, kernel=kernel, gamma = gamma, coef0 = coef0,
                degree = degree)
        Block.__init__(self)
        Model.__init__(self)
        super(DenseMulticlassSVM,self).__init__(estimator)

    def train_all(self, dataset):
        """"""
        If implemented, performs one epoch of training.
",pylearn2/models/svm.py,CIFASIS/pylearn2,1
"            clf = KNN(n_neighbors=120)
        
    elif model_name == ""rf"":
        clf = RF(n_estimators=1000,max_features=""auto"",max_depth=8,min_samples_split=10,min_samples_leaf=2)
        
    elif model_name == ""gbdt"":
        clf = GBDT(n_estimators=400,max_features=""auto"",max_depth=8,min_samples_split=10,min_samples_leaf=2)
        
    elif model_name == ""svm"":
        if feature == ""word"" or feature == ""length"":
            clf = svm.SVC(C=0.8,kernel='rbf',gamma=0.08)
        elif feature == ""structure"":
            clf = svm.SVC(C=0.1,kernel='rbf',gamma=0.08)
        else:
            sp = feature.split(',')
            if ""struct"" in sp and ""lsa"" in sp:
                clf = svm.SVC(C=0.9,kernel='rbf',gamma=0.08)
            else:
                clf = svm.SVC(C=3,kernel='rbf',gamma=0.08)
    else:",difficulty/learner.py,lavizhao/easy_learner,1
"ys[ys == -1] = 0


# Train the non-GP classifiers ------------------------------------------------

print(""\nLearning the support vector classifier"")
C_range = 10.0 ** np.arange(-2, 9)
gamma_range = 10.0 ** np.arange(-5, 4)
param_grid = dict(gamma=gamma_range, C=C_range)
cv = StratifiedKFold(y=y, n_folds=3)
grid = GridSearchCV(SVC(kernel='rbf', probability=True), param_grid=param_grid,
                    cv=cv, verbose=1)
grid.fit(x.T, y)
svc = grid.best_estimator_

print(""\nLearning the logistic regression classifier"")
lreg = LogisticRegression(penalty='l2')
lreg.fit(x.T, y)

",experiments/uspsbclass.py,NICTA/linearizedGP,1
"    negativelist.fill(-1.0)
    datanegative = np.c_[negatives[:,1], negatives[:,2], negativelist]
    newsample = np.concatenate((datanegative, datapositive), axis=0)
    np.random.shuffle(newsample)
    return newsample

def predictEinRBF(in_data, num, c, vs=-1.0): 
    sample = classifyPoints(num, in_data, vs)
    X = np.c_[sample[:,0], sample[:,1]]
    y = sample[:,2]
    clf = SVC(C = c, kernel = 'rbf', gamma=1.0)  
    clf.fit(X,y)
    #print(1-clf.score(X,y))
    #print(float(len(sample) - np.sum(np.sign(clf.predict(X)) == np.sign(sample[:,2])))/len(sample))
    return 1-clf.score(X,y)

def predictEoutRBF(in_data, out_data, num, c, vs=-1.0): 
    sample = classifyPoints(num, in_data, vs)
    out_sample = classifyPoints(num, out_data, vs)
    X = np.c_[sample[:,0], sample[:,1]]",Week 8/test.py,pramodh-bn/learn-data-edx,1
"
        # Split of dataset subset
        train = self.train[shuffle[:size]]
        train_targets = self.train_targets[shuffle[:size]]

        # Transform hyperparameters to linear scale
        C = np.exp(float(x[0]))
        gamma = np.exp(float(x[1]))

        # Train support vector machine
        clf = svm.SVC(gamma=gamma, C=C, random_state=self.rng)
        clf.fit(train, train_targets)

        # Compute validation error
        y = 1 - clf.score(self.valid, self.valid_targets)
        c = time.time() - start_time

        return {'function_value': y, ""cost"": c}
    
    @AbstractBenchmark._check_configuration",hpolib/benchmarks/ml/svm_benchmark.py,automl/HPOlib2,1
"import pandas
import numpy
from sklearn.svm import SVC

data = pandas.read_csv('./data/svm.csv', header=None)
objects = data.ix[:, 1:]
classes = data.ix[:, 0]

model = SVC(kernel='linear', C=100000, random_state=241)
model.fit(objects, classes)

answer = ' '.join(str(x + 1) for x in model.support_)

submission_file = open('submissions/svm/support_vectors.txt', 'w+')
submission_file.write(answer)
submission_file.close()

print(answer)",intro-into-ml/tasks/svm/svm.py,universome/courses,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,choldgraf/mne-python,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,nicproulx/mne-python,1
"    from sklearn.grid_search import GridSearchCV
else:
    from sklearn.model_selection import train_test_split, GridSearchCV

R_TOL = 1e-2


def test_imblearn_classification_scorers():
    X, y = make_blobs(random_state=0, centers=2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    clf = LinearSVC(random_state=0)
    clf.fit(X_train, y_train)

    # sensitivity scorer
    scorer = make_scorer(sensitivity_score, pos_label=None, average='macro')
    grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=scorer)
    grid.fit(X_train, y_train).predict(X_test)
    assert_allclose(grid.best_score_, 0.92, rtol=R_TOL)

    scorer = make_scorer(sensitivity_score, pos_label=None, average='weighted')",imblearn/metrics/tests/test_score_objects.py,chkoar/imbalanced-learn,1
"# Building SVC from database

FACE_DIM = (50,50) # h = 50, w = 50

# Load training data from face_profiles/
face_profile_data, face_profile_name_index, face_profile_names  = ut.load_training_data(""./Paragon/Drivers/Backend/m_lowNeurals/vInter/face_profiles/"")

print(""\n"", face_profile_name_index.shape[0], "" samples from "", len(face_profile_names), "" people are loaded"")

# Build the classifier
clf, pca = svm.build_SVC(face_profile_data, face_profile_name_index, FACE_DIM)


###############################################################################
# Facial Recognition In Live Tracking


DISPLAY_FACE_DIM = (600, 600) # the displayed video stream screen dimension
SKIP_FRAME = 2      # the fixed skip frame
frame_skip_rate = 0 # skip SKIP_FRAME frames every other frame",Protocols/vInter.py,Klaminite1337/Paragon,1
"from .tools import make_handcrafted_dataset


def plot_rbf_svm_parameters():
    X, y = make_handcrafted_dataset()

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, C in zip(axes, [1e0, 5, 10, 100]):
        ax.scatter(X[:, 0], X[:, 1], s=60, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='rbf', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)

    fig, axes = plt.subplots(1, 4, figsize=(15, 3))
    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):
        ax.scatter(X[:, 0], X[:, 1], s=60, c=np.array(['red', 'blue'])[y])
        svm = SVC(gamma=gamma, kernel='rbf', C=1).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""gamma = %f"" % gamma)",mglearn/plot_rbf_svm_parameters.py,amueller/advanced_training,1
"import helper as hlp
import pandas as pd
import sklearn.svm as sv

@hlp.timeit
def fitSVM(data):
    '''
        Build the SVM classifier
    '''
    # create the classifier object
    svm = sv.SVC(kernel='linear', C=20.0)

    # fit the data
    return svm.fit(data[0],data[1])

# the file name of the dataset
r_filename = '../../Data/Chapter03/bank_contacts.csv'

# read the data
csv_read = pd.read_csv(r_filename)",Codes/Chapter03/classification_svm_alternative.py,drabastomek/practicalDataAnalysisCookbook,1
"    return build_classifier(X, Y, trainer)

def build_classifier(X, Y, trainer):    
    steps = []
    if normalise:
        normaliser = skpp.StandardScaler()
        steps.append(('normaliser', normaliser))
    if select:
        #selector = VarianceThreshold(threshold=0.05) 
        # selector = PCA(n_components=""mle"")
        #selector = LinearSVC(penalty=""l1"", dual=False)
        #selector = RandomForestClassifier(n_jobs=-1, n_estimators=300)
        #selector = RFECV(estimator=LinearSVC(penalty=""l1"", dual=False))
        selector = LDA(n_components=9)
        print type(selector)
        steps.append(('selector', selector))

    steps.append(('classification', trainer))
    trainer = Pipeline(steps)
",project3/methods.py,ethz-nus/lis-2015,1
"    # Sane?
    assert X.shape[0] == chunks.shape[0], ""X and chunks length mismatch""       
    assert np.sum(chunks == -1) == 0, ""Chunks is malformed""  

# ----
# Classify
# ----

# ----
# Using RFE and linear SVM
clf = SVC(C=10, kernel=""linear"")
rfecv = RFECV(estimator=clf, step=1, cv=cv, scoring=""accuracy"")
rfecv.fit(X, y)
prediction = rfecv.predict(X)
print(""Optimal feature number {0}"".format(rfecv.n_features_))
print(""Feature ranks {0}"".format(rfecv.ranking_))
accs = accuracy_score(y, prediction)
#print(classification_report(y, prediction))
#print(""Overall accuracy: {0}, Chance: {1}."".format(
#    np.round(accuracy_score(y, prediction), decimals=2), ",meta/classify.py,parenthetical-e/wheelerexp,1
"# Store to my_dataset for easy export below.
my_dataset = data_dict

# ---------------------------------------------------------------------
# classifiers and hyper-parameters

clf_names = ['GaussianNB', 'LinearSVC', 'SVC',
             'DecisionTree', 'RandomForest',
             'KNeighbors', 'AdaBoost', 'LogisticRegression']

clf_list = [GaussianNB(), LinearSVC(), SVC(),
            DecisionTreeClassifier(), RandomForestClassifier(),
            KNeighborsClassifier(), AdaBoostClassifier(),
            LogisticRegression(max_iter=1000)]

classify_params_list = [my_params.gaussian_nb_params,
                        my_params.linear_svc_params,
                        my_params.svc_params,
                        my_params.decision_tree_params,
                        my_params.random_forest_params,",IdentifyFraudFromEnronEmails/poi_id.py,zhujun98/dataAnalysis,1
"elif folding == 'kfolding':
    cv = KFold(n=y.shape[0], k=n_folds)
elif folding == 'leaveoneout':
    n_folds[0] = y.shape[0]
    cv = LeaveOneOut(n=y.shape[0])
else:
    print(""unknown crossvalidation method!"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
fs_n = round(n_features * fs_n) / n_features
if fs_n == 100.00:
    fs = SelectKBest(f_classif, k=n_features)
else:",JR_toolbox/_skl_king_parallel.py,cjayb/kingjr_natmeg_arhus,1
"from sklearn.svm import LinearSVC
import numpy as np
from sklearn.externals import joblib
import pickle

def main():
    settings = get_settings_from_file(""spec.json"")
    print(settings)
    X = np.genfromtxt(settings.Input.X, delimiter=',', skip_header=1)
    Y = np.genfromtxt(settings.Input.Y, delimiter=',', skip_header=1)
    svc = LinearSVC(C=float(settings.Param.C), loss=settings.Param.loss, penalty=settings.Param.penalty)
    svc.fit(X,Y)
    # joblib.dump(svc, settings.Output.MODEL, cache_size=1e9)
    with open(settings.Output.MODEL, ""w"") as f:
        pickle.dump(svc, f)
    print(""Done"")

if __name__ == ""__main__"":
    main()
",modules/modeling/basic/linear_svc/main.py,dkuner/example-modules,1
"
tic = time.clock()

df = pd.read_csv('dataset/winequality-white.csv', header=0, sep=';')

X = df[list(df.columns)[:-1]]
y = df['quality']
print ""1""
X_train, X_test, y_train, y_test = train_test_split(X, y)
print ""splitted""
model_lin = svm.LinearSVC()
model_rbf = svm.SVC()
print ""made model""
model_lin.fit(X_train, y_train)
model_rbf.fit(X_train, y_train)
print ""fitting""
y_predict_lin = model_lin.predict(X_test)
y_predict_rbf = model_rbf.predict(X_test)
print ""predicted""
mse_lin = mean_squared_error(y_predict_lin, y_test)",svm-white.py,behrtam/wine-quality-prediction,1
"
    estimator = EstimatorWithFit()
    scorer = check_scoring(estimator, allow_none=True)
    assert_true(scorer is None)


def test_check_scoring_gridsearchcv():
    # test that check_scoring works on GridSearchCV and pipeline.
    # slightly redundant non-regression test.

    grid = GridSearchCV(LinearSVC(), param_grid={'C': [.1, 1]})
    scorer = check_scoring(grid, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    pipe = make_pipeline(LinearSVC())
    scorer = check_scoring(pipe, ""f1"")
    assert_true(isinstance(scorer, _PredictScorer))

    # check that cross_val_score definitely calls the scorer
    # and doesn't make any assumptions about the estimator apart from having a",scikit-learn-0.17.1-1/sklearn/metrics/tests/test_score_objects.py,RPGOne/Skynet,1
"    return X, y


def plot_rbf_svm_parameters():
    X, y = make_handcrafted_dataset()

    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, C in zip(axes, [1e0, 5, 10, 100]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])

        svm = SVC(kernel='rbf', C=C).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""C = %f"" % C)

    fig, axes = plt.subplots(1, 4, figsize=(15, 3))
    for ax, gamma in zip(axes, [0.1, .5, 1, 10]):
        ax.scatter(X[:, 0], X[:, 1], s=150, c=np.array(['red', 'blue'])[y])
        svm = SVC(gamma=gamma, kernel='rbf', C=1).fit(X, y)
        plot_2d_separator(svm, X, ax=ax, eps=.5)
        ax.set_title(""gamma = %f"" % gamma)",scikit/Chapter 2/figures/plot_rbf_svm_parameters.py,obulpathi/datascience,1
"
    # Shuffle the data and split up the request subset of the training data
    s_max = y_train.shape[0]
    shuffle = np.random.permutation(np.arange(s_max))
    train_subset = X_train[shuffle[:s]]
    train_targets_subset = y_train[shuffle[:s]]

    # Train the SVM on the subset set
    C = np.exp(float(x[0]))
    gamma = np.exp(float(x[1]))
    clf = svm.SVC(gamma=gamma, C=C)
    clf.fit(train_subset, train_targets_subset)
    
    # Validate this hyperparameter configuration on the full validation data
    y = 1 - clf.score(X_val, y_val)

    c = time.time() - start_time

    return y, c
",examples/example_fabolas.py,numairmansur/RoBO,1
"
    def __init__(self, X_whole, y_whole, score_func, k=None):
        self.X_whole = X_whole
        self.y_whole = y_whole

        self.score_func = score_func
        self.k = k
        self.kbest = None
        self.kbest_unfitted = True

        self.svc = SVC(kernel='linear', C=1, verbose=False)  # TODO C=1 linear / rbf ??

    def fit(self, X, y):
        if self.kbest_unfitted:
            self.kbest = SelectKBest(score_func=self.score_func, k=self.k)
            self.kbest.fit(self.X_whole, self.y_whole)
            self.kbest_unfitted = False

        X_new = self.kbest.transform(X)
",scripts/util.py,Rostlab/LocText,1
"from sklearn import datasets
from sklearn import svm


def main():
    iris = datasets.load_iris()

    X_train, X_test, y_train, y_test = train_test_split(
        iris.data, iris.target, test_size=0.4, random_state=0)

    clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
    print(""Accuracy score on the IRIS dataset using a 60/40 split: {}""
          .format(clf.score(X_test, y_test)))

if __name__ == '__main__':
    print(""This is not printed when run with PEX ..."")
    main()",examples/iris-classification/src/classify_iris/run.py,sixninetynine/pygradle,1
"
###############################################################################
# Classification with linear discrimant analysis

from sklearn.lda import LDA  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from sklearn import svm

# Assemble a classifier
svc = LDA()
#svc = svm.SVC(decision_function_shape=""ovr"")
#svc = svm.SVC(kernel='rbf')
csp = CSP(n_components=4, reg=None, log=True)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()
epochs_data_train = epochs_train.get_data()
",plot_csp_patterns.py,jpirsch/myBCI,1
"    trainY = np.array(trainY)
    testY = np.array(testY)
    print (trainY.shape , testY.shape)
    np.savetxt('ohsumed_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+ '_train_output_labels.txt', trainY, fmt = '%s')
    np.savetxt('ohsumed_' + str(MAX_SEQUENCE_LENGTH) + ' D_' + str(Classes)+ '_test_output_labels.txt', testY, fmt = '%s')
    return trainY, testY 


def evaluate_with_SVM(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting SVM"")
    clf = svm.SVC(kernel='linear')
    clf.fit(train_X, train_Y)
    predict_Y = clf.predict(test_X) 
    s=metrics.accuracy_score(test_Y, predict_Y) 
    print (""SVM Testing Acc: "", s)
    return s


def evaluate_with_KNN(data, labels, train_X, train_Y,test_X, test_Y):
    print (""Starting KNN"")",Model/Ohsumed/ohsumed_LSTM-SVM_CV.py,irisliu0616/Short-text-Classification,1
"                from Tribler.Core.Video.VideoSource import PiecePickerSource

                self.picker = PiecePickerSource(self.len_pieces, config['rarest_first_cutoff'], 
                             config['rarest_first_priority_cutoff'])
            elif self.play_video:
                # Jan-David: Start video-on-demand service
                self.picker = PiecePickerVOD(self.len_pieces, config['rarest_first_cutoff'], 
                             config['rarest_first_priority_cutoff'], piecesize=self.piecesize)
            elif self.svc_video:
                # Ric: Start SVC VoD service TODO
                self.picker = PiecePickerSVC(self.len_pieces, config['rarest_first_cutoff'], 
                             config['rarest_first_priority_cutoff'], piecesize=self.piecesize)
            else:
                self.picker = PiecePicker(self.len_pieces, config['rarest_first_cutoff'], 
                             config['rarest_first_priority_cutoff'])
            
        except:
            print_exc()
            print >> sys.stderr,""BT1Download: EXCEPTION in __init__ :'"" + str(sys.exc_info()) + ""' '""
",Tribler/Core/BitTornado/download_bt1.py,egbertbouman/tribler-g,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.SVC(C=1., gamma=0.001, kernel='rbf', random_state=0)
clf.fit(X, y)

output = Porter(clf, language='c').export()
print(output)

""""""
#include <stdlib.h>
#include <stdio.h>
#include <math.h>",examples/classifier/SVC/c/basics.py,nok/sklearn-porter,1
"Y = iris.target


pca = decomposition.PCA(n_components=2)
pca.fit(X)
X = pca.transform(X)

h = 0.03  # step size in the mesh

# we create an instance of SVM and fit out data.
#clf = svm.SVC(kernel=my_kernel)
trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3)
testX = testX.astype(dtype = 'float32')
clf = svm.SVC(kernel=arc_cosine, C=0.05)
clf.fit(trainX, trainY)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1",arc_cosine/desicionBoundary/db_arcCosine.py,akhilpm/Masters-Project,1
"from sklearn import datasets, svm

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
svc = svm.SVC(C=1, kernel='linear')
score = svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])
print(score)


# split the data in folds that we use for training and testing
# KFold cross validation
import numpy as np
X_folds = np.array_split(X_digits, 3)
y_folds = np.array_split(y_digits, 3)",sklearnLearning/modelSelection/scoreAndCress-validated.py,zhuango/python,1
"    _ = joblib.dump(model, path, compress=9)

def load_model(path):
    """"""
    Load trained model from file
    :param path:
    :return:
    """"""
    return joblib.load(path)

CLASSIFIERS = [DummyClassifier(), GaussianNB(), LinearSVC(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]
PARAMETERS = [{}, {}, {},{'kernel':('linear', 'rbf', 'poly', 'sigmoid')},{}, {}]",solution/application/modules/ml_utils.py,ssip16teamb/ssip16teamb.github.io,1
"    return train_sizes, train_scores, test_scores


if __name__ == ""__main__"":
    from sklearn.datasets import load_digits
    from sklearn.svm import SVC
    import time

    digits = load_digits()
    tic = time.time()
    plot_learning_curves(SVC(gamma=0.01), digits.data, digits.target, steps=5,
                         n_jobs=-1)
    print(""Computed and plotted learning curves in %0.3fs""
          % (time.time() - tic))
    plt.show()",oglearn/evaluation.py,ogrisel/oglearn,1
"from tools import ProgressIterator, DataML, classified_error, allexcept, a_vs_b, output

from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score, KFold

import numpy as np 

def trial_all_except(training_set, testing_set, digit, kernel, c, degree=None):
    training_set, testing_set = allexcept(digit, training_set, testing_set)
    if degree == None:
        svm = SVC(kernel=kernel, C=c)
    else:
        svm = SVC(kernel=kernel, C=c, degree=degree)
    svm.fit(training_set.z, training_set.y)
    training_predicted = svm.predict(training_set.z)
    in_sample_error = classified_error(training_predicted, training_set.y)
    testing_predicted = svm.predict(testing_set.z)
    out_of_sample_error = classified_error(testing_predicted, testing_set.y)
    return svm.n_support_, (in_sample_error, out_of_sample_error)
",ass8/hw8.py,zhiyanfoo/caltech-machine-learning,1
"ys = np.array([-1]*N)
sidx = random.sample(np.where(ytrue == 0)[0], supevised_data_points/2)+random.sample(np.where(ytrue == 1)[0], supevised_data_points/2)
ys[sidx] = ytrue[sidx]

Xsupervised = Xs[ys!=-1, :]
ysupervised = ys[ys!=-1]
    
# compare models     
lbl = ""Purely supervised SVM:""
print lbl
model = sklearn.svm.SVC(kernel=kernel, probability=True)
model.fit(Xsupervised, ysupervised)
evaluate_and_plot(model, Xs, ys, ytrue, lbl, 1)

lbl =  ""S3VM (Gieseke et al. 2012):""
print lbl
model = scikitTSVM.SKTSVM(kernel=kernel)
model.fit(Xs, ys.astype(int))
evaluate_and_plot(model, Xs, ys, ytrue, lbl, 2)
",examples/compare_linsvm_methods.py,carloslds/semisup-learn,1
"    grid_search.transform(X)

    # Test exception handling on scoring
    grid_search.scoring = 'sklearn'
    assert_raises(ValueError, grid_search.fit, X, y)


@ignore_warnings
def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)
    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')
    grid_search.fit(X, y)

    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
                                        scoring='accuracy')
    # smoketest grid search",projects/scikit-learn-master/sklearn/tests/test_grid_search.py,DailyActie/Surrogate-Model,1
"	for k in p_d:
		param_d[ k] = p_d[ k]


def clst( X_train, y_train, X_test, y_test, nb_classes):
	model = tree.DecisionTreeClassifier()
	model.fit( X_train, y_train)
	dt_score = model.score( X_test, y_test)
	print( ""DT-C:"", dt_score)

	model = svm.SVC( kernel = 'linear')
	model.fit( X_train, y_train)
	sv_score = model.score( X_test, y_test)
	print( ""SVC:"", sv_score)

	model = kkeras.MLPC( [X_train.shape[1], 30, 10, nb_classes])
	model.fit( X_train, y_train, X_test, y_test, nb_classes)
	mlp_score = model.score( X_test, y_test)
	print( ""DNN:"", mlp_score)
",kcell_r2.py,jskDr/jamespy_py3,1
"
        start = time.clock()

        C_range = np.logspace(-2, 10, 13)
        gamma_range = np.logspace(-9, 3, 13)

        param_grid_rbf = dict(gamma=gamma_range, C=C_range)
        # cv = StratifiedKFold(target,n_folds=5,shuffle=True,random_state=42)
        # cv = ShuffleSplit(len(target),n_iter=5,test_size=0.1,random_state=42)
        cv = StratifiedShuffleSplit(target, n_iter=5, test_size=0.1, random_state=42)
        grid_rbf = GridSearchCV(SVC(), param_grid=param_grid_rbf, cv=cv)
        grid_rbf.fit(data, target)
        cost = time.clock()-start

        print(""RBF The best parameters are %s with a score of %0.2f""
              % (grid_rbf.best_params_, grid_rbf.best_score_))
        print ""cost_di:"", cost

        # param_grid_linear = dict(C=C_range)
        # grid_linear = GridSearchCV(SVC(kernel='linear'), param_grid=param_grid_linear, cv=cv)",ryu/app/reduce_t/active_pair_selector.py,Zouyiran/ryu,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0625_2015.py,magic2du/contact_matrix,1
"    df = sm.datasets.get_rdataset(""Guerry"", ""HistData"").data
    smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=df).fit()


def test_scikit_learn(df):

    sklearn = import_module('sklearn')  # noqa
    from sklearn import svm, datasets

    digits = datasets.load_digits()
    clf = svm.SVC(gamma=0.001, C=100.)
    clf.fit(digits.data[:-1], digits.target[:-1])
    clf.predict(digits.data[-1:])


def test_seaborn():

    seaborn = import_module('seaborn')
    tips = seaborn.load_dataset(""tips"")
    seaborn.stripplot(x=""day"", y=""total_bill"", data=tips)",pandas/tests/test_downstream.py,Winand/pandas,1
"

def test_Pipeliner_simple_experiment():

    X, y = make_classification()

    pipeline0 = Pipeline([('Scaler', MinMaxScaler()),
                          ('Classifier', LogisticRegression())])

    pipeline1 = Pipeline([('Scaler', MinMaxScaler()),
                          ('Classifier', SVC())])

    pipeline2 = Pipeline([('Scaler', StandardScaler()),
                          ('Classifier', LogisticRegression())])

    pipeline3 = Pipeline([('Scaler', StandardScaler()),
                          ('Classifier', SVC())])

    param_grid_LR = {'Classifier__penalty': ['l1',
                                             'l2']}",reskit/tests/test_core.py,neuro-ml/reskit,1
"        D[f] = data[instance][f]
    mapping.append(D)
data_enc = DictVectorizer(sparse=False)
encoded_data = data_enc.fit_transform(mapping)


X, y = encoded_data, encoded_labels

param_range = np.logspace(-7, 3, 5)
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name=""gamma"", param_range=param_range,    #Gamma is the kernel coefficient for SVM
    cv=10, scoring=""accuracy"", n_jobs=1)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.title(""Validation Curve with SVM"")
plt.xlabel(""$\gamma$"")
plt.ylabel(""Score"")",evaluation/valcurve.py,rebeccabilbro/orlo,1
"        self.labels = []
        self.pred = []
        self.clusters = word2vec.load_clusters(""corpora/Thaliana/documents-processed-clusters.txt"")
        self.posfmeasure = make_scorer(f1_score, average='binary', pos_label=True)
        self.generate_data(corpus, modelname, relationtype)
        self.text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3,20), min_df=0.0, max_df=0.7)),
                                  #('vect', CountVectorizer(ngram_range=(1,3), binary=False, max_features=None)),
                                  #('tfidf', TfidfTransformer(use_idf=True, norm=""l2"")),
                                  #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0001, n_iter=5, random_state=42)),
                                  #('clf', SGDClassifier())
                                  #('clf', svm.NuSVC(nu=0.01 ))
                                   #('clf', RandomForestClassifier(class_weight={False:1, True:2}, n_jobs=-1))
                                  ('clf', MultinomialNB(alpha=0.01, fit_prior=False))
                                  #('clf', DummyClassifier(strategy=""constant"", constant=True))
                                 ])

    def generate_data(self, corpus, modelname, pairtypes):
       # TODO: remove old model
        pcount = 0
        truepcount = 0",src/classification/rext/scikitre.py,AndreLamurias/IBEnt,1
"                    )}

#########################
# Creating Classifiers #
#########################

classifiers = {
                ""LREG"": LogisticRegression(C=1000, penalty=""l1"", dual=False),
                ""BernoulliNB"": BernoulliNB(alpha=.01),
                # ""svm_cv"": GridSearchCV(
                #     LinearSVC(penalty=""l1"", dual=False),
                #     [{'C': [0.0001, 0.001, 0.03, 0.1, 1, 3, 10, 100, 1000]}] #range of C coefficients to try
                #     ),
                ""LREG_CV"": GridSearchCV(
                    LogisticRegression(penalty=""l1"", dual=False),
                    [{'C': [0.0001, 0.001, 0.1, 1, 10, 100]}] #range of C coefficients to try
                    )
                # ""svc"": GridSearchCV(SVC(probability=True),
                #                     [{'C': [0.001, 0.03, 0.1, 1, 3, 10]}] #range of C coefficients to try
                #     )",run.py,lukovnikov/hatespeech,1
"	kernel, svm_c, chi2_sampler: parameters for the svm and the kernel

	Todo: use SGD Classifier
	  Problem: No multi-class probability support
	  Sln: check source version of sklearn? i think it might be in the dev version
	  frame_classifier = SGDClassifier(loss=""log"", penalty=""l2"")
	'''

	def __init__(self, kernel='rbf', svm_c=1000., chi2_sampler=1):
		self.chi2 = AdditiveChi2Sampler(chi2_sampler)
		self.clf = SVC(C=svm_c, kernel=kernel, probability=True)

	def fit(self, data, labels):
		'''
		data: a set of histograms for each sample
		labels: the truth for each sample
		'''
		data = [x/np.sum(x, dtype=np.float) for x in data]
		# data = [np.nan_to_num(x) for x in data]
		chi2_hists = self.chi2.fit_transform(data)",StructuredModels/models/CRF_Functions.py,colincsl/StructuredModels,1
"

import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)

scores = list()
scores_std = list()
for C in C_s:
    svc.C = C
    this_scores = cross_val_score(svc, X, y, n_jobs=1)
    scores.append(np.mean(this_scores))
    scores_std.append(np.std(this_scores))",examples/exercises/plot_cv_digits.py,mjudsp/Tsallis,1
"        'max_iter':-1, 
        'decision_function_shape':None, 
        'random_state':None
    }

    def __init__(
        self,data_block, predictors=[],cv_folds=10,
        scoring_metric='accuracy',additional_display_metrics=[]):

        base_classification.__init__(
            self, alg=SVC(), data_block=data_block, predictors=predictors,
            cv_folds=cv_folds,scoring_metric=scoring_metric, 
            additional_display_metrics=additional_display_metrics
            )
        
        self.model_output=pd.Series(self.default_parameters)
        self.model_output['Coefficients'] = ""-""

        #Set parameters to default values:
        self.set_parameters(set_default=True)",easyML/models_classification.py,aarshayj/easyML,1
"import time

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.svm import LinearSVC

from modules.cleaner import clean
categories = ['traffic', 'non_traffic']
count_vect = CountVectorizer(preprocessor=clean)
clf = LinearSVC(max_iter=10000)
cv = StratifiedKFold(n_splits=10, shuffle=True)

calculations = [
    'cosine',
    'dice',
    'jaccard',
    'overlap',
    'lcs',
]",svm_10folds_cv.py,dwiajik/twit-macet-mining-v3,1
"    predictions = dummy.predict(X_test)
    accuracy = accuracy_score(test_label_list,predictions)
    
    if verbose:
        print('Got baseline of %f with dummy classifier'%accuracy)

    return accuracy

def get_baseline_svm(dataset,train_label_list,test_label_list,verbose=True):
    (X_train, Y_train), (X_test, Y_test) = dataset
    linear = LinearSVC(penalty='l1',dual=False)
    grid_linear = GridSearchCV(linear, {'C':[0.1, 0.5, 1, 5, 10]})
    grid_linear.fit(X_train,train_label_list)
    accuracy = grid_linear.score(X_test, test_label_list)
    
    if verbose:
        print('Got baseline of %f with svm classifier'%accuracy)

    return accuracy
",pyScripts/kerasExperiments.py,andreykurenkov/emailinsight,1
"    ----------
    .. [1] `Wikipedia entry on the Hinge loss
            <http://en.wikipedia.org/wiki/Hinge_loss>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
         random_state=0, tol=0.0001, verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS
    0.30...",sklearn/metrics/metrics.py,loli/sklearn-ensembletrees,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_03_24_2015_parallel.py,magic2du/contact_matrix,1
"        num_data = 0
        num_correct = 0
        for train_index, test_index in loo:
            X_train, X_test = data_sets.train.inputs[train_index], data_sets.train.inputs[test_index]
            y_train, y_test = data_sets.train.labels[train_index], data_sets.train.labels[test_index]

            X_train, _, X_test = max_normalization(X_train, X_test, X_test)
            # X_train, _, X_test = maxall_normalization(X_train, X_test, X_test)
            # X_train, _, X_test = sum_normalization(X_train, X_test, X_test)

            clf = SVC()
            clf.fit(X_train, y_train)

            y_test_pred = clf.predict(X_test)

            if y_test == y_test_pred:
                num_correct = num_correct + 1

            num_data = num_data + 1
",cmarker/universal_marker/loocv_svm.py,bgshin/cmarker,1
"
X_train = all_train_data
y_train = all_train_targets
X_val = all_val_data
y_val = all_val_targets

X_train = X_train.reshape(len(X_train), -1)
X_val = X_val.reshape(len(X_val), -1)

# Train pipeline
bg = make_pipeline(StandardScaler(), SVC())
print(""Training..."")
bg.fit(X_train, y_train)

y_pred = bg.predict(X_train)
print(""Accuracy on training data"")
print(accuracy_score(y_train, y_pred))

y_pred = bg.predict(X_val)
print(""Accuracy on validation data"")",svm_test.py,kastnerkyle/kaggle-decmeg2014,1
"                count += 1
    else:
        for ele in unique_y:
            y[y == ele] = count
            count += 1

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of selected features
        idx = OSFS.OSFS(X[train], y[train])

        # obtain the dataset on the selected features
        selected_features = X[:, idx]
",skfeast/example/test_OSFS.py,jundongl/scikit-feast,1
"    result += ""positive"" if p == 1 else ""negative""
    result += "" - distance: %.5f"" % prediction
    return result

def get_svm_importances(coef):
    """"""Normalize the SVM weights.""""""
    factor = 1.0 / np.linalg.norm(coef)
    return (coef * factor).ravel()

if __name__ == ""__main__"":
    svm = LinearSVC(C=0.1)

    category = ""Faces""
    dataset = ""all""
    datamanager = CaltechManager()
    datamanager.PATHS[""RESULTS""] = os.path.join(datamanager.PATHS[""BASE""], ""results_Faces_LinearSVC_normalized"")
    vcd = VisualConceptDetection(svm, datamanager)

    clf = vcd.load_object(""Classifier"", category)
    importances = get_svm_importances(clf.coef_)",svm_visualization.py,peret/visualize-bovw,1
"

print 'Training...'
#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(n_estimators=100)#0.8035

#from sklearn.neighbors import KNeighborsClassifier  
#clf=KNeighborsClassifier(n_neighbors=7)#0.7800

from sklearn import svm   
clf=svm.SVC(C=10,gamma=0.0029)#0.7867
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))

#from sklearn.naive_bayes import GaussianNB      #nb for 高斯分布的数据
#clf=GaussianNB() #0.7609       

#from sklearn.linear_model import LogisticRegression
#clf = LogisticRegression(C=1,penalty='l2',tol=0.0001)#0.7833",my_1.py,zlykan/my_test,1
"
from masque import datasets
from masque.transform import PatchTransformer
from masque.rbm import GaussianBernoulliRBM
from masque.playground.runners import plain_classify, pretrain_classify



rbm_svc = {
    'pretrain_model' : BernoulliRBM(n_components=128, verbose=True),
    'model' : SVC(kernel='linear', verbose=True),
    'pretrain_data' : lambda: datasets.cohn_kanade_shapes(),
    'data' : lambda: datasets.cohn_kanade_shapes(labeled_only=True),
}

# TODO: sample
dbn_svc = {
    'pretrain_model' : Pipeline([        
        ('rbm0', BernoulliRBM(n_components=60, verbose=True)),
        ('rbm1', BernoulliRBM(n_components=80, verbose=True)),",masque/playground/landmark_based.py,dfdx/masque,1
"i_STTRB  = i_STR
i_STTRH  = i_STR
i_STUR   = i_STR
i_STURB  = i_STR
i_STURH  = i_STR

def i_SMC(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    ext('EXCEPTION.EL3 %s'%i.imm,size=pc.size).call(fmap)

def i_SVC(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    ext('EXCEPTION.EL1 %s'%i.imm,size=pc.size).call(fmap)

def i_SYS(i,fmap):
    fmap[pc] = fmap[pc]+i.length
    logger.warning('semantic undefined for %s'%i.mnemonic)

def i_SYSL(i,fmap):
    fmap[pc] = fmap[pc]+i.length",amoco/arch/arm/v8/asm64.py,x86Labs/amoco,1
"
## 2. predict
print reg.predict([[3, 3], [4, 4]])

# SVM
print get_model_msg(""SVM"")
from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]

clf = svm.SVC()
clf.fit(X, y)

print clf.predict([[2.,2.]])

# Naive Bayes
print get_model_msg(""Navie Bayes"")
from sklearn import datasets
iris = datasets.load_iris()
from sklearn.naive_bayes import GaussianNB",scikit-learn/glm.py,WaysonKong/blog,1
"    X = mat['X']    # data
    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the idx of selected features from the training set
        idx = svm_backward.svm_backward(X[train], y[train], n_features)

        # obtain the dataset on the selected features
        X_selected = X[:, idx]
",skfeast/example/test_svm_backward.py,jundongl/scikit-feast,1
"y = data.iloc[:,4].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

# standard Scaling the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Implemeting Logistic Reg
clf = SVC(kernel='linear', random_state=0)
clf.fit(X_train, y_train)

# predict
pred_y = clf.predict(X_test)

# confusion prediction
cm = confusion_matrix(y_test, pred_y)
print(""Confusion Matrix:\n"",cm)
",ML_A2Z/10_SVM_Classifier.py,atulsingh0/MachineLearning,1
"    sel = SelectKBest(chi2, k=n)
elif (selector == ""kbest_anova""):
    sel = SelectKBest(f_classif, k=n)
elif (selector == ""rfecv""):
    sel = RFECV()
elif (selector == ""lasso""):
    sel = SelectFromModel(LassoCV(), threshold=0.005)
elif (selector == ""rlregr""):
    sel = RandomizedLogisticRegression()
elif (selector == ""svm""):
    sel = eval( ""SelectFromModel(LinearSVC(%s))"" % (args.selector_params) )
elif (selector == ""extra_trees""):
    sel = SelectFromModel(ExtraTreesClassifier())
elif (selector == ""random_forest""):
    sel = SelectFromModel(RandomForestClassifier())

print sel.estimator
if (type(sel) == SelectFromModel and type(sel.estimator) == LassoCV):
    sel.fit(Xtr, Ytr)
    top_ranked = sorted(enumerate(sel.estimator_.coef_), key=lambda x:x[1], reverse=True)",mlfix/scripts/scikit-f_select.py,varisd/school,1
"if MNIST :
    test_LogisticRegression_mnist(logging = False).process()
# - Test FNN
test_FNN(logging = False).process()
if IRIS :
    test_FNN_iris(logging = False).process()
if MNIST:
    test_FNN_mnist(logging = False).process()
# - Test SVM
test_SVM(logging = False).process()
test_SVC(logging = False).process()
if IRIS:
    test_SVC_iris(logging = False).process()
if MNIST:
    test_SVC_mnist(logging = False).process()

# 1-3. Sequential variables
# - Test HMM
test_HMM(logging = False).process()
test_HMM_BaumWelch(logging = False).process()",test_pytrain/test_main.py,becxer/pytrain,1
"X =  tfv.transform(traindata) 
X_test = tfv.transform(testdata)

# Initialize SVD
svd = TruncatedSVD()

# Initialize the standard scaler 
scl = StandardScaler()

# We will use SVM here..
svm_model = SVC()

# Create the pipeline 
clf = pipeline.Pipeline([('svd', svd),
						 ('scl', scl),
                	     ('svm', svm_model)])

# Create a parameter grid to search for best parameters for everything in the pipeline
param_grid = {'svd__n_components' : [120, 140],
              'svm__C': [1.0, 10]}",code/BenchmarkBeat2_withcv.py,jasonwbw/tmp_game,1
"
        # weight components in FeatureUnion
        transformer_weights={
            'subject': 0.8,
            'body_bow': 0.5,
            'body_stats': 1.0,
        },
    )),

    # Use a SVC classifier on the combined features
    ('svc', SVC(kernel='linear')),
])

# limit the list of categories to make running this exmaple faster.
categories = ['alt.atheism', 'talk.religion.misc']
train = fetch_20newsgroups(random_state=1,
                           subset='train',
                           categories=categories,
                           )
test = fetch_20newsgroups(random_state=1,",Machine Learning/hetero_feature_union.py,samouri/kinja-api,1
"        X.append(text)
        y.append(syntax.lower())
    return (X, y)

if __name__ == '__main__':
    X, y = load_data()

    pipe = make_pipeline(
        CountVectorizer(ngram_range=(1, 1),
                        token_pattern='(?u)\\b\\w\\w+\\b|\:|\;|\""|\'|#'),
        LinearSVC()
        # TruncatedSVD(),
        # SGDClassifier()
    )

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
    pipe.fit(X_train, y_train)

    p = pipe.predict(X_val)
",clasificador.py,micabressan/trabajo_final,1
"    :param y_train: A dictionary with the following structure
            { instance_id : sense_id }

    :return: svm_results: a list of tuples (instance_id, label) where labels are predicted by LinearSVC
             knn_results: a list of tuples (instance_id, label) where labels are predicted by KNeighborsClassifier
    '''

    svm_results = []
    knn_results = []

    svm_clf = svm.LinearSVC()
    knn_clf = neighbors.KNeighborsClassifier()

    # implement your code here
    svm_clf.fit(X_train.values(), y_train.values())
    labels_svm = svm_clf.predict(X_test.values())
    i = 0
    for key in X_test:
        t = (key, labels_svm[i])
        svm_results.append(t)",A.py,hristo-vrigazov/Word-sense-disambiguator,1
"#result = eclf1.predict_proba(test_data)

#eclf2 = VotingClassifier(estimators=[('lr', clf1), ('gb', clf2), ('gnb', clf3)], voting='soft')

#clf = ExtraTreesClassifier(n_estimators=10000, verbose=1)
#clf.fit(train_data, train_label)
#result = clf.predict_proba(test_data)
#clf  = RandomForestClassifier(n_estimators=6000, max_depth = 4, verbose=1).fit(train_data, train_label)
#knn = neighbors.KNeighborsClassifier()
#logistic = linear_model.LogisticRegression()
#clf = svm.SVC(probability = True)
#clf = tree.DecisionTreeClassifier()


#print('KNN score: %f' % knn.fit(train_data, train_label).score(valid_data, valid_label))
#result = knn.fit(train_data, train_label).predict_proba(test_data)
#train_data = train_data[0:5000,:]
#train_label = train_label[0:5000]
#result = logistic.fit(train_data, train_label).predict_proba(test_data)
#clf.fit(train_data, train_label)",liao/model.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"        'params': [
            {
                'solver': ['liblinear'],
                'penalty': ['l2'],
                'random_state': [77]
            },
        ]
    },
    {
        'name': 'SVM',
        'model': SVC(),
        'params': [
            {
                'kernel': ['poly'],
                'degree': [5],
                'random_state': [77]
            },
        ]
    },
    {",scripts/train_classifier.py,yohanesgultom/id-openie,1
"                preprocessor_list,
                [0.01, 0.1, 0.5, 1., 10., 50., 100.],
                [0.01, 0.1, 0.5, 1., 10., 50., 100., 'auto'],
                [0., 0.1, 0.5, 1., 10., 50., 100.]):
        features = input_data.drop('class', axis=1).values.astype(float)
        labels = input_data['class'].values

        try:
            # Create the pipeline for the model
            clf = make_pipeline(preprocessor,
                                SVC(C=C,
                                    gamma=gamma,
                                    kernel='sigmoid',
                                    coef0=coef0,
                                    random_state=324089))
            # 10-fold CV score for the pipeline
            cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
            accuracy = accuracy_score(labels, cv_predictions)
            macro_f1 = f1_score(labels, cv_predictions, average='macro')
            balanced_accuracy = balanced_accuracy_score(labels, cv_predictions)",preprocessor_model_code/SVCSigmoid.py,rhiever/sklearn-benchmarks,1
"
	# separating the binary predictor into different arryays so the algo knows what to predict on
	X_train = np.array(train_df.drop(['UpDown'],1))
	y_train = np.array(train_df['UpDown'])
	X_test = np.array(test_df.drop(['UpDown'],1))
	y_test = np.array(test_df['UpDown'])
	
	#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.5)

	# performing the classifier
	clf = svm.SVC()
	clf.fit(X_train,y_train)

	accuracy = clf.score(X_test,y_test)

	# iterate and print average accuracy rate
	print accuracy
	accuracies.append(accuracy)	

	# test value",tinkering/ml/sklearn_svm.py,Darthone/Informed-Finance-Canary,1
"                        n_entities += 1
            logging.info(""Combined {} docs, {} sentences, {} entities"".format(n_docs, n_sentences, n_entities))
            base_result.save(options.models + "".pickle"")
        elif options.action == ""savetocorpus"":
            base_result.corpus.save(options.output + "".pickle"")
        elif options.action == ""train_ensemble"":
            pipeline = Pipeline(
                [
                    #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0001, n_iter=5, random_state=42)),
                    #('clf', SGDClassifier())
                     #('clf', svm.NuSVC(nu=0.01 ))
                    # ('clf', RandomForestClassifier(class_weight={False:1, True:1}, n_jobs=-1, criterion=""entropy"", warm_start=True))
                    #('clf', tree.DecisionTreeClassifier(criterion=""entropy"")),
                     #('clf', MultinomialNB())
                    #('clf', GaussianNB())
                    ('clf', svm.SVC(kernel=""rbf"", degree=2, C=1))
                    #('clf', DummyClassifier(strategy=""constant"", constant=True))
                ])
            print pipeline
            base_result.train_ensemble(pipeline, options.models, options.etype)",src/evaluate.py,AndreLamurias/IBEnt,1
"    #    for j in range(len(gram)):
    #        gram[i,j]=gram[i,j]/sqrt(gram[i,i]+gram[j,j])
    
    sc=[]
    for train_index, test_index in kf:
        #print(""TRAIN:"", train_index, ""TEST:"", test_index)
    
        #generated train and test lists, incuding indices of the examples in training/test
        #for the specific fold. Indices starts from 0 now
        
        clf = svm.SVC(C=c, kernel='precomputed',probability=True)
        train_gram = [] #[[] for x in xrange(0,len(train))]
        test_gram = []# [[] for x in xrange(0,len(test))]
          
        #generate train matrix and test matrix
        index=-1    
        for row in gram:
            index+=1
            if index in train_index:
                train_gram.append([gram[index,i] for i in train_index])",scripts/cross_validation_from_matrix_AUC_norm.py,nickgentoo/scikit-learn-graph,1
"
# We transition into the instruction arguments
def t_mulopcode_SPACEORTAB(t):
    r'[ \t]+'
    t.lexer.begin('mulinstr')
    return t


# SVC/SWI
@lex.TOKEN(r'(' + r'(?=[A-Z\t ])|'.join([k for k,v in instrInfos.exportInstrInfo.items() if v == instrInfos.InstrType.softinterrupt])+r'(?=[A-Z\t ]))')
def t_OPSVC(t):
    t.lexer.begin('svcopcode')
    t.lexer.currentMnemonic = t.value
    t.lexer.countArgs = 0
    t.lexer.instrType = instrInfos.InstrType.softinterrupt
    t.lexer.expectedArgs = 1
    t.lexer.suffixesSeen = set()
    return t

# We transition into the instruction arguments",tokenizer.py,mgard/epater,1
"classes = np.unique(y2)
for c in range(classes.shape[0]):
    sample_weight[y2 == classes[c]] = 1. / (np.sum(y2 == classes[c]))


###############################################################################
print(""PREPARE CLASSIFICATION"")


#-- classifier
clf = svm.SVC(kernel='linear', probability=True, C=svm_C)

#-- normalizer
scaler = Scaler()

#-- feature selection
if fs_n < 99.00:
    fs = SelectPercentile(f_classif, percentile=fs_n)
elif fs_n > 99 and fs_n < 101:
    fs = SelectKBest(f_classif, k=n_features)",JR_toolbox/_skl_king_parallel_201212.py,kingjr/natmeg_arhus,1
"from sklearn.svm.classes import LinearSVC

from ..Classifier import Classifier
from ...language.C import C


class LinearSVCCTest(C, Classifier, TestCase):

    def setUp(self):
        super(LinearSVCCTest, self).setUp()
        self.mdl = LinearSVC(C=1., random_state=0)

    def tearDown(self):
        super(LinearSVCCTest, self).tearDown()",tests/classifier/LinearSVC/LinearSVCCTest.py,nok/sklearn-porter,1
"norm_tst_data = (tst_data - tst_data.mean(axis=0)) / np.sqrt(tst_data.var(axis=0,ddof=1))
N = tst_data.shape[0]
D = trn_data.shape[1]
norm_tst_data0 = norm_tst_data[:N/2,:]
norm_tst_data1 = norm_tst_data[N/2:,:]

trn_labels = np.hstack(( np.zeros(Ntrn/2), np.ones(Ntrn/2) ))
tst_labels = np.hstack(( np.zeros(N/2), np.ones(N/2) ))
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)

output = {}
output['ldaerr'] = (1-sklda.score(norm_tst_data, tst_labels))
output['knnerr'] = (1-skknn.score(norm_tst_data, tst_labels))
output['svmerr'] = (1-sksvm.score(norm_tst_data, tst_labels))
",tests/poisson_synth.py,binarybana/samcnet,1
"            
            if vocab.has_key(term):
                print term, vocab[term], len(x)
                #i = vocab_list.index(term)
                x[vocab_list.index(term)] += 1
        X.append(x)
    print len(X[0]), ""# Features""
    classes = [item[2] for item in training_data]
    
    
    linsvm = svm.LinearSVC()
    Cs = range(1, 20)
    ssvm = GridSearchCV(estimator = linsvm, param_grid = dict(C=Cs), cv = 10)
    print len(X), len(X[0]), len(classes)
    ssvm.fit(X, classes)
    svmm = ssvm.best_estimator_
    
    f = open('svmm_model.pkl', 'w')
    f.write(pickle.dumps(svmm))
    f.close()",SVM/svm_code.py,ezeissler90/CSI_431,1
"	return build_classifier(X, Y, trainer)

def build_classifier(X, Y, trainer):	
	steps = []
	if normalise:
		X = cast_X_to_floats(X)
		normaliser = skpp.StandardScaler()
		steps.append(('normaliser', normaliser))
	if select:
		# selector = VarianceThreshold(threshold=0.05) 
		selector = LinearSVC(penalty=""l1"", dual=False)
		steps.append(('selector', selector))

	steps.append(('classification', trainer))
	trainer = Pipeline(steps)

	Xtrain, Xtest, Ytrain, Ytest = skcv.train_test_split(X, Y, train_size=0.75)
	classifier = trainer.fit(Xtrain, Ytrain)

	Ypred = trainer.predict(Xtest)",project2/forest.py,ethz-nus/lis-2015,1
"      self.xform_.fit(xScale)
   def transform(self,xTest):
      nRow = len(xTest)
      sc = self.scale_(xTest)
      xf = self.xform_.transform(self.scale_(xTest))  
      # return transformed plus original scaled columns
      return [ numpy.array(sc[i]).tolist() + numpy.array(xf[i]).tolist() for i in range(nRow) ]


# map of modelfactories (each takes a random state on instantiation)
model_dict = {""SVM"" : (lambda rs: sklearn.svm.SVC(kernel='rbf',gamma=0.001,C=10.0,probability=True,random_state=rs)),\
              ""Random Forest"" : (lambda rs: sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1,random_state=rs)),\
              ""Gradient Boosting"" : (lambda rs: sklearn.ensemble.GradientBoostingClassifier(random_state=rs)), \
              ""Decision Tree"" : (lambda rs: sklearn.tree.DecisionTreeClassifier(random_state=rs)), \
              ""Naive Bayes"" : (lambda rs: sklearn.naive_bayes.GaussianNB()), \
              ""Logistic Regression"" : (lambda rs: sklearn.linear_model.LogisticRegression(random_state=rs)), \
              ""KNN5"" : (lambda rs: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5))}


",ScoreModels/processArffs.py,WinVector/ExploreModels,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",Python/Lib/test/test_email/test_email.py,ArcherCraftStore/ArcherVMPeridot,1
"    df_input_target = df_input[list(range(0,1))].as_matrix()

    colors = numpy.random.rand(len(df_input_target))

    # splitting the data into training and testing sets
    from sklearn.cross_validation import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(df_input_data, df_input_target.tolist())

    # SVM
    from sklearn.svm import SVC
    svc = SVC(probability=True, max_iter=10000) # kernel='rbf'
    # svc = SVC(kernel='linear', probability=True, max_iter=10000)
    # svc = SVC(kernel='poly', probability=True, max_iter=10000)
    # svc = SVC(kernel='precomputed', probability=True, max_iter=10000)
    # svc = SVC(kernel='sigmoid', probability=True, max_iter=10000) #results best for sigmoid
    svc.fit(X_train[:],numpy.ravel(y_train[:]))
    predicted = svc.predict(X_test)

    print y_test[60:90] , len(y_test[60:90])
    print predicted[60:90] , len(predicted[60:90])",Code/Machine_Learning_Algos/10k_Tests/ml_classification_svm.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"class _NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,peterbraden/tensorflow,1
"if __name__ == '__main__':
    # Load dataset
    featureVecs, labels, numFeatures, numLabelTypes = loadArffDataset(
                                           'data/faces_vegetables_dataset.arff',
                                            normalise=True,
                                            displayData=True)

    # Construct all classifiers we wish to test, with 'standard' parameters
    classifiers = {
        'SVM':
            svm.SVC(kernel='linear', C=1),
        'Decision Tree':
            tree.DecisionTreeClassifier(criterion='gini', splitter='best'),
        'Feed-Forward Neural Network (Sigmoid)':
            mlp.Classifier(layers=[
                mlp.Layer('Sigmoid', units=numFeatures),
                mlp.Layer('Sigmoid', units=numLabelTypes),
            ],
            n_iter=100),
        'Gaussian Naive Bayes':",demo/spotcheck_facesvegetables.py,DonaldWhyte/intro-to-ml,1
"    accuracy = clf.score(X_test, y_test)
    
    return accuracy

def performSVMClass(X_train, y_train, X_test, y_test, parameters, fout, savemodel):
    """"""
    SVM binary Classification
    """"""
    # c = parameters[0]
    # g =  parameters[1]
    clf = SVC(kernel = 'linear', C = 0.025)
    clf.fit(X_train, y_train)

    if savemodel == True:
        #fname_out = '{}-{}.pickle'.format(fout, datetime.now().date())
        fname_out = fout+'.pickle'
        with open(fname_out, 'wb') as f:
            pickle.dump(clf, f, -1)    
    
    accuracy = clf.score(X_test, y_test)",smap_nepse/prediction/classify.py,samshara/Stock-Market-Analysis-and-Prediction,1
"dataset=np.zeros((1,c))
labels=np.zeros(1)

for l,i in feature:
	dataset=np.vstack((dataset,i))
	labels=np.hstack((labels,l))
dataset=np.delete(dataset,0,0)
labels=np.delete(labels,0)


clf=svm.LinearSVC()

labelValid={}
f=open(""./Validation/labels_valid.txt"")
for x in f.read().split(""\n""):
	d=x.split()
	if d!=[]:
		labelValid[d[0]]=d[1]

training=dataset",say-trees/tree-finder/TagMe/classify.py,ebayohblr2014/eBay-Opportunity-Hack-Blr-2014,1
"parser = argparse.ArgumentParser(description='Classify cluster.')
parser.add_argument( '-i', '--input', help = 'file we wish to perform classification on', required=True)
parser.add_argument( '-o', '--output', help = 'file where results are saved', required=True)

args = parser.parse_args()


# load X, load y
X,Y = load_svmlight_file(args.input)

clf = svm.SVC()
clf.fit(X,Y)

import pickle
",ranking/src/bin/svm.py,deathly809/research,1
"    
    saveName = '{}data/BWtrainingRF-{}.pkl'.format(baseDir,'C3D')
    with open(saveName,'w') as f:
            pickle.dump(clf,f)
            
    print 'training svms '
    Cs = [0.01,0.1,1,10];
    bestclf = {};
    bestscore = 0;
    for cc in Cs:
        clf = LinearSVC(C = cc)
        clf = clf.fit(xtrain, ytrain)
        preds = clf.predict(xval)
        correctPreds = preds == yval;
        score = 100*float(np.sum(correctPreds))/numSamples;
        print 'Overall Accuracy is ',score, '% ', ' C = ',str(cc)
        if score>bestscore:
            bestclf = clf
            bestscore = score
            ",processing/trainC3dFramewise.py,gurkirt/actNet-inAct,1
"
print ""Feature selection / dimensionality reduction (using training )...""
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
ch2 = SelectKBest(chi2, k=25000)
X_train_vect_red = ch2.fit_transform(X_train_vect, y_train)
X_test_vect_red = ch2.transform(X_test_vect)

print ""Training a Support Vector Machine (SVM) classifier using LinearSVC and the training dataset...""
from sklearn.svm import LinearSVC
clf = LinearSVC(C=6.5)
clf.fit(X_train_vect_red, y_train)

print ""Storing the predictions of the trained classifier on the testing dataset...""
predicted = clf.predict(X_test_vect_red)

#print ""Evaluation results of the content-based engine working alone, predicting the top-1 developer: ""
#
import numpy as np
#print ""Accuracy: %s"" % np.mean(predicted == y_test)",Archive/RecoDev-Evaluator/recodev_eval3.py,amirhmoin/recodev,1
"import matplotlib.pyplot as plt
import copy
import numpy as np
import pylab as pl

features_train, labels_train, features_test, labels_test = makeTerrainData()

########################## SVM #################################
# we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel=""linear"")

# now your job is to fit the classifier
# using the training features/labels, and to
# make a set of predictions on the test data

clf.fit(features_train, labels_train)

# store your predictions in a list named pred
",002 - SVM/exercise/main.py,mdegis/machine-learning,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_15_2015_02.py,magic2du/contact_matrix,1
"ax.scatter(x[:,0], x[:,1], x[:,2], c=yTr, cmap=matplotlib.cm.rainbow)


t0 = time.time()
clf_lr = LogisticRegression(C=2.0)
clf_lr.fit(xTr, yTr)
lr_score = clf_lr.score(xTe, yTe)
lr_t = time.time()-t0

t0 = time.time()
clf_svc = SVC()
clf_svc.fit(xTr, yTr)
svc_score = clf_svc.score(xTe, yTe)
svc_t = time.time()-t0


t0 = time.time()
clf_gp = GPClassifier(xTr, yTr, \
                   alpha=0.05, max_iter=100, num_inducing_points=1500, \
                   kernel_type='rbf', kernel_args={'gamma':2.0}, \",GPSVI/test/test20newsgroups.py,AlchemicalChest/Gaussian-Process-with-Stochastic-Variational-Inference,1
"    trainLabel5=npy.ravel(dataTemp[""labels""])
    
    trainData=npy.concatenate((trainData1,trainData2,trainData3,trainData4,trainData5),axis=0)
    trainLabel=npy.concatenate((trainLabel1,trainLabel2,trainLabel3,trainLabel4,trainLabel5))
    
    return (trainData, trainLabel,testData,testLabel)
        
if __name__==""__main__"":
    trainData,trainLabel,testData,testLabel=ReadCIFAR10Gist()
    print(""Training SVM..."")
    svmClf=SVC(10,""linear"")
    svmClf.fit(trainData,trainLabel)
    predLabel=svmClf.predict(testData)
    confMat,oa=EvalAccuracy(predLabel,testLabel,range(10))
    print(""Overall Accuracy=""+str(oa))
        ",TestImgClassify.py,galad-loth/DescHash,1
"features_train_transformed = vectorizer.fit_transform(protrainingdata)
features_test_transformed  = vectorizer.transform(testdata)



bagvectorizer = CountVectorizer()

bag_of_words = bagvectorizer.fit(protrainingdata)
bag_of_words = bagvectorizer.transform(protrainingdata)

clf = svm.SVC()

fulldata = len(protrainingdata)
halfdata = len(protrainingdata)/2
print (halfdata)
print fulldata
#newtrain = features_train_transformed[0:halfdata]
newtrain = bag_of_words[0:halfdata]

clf.fit(features_train_transformed  ,labels)",datatransformation.py,archiekey/AuroraBorealis,1
"    grid_search.transform(X)

    # Test exception handling on scoring
    grid_search.scoring = 'sklearn'
    assert_raises(ValueError, grid_search.fit, X, y)


@ignore_warnings
def test_grid_search_no_score():
    # Test grid-search on classifier that has no score function.
    clf = LinearSVC(random_state=0)
    X, y = make_blobs(random_state=0, centers=2)
    Cs = [.1, 1, 10]
    clf_no_score = LinearSVCNoScore(random_state=0)
    grid_search = GridSearchCV(clf, {'C': Cs}, scoring='accuracy')
    grid_search.fit(X, y)

    grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs},
                                        scoring='accuracy')
    # smoketest grid search",scikit-learn-0.18.1/sklearn/tests/test_grid_search.py,RPGOne/Skynet,1
"
# Perform Tf-Idf vectorization
nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)
idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')

# Scaling the words
stdSlr = StandardScaler().fit(im_features)
im_features = stdSlr.transform(im_features)

# Train the Linear SVM
clf = LinearSVC()
clf.fit(im_features, np.array(image_classes))

# Save the SVM
joblib.dump((clf, training_names, stdSlr, k, voc), ""bof.pkl"", compress=3)    

# cd to the file in cmd.exe and type python findFeatures.py -t dataset/train/
    ",findFeatures.py,faraz117/Traffic-Congestion-Estimator,1
"
            # LDA :
            if clf == 'lda' or clf == 0:
                clfObj = LinearDiscriminantAnalysis(
                    priors=priors, **kwargs)
                clfObj.lgStr = 'Linear Discriminant Analysis'
                clfObj.shStr = 'LDA'

            # SVM : ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable
            elif clf == 'svm' or clf == 1:
                clfObj = SVC(kernel=kern, probability=True, **kwargs)
                clfObj.lgStr = 'Support Vector Machine (kernel=' + kern + ')'
                clfObj.shStr = 'SVM-' + kern

            # Linear SVM:
            elif clf == 'linearsvm' or clf == 2:
                clfObj = LinearSVC(**kwargs)
                clfObj.lgStr = 'Linear Support Vector Machine'
                clfObj.shStr = 'LSVM'
",brainpipe/clf/utils/_classif.py,EtienneCmb/brainpipe,1
"    #vectorizer = HashingVectorizer(stop_words=stop_words,ngram_range=(1,2),analyzer=""char"", non_negative=True, norm='l2', n_features=2**18)
    #vectorizer = TfidfVectorizer(min_df=5,  max_features=None, strip_accents='unicode', analyzer='word',ngram_range=(2, 6), use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words = stop_words,token_pattern=r'\w{1,}',norm='l2')#default
    vectorizer = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode', analyzer='char',ngram_range=(1, 2), use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words = stop_words,token_pattern=r'\w{1,}',norm='l2')#default
    Xtrain, ytrain, Xtest,idx, sample_weight  = prepareDataset(seed=seed,nsamples=nsamples,vectorizer=vectorizer,cleanse=cleanse,useOnlyTrain=useOnlyTrain,doBenchMark=doBenchMark,computeWord2Vec=computeWord2Vec,computeFeatures=computeFeatures,computeSynonyma=computeSynonyma,computeKaggleDistance=computeKaggleDistance,computeKaggleTopics=computeKaggleTopics,addNoiseColumns=addNoiseColumns,concat=concat,doTFID=doTFID,doSeparateTFID=doSeparateTFID,vectorizeFirstOnly=vectorizeFirstOnly,stop_words=stop_words,computeSim=computeSim,doSVD=doSVD,doSVDseparate=doSVDseparate,doSVDseparate_2nd=doSVDseparate_2nd,standardize=standardize,useAll=useAll,concatTitleDesc=concatTitleDesc,doKmeans=doKmeans)
    sample_weight=None
    Xtrain.to_csv('./data/Xtrain.csv',index=False)
    pd.DataFrame(ytrain,columns=['class']).to_csv('./data/ytrain.csv',index=False)
    #model = Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), ('svd', TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)), ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=10.0, kernel='rbf', degree=3, gamma=0.001, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None))])

    #model = Pipeline([('reducer', TruncatedSVD(n_components=400)), ('scaler', StandardScaler()), ('model', model)])
    #model = Pipeline([('scaler', StandardScaler()), ('model', SVC(C=10,gamma='auto') )])
    #model = SVC(C=10,gamma=0.001)
    #model = Pipeline([('scaler', StandardScaler()), ('model', LinearSVC(C=0.01))])
    #model = Pipeline([('reducer', MiniBatchKMeans(n_clusters=400,batch_size=400,n_init=3)), ('scaler', StandardScaler()), ('SVC', model)])
    #model = Pipeline([('feature_selection', LinearSVC(penalty=""l1"", dual=False, tol=1e-3)),('classification', LinearSVC())])
    #model = LinearSVC(C=0.01)


    #model =  RandomForestClassifier(n_estimators=250,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=100)#0.627
    #model =  RandomForestClassifier(n_estimators=250,max_depth=None,min_samples_leaf=1,n_jobs=2,criterion='gini', max_features='auto')",competition_scripts/crowd.py,chrissly31415/amimanera,1
"    features_train = lda.transform(features_train)
    features_test = lda.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))

        # Fit on the whole data:",nytimes/step4_analysis_supervised_4(lda).py,dikien/Machine-Learning-Newspaper,1
"
class MyCodeManager(CodeManager):

    @classmethod
    def name(cls):
        return 'svc'

    def load(self):
        # raise NotImplemented
        print(""Entered worker"")
        clf = SVC()
        iris = datasets.load_iris()
        clf.fit(iris.data, iris.target_names[iris.target])
        return clf

    def evaluate(self, trained_algorithm, input):
        # raise NotImplemented
        return trained_algorithm.predict(input)

    def validate_input(self, input):",nanoservice/example/code_example.py,nave91/nanoservice,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",tools/project-creator/Python2.6.6/Lib/email/test/test_email_renamed.py,babyliynfg/cross,1
"		labelsTrain.append(all_images_class_labels[t])
	true_classes = []
	for t in selTest:
		true_classes.append(all_images_class_labels[t])
	if (not exists(conf.modelPath)) | OVERWRITE:
		if VERBOSE: print (str(datetime.now()) + ' training liblinear svm')
		if VERBOSE == 'SVM':
			verbose = True
		else:
			verbose = False
		clf = svm.LinearSVC(C=conf.svmC)
		if VERBOSE: print (clf)
		clf.fit(train_data, labelsTrain)
		with open(conf.modelPath, 'wb') as fp:
			dump(clf, fp)
	else:
		if VERBOSE: print (""loading old SVM model"")
		with open(conf.modelPath, 'rb') as fp:
			clf = load(fp)
",phow_crop_aug.py,lbarnett/BirdID,1
"train_data = pca.fit_transform(train_data)
# pca.fit(test_data)
test_data = pca.transform(test_data)


# ## SVM训练

# In[29]:

print('使用SVM进行训练...')
svc = SVC(kernel='rbf',C=2)
svc.fit(train_data, label)
print('训练结束.')


# In[30]:

print('对测试集进行预测...')
predict = svc.predict(test_data)
print('预测结束.')",competitions/digit_recognizer/digit_recognizer.py,lijingpeng/kaggle,1
"tstIndeces = randIndeces[maxSize:maxSize+maxSize]

trnTF = allDataGood[:26, trnIndeces]
trnZones = allDataGood[26, trnIndeces]

tstTF = allDataGood[:26, tstIndeces]
tstZones = allDataGood[26, tstIndeces]

# train SVM
print 'Train SVM'
clf = svm.SVC(C=1.0, gamma=0.1)
clf.fit(trnTF.T, trnZones)

# save SVM into file
pickle.dump(clf, open(svmFile, ""wb"" ))

# test SVM on small independent subsample
print 'Test on independent sample'
svmZones = clf.predict(tstTF.T)
",05train_svm.py,nansencenter/sentinel1ice,1
"
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

pca = decomposition.PCA(n_components=2)
pca.fit(X_train)

X_train_transformed = pca.transform(X_train_scaled)
X_test_transformed = pca.transform(X_test_scaled)

clf = svm.SVC(C=0.1, kernel='rbf')
clf.fit(X_train_transformed, y_train)

# print clf.score(X_train_transformed, y_train)
print clf.score(X_test_transformed, y_test)

'''
y_predict = clf.predict(X_test_transformed)

print metrics.accuracy_score(y_test, y_predict)",abalone.py,reetawwsum/Supervised-Learning,1
"data=zip(*map(lambda x: map(lambda y: float(y), x), csv_reader(data_file)))
## Stats for numeric domains
stats = map(base_stats, data)
#print stats
##
names = [""Nearest Neighbors"",  ""Linear SVM"", ""RBF SVM"", 
         ""Decision Tree"", ""Random Forest"", ""AdaBoost"", 
         ""Naive Bayes"", ""LDA""] #, ""QDA""]         
anova_filter = SelectFwe(chi2) #SelectKBest(chi2, k=4)
classifiers = [ KNeighborsClassifier(3), SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(), GaussianNB(), LDA()] #, QDA()]
##
X = zip(*data[:len(data)-1]); Y = data[len(data)-1]
X_trg = min_max_scaler.fit_transform(X[:TRG_SIZE]); Y_trg = Y[:TRG_SIZE]
X_tst = min_max_scaler.fit_transform(X[TRG_SIZE:]); Y_tst = Y[TRG_SIZE:]
for name, classifier in zip(names, classifiers):
    classifier.fit(X_trg, Y_trg)
    print '%s: %f' %(name, classifier.score(X_tst, Y_tst))",knn/scripts/exp1.py,xulesc/algos,1
"from sklearn.datasets import load_iris
from sklearn.cross_validation import StratifiedKFold, KFold
iris = load_iris()
X, y = iris.data, iris.target

print(cross_val_score(LinearSVC(), X, y, cv=KFold(len(X), 3)))
print(cross_val_score(LinearSVC(), X, y, cv=StratifiedKFold(y, 3)))",day3-machine-learning/solutions/cross_validation_iris.py,lvrzhn/AstroHackWeek2015,1
"def plot_curve(fpr, tpr, title):
    plt.plot(fpr, tpr)
    plt.plot(fpr, fpr)
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title(title)
    plt.show()


def train_svm(x_train, y_train, x_cv, y_cv):
    clf = SVC(probability=True)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf",examples/sara/titanic_sara_7.py,remigius42/code_camp_2017_machine_learning,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - Cparam:           SVM parameter C (cost of constraints violation)
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    svm = sklearn.svm.SVC(C = Cparam, kernel = 'linear',  probability = True)        
    svm.fit(X,Y)

    return svm

def trainSVM_RBF(features, Cparam):
    '''
    Train a multi-class probabilitistic SVM classifier.
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training
              See function trainSVM_feature() to use a wrapper on both the feature extraction and the SVM training (and parameter tuning) processes.",audioTrainTest.py,tyiannak/pyAudioAnalysis,1
"from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

svc = svm.SVC(C=1, kernel='linear')
k_fold = cross_validation.KFold(n = len(X_digits), n_folds = 3)
for train_indices, test_indices in k_fold:
    print(""Train: %s | test: %s"" % (train_indices, test_indices))

results = [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train , test in k_fold]
print(results)


# n_jobs=-1 means that the computation will be dispatched on all the CPUs of the computer.",sklearnLearning/modelSelection/cross-validationGenerators.py,zhuango/python,1
"from sklearn.grid_search import GridSearchCV
from openml import tasks, runs
import xmltodict
import numpy as np

def challenge():    
    ## use dev openml to run
    # Download task, run learner, publish results
    task = tasks.get_task(14951)
    
    ## clf = BaggingClassifier(SVC(), n_estimators = 128)
    
    '''
    clf = RandomForestClassifier(n_estimators = 128, class_weight = 'balanced_subsample')
    '''
    '''
    clf = BaggingClassifier(ExtraTreeClassifier(), n_estimators = 20)
    '''
    '''
    param_grid = {'max_depth': np.linspace(1, 15, num = 15, dtype = np.int64),",challenge.py,lidalei/DataMining,1
"X_cols = [""F"" + str(i+1) for i in range(0, 2048)]
y_cols = [""L"" + str(i+1) for i in range(0, 9)]

X = df[X_cols].values
y = df[y_cols].values

random_state = np.random.RandomState(2016)

# ----------setup models----------

svc = svm.SVC(
    cache_size=1000,
    kernel='linear',
    probability=True,
    random_state=random_state,
    C=0.10)
ovr_svc = OneVsRestClassifier(estimator=svc, n_jobs=1)

rfc = RandomForestClassifier(
    n_jobs=-1,",scripts/train_ml.py,bzshang/yelp-photo-classification,1
"import sklearn
import sklearn.feature_extraction
import sklearn.svm

df = pandas.read_csv(""../data/spam/SMSSpamCollection"", sep=""\t"", header=-1)

tf_transformer = sklearn.feature_extraction.text.TfidfVectorizer()
X = tf_transformer.fit_transform(df[1])
y = pandas.get_dummies(df[0], drop_first=True).values.ravel()

learner = sklearn.svm.LinearSVC()
scores = sklearn.cross_validation.cross_val_score(learner, X, y, cv=5, scoring=""accuracy"")
print(scores.mean())
scores = sklearn.cross_validation.cross_val_score(learner, X, y, cv=5, scoring=""recall"")
print(scores.mean())
scores = sklearn.cross_validation.cross_val_score(learner, X, y, cv=5, scoring=""precision"")
print(scores.mean())",tyrehug/exp/spamexp.py,charanpald/tyre-hug,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",imblearn/metrics/tests/test_classification.py,scikit-learn-contrib/imbalanced-learn,1
"
train = pd.read_csv(train_fn,header=0)

#Separate out label data and convert to numpy array
train_predictor_data = train.iloc[ :, 1:].values
train_target_data = train.iloc[ :, 0].values

#Train dat sucker
Cs = np.linspace(1,20,8)
gammas = np.linspace(1e-7, 1e-4, 8)
svc = svm.SVC()
params = dict(C=Cs,gamma=gammas)
train_size = num_fit
test_size = num_fit // 2 
cv = ShuffleSplit(len(train_predictor_data), test_size=test_size, train_size=train_size, random_state=0)
clf = GridSearchCV(estimator=svc, cv=cv, param_grid=params, n_jobs=-1)
clf.fit(train_predictor_data, train_target_data)

#Save model
barrel = open(model_fn, 'wb')",svm-grid.py,josh314/squinty,1
"if (X_train.size != len(exercise_train_text)):
    print ""Training sample size %d, Exercise Labels %d Do not match"" % ( X_train.size, len(exercise_train_text))
    sys.exit()    
   
lb = preprocessing.MultiLabelBinarizer()
Y = lb.fit_transform(exercise_train_text)

ex_classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])

ex_classifier.fit(X_train, Y)
# Test a little
#X_test = np.array([""400 meter run   Overhead squat 95 lbs x 15"",
#                   ""Snatch 135 pounds"",
#                  ""Run 1 mile"",
#                   ""Ring Dips"",
#                   ""push up, pull ups"",
#                   ""Box jumps"",",multilabel/cf_ml_trainer.py,praveen049/text_classification,1
"import numpy
import os
import sklearn.datasets as datasets
import sklearn.svm as svm
import sklearn.cross_validation as cross_validation
import sklearn.metrics as metrics
import sklearn.preprocessing as preprocessing
from tyrehug.settings import DATA_DIR, get_dir

learner = svm.SVC(kernel='linear', C=1)
data_dir = get_dir(os.path.join(DATA_DIR, ""mlbenchmark""))
print(data_dir)

def benchmark(learner, dataset, filename):
    num_folds = 5
    num_metrics = 4
    scores = numpy.zeros((num_folds, num_metrics))

    X = preprocessing.scale(dataset.data)",tyrehug/mlbenchmarks/mlbenchmark.py,charanpald/tyre-hug,1
"y = train_labels

# Prepare Classifier Training and Testing Data
print('\nPreparing Classifier Training and Testing Data...')
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.1)


# Pickle the Classifier for Future Use
print('\nSVM Classifier with gamma = 0.1; Kernel = polynomial')
print('\nPickling the Classifier for Future Use...')
clf = svm.SVC(gamma=0.1, kernel='poly')
clf.fit(X_train,y_train)

with open('MNIST_SVM.pickle','wb') as f:
	pickle.dump(clf, f)

pickle_in = open('MNIST_SVM.pickle','rb')
clf = pickle.load(pickle_in)

print('\nCalculating Accuracy of trained Classifier...')",2. SVM/svm.py,anujdutt9/Handwritten-Digit-Recognition-using-Deep-Learning,1
"		return self.target_names[self.pipeline.predict([featureset])[0]]
	
	@classmethod
	def train(cls, labeled_featuresets):
		train, target_labels = zip(*labeled_featuresets)
		target_names = sorted(set(target_labels))
		targets = [target_names.index(l) for l in target_labels]
		
		pipeline = Pipeline([
			('bow', BagOfWordsVectorizer()),
			('clf', LinearSVC(C=1000)),
		])
		
		pipeline.fit(train, targets)",nltk_trainer/classification/sci.py,mayk93/nltk-trainer,1
"            else:
                self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # Todo : because of probability = True => very slow due to
            # cross-validation. Need to improve the speed here
            model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=True,
                probability=True)

            model.fit(x_train, y_train)",lib/python2.7/site-packages/nltk/parse/transitionparser.py,akhilari7/pa-dude,1
"    # se excluyen las palabras vacias
    # uso 1-gramas, luego no tengo en cuenta el orden de las palabras, se toma cada token por
    # separado. El tokenizador incluido no tiene en cuenta ningun tipo de análisis semántico,
    # se puede implementar en un futuro creando una funcion que procesaria con spaCy cada
    # entrada nlp(sample)
    vectorizador = CountVectorizer(stop_words=STOPLIST, ngram_range=(1, 1))
    # TfidfVectorizer evita estadisiticamente toknes que se repitan mucho. Por ahora no
    # aporta mejoras

    # Algoritmo clasificador
    clasificador = svm.SVC(kernel='linear', C=1)
    # Otras opciones
    # SVC(kernel='rbf', gamma=0.7, C=1)
    # SVC(kernel='poly', degree=3, C=1)
    # SVC(kernel='linear', C=1)
    # LinearSVC(C=1)

    # Creación de la secuencia de procesos para la creación del modelo.
    # Se define una cadena de pares (nombre, objetos transformacion)
    # Es posible incorporar parámetros a la pipeline completa utilizando",SW/core/filtros/MC-C1/generadorModeloC1.py,aoltra/Maya_IDA,1
"# --- Hotmail ---
* ""by \S+\.hotmail\.msn\.com ""

  # from 213.123.174.21 by lw11fd.law11.hotmail.msn.com with HTTP; Wed, 24 Jul 2002 16:36:44 GMT
  ""^from ((?#IP)).*?by (\S+\.hotmail\.msn\.com) ""
  IP,BY

# --- Microsoft ---
* ""^from .+ by \S+ with Microsoft""

  # from inet-vrs-05.redmond.corp.microsoft.com ([157.54.6.157]) by INET-IMC-05.redmond.corp.microsoft.com with Microsoft SMTPSVC(5.0.2195.6624); Thu, 6 Mar 2003 12:02:35 -0800
  # from tthompson ([217.35.105.172] unverified) by mail.neosinteractive.com with Microsoft SMTPSVC(5.0.2195.5329); Tue, 11 Mar 2003 13:23:01 +0000
  ""^from (\S+) \(\[((?#IP))\](?: unverified)?\) by (\S+) ""
  HELO,IP,BY

  # from mail pickup service by mail1.insuranceiq.com with Microsoft SMTPSVC; Thu, 13 Feb 2003 19:05:39 -0500
  ""^from mail pickup service by \S+ ""
  None

  # from xxxx (IP) by YYYY (IP) with Microsoft SMTP Server (TLS) id 14.3.224.2; Thu, 24 Feb 2013 09:40:11 -6000",mantis_authoring/EmailObjectFunctions.py,siemens/django-mantis-authoring,1
"# y is filled with integers coding for the class to predict
# We must have X.shape[0] equal to y.shape[0]
X = [e.get_data()[:, data_picks, :] for e in epochs_list]
y = [k * np.ones(len(this_X)) for k, this_X in enumerate(X)]
X = np.concatenate(X)
y = np.concatenate(y)

from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score, ShuffleSplit

clf = SVC(C=1, kernel='linear')
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(X), 10, test_size=0.2)

scores = np.empty(n_times)
std_scores = np.empty(n_times)

for t in xrange(n_times):
    Xt = X[:, :, t]
    # Standardize features",examples/decoding/plot_decoding_sensors.py,christianbrodbeck/mne-python,1
"    feature_names = [Rfeature_names[i] for i in ch2.get_support(indices=True)]
    print(""done in %fs"" % (time() - t0))
    print()
    feature_names = np.asarray(feature_names)
    save_obj(ch2,path + Models[i]+""/ch2Model"")
    results = []

    print('=' * 80)
    print(""L2 penalty"")
    # Train Liblinear model
    clf = benchmark(LinearSVC(loss='l2', penalty=""l2"",dual=False, tol=1e-3))

    save_obj(clf,path + Models[i]+""/classifier"")
",Project/BaggedClassifierModeler.py,tpsatish95/Youtube-Comedy-Comparison,1
"from sklearn import tree, feature_extraction, svm
from sklearn.feature_extraction.text import CountVectorizer
from multiprocessing import Pool
import numpy as np
import datetime

import app.analytics.filterSentences as fl
import matplotlib.pyplot as plt
import networkx as nx
listOfYears = []
clf = svm.SVC(probability=True)
probs = []
titles = []
trainData = eval(open('/tmp/trainDoubleSet').readlines()[0])#open('trainDataTitle','r').readlines()
testData = open('/tmp/testDoubleSet','r').readlines()
B = None
#C
def train(features):
    features = [item for item in features if len(item[0]) != 0]
    feats = [item[0] for item in features]",classifier.py,JFriel/honours_project,1
"
### features_train and features_test are the features for the training
### and testing datasets, respectively
### labels_train and labels_test are the corresponding item labels
features_train, features_test, labels_train, labels_test = preprocess()

#########################################################
from sklearn.svm import SVC
from class_vis import prettyPicture
c = 10000
clf = SVC(kernel=""rbf"", C=c)

print(""RBF and c=%d""%c)
#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data

#slice training sets to 1% of original size
#features_train = features_train[:len(features_train)/100]
#labels_train = labels_train[:len(labels_train)/100]",svm/svm_author_id.py,ncfausti/udacity-machine-learning,1
"    elif name == 'GaussianNB':
        param_dist = {""priors"": [None]}
        clf = GaussianNB()
    elif name == 'QuadraticDiscriminantAnalysis':
        param_dist = {""priors"": [None],
                      ""reg_param"": [0.0, 0.01, 0.1, 0.9]}
        clf = QuadraticDiscriminantAnalysis()
    elif name == 'RBF_SVM':
        param_dist = {""C"": [0.1, 0.5, 1.0],
                      ""gamma"": [0.1, 0.5, 1.0, 2.0, 3.0, 'auto']}
        clf = SVC(random_state = random_state)
    elif name == 'LINEAR_SVM':
        param_dist = {""C"": [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]}
        clf = SVC(random_state = random_state, kernel=""linear"")
        
        
    cv_results = fitClassifier(X, y, clf, param_dist)
    if print_report:
        printReport(cv_results)
        ",src/classifier.py,yaricom/brainhash,1
"    
    # create mixed SVM
    meta_svm = metaSVM(regime_weights)
    
    # train on each regime
    for regime in [i for i in range( len(train_dict)/2 ) ]:
        # training data
        train_data, train_labels = normalize(train_dict[regime, ""data""]), train_dict[regime, ""states""]
        
        # make classifier
        curr_clf  = SVC( kernel='linear', probability=True, C=regime_c_terms[regime], cache_size=500 )
        #curr_clf = LogisticRegression( penalty='l1', fit_intercept=True, C=regime_c_terms[regime] )
        
        # train & add to meta SVM
        curr_clf.fit(train_data, train_labels)
        meta_svm.add_trained_SVM(curr_clf, regime)
    
    return meta_svm

###############################################################################",learn.py,rronen/SFselect,1
"from sklearn.calibration import CalibratedClassifierCV
from ml.clf.wrappers import SKL, SKLP
from ml.models import MLModel


class SVC(SKL):
    def prepare_model(self):
        from sklearn import svm

        reg = CalibratedClassifierCV(
            svm.LinearSVC(C=1, max_iter=1000), method=""sigmoid"")
        reg = reg.fit(self.dataset.train_data, self.dataset.train_labels)
        sig_clf = CalibratedClassifierCV(reg, method=""sigmoid"", cv=""prefit"")
        sig_clf.fit(self.dataset.validation_data, self.dataset.validation_labels)
        return self.ml_model(sig_clf)

    def prepare_model_k(self):
        from sklearn import svm
        
        model = CalibratedClassifierCV(",src/ml/clf/extended/w_sklearn.py,elaeon/ML,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.svm

import submissions
from data import *

svm = sklearn.svm.LinearSVC()
svm.fit(train, target)
pred = svm.predict(test)

submissions.save_csv(pred, ""support_vector_machine.csv"")",digit_recognizer/support_vector_machine.py,wjfwzzc/Kaggle_Script,1
"Y_real_test = load('../data/test_label.txt').flatten()


# PCA
pca = decomposition.PCA(n_components=ndim)
pca.fit(X)
X_red = pca.transform(X)
X_red_test = pca.transform(X_test)

# SVM 
svm = SVC(kernel=kern)
svm.fit(X_red, Y)
Y_test = svm.predict(X_red_test)

cnt, tot = 0, len(Y_test)
for i in range(len(Y_test)):
    if Y_test[i] == Y_real_test[i]:
        cnt += 1

print('svm: pca %d' % (ndim))",classical/pca-svm.py,MihawkHu/Gene_Chip,1
"            
        feature_row.append(difference)
        
    data[i,:] = feature_row

train_data,test_data,train_label,test_label = cross_validation.train_test_split(data,class_labels,test_size=0.3)

rf = RandomForestClassifier(n_estimators=101)
ada = AdaBoostClassifier(n_estimators=101)
gradboost = GradientBoostingClassifier(n_estimators=101)
svm = SVC()
gnb = GaussianNB()

classifiers = [rf,ada,gradboost,svm,gnb]
classifier_names = [""Random Forests"",""AdaBoost"",""Gradient Boost"",""SVM"",""Gaussian NB""]

for classifier,classifier_name in zip(classifiers,classifier_names):
    
    classifier.fit(train_data,train_label)
    predicted_label = classifier.predict(test_data)",PizzaDataModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"        # Possible alternative solvers:
        # parameters = {'loss':'modified_huber',  'n_jobs': -1}
        # solver = SGDClassifier

        # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’
        # parameters = {'kernel': 'rbf', 'probability': True}
        # solver = SVC

        # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’
        # parameters = {'kernel': 'linear', 'probability': True}
        # solver = OneVsRestClassifier(SVC(**parameters))  # XXX won't work because ** in parameters...

        self._model = solver(**parameters)
        self._dataSizes = options['dataSizes']
        self._tagField = options['tagField']
        self._modelFileName = options['modelFileName']
        self._parameters = options['trainParams']
        self._cutoff = options['cutoff']
        self._featCounterFileName = options['featCounterFileName']
        self._labelCounterFileName = options['labelCounterFileName']",Lang_Hungarian/resources/huntag3/trainer.py,dlt-rilmta/hunlp-GATE,1
"
###############################################################################
# Univariate feature selection with F-test for feature scoring
# We use the default selection function: the 10% most significant features
selectTopPercentile = True
selector = SelectPercentile(f_classif, percentile=65)
selector.fit(data, y.ravel())
###############################################################################

params = [{'C': [0.1, 1, 10, 20, 50, 70, 100, 1000], 'gamma': [0.08, 5, 10, 1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}]
clf = grid_search.GridSearchCV(svm.SVC(), params, n_jobs=8,cv=cross_validation.ShuffleSplit(data.shape[0], n_iter=10, test_size=0.2,random_state=0))
clf.fit(selector.transform(data) if selectTopPercentile else data, y.ravel())
#clf.fit(data,y.ravel())

print clf.grid_scores_
print clf.best_estimator_

Xtrain,Xtest,ytrain,ytest = cross_validation.train_test_split(data,y,test_size=0.2)
Xtrain.shape,ytrain.shape
Xtest.shape,ytest.shape",machinelearning.py,geniass/LiveVsStudioML,1
"    train_indices = []
    test_indices = []

    for indx in range(0, data_size):
        if indx in indices:
            train_indices.append(indx)
        else:
            test_indices.append(indx)

    # Train and Eval Model
    clf = svm.SVC()
    clf.fit(X,Y)

    y_predicted = clf.predict(X_test)
    y_true = Y
",Python/MachineLearning_SciKit.py,melissadale/MyToolbox,1
"#   paint(v, w)
#   print v.outOfSampleError(w, 1000)
#   writeV(points)
  v = readV()
  X = v.X[:,1:]
#   X = sklearn.preprocessing.normalize(X)
#   print X_norm
  Y = v.Y
#   Y = [(y+1)/2 for y in Y]
#   sum()
  clf = svm.SVC(kernel='linear', verbose=True, tol=1e-9, C=5.)
#   clf = svm.NuSVC(kernel='linear', verbose=True, tol=1e-9, nu=1e-6)
#   clf = svm.LinearSVC(verbose=True, tol=1e-9)
#   clf = svm.SVC(kernel='rbf', verbose=True, tol=1e-3)
  clf.fit(X, Y)
#   print Y
  print len(clf.support_vectors_)
#   print clf.support_vectors_
  print clf.dual_coef_
  # get the separating hyperplane",Final/Python/by_Mark_B2/hw7.py,JMill/edX-Learning-From-Data-Solutions-jm,1
"                 map(os.path.split,
                     map(os.path.dirname, labels)))  # Get the directory.
    fname = ""{}/reps.csv"".format(args.workDir)
    embeddings = pd.read_csv(fname, header=None).as_matrix()
    le = LabelEncoder().fit(labels)
    labelsNum = le.transform(labels)
    nClasses = len(le.classes_)
    print(""Training for {} classes."".format(nClasses))

    if args.classifier == 'LinearSvm':
        clf = SVC(C=1, kernel='linear', probability=True)
    elif args.classifier == 'GridSearchSvm':
        print(""""""
        Warning: In our experiences, using a grid search over SVM hyper-parameters only
        gives marginally better performance than a linear SVM with C=1 and
        is not worth the extra computations of performing a grid search.
        """""")
        param_grid = [
            {'C': [1, 10, 100, 1000],
             'kernel': ['linear']},",demos/classifier.py,cmusatyalab/openface,1
"	pr = io.param_reader(os.path.join(PAR_DIR, 'etc', '%s.yaml' % common_cfg.setdefault('mdl_cfg', 'mdlcfg')))
	filt_names = []
	for filt_name, filter in [
#		('Var Cut', VarianceThreshold()),
#		('Chi2 Pval on FPR', SelectFpr(chi2, alpha=0.05)),
#		('ANOVA-F Pval on FPR', SelectFpr(f_classif, alpha=0.05)),
#		('Chi2 Top K Perc', SelectPercentile(chi2, percentile=30)),
#		('ANOVA-F Top K Perc', SelectPercentile(f_classif, percentile=30)),
#		('Chi2 Top K', SelectKBest(chi2, k=1000)),
#		('ANOVA-F Top K', SelectKBest(f_classif, k=1000)),
#		('LinearSVC', LinearSVC(loss='squared_hinge', dual=False, **pr('Classifier', 'LinearSVC') if tuned else {})),
#		('Logistic Regression', SelectFromModel(LogisticRegression(dual=False, **pr('Feature Selection', 'Logistic Regression') if tuned else {}))),
#		('Lasso', SelectFromModel(LassoCV(cv=6), threshold=0.16)),
#		('Lasso-LARS', SelectFromModel(LassoLarsCV(cv=6))),
#		('Lasso-LARS-IC', SelectFromModel(LassoLarsIC(criterion='aic'), threshold=0.16)),
#		('Randomized Lasso', SelectFromModel(RandomizedLasso(random_state=0))),
#		('Extra Trees Regressor', SelectFromModel(ExtraTreesRegressor(100))),
		# ('U102-GSS502', ftslct.MSelectKBest(ftslct.gen_ftslct_func(ftslct.utopk, filtfunc=ftslct.gss_coef, fn=100), k=500)),
		# ('GSS502', ftslct.MSelectKBest(ftslct.gss_coef, k=500)),
#		('Combined Model', FeatureUnion([('Var Cut', VarianceThreshold()), ('Chi2 Top K', SelectKBest(chi2, k=1000))])),",bin/chm_annot.py,cskyan/chmannot,1
"# startYear = 2015; yearNum = 1
startYear = 2014; yearNum = 2

ohlcva = readWSDFile(baseDir, instrument, startYear=startYear, yearNum=yearNum)
print 'Day count:', len(ohlcva)
X, y, actionDates = prepareData(ohlcva)
normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
X_norm = normalizer.transform(X)
gamma, C, score = optimizeSVM(X_norm, y, kFolds=10)
print 'gamma=',gamma, 'C=',C, 'score=',score
clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)


pathName, df = readAndReWriteCSV(baseDir, instrument, startYear=startYear, yearNum=yearNum)
print pathName
# print df.sample(3)

feed = yahoofeed.Feed()
feed.addBarsFromCSV(instrument, pathName)
",finance/PyAlgoSVM.py,Ernestyj/PyStudy,1
"            feature_set[cnt, 5] = self.NBClassifier.test([row])
            pbar.update(1)
        pbar.close()
        return feature_set

    def train(self, pos_twt, neg_twt):
        target = np.concatenate((np.zeros((len(neg_twt), 1)), np.ones((len(pos_twt), 1))), axis=0)
        feature_set = self.get_feature_set(neg_twt + pos_twt)

        # train svm classifier here
        clf = svm.SVC(probability=True)
        clf.fit(feature_set, target)

        self.senti_score_classifier = clf
        self.is_trained = True

    def test(self, twt):
        if self.is_trained:
            feature_set = self.get_feature_set(twt)
            score = self.senti_score_classifier.predict_proba(feature_set)",bhtsa/SCClassifier.py,qingshuimonk/bhtsa,1
"

class SVMPredictor(object):
    """"""""
    A simple application of SVM classifier

    @author: Shaun
    """"""

    def __init__(self):
        self.clf = SVC(probability=True)

    @abstractmethod
    def fit(self, X, y):
        """"""
        Method to fit the model.

        Parameters:
        X - 2d numpy array of training data
        y - 1d numpy array of training labels",code/python/seizures/prediction/SVMPredictor.py,vincentadam87/gatsby-hackathon-seizure,1
"class TestWorkFlow(unittest.TestCase):

    def test_peristence_load_and_fit_predict(self):
        X, y = datasets.make_classification(n_samples=20, n_features=10,
                                            n_informative=2)
        n_folds = 2
        n_folds_nested = 3
        k_values = [1, 2]
        C_values = [1, 2]
        pipelines = Methods(*[Pipe(SelectKBest(k=k),
                                   Methods(*[SVC(kernel=""linear"", C=C)
                                             for C in C_values]))
                              for k in k_values])

        pipeline = CVBestSearchRefitParallel(pipelines,
                                             n_folds=n_folds_nested)

        tree_mem = CV(pipeline, n_folds=n_folds,
                      reducer=ClassificationReport(keep=False))
        # Save Tree",epac/tests/test_persistence.py,neurospin/pylearn-epac,1
"            #cv_obj = StratifiedKFold(n_splits=cv_n_folds, shuffle=False)
            cv_obj = cv_n_folds  # temporary hack (due to piclking issues otherwise, this needs to be fixed)
        else:
            cv_obj = None

        _rename_main_thread()

        if method == 'LinearSVC':
            from sklearn.svm import LinearSVC
            if cv is None:
                cmod = LinearSVC(**options)
            else:
                try:
                    from freediscovery_extra import make_linearsvc_cv_model
                except ImportError:
                    raise OptionalDependencyMissing('freediscovery_extra')
                cmod = make_linearsvc_cv_model(cv_obj, cv_scoring, **options)
        elif method == 'LogisticRegression':
            from sklearn.linear_model import LogisticRegression
            if cv is None:",freediscovery/categorization.py,rth/FreeDiscovery,1
"from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 7)

#------------------------------------------------------------------------------
# TODO: Create an *SVC classifier* named svc
# Use a linear kernel, and set the C value to C
#
# .. your code here ..

from sklearn.svm import SVC
svc = SVC(C = C, kernel = kernel) #C=1, kernel = 'linear' defined in the beginning of this code

#------------------------------------------------------------------------------
# TODO: Create an *KNeighbors classifier* named knn
# Set the neighbor count to 5
#
# .. your code here ..

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5)",Module 6/assignment4.py,LamaHamadeh/Microsoft-DAT210x,1
"        A 3-tuple of the form (tpr, fpr, roc_auc), where
        tpr is a list of true positive rates, 
        fpr is a list of false positive rates, and 
        roc_auc is the area under the ROC curve
    """"""
    # Split the data into training and testing sets
    x_train, x_test, y_train, y_test = \
            train_test_split(data, targets, test_size=test_size, random_state=0)

    # Run the classifier
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    y_pred = clf.fit(x_train, y_train).predict_proba(x_test)
    fpr, tpr, thresholds = roc_curve(y_test, y_pred[:, 1])
    roc_auc = auc(fpr, tpr)
    return (fpr, tpr, roc_auc)

def display_curve(fpr, tpr, roc_auc):
    """"""Display an ROC curve

    Args:",roc.py,bgold09/tweet_learn,1
"
    # Prescaling
    scaler = preprocessing.Scaler().fit(data.features)
    scaledFeatures = scaler.transform(data.features)

    # Feature selection
    selector = feature_selection.SelectKBest(feature_selection.f_regression).fit(scaledFeatures, data.labels)
    selectedFeatures = selector.transform(scaledFeatures)

    # Train a classifier
    clf = SVC(kernel='linear', C=1).fit(selectedFeatures, data.labels)

    # Save to files
    if not os.path.exists(modelDir(sensor)):
        os.makedirs(modelDir(sensor))
    joblib.dump(clf, modelDir(sensor) + 'model.pkl')
    joblib.dump(scaler, modelDir(sensor) + 'scaler.pkl')
    joblib.dump(selector, modelDir(sensor) + 'selector.pkl')
    saveObject(labels, modelDir(sensor)+'labels.pkl')
",virtual_sensor/train.py,IoT-Expedition/Edge-Analytics,1
"    def test_train_apply(self):
        d = self.d
        self.c.train_apply(d)
        np.testing.assert_equal([n.train_calls for n in self.nodes], [1, 1, 1])
        np.testing.assert_equal([n.test_calls for n in self.nodes], [1, 1, 1])

    def test_train_sklearn(self):
        if not sklearn_present:
            return

        ch = Chain([NOPNode(), svm.LinearSVC()])
        ch.train(self.d)
        self.assertTrue(hasattr(ch.nodes[1], 'coef_'))

        ch = Chain([svm.LinearSVC(), NOPNode()])
        ch.train(self.d)
        self.assertTrue(hasattr(ch.nodes[0], 'coef_'))
        self.assertEqual(ch.nodes[1].d.data.shape, (1, 10))

    def test_apply_sklearn(self):",psychic/tests/testchain.py,wmvanvliet/psychic,1
"#
#     models = toolkit.make_it_list(models)
#     we_report_folder = toolkit.make_it_list(we_report_folder)
#
#     privatized_set_folder = we_report_folder[0]
#
#     clfs = list()
#
#     if settings.predict_mode is 'CLASSIFICATION_BIN':
#         print(""Predicting at the classification mode (binary)"")
#         svm_clf = svm.SVC()
#         cart_clf = tree.DecisionTreeClassifier()
#         nb_clf = GaussianNB()
#         clfs = [('svm', svm_clf), ('cart', cart_clf), 'naive bayes', nb_clf]
#
#     if settings.predict_mode is 'CLASSIFICATION_MUL':
#         print(""Predicting at the classification mode (multiple)"")
#         svm_clf = OneVsRestClassifier(svm.SVC(kernel='linear'))
#         cart_clf = tree.DecisionTreeClassifier()
#         nb_clf = OneVsRestClassifier(GaussianNB())",evaluate/predict.py,ai-se/privacy_sharing,1
"
    for k in p_d:
        param_d[k] = p_d[k]


def _clst_r0(X_train, y_train, X_test, y_test, nb_classes, disp=True):
    model = tree.DecisionTreeClassifier()
    model.fit(X_train, y_train)
    dt_score = model.score(X_test, y_test)

    model = svm.SVC(C=param_d['SVC:C'], gamma=param_d['SVC:gamma'])
    # print(model)
    model.fit(X_train, y_train)
    sv_score = model.score(X_test, y_test)

    model = kkeras.MLPC([X_train.shape[1],
                         param_d[""DNN:H1""], param_d[""DNN:H2""], nb_classes])
    model.fit(X_train, y_train, X_test, y_test, nb_classes)
    mlp_score = model.score(X_test, y_test)
",kcellml.py,jskDr/jamespy_py3,1
"       spf=neutral (google.com: 68.232.133.53 is neither permitted nor denied by best guess record for domain of ekvinogradova@deloitte.ru) smtp.mail=ekvinogradova@deloitte.ru
X-IronPort-AV: E=Sophos;i=""4.97,751,1389729600""; 
   d=""png'150?jpg'150,145?scan'150,145,208,145,150,217"";a=""12421572""
Received: from esa1.deloitteie.c3s2.iphmx.com ([68.232.133.38])
  by esa1.deloitteru.c3s2.iphmx.com with ESMTP/TLS/RC4-SHA; 28 Mar 2014 17:29:44 +0400
X-IronPort-AV: E=McAfee;i=""5400,1158,7390""; a=""17860164""
X-IronPort-AV: E=Sophos;i=""4.97,751,1389740400""; 
   d=""png'150?jpg'150,145?scan'150,145,208,145,150,217"";a=""17860164""
Received: from unknown (HELO UKATREMA0474.atrema.deloitte.com) ([170.194.10.120])
  by esa1.deloitteie.c3s2.iphmx.com with ESMTP/TLS/AES128-SHA; 28 Mar 2014 14:29:44 +0100
Received: from UKATREMA1405.atrema.deloitte.com ([10.236.64.26]) by UKATREMA0474.atrema.deloitte.com over TLS secured channel with Microsoft SMTPSVC(7.0.6002.18222);
	 Fri, 28 Mar 2014 13:29:44 +0000
Received: from RUMOS1401.atrema.deloitte.com (10.163.1.82) by
 UKATREMA1405.atrema.deloitte.com (10.236.64.170) with Microsoft SMTP Server
 (TLS) id 14.3.174.1; Fri, 28 Mar 2014 13:29:41 +0000
Received: from RUMOS1421.atrema.deloitte.com ([fe80::2119:c736:ab61:78c4]) by
 RUMOS1401.atrema.deloitte.com ([fe80::94dd:87f1:48e6:724f%11]) with mapi id
 14.03.0174.001; Fri, 28 Mar 2014 17:29:38 +0400
From: ""Vinogradova, Ekaterina (RU - Moscow)"" <ekvinogradova@deloitte.ru>
To: ""s.a.kovalev@gmail.com"" <s.a.kovalev@gmail.com>",django/learning-friday-nuggets/lfn/lfn/test_imaptest.py,skoval00/hacks,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_Server_0715_2015_pre_activation_rectifier_didnotwork.py,magic2du/contact_matrix,1
"from sklearn import svm
from sklearn import neighbors
from sklearn import discriminant_analysis
from sklearn import linear_model


dt = tree.DecisionTreeClassifier()

# CHALLENGE - create 3 more classifiers...
# 1
lsvc = svm.LinearSVC()

# 2
kn = neighbors.KNeighborsClassifier(3)

# 3
svc = svm.SVC()

classifiers = [ dt, lsvc, kn, svc ]
",learn-python-for-data-science-1/challenge.py,jlcanela/learn-data,1
"train_index = index[:N]
test_index  = index[N:]

X = df.ix[:, 1:9]
y = df[10]

imp = Imputer(strategy='mean', axis=0)
X = imp.fit_transform(X)

#print 'Training..'
#clf = SVC()
clf = RandomForestClassifier()
clf.fit(X[train_index, :], y[train_index])

print 'Test'
#print clf.predict(X[test_index, :])
#print y[test_index]",src/python-lesson/glass.py,minhouse/python-lesson,1
"        kwargs['mapping'] = 'ZeroMeanUnitVarianceMapper'
        super(TrainLinearResolver, self).__init__(**kwargs)

    def init_model(self):
        hparams = {
            'kernel': 'rbf',
            'C': 1000.,
            'probability': True
        }

        return SVC(**hparams)

    def iter_instances(self, docs):
        toggle = True

        for doc in docs:
            for chain in doc.chains:
                if not chain.candidates:
                    # skip mentions without candidates
                    continue",nel/learn/resolving.py,henningpeters/nel,1
"pls = PLSRegression(3)
pls.fit(X, y)
X2 = pls.transform(X)
print(numpy.var(X2, 0).sum())

# Make predictions using an SVM with PCA and PLS
pca_error = 0
pls_error = 0
n_folds = 10

svc = LinearSVC()

for train_inds, test_inds in KFold(X.shape[0], n_folds=n_folds):
    X_train, X_test = X[train_inds], X[test_inds]
    y_train, y_test = y[train_inds], y[test_inds]

    # Use PCA and then classify using an SVM
    X_train2 = pca.fit_transform(X_train)
    X_test2 = pca.transform(X_test)
",tyrehug/exp/feexp.py,charanpald/tyre-hug,1
"    traindata = normalizer.fit_transform(traindata)

    lkf = LabelKFold(subjects, n_folds=len(np.unique(subjects)))

    delta = 0.1
    parameters = {'kernel': ['rbf'],
                  'C': [2 ** x for x in np.arange(-12, 12, 0.5)],
                  'gamma': [2 ** x for x in np.arange(-12, 12, 0.5)],
                  'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)]}

    svc = svm.SVC(probability=True, verbose=False, cache_size=2000)

    if args.scorer == 'f1':
        scorer = f1Bias_scorer_CV
    else:
        scorer = Twobias_scorer_CV

    if args.whichsearch == 'grid':
        clf = ModifiedGridSearchCV(svc, parameters, cv=lkf, n_jobs=-1, scoring=scorer, verbose=1, iid=False)
    else:",cStress.py,MD2Korg/cStress-model,1
"    train (numpy.array): training data in numpy array format
    test (numpy.array): test data in numpy array format
    y_train (numpy.array): dependent variable of training data
    y_test (numpy.array): dependent variable of test data
    Outputs
    ----------
    loss (float): error of the model in test data
    """"""
    if model in (1, 2, 3, 4, 7):
##        clf = RandomForestRegressor(n_estimators=100, random_state=1234, max_features=100, n_jobs=4)
        clf1 = SVC(C=10.0, random_state=1234)
        clf2 = SVR(C=7.0)
    elif model == 5:
        clf = LogisticRegression(C=3.0, random_state=1234, class_weight='auto')
    elif model == 6:
        clf = LinearSVR(C=0.6, random_state=1234)
    elif model in (8, 9, 10, 11, 12):
        dims = train.shape[1]
        num_classes = 4
        prediction = np.array([0 for i in range(len(y_test))])",src/enhancement/ManipulateAndValidation.py,JimingAndYuqi/secret,1
"    :param y_train: A dictionary with the following structure
            { instance_id : sense_id }

    :return: svm_results: a list of tuples (instance_id, label) where labels are predicted by LinearSVC
             knn_results: a list of tuples (instance_id, label) where labels are predicted by KNeighborsClassifier
    '''

    svm_results = []
    knn_results = []

    # svm_clf = svm.SVC(gamma=0.001, C=1.)
    #C=100,10,1.0,0.1,0.1,0.01,0.001,0.0001
    # 1.0:0.536, 0.51, 0.1: , 0.01:0.543, 0.005:0.536, 0.075:0.538, 0.015:0.541, 0.011:0.541, 0.0109: 0.540
    svm_clf = svm.LinearSVC(C=1.01, verbose=0, random_state=0)
    # k=4:0.48, 5:0.492, 6: 0.499, 7:0.510, 8:0.518, 9:0.519, 10:0.529, 11:0.532, 14:0.535
    knn_clf = neighbors.KNeighborsClassifier(14)
    # the label encoder seems not necessary
    label_encoder = preprocessing.LabelEncoder()
    label_encoder.fit(y_train.values())
",coursera/nlpintro-001/Assignment3/A.py,Alexoner/mooc,1
"le = LabelEncoder()
labels = le.fit_transform(labels)


print(""[INFO] constructing training/testing split..."")
(trainData, testData, trainLabels, testLabels) = train_test_split(
	np.array(data), labels, test_size=0.25, random_state=1000)


print(""[INFO] training Linear SVM classifier..."")
model = LinearSVC()
model.fit(trainData, trainLabels)
joblib.dump(model, ""D:/svm1.model"")

print(""[INFO] evaluating classifier..."")
predictions = model.predict(testData)
datas = []
imre = cv2.imread(""C:\\Users\\ZÝYA\\Desktop\\Datasets\\red1\\1051.png"")
imre_hist = extract_color_histogram(imre)
datas.append(imre_hist)",classifier.py,ziyaozclk/Traffic-Light-Detection-and-Recognition,1
"train_images, test_images, train_labels, test_labels = train_test_split(images, labels, train_size=0.7, random_state=21)
i = 2

std_weight = 0.1
bt_tst_imgs = mean_split(test_images,std_weight)
bt_trn_imgs = mean_split(train_images,std_weight)
img = bt_trn_imgs.iloc[i].as_matrix().reshape((28,28))
plt.clf()
plt.imshow(img, cmap='viridis')
#plt.show()
clf_bucket = svm.SVC(random_state=seed)
clf_bucket.fit(bt_trn_imgs, train_labels.values.ravel())
score = clf_bucket.score(bt_tst_imgs,test_labels)
print('bucket score ' + str(score))
print()
print('grid searching')

grid_vals = np.arange(0.0,1.025,0.025)

def grid_search(gridVals):",Python/Machine Learning/ScikitClassifiers/Classifiers/SVM_digits.py,sindresf/The-Playground,1
"        # Smoke test
        scores = cval.cross_val_score(clf, X, y)
        assert_array_equal(scores, clf.score(X, y))

        scores = cval.cross_val_score(clf, X_sparse, y)
        assert_array_equal(scores, clf.score(X_sparse, y))


def test_cross_val_score_precomputed():
    # test for svm with precomputed kernel
    svm = SVC(kernel=""precomputed"")
    iris = load_iris()
    X, y = iris.data, iris.target
    linear_kernel = np.dot(X, X.T)
    score_precomputed = cval.cross_val_score(svm, linear_kernel, y)
    svm = SVC(kernel=""linear"")
    score_linear = cval.cross_val_score(svm, X, y)
    assert_array_equal(score_precomputed, score_linear)

    # Error raised for non-square X",python/sklearn/sklearn/tests/test_cross_validation.py,seckcoder/lang-learn,1
"            avg_r += r
            print(""    %c     %.4f   %.4f"" % (chr(i + 65), p, r))
        avg_p /= 26
        avg_r /= 26
        print(""-------------------------"")
        print(""  AVG     %.4f   %.4f"" % (avg_p, avg_r))

    # use the default model
    if model_method == 2:
        # Training
        model = SVC(kernel=kernel_type)
        model.fit(images, labels)

        # Testing
        test_pred = model.predict(test_images)
        names = [chr(i + 65) for i in range(num_class)]
        print(classification_report(test_labels, test_pred,
                                    target_names=names, digits=4))

",main.py,anarchih/MCML_HW2,1
"
    def set_params(self, **params):
        return self


def test_rfe_set_params():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target
    clf = SVC(kernel=""linear"")
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    y_pred = rfe.fit(X, y).predict(X)

    clf = SVC()
    with warnings.catch_warnings(record=True):
        # estimator_params is deprecated
        rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1,
                  estimator_params={'kernel': 'linear'})
        y_pred2 = rfe.fit(X, y).predict(X)",scikit-learn-0.17.1-1/sklearn/feature_selection/tests/test_rfe.py,RPGOne/Skynet,1
"            ('mlp', estimator)
        ])

        results = cross_val_score(estimator, self.X, self.Y, cv=kfold)
        print('Accuracy: %.2f%% (+-%.2f%%)' % (results.mean() * 100, results.std() * 100))

        results = cross_val_score(pipeline, self.X, self.Y, cv=kfold)
        print('Accuracy: %.2f%% (+-%.2f%%) [Feature normalized]' % (results.mean() * 100, results.std() * 100))

    def baseline_svm(self):
        clf = svm.SVC(kernel='rbf', C=1).fit(self.train_x, self.train_y)
        accuracy = clf.score(self.test_x, self.test_y)
        print('SVM accuracy %.2f%%' % (accuracy * 100))

    def baseline_randomforest(self):
        clf = RandomForestClassifier(n_estimators=100).fit(self.train_x, self.train_y)
        accuracy = clf.score(self.test_x, self.test_y)
        print('RF accuracy %.2f%%' % (accuracy * 100))",network/evaluate.py,NTHU-CVLab/ActivityProps,1
"    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact be computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.96) than than the non
    # shuffling variant (around 0.86).

    digits = load_digits()
    X, y = digits.data[:800], digits.target[:800]
    model = SVC(C=10, gamma=0.005)
    n = len(y)

    cv = cval.KFold(n, 5, shuffle=False)
    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.88, mean_score)
    assert_greater(mean_score, 0.85)

    # Shuffling the data artificially breaks the dependency and hides the
    # overfitting of the model with regards to the writing style of the authors",projects/scikit-learn-master/sklearn/tests/test_cross_validation.py,DailyActie/Surrogate-Model,1
"    t_tc = time.clock()

    print ""Training...""

    X_dim = len(data_X[0])

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.1.py,totuta/deep-supertagging,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_02_19_2015.py,magic2du/contact_matrix,1
"# Split the dataset in two equal parts
X, y = data, training_data['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': [10,1,1e-1,1e-2,1e-3, 1e-4],
                     'C': [0.1, 1, 10, 100, 1000, 10000]}]

param_dist = {'C': scipy.stats.uniform(0.1, 1000), 'gamma': scipy.stats.uniform(.001, 1.0),
  'kernel': ['rbf'], 'class_weight':['balanced', None]}

clf = SVC()

# run randomized search
n_iter_search = 1000
random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
                                   n_iter=n_iter_search, n_jobs=-1, cv=4)

start = time()
random_search.fit(X, y)
print(""RandomizedSearchCV took %.2f seconds for %d candidates""",titanic_tuned_SVM.py,michael-hoffman/titanic,1
"    axes.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])
    axes.legend(loc='upper center', bbox_to_anchor=(0.5, 0.1), ncol=3, fancybox=True, shadow=True)
    axes.set_xticklabels([])
    axes.set_yticklabels([])
    pl.show()

def ensemble(training_df, testing_df):
    # Train an ensemble of classifiers on resampled data
    N, clfs = 10, []
    for _ in range(N):
        clf = sklearn.svm.LinearSVC()
        small = resample_to(training_df, ""ATTACK"")
        print ""Training classifier {}"".format(_)
        clf.fit(small.drop(""class"", axis=1), small[""class""])
        clfs.append(clf)

    # Take the modal prediction as the ensemble's consensus
    res = pd.DataFrame({""ground"": testing_df[""class""]})
    for i, clf in enumerate(clfs):
        res[i] = clf.predict(testing_df.drop(""class"", axis=1))",manipulate.py,katrielalex/cdt-modelling,1
"    N = len(Y)
    label_enc = LabelEncoder()
    label_enc.fit(list(causes))
    print 'found %d causes' % len(label_enc.classes_)
    Y = [list(label_enc.transform(yi)) for yi in Y]
    print 'labels:', label_enc.classes_
    # LabelBinarizer buggy with np arrays. See https://github.com/scikit-learn/scikit-learn/issues/856
    Y = LabelBinarizer().fit_transform(Y)
    #clf = OneVsRestClassifier(MultinomialNB())
    #clf = OneVsRestClassifier(LogisticRegression())
    clf = OneVsRestClassifier(LinearSVC(random_state=0))

    cv = cross_validation.KFold(len(Y), 10, shuffle=True, random_state=1234)
    losses = []
    for train, test in cv:
        truth = Y[test]
        pred = clf.fit(X[train], Y[train]).predict(X[test])
        losses.append(metrics.precision_score(truth.reshape(-1), pred.reshape(-1)))
    print 'F1 score=%.3f stderr=%.3f' % (np.average(losses), np.std(losses))
",givinggraph/analysis/company_cause_classifier.py,math4youbyusgroupillinois/givinggraph,1
"# we create 40 separable points
np.random.seed(0)
X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]
Y = [0] * 20 + [1] * 20

# figure number
fignum = 1

# fit the model
for name, penalty in (('unreg', 1), ('reg', 0.05)):
    clf = svm.SVC(kernel='linear', C=penalty)
    clf.fit(X, Y)

    # get the separating hyperplane
    w = clf.coef_[0]
    a = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = a * xx - (clf.intercept_[0]) / w[1]

    # plot the parallels to the separating hyperplane that pass through the",projects/scikit-learn-master/examples/svm/plot_svm_margin.py,DailyActie/Surrogate-Model,1
"        classifier is used for testing and a ROC curve is computed
        and saved as property and locally.

        :param rocs_filename: the file to save all rocs computed
        :param iterations: number of runs (training/testing)
        """"""
        for i in xrange(iterations):
            print ""[*] Iteration {0}"".format(i)
            print ""[*] Randomizing dataset...""
            self.randomize_dataset()
            clf = GridSearchCV(svm.LinearSVC(), {'C': np.logspace(-3, 3, 7)})
            print ""[*] Training...""
            clf.fit(self.X_train, self.Y_train)
            out = clf.best_estimator_.decision_function(self.X_test)
            print ""[*] Testing...""
            roc = eval.compute_roc(np.float32(out.flatten()),
                                   np.float32(self.Y_test))
            self.rocs.append(roc)
            print ""[*] ROC saved.""
        pz.save(self.rocs, rocs_filename)",adagio/core/analysis.py,hgascon/adagio,1
"train_sizes = np.linspace(.1, 1.0, 5)
train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=1, train_sizes=train_sizes)
plot.learning_curve(train_scores, test_scores, train_sizes)
plt.show()

# SVC is more expensive so we do a lower number of CV iterations:
cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,
                                   test_size=0.2, random_state=0)
cv = 5
estimator = SVC(gamma=0.001)
train_sizes = np.linspace(.1, 1.0, 5)
train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=1, train_sizes=train_sizes)

plot.learning_curve(train_scores, test_scores, train_sizes)
plt.show()",examples/learning_curve.py,edublancas/sklearn-evaluation,1
"        data_dict=pickle.load(f)
    with open('inner_cv.pickle','rb') as f:
        inner_cv=pickle.load(f) 
    
    scores= {'train': [], 'test': []}
    for i in range(25):
        X_train= np.array([data_dict['data'][inner_cv['X_train'][i][j]] for j in range(len(inner_cv['X_train'][i]))])
        X_test= np.array([data_dict['data'][inner_cv['X_test'][i][j]] for j in range(len(inner_cv['X_test'][i]))])
        y_train= inner_cv['y_train'][i]
        y_test= inner_cv['y_test'][i]
        est = svm.SVC()
        est.fit(X_train, y_train)
        scores['train'].append(est.score(X_train, y_train))
        scores['test'].append(est.score(X_test, y_test))
    
    with open('csvc_scores.pickle','wb') as f:
        pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL) 

    return 
    ",estimators.py,jrabenoit/shopvec,1
"            self.mongo_client = MongoClient(self.MONGO_ADDR, self.MONGO_PORT)
        except Exception as error:
            pretty_print('ERROR', str(error))
    
    """"""
    ## Initialize SKlearn
    def init_sklearn(self):     
        pretty_print('SKLEARN', 'Initializing SKlearn')
        try:
            ## Learning Estimators
            self.svc_health = svm.SVC(kernel='rbf')
            self.svc_environment = svm.SVC(kernel='rbf')
            self.svc_activity = svm.SVC(kernel='rbf')
        except Exception as error:
            pretty_print('ERROR', str(error))
    """"""
    """"""    
    ## Query Samples in Range to JSON-file
    def query_db(self, days, query_type):
        pretty_print('MONGO', 'Querying samples for last %s days' % str(days))",hive-aggregator-update.py,nathanielrindlaub/hive-aggregator,1
"
#add 200 non-informative features
X = np.hstack((X,2 * np.random.random((n_samples,200))))

###############################
#Create a feature-selectio transform and an instance of SVM that while :
# we combine together to have an full-blown estimatoor

transform = feature_selection.SelectPercentile(feature_selection.f_classif)

clf = Pipeline([('anova',transform),('svc',svm.SVC(C=1.0))])

##################################
#Plot the cross-validation score as a function 
#of percentile of features
score_means = list()
score_stds = list()
percentiles = (1,3,6,10,15,20,30,40,60,80,100)

for percentile in percentiles:",1_supervised_classification/15-SVM/svm/plot_svm_anova.py,PhenixI/machine-learning,1
"                                      strip_accents=""unicode"",
                                      dtype=np.float32,
                                      decode_error=""replace"")),
            (""scaling"", Normalizer())
        ])

    X = transformer.fit_transform(records)
    y = np.array([r[0][""decision""] for r in records])

    # FIXME: Perhaps add , n_jobs=-1 for parallel jobs
    grid = GridSearchCV(LinearSVC(),
                        param_grid={""C"": np.linspace(start=0.2, stop=0.5,
                                                     num=20)},
                        scoring=""accuracy"", cv=3, verbose=3)
    grid.fit(X, y)

    return Pipeline([(""transformer"", transformer),
                     (""classifier"", grid.best_estimator_)])

",inspire/modules/predicter/arxiv.py,jalavik/inspire-next,1
"
    def gen_svm_model(self):
        df = pd.read_csv('../data/eye_tests/combined_calibration_log.csv',sep=',')
        X = df.iloc[:,[0,1]].values
        #standardize feature vectors 
        stdsc = StandardScaler()
        X = stdsc.fit_transform(X)
        y=df.iloc[:,3].values
        C = 1.0  # SVM regularization parameter
        if (settings.SVM_model_type == 1):
            self.svm_model = svm.SVC(kernel='linear', C=C).fit(X, y)
        elif (settings.SVM_model_type == 2):
            self.svm_model = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
        elif (settings.SVM_model_type == 3):
            self.svm_model = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)
        elif (settings.SVM_model_type == 4):
            self.svm_model = svm.LinearSVC(C=C).fit(X, y)
        else:
            raise Exception(""Invalid SVM model selection"")
",src/wtfj/__init__.py,WritingTechForJarrod/gui,1
"
# 
# TODO: Split your data into test / train sets
# Your test size can be 30% with random_state 7.
# Use variable names: X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)

#
# TODO: Create an SVC classifier named svc
# Use a linear kernel, and set the C value to C
svc = SVC(kernel=kernel, C=C)

#
# TODO: Create an KNeighbors classifier named knn
# Set the neighbor count to 5
knn = KNeighborsClassifier(n_neighbors=5)

benchmark(knn, X_train, X_test, y_train, y_test, 'KNeighbors')
drawPlots(knn, X_train, X_test, y_train, y_test, 'KNeighbors')
",Module6/assignment1.py,Wittlich/DAT210x-Python,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",imblearn/tests/test_pipeline.py,dvro/UnbalancedDataset,1
"def train_svm_model(X,y):
    t=time.time()

    # Scaler
    scaler = StandardScaler()
    scaler.fit(X)
    save_model(scaler,""standard_scaler"")
    X = scaler.transform(X)

    # SVM Fit
    svc = LinearSVC(penalty='l2')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    svc.fit(X_train, y_train)
    print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))
    # Save the model
    save_model(svc,""linear_svm"")

#  Models.
def build_cnn(data, one_hot_y, loss_beta = 0.001):
    # Variables.",vehicle_detection/src/train.py,dzorlu/sdc,1
"##########################################################################
##  Tests
##########################################################################

class ROCAUCTests(VisualTestCase):

    def test_roc_auc(self):
        """"""
        Assert no errors occur during ROC-AUC integration
        """"""
        model = LinearSVC()
        model.fit(X,y)
        visualizer = ROCAUC(model, classes=[""A"", ""B""])
        visualizer.score(X,y)",tests/test_classifier/test_rocauc.py,pdamodaran/yellowbrick,1
"print ""Class Label Vector Y of size "", len(YLabels), "" extracted"";

#Setting up scaler for standardisation
from sklearn import preprocessing
scaler = preprocessing.StandardScaler();

# Training SVM
from sklearn import svm
from sklearn import linear_model
print ""Declaring SVM""
#clf = svm.LinearSVC(); # linearsvc1
clf = svm.LinearSVC(C=2000.0, class_weight='auto', penalty='l1', dual=0); # linearsvc2
#clf = svm.SVC(cache_size = 1000, class_weight='auto', kernel = 'poly'); # Predicts all as POSITIVE :((
#clf = linear_model.SGDClassifier();  # not tried yet
print ""standardising training data""
XFeatures = scaler.fit_transform(XFeatures, YLabels);
print ""Fitting Data To SVM""
clf.fit(XFeatures, YLabels);
print ""SVM trained""
",Preliminary experimentation/code/triBiUnigramSVMIgnoreUNK.py,prernaa/NLPCourseProj,1
"    y = []
    for i in range(win-1, len(dailyreturn)):
        if dailyreturn[i]<0: y.append(-1)
        elif dailyreturn[i]>0: y.append(1)
        else: y.append(y[-1])   # 按前一个值填充
    return X, y


def optimizeSVM(X_norm, y, kFolds=10):
    clf = pipeline.Pipeline([
        ('svc', svm.SVC(kernel='rbf')),
    ])
    # grid search 多参数优化
    parameters = {
        'svc__gamma': np.logspace(-1, 3, 20),
        'svc__C': np.logspace(-1, 3, 10),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['svc__gamma'], gs.best_params_['svc__C'], gs.best_score_",finance/DaysTest/DaysDataPrepare.py,Ernestyj/PyStudy,1
"        else:
            embeddings = np.zeros((2, 150))  # creating an empty array since csv is empty

        self.le = LabelEncoder().fit(
            labels)  # LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1
        # Fits labels to model
        labelsNum = self.le.transform(labels)
        nClasses = len(self.le.classes_)

        if classifier == 'LinearSvm':
            self.clf = SVC(C=1, kernel='linear', probability=True)
        elif classifier == 'GMM':
            self.clf = GMM(n_components=nClasses)

        if ldaDim > 0:
            clf_final = self.clf
            self.clf = Pipeline([('lda', LDA(n_components=ldaDim)),
                                 ('clf', clf_final)])

        self.clf.fit(embeddings, labelsNum)  # link embeddings to labels",facelib/monitor/FaceHandle.py,golden-tech-native/gd_facerecognize,1
"

## 1) Design your classifier
## ===========================================================================
class MySVC:
    def __init__(self, C=1.0):
        self.C = C

    def transform(self, X, y):
        from sklearn.svm import SVC
        svc = SVC(C=self.C)
        svc.fit(X, y)
        # ""transform"" should return a dictionary
        return {""y/pred"": svc.predict(X), ""y"": y}


## 2) Design your reducer which compute, precision, recall, f1_score, etc.
## ===========================================================================
class MyReducer(Reducer):
    def reduce(self, result):",epac/tests/test_dump_class_definition_sfw.py,neurospin/pylearn-epac,1
"from specparser import get_settings_from_file

from sklearn.svm import LinearSVC
import numpy as np
import pickle

def main():
    settings = get_settings_from_file(""spec.json"")
    X = np.genfromtxt(settings.Input.X, delimiter=',', skip_header=1)
    Y = np.genfromtxt(settings.Input.Y, delimiter=',', skip_header=1)
    svc = LinearSVC(C=float(settings.Param.C))
    svc.fit(X,Y)
    with open(settings.Output.MODEL, ""w"") as f:
        pickle.dump(svc, f)
    print(""Done"")

if __name__ == ""__main__"":
    main()",tutorials/basic/svm/main.py,dkuner/example-modules,1
"class SimpleLogisticRegression(LinearRegression):
    def predict_proba(self, X):
        predictions = self.predict(X)
        predictions = sklearn.preprocessing.scale(predictions)
        predictions = 1.0 / (1.0 + np.exp(-0.5 * predictions))
        return np.vstack((1.0 - predictions, predictions)).T


def make_svm(gamma, C):
    cls = sklearn.pipeline.make_pipeline(StandardScaler(),
        SVC(gamma=gamma, C=C, probability=True, cache_size=500, random_state=0))
    name = 'ss-svc-g%.4f-C%.1f' % (gamma, C)
    return (cls, name)


def make_lr(C):
    cls = sklearn.pipeline.make_pipeline(StandardScaler(), LogisticRegression(C=C))
    name = 'ss-lr-C%.4f' % C
    return (cls, name)
",seizure_prediction/classifiers.py,MichaelHills/seizure-prediction,1
"                Y=np.array(Y)
                Y=Y.astype(float)
                X_full=np.array(X_full)
                X_full=X_full.astype(float)
                #logistic = linear_model.LogisticRegression(C=1e5)
                #logistic.fit(X,Y)
                #pred=logistic.predict(X_full)
                #probs=logistic.predict_proba(X_full)
                
                
                mdl=svm.SVC(kernel='poly',degree=2)
                mdl.fit(X,Y)
                pred = mdl.predict(X_full)
                probs= mdl.predict_log_proba(X_full)
                #pred=logistic.predict(X_full)
                
                ##Test if the other player thinks like me
                #self.myPredActions.append(predme[-1])
                
                ",ipstrategy.py,rnyang/learningprisoners,1
"
    accu_p = np.zeros(shape=(2,))
    accu_r = np.zeros(shape=(2,))
    accu_f = np.zeros(shape=(2,))
    accu_a = 0.0
    folds = 10
    for train_idx, test_idx in StratifiedKFold(y=y, n_folds=folds, shuffle=True):
        train_x, train_y = x[train_idx], y[train_idx]
        test_x, test_y = x[test_idx], y[test_idx]

        cls = svm.NuSVC(nu=0.5, kernel='rbf')

        # train
        train_x = vectorizer.fit_transform(train_x).toarray()

        cls.fit(train_x, train_y)

        # test
        test_x = vectorizer.transform(test_x).toarray()
",yelp-sentiment/experiments/sentiment_svmnu.py,canast02/csci544_fall2016_project,1
"
  columns = list(x.columns)

  sc_dict = sc_fit(x,columns)

  for col in columns:
    x = scale_col(x, col, sc_dict)

  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
  
  clf = SVC(probability=True)
  svc_filter = SelectKBest(f_regression, k=5)
  svc_pipe = Pipeline([('anova',svc_filter), ('svc',clf)])
  svc_pipe.fit(x_train, y_train)
  scores = cross_val_score(svc_pipe, x, y, cv=5)
  obj = {'clf':svc_pipe,'type':'SVC'}
  hook(SCRIPT_NAME, ""INFO"", ""LOW"", lineno(), ""SVC score: {0:.3f}"".format(scores.mean()))
  return obj,x,sc_dict

#------------------------------------------------------------------------------",test_bets.py,cgrohman/ponies,1
"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]
",learn-python-for-data-science-1/classifiers.py,jlcanela/learn-data,1
"    logger.info(""Forming CSR Matrices"")
    x_train, x_test = create_csr_matrix(X_train, n_symbols), create_csr_matrix(X_test, n_symbols)
    logger.info(""Starting PCA"")
    # pseudo-supervised PCA: fit on positive class only
    pca = pca.fit(x_train[y_train > 0])

    x_train_pca = pca.transform(x_train)
    x_test_pca = pca.transform(x_test)

    logger.info(""Starting SVM"")
    svc = SVC(probability=probability, cache_size=cache_size, **svm_kwargs)
    svc.fit(x_train_pca, y_train)
    logger.info(""Scoring SVM"")
    score = svc.score(x_test_pca, y_test)
    logger.info(score)
    svc.test_score = score
    pca.n_symbols = n_symbols
    return svc, pca, x_train_pca, x_test_pca

",chatnet/svm_model.py,bhtucker/chatnet,1
"    X.append( temp_arr )
    Y.append( index_feat[i][""Target""] )
    
X =  np.array( X )
Y =  np.array( Y )  
X_train = X[ : int( training_length*len( X ) ) ]
Y_train = Y[ : int( training_length*len( Y ) ) ]
X_test = X[ int( training_length*len( X ) ) : ]
Y_test = Y[ int( training_length*len( Y ) ) : ]

clf = svm.SVC( kernel='rbf' )
clf.fit( X_train, Y_train )
Y_pred = clf.predict( X_test ) 

cnt = 0
pred_file = open(""../Output-files/Predictions.txt"", ""w"")
pred_file.write(""Actual    Predicted\n"")
for i in range( 0, len( Y_pred ) ):
    pred_file.write( (' '+str(Y_test[i]))[-2:] + ""       "" + (' '+str(Y_pred[i]))[-2:] + ""\n"" )
    if( Y_pred[i] != Y_test[i] ):",Python-PHP-Codes/svm_classifier.py,Chinmoy07/ML-Term-Project-Team-Pikachu,1
"    model = joblib.load(""model.pkl"")
    scaler = joblib.load(""scaler.pkl"")
    # raise ""BOOM""
except:
    # Train the model
    with Timed(""Pipeline Total""):
        pipe = Pipeline(config=config)
        pipe.load_images()
        with Timed(""Grabbing features""):
            X, y = pipe.X_and_y()
        pipe.train(LinearSVC(), X, y)

        # Save the model
        joblib.dump(pipe.model, ""model.pkl"")
        joblib.dump(pipe.scaler, ""scaler.pkl"")
        model = pipe.model
        scaler = pipe.scaler

# raise ""BOOM""
",projects/MOOCs/udacity/drive/project-5-vehicle-detection/src/main.py,seansu4you87/kupo,1
"import app.analytics.filterSentences as fl
import matplotlib.pyplot as plt
G = {}
np.seterr(divide='ignore',invalid='ignore')

trainArticles= open('data/singleShort.txt','r').readlines()#=importArticles.getData('train')
testArticles = open('data/singleShortTest.txt','r').readlines()#= importArticles.getData('test')
print len(trainArticles)
print len(testArticles)
listOfYears = []
clf = svm.SVC(probability=True)
probs = []
titles = []
#A
def getArticle(article):
    singleSets = []
    try:
        chunks = gc.getChunks(article[1])
        tags =  tag.getTags(article[1],chunks)
        #if tags == []:",testing.py,JFriel/honours_project,1
"    #                                                mask_file2='face_scene/mask.nii.gz')
    epochs_per_subj = int(sys.argv[5])
    num_subjs = int(sys.argv[6])
    # the following line is an example to leaving a subject out
    #vs = VoxelSelector(labels[0:204], epochs_per_subj, num_subjs-1, raw_data[0:204])
    # if using all subjects
    vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data)
    # if providing two masks, just append raw_data2 as the last input argument
    #vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data, raw_data2=raw_data2)
    # for cross validation, use SVM with precomputed kernel
    clf = svm.SVC(kernel='precomputed', shrinking=False, C=10)
    results = vs.run(clf)
    # this output is just for result checking
    if MPI.COMM_WORLD.Get_rank()==0:
        logger.info(
            'correlation-based voxel selection is done'
        )
        #print(results[0:100])
        mask_img = nib.load(mask_file)
        mask = mask_img.get_data().astype(np.bool)",examples/fcma/voxel_selection.py,TuKo/brainiak,1
"class EvalTree(object):
    """"""docstring for EvalTree""""""
    b_preict = []
    def __init__(self):
        super(EvalTree, self).__init__()


  

    def init_classifier(self):
        # clf = svm.SVC(kernel = 'rbf', gamma=self.gamma_value, C=self.c_value)
        # print ""SVM configuration... \n\n"", clf
        clf = tree.DecisionTreeClassifier()
        return clf



    def fit_train_data(self, clf, a_train, b_train):
        # clf = svm.SVC(kernel = 'rbf', gamma=gamma_value, C=c_value)
        # print ""SVM configuration... \n\n"", clf",implementation/evaluation/tree.py,imink/UCL_COMPIG15_Project,1
"        return numpy.vstack(res)

def evaluate(cat, fold, txt_train, txt_test, y_train, y_test):
    d2vmodelfile = os.path.join(conf.D2V_DIR, 'model')
    if not os.path.exists(d2vmodelfile):
        print('Doc2vec model file ""%s"" not found' % d2vmodelfile)
        print('Did you run train_doc2vec.py?')
        sys.exit(1)

    fe = D2V_feature_extractor(d2vmodelfile)
    predictor = SVC(
        kernel=conf.SVM_KERNEL,
        class_weight=conf.SVM_CLWEIGHT,
        C=conf.SVM_C,
        random_state=conf.SEED,
    )
    X = fe.transform(txt_train)
    predictor.fit(X, y_train)
    X_test = fe.transform(txt_test)
    y_pred = predictor.predict(X_test)",experiments/src/evaluate_d2v.py,OFAI/million-post-corpus,1
"    ## TEST START
    n_cv = []
    for train,test in cv:
        n_cv.append(len(train))

    n_cv = min(n_cv)
    #cv = StratifiedKFold(y=self.y, n_folds=folds)
    json_cv_time = []

    grid_cv = KFold(n_cv, n_folds=5, shuffle=True)
    linear_clf = grid_search.GridSearchCV(SVC(kernel=""linear"", probability=True),param_grid={'C': [2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]},cv=grid_cv,n_jobs=-1)
    # rbf_clf = grid_search.GridSearchCV(SVC(probability=True),param_grid={'gamma': [2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1], 'C': [2**-1, 2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]},cv=grid_cv,n_jobs=-1)

    # models = [linear_clf, rbf_clf]
    ## TEST END

    # Get accuracy
    def scorer(model, X, y):
        return model.score(X, y)
",tornado-server/app.py,daviddao/luminosity,1
"#=======================
#TODO Perform a train/test split. 30% test group size, with a random_state equal to 7.

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(T, y, test_size = 0.30, random_state = 7)

#TODO Create a SVC classifier. Don't specify any parameters, just leave everything as default. 
#Fit it against your training data and then score your testing data.

from sklearn.svm import SVC
model = SVC()
model.fit(X_train, y_train)
score = model.score(X_test, y_test)
print(score) 

#-----------------------------------------------------------------------------------------
#==============
#Second Question
#==============
#That accuracy was just too low to be useful. We need to get it up. ",Module 6/assignment3.py,LamaHamadeh/Microsoft-DAT210x,1
"                                scores.max(),]))
        row = pandas.DataFrame([row])
        df = df.append(row)
    df.index = df.name
    df = df.drop(""name"",1)
    df = df.sort(""ROC_score"",ascending=False)
    print df.to_string(float_format=stringify)
    return df

CLASSIFIERS = [
    svm.SVC(), 
    LogisticRegression(C=0.16,penalty='l1', ## given in the forums
                       tol=0.001, fit_intercept=True)
    Pipeline([('pca',decomposition.PCA()),
              ('svm',svm.SVC()),]),
    Pipeline([(""rfe_Lsvc"",
               RFE(estimator=svm.LinearSVC(), 
                   n_features_to_select=240,step=1)),
              (""svc_3"",svm.SVC(gamma=.1,
                             degree=3,",make_benchmarks.py,afoss925/kaggle_schizophrenia_2014,1
"from masque.utils import get_patch, implot, conv2, conv_transform


_IM_SHAPE = (64, 64)

prbm_svc = {
    'pretrain_model' : Pipeline([
        ('patch_trans', PatchTransformer(_IM_SHAPE, (12, 12), n_patches=10)),
        ('rbm', BernoulliRBM(n_components=72, verbose=True)),
    ]),
    'model' : SVC(kernel='linear', verbose=True),
    'pretrain_data' : lambda: datasets.cohn_kanade(im_shape=_IM_SHAPE),
    'data' : lambda: datasets.cohn_kanade(im_shape=_IM_SHAPE,
                                     labeled_only=True),
    'x_shape' : _IM_SHAPE,
    'filter_shape' : (12, 12),
}

_PS2 = (8, 8)
prbm_svc2 = {",masque/playground/patch_rbm.py,dfdx/masque,1
"
    # START_OBSESSING_OVER_HOST_CHECKS
    def START_OBSESSING_OVER_HOST_CHECKS(self):
        if not self.conf.obsess_over_hosts:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            self.conf.obsess_over_hosts = True
            self.conf.explode_global_conf()
            self.sched.get_and_register_update_program_status_brok()

    # START_OBSESSING_OVER_SVC;<host_name>;<service_description>
    def START_OBSESSING_OVER_SVC(self, service):
        if not service.obsess_over_service:
            service.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value
            service.obsess_over_service = True
            self.sched.get_and_register_status_brok(service)

    # START_OBSESSING_OVER_SVC_CHECKS
    def START_OBSESSING_OVER_SVC_CHECKS(self):
        if not self.conf.obsess_over_services:
            self.conf.modified_attributes |= DICT_MODATTR[""MODATTR_OBSESSIVE_HANDLER_ENABLED""].value",shinken/external_command.py,rednach/krill,1
"	import numpy as np
	from sklearn import svm
	pos_neg_samps = num_training / 2
	train_labels_pos = np.array([i for i, elem in enumerate(train_labels == 1, 1) if elem]) - 1
	train_labels_neg = np.array([i for i, elem in enumerate(train_labels == 0, 1) if elem]) - 1

	tr_idx = np.array(range(0,int(num_estimates*pos_neg_samps))).reshape(num_estimates,int(pos_neg_samps))
	#te_idx = np.repeat(range(0,num_estimates),num_testing,axis=0)
	acc = [];
	for idx in range(0,num_estimates):
		clf = svm.SVC()
		it_train = np.vstack((train_data[train_labels_pos[tr_idx[idx,:]],:],
			train_data[train_labels_neg[tr_idx[idx,:]],:]))
		it_lab = np.hstack((np.repeat(1,pos_neg_samps),np.repeat(0,pos_neg_samps)))
		clf.fit(it_train,it_lab)
		y_hat = clf.predict(test_data[idx,:].reshape(1, -1))
		it_acc = np.sum(test_labels[idx]==y_hat) / num_testing
		#Store stuff
		acc = np.hstack((acc,it_acc))
	return acc",SVRT_analysis_helper_functions.py,drewlinsley/draw_classify,1
"from sklearn import svm, datasets
from sklearn.externals import joblib

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :4]
y = iris.target

# we create an instance of SVM and fit out data.
C = 1.0  # SVM regularization parameter
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C, probability=True)\
          .fit(X, y)
joblib.dump(rbf_svc, 'models/iris.pickle', compress=3)",iris_create_model.py,dukebody/weblearn,1
"#print ""run OneVsRest with our implementation""
run_OneVsRest(data, OneVsRest, NaiveBayes)

print ""run OneVsrest with sklearn""
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
print "">> Multinomial Naive Bayes""
run_OneVsRest(data, OneVsRestClassifier, MultinomialNB())
print "">> LinearSVC""
run_OneVsRest(data, OneVsRestClassifier, LinearSVC())",main.py,CMPS242-fsgh/project,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,agramfort/mne-python,1
"
Training = np.load(DATA_TRAIN_FILE)
Labels = np.load(DATA_TRAIN_LABELS_FILE)
Test = np.load(DATA_TEST_FILE)

#print Test.size
#print Labels.size
#print Training.size

''' This is important. It's also magic. '''
clf = SVC()
#print shape(Training)
print shape(Labels)

#print Labels
#print Training[0:3]
clf.fit(Training, Labels)

''' This is just graphing the Data '''
fig1 = plt.figure()",4155/assignments/a4/q2.py,moriarty/csci-homework,1
"
    m, n = X.shape
    plt.figure()
    plotData(X, y)

    raw_input('Program paused. Press enter to continue')

    # =================== Part 2: Training Linear SVM ===================

    C = 1
    model = svm.SVC(C=C, kernel='linear', max_iter=20)
    model.fit(X, y)
    visualizeBoundaryLinear(X, y, model)
    
    raw_input('Program paused. Press enter to continue')

    # =================== Part 3: Implementing Gaussian Kernel ===================

    print('Evaluating the Gaussian Kernel ...')
    ",skeletons/ex6/ex6.py,cameronlai/ml-class-python,1
"# for machine learning with scikit-learn
fmri_masked = nifti_masker.fit_transform(data.func[0])

# Restrict the classification to the face vs cat discrimination
fmri_masked = fmri_masked[condition_mask]

### Prediction ################################################################

# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

# And we run it
svc.fit(fmri_masked, target)
prediction = svc.predict(fmri_masked)

### Cross-validation ##########################################################

from sklearn.cross_validation import KFold
",plot_haxby_simple.py,abenicho/isvr,1
"

def svm_predict_proba(args):
    svm, (i, j), X = args
    return svm[i][j].predict_proba(X)[..., 1]  # (K * D) * N * 2 -> (K * D) * N


class SvmArray:
    def __init__(self, D, proba=True):
        self.D = D
        self.svm = [[SVC(kernel=k, probability=proba) for _ in range(0, D)] for k in kernels]  # K * D svm array

    def fit(self, X, Y):
        coords = itertools.product(range(0, len(kernels)), range(0, self.D))  # if iterating more than once, use list
        # parallel(delayed(svm_fit)((self.svm, c, X, Y)) for c in coords)  # double parenthesis to make tuple
        map(svm_fit, [(self.svm, c, X, Y) for c in coords])

    def predict(self, X, out, kernel=None):
        assert out == 'proba' or out == 'dist'
        if kernel:  # is not None",svm_array.py,liboyin/iucg,1
"    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact be computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.96) than than the non
    # shuffling variant (around 0.86).

    digits = load_digits()
    X, y = digits.data[:800], digits.target[:800]
    model = SVC(C=10, gamma=0.005)
    n = len(y)

    cv = cval.KFold(n, 5, shuffle=False)
    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.88, mean_score)
    assert_greater(mean_score, 0.85)

    # Shuffling the data artificially breaks the dependency and hides the
    # overfitting of the model with regards to the writing style of the authors",sklearn/tests/test_cross_validation.py,loli/sklearn-ensembletrees,1
"    :param y_test: Class labels from the testing set
    :return: results: Library with classifier names and scores
    """"""
    rf = RandomForestClassifier(n_estimators=50)
    lr = LogisticRegression()
    mlp = MLPClassifier()
    lda = LinearDiscriminantAnalysis()
    sgd = SGDClassifier()
    ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50)
    gbc = GradientBoostingClassifier()
    svc = SVC(kernel='rbf', probability=True)
    knn = KNeighborsClassifier()
    et = ExtraTreeClassifier()

    clf_names = ['LogisticRegression', 'MLPClassifier', 'LinearDicriminantAnalysis',
                 'SGD Classifier', 'AdaBoostClassifier', 'GradientBoostClassifier', 'SVC(rbf)',
                 'KNearestNeighbors', 'ExtraTreesClassifier', 'RandomForestClassifier']
    clfs = [lr, mlp, lda, sgd, ada, gbc, svc_l, knn, et, rf]

    results = {}",pymlkit/model_select.py,xfaxca/pymlkit,1
"from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import label_binarize
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import KFold
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import cross_val_score
from sklearn.multiclass import OneVsRestClassifier

rdf = RandomForestClassifier(n_estimators=100)
ada = OneVsRestClassifier(AdaBoostClassifier())
lsvc = SVC(kernel=""linear"", C=4, probability=True)
rsvc = SVC(probability=True, C=32, gamma=0.125) # tuned HP for HELA
log = LogisticRegression()
lda = LDA()
knn = KNeighborsClassifier()

# estimators=[('lr', rdf), ('rf', log), ('lda', lda)]
estimators=[('l',rdf),('lda', lda),('svc',rsvc),('lsvc',lsvc)]
clf = VotingClassifier(estimators=estimators, voting=""soft"")
# self.models = [RandomForestClassifier(n_estimators=100), LDA(), DecisionTreeClassifier()]",sklearn-server/app.py,daviddao/luminosity,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_10_30_2014_server_8.py,magic2du/contact_matrix,1
"
    x_train = x[0:20000,:]
    y_train = y[0:20000]
    x_test = x[20000:40000,:]
    y_test = y[20000:40000]

    # Set the parameters by cross-validation
    C=10
    gamma=1e-7
    
    clf = svm.SVC(C=C, gamma=gamma)

    # We learn the digits on the first half of the digits
    clf.fit(x_train, y_train)

    # Pickle the model!
    outf = open('model.pkl', 'wb')
    pickle.dump(clf, outf)
    outf.close()
",DigitRecognizer/digitr2.py,n7jti/kaggle,1
"norm_trn_data, norm_tst_data = norm(trn_data, tst_data)

norm_trn_data0, norm_trn_data1 = split(norm_trn_data, trn_labels)
norm_tst_data0, norm_tst_data1 = split(norm_tst_data, tst_labels)
trn_data0, trn_data1 = split(trn_data, trn_labels)
tst_data0, tst_data1 = split(tst_data, tst_labels)

#################### CLASSIFICATION ################
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, trn_labels)
skknn.fit(norm_trn_data, trn_labels)
sksvm.fit(norm_trn_data, trn_labels)
errors['lda'] = (1-sklda.score(norm_tst_data, tst_labels))
errors['knn'] = (1-skknn.score(norm_tst_data, tst_labels))
errors['svm'] = (1-sksvm.score(norm_tst_data, tst_labels))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",exps/expr_sweep.py,binarybana/samcnet,1
"	scaler = preprocessing.MinMaxScaler()
	train = scaler.fit_transform(train)
	target = [x[4] for x in dataset]

	dataset = genfromtxt(open('test_data.csv','r'), delimiter=',', dtype=None)
	test = tokenize(dataset)
	test = scaler.transform(test)

	clf = tree.DecisionTreeClassifier()
	# clf = RandomForestClassifier(n_estimators=100)
	# clf = svm.SVC()
	clf.fit(train, target)
	result = clf.predict(test)

	dataset = dataset.astype(object)
	dataset = [dataset[i] + (result[i],) for i in range(len(dataset))]
	savetxt('result.csv', dataset, delimiter = ',', fmt=""%s"", header='language, owner_org, following_owner,no_of_stars,class', comments = '' )

if __name__ == '__main__':",GithubStarringDataset/classifier.py,NagabhushanS/DataMining,1
"        words = p.get_words(filter=lambda x: x[""num""], flatten=True)
        
        return X, words


    def model(self, C=1.0):
        # clf = Pipeline([
        #                 ('feature_selection', RandomizedLogisticRegression()),
        #                 ('classification', SVC(probability=True))
        #                ])
        # clf = SVC(C=C, kernel='linear', probability=True)
        clf = LogisticRegression(C=C, penalty=""l1"")
        
        
        return clf




    def predict_population_text(self, text, clf):",bilearn_unsupervised.py,ijmarshall/cochrane-nlp,1
"precision, recall, f1, accuracy, support, fn, roc_auc = 0, 0, 0, 0, 0, 0, 0
colors = ['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange']

k = 10
kf = KFold(n_splits = k)

start = timer()
for train, test in kf.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)
	y_pred = clf.predict(X_test)
	
	#ROC curve
	y_prob = clf.predict_proba(X_test)[:,1]
	fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=1)
	roc_auc += auc(fpr, tpr)
	plt.plot(fpr, tpr, color=colors[randint(0, len(colors)-1)])
	
	precision += precision_score(y_test, y_pred, average = 'macro')",K-Fold/SVM.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"      fpos.close()
      fneg.close()
      fpos_prob.close()
      fneg_prob.close()
                 
if __name__ == '__main__' :
      init_terms_file()
      pos_eg, neg_eg = load_examples()
      Y_train, X_train, eg = make_XY(pos_eg, neg_eg)
      print X_train.shape, Y_train.shape
      clf = SVC(probability=True, kernel='linear')      
      scores = cross_validation.cross_val_score(clf, X_train, Y_train, cv=5)
      print 'cross validation scores - ', scores
      clf.fit(X_train, Y_train) 
      predict(clf)",src/python/svm_clf.py,iesl/fuse_ttl,1
"        print (""accuracy : %0.2f"" %  accuracy_score(y_test, y_pred) '''


        '''C_2d_range = [1e-2, 1, 1e2]
        gamma_2d_range = [1e-1, 1, 1e1]
        classifiers = []
        for C in C_2d_range:
            print 'C = ' + str(C)
            for gamma in gamma_2d_range:
                print 'gamma = ' + str(gamma)
                clf = svm.SVC(C=C, gamma=gamma)
                pepe = clf.fit(features_train, samples_train).score(features_test, samples_test)
                print pepe'''

        score = classifier.fit(features_train, samples_train).score(features_test, samples_test)
        print score
        scores.append(score)

    print 'Mean accuracy rate: %.4f' % np.mean(np.array(scores))
    print 'Median accuracy rate: %.4f' % np.median(np.array(scores))",airlines.py,fernandoflores83/kaggle-airlines,1
"    return Pipeline([(""count_vectorizer"", CountVectorizer(**kwargs)),
                     (""nb_features"", CountToNBFeatures(alpha=alpha))])


def NBSVM(alpha=1, vectorizer_kwargs=None, svm_kwargs=None):
    if vectorizer_kwargs is None:
        vectorizer_kwargs = {}
    if svm_kwargs is None:
        svm_kwargs = {}
    return Pipeline([(""vectorizer"", NBVectorizer(alpha, **vectorizer_kwargs)),
                     (""svm"", LinearSVC(**svm_kwargs))])",du/model/nbsvm.py,diogo149/du,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",stable/_downloads/plot_decoding_spatio_temporal_source.py,mne-tools/mne-tools.github.io,1
"if __name__ == ""__main__"":
    spark = SparkSession\
        .builder\
        .appName(""linearSVC Example"")\
        .getOrCreate()

    # $example on$
    # Load training data
    training = spark.read.format(""libsvm"").load(""data/mllib/sample_libsvm_data.txt"")

    lsvc = LinearSVC(maxIter=10, regParam=0.1)

    # Fit the model
    lsvcModel = lsvc.fit(training)

    # Print the coefficients and intercept for linearsSVC
    print(""Coefficients: "" + str(lsvcModel.coefficients))
    print(""Intercept: "" + str(lsvcModel.intercept))

    # $example off$",examples/src/main/python/ml/linearsvc.py,maropu/spark,1
"from sklearn.svm.classes import SVC

from ..Classifier import Classifier
from ...language.Java import Java


class SVCJavaTest(Java, Classifier, unittest.TestCase):

    def setUp(self):
        super(SVCJavaTest, self).setUp()
        self.mdl = SVC(C=1., kernel='rbf', gamma=0.001, random_state=0)

    def tearDown(self):
        super(SVCJavaTest, self).tearDown()

    def test_linear_kernel(self):
        self.mdl = SVC(C=1., kernel='linear',
                       gamma=0.001,
                       random_state=0)
        self.load_iris_data()",tests/classifier/SVC/SVCJavaTest.py,nok/sklearn-porter,1
"
def qdump__std____1__complex(d, value):
    qdump__std__complex(d, value)


def qdump__std__deque(d, value):
    if d.isQnxTarget():
        qdump__std__deque__QNX(d, value)
        return
    if d.isMsvcTarget():
        qdump__std__deque__MSVC(d, value)
        return

    innerType = value.type[0]
    innerSize = innerType.size()
    bufsize = 1
    if innerSize < 512:
        bufsize = 512 // innerSize

    (mapptr, mapsize, startCur, startFirst, startLast, startNode,",share/qtcreator/debugger/stdtypes.py,qtproject/qt-creator,1
"            with open(svmTrainedData):
                print(' Opening SVM training model...')
                clf = joblib.load(svmTrainedData)
        else:
            raise ValueError('Force retraining SVM model')
    except:
        #**********************************************
        ''' Retrain NN data if not available'''
        #**********************************************
        print(' Retraining SVM data...')
        clf = svm.SVC(C = Cfactor, decision_function_shape = 'ovr', probability=True)
        clf.fit(A,Cl)
        Z= clf.decision_function(A)
        print(' Number of classes = ' + str(Z.shape[1]))
        joblib.dump(clf, svmTrainedData)
        if showClasses == True:
            print(' List of classes: ' + str(clf.classes_))

    R_pred = clf.predict(R)
    prob = clf.predict_proba(R)[0].tolist()",Other/obsolete/multifile/ML_SLP.py,feranick/SpectralMachine,1
"    cv2.waitKey(0)
    cv2.destroyAllWindows()

# test_recognition(0.3, 0.05)

# ------------------- LIVE FACE RECOGNITION -----------------------------------


if __name__ == ""__main__"":

    svc_1 = SVC(kernel='linear')  # Initializing Classifier

    trainer = Trainer()
    results = json.load(open(""../results/results.xml""))  # Loading the classification result
    trainer.results = results

    indices = [int(i) for i in trainer.results]  # Building the dataset now
    data = faces.data[indices, :]  # Image Data

    target = [trainer.results[i] for i in trainer.results]  # Target Vector",source/Train Classifier and Test Video Feed.py,its-izhar/Emotion-Recognition-Using-SVMs,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
\tlocQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
\tWed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
\tMicrosoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",xbmc/lib/libPython/Python/Lib/email/test/test_email.py,xbmc/atv2,1
"                     

# Ignore linear for now, too slow

scores = [('f1',make_scorer(f1_score))]

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score[0])
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=k,
                       scoring=score[1])
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_params_)
    print()
    print(""Grid scores on development set:"")
    print()",c_grid.py,Tweety-FER/tar-polarity,1
"    for train_index, test_index in cv:

        # Extract training and test set for current CV fold
        X_train = X[train_index,:]
        y_train = y[train_index,:]
        X_test = X[test_index,:]
        y_test = y[test_index,:]

        # Fit the different classifiers
        if algorithm == ""svm"":
            clf = svm.SVC()
        elif algorithm == ""random_forest"":
            clf = ensemble.RandomForestClassifier()
        elif algorithm == ""logistic_regression"":
            clf = linear_model.LogisticRegression()
        elif algorithm == ""ada_boost"":
            clf = ensemble.AdaBoostClassifier()

        # Evaluate the different classifiers
        clf.fit(X_train, y_train.ravel())",infomine/gender_classifier.py,rluch/InfoMine,1
"		self._normalized_t2  = self.s_t2.normalize(self._g_noNones_t2)
		self._segments_t2 = _su.segmentateData(self._normalized_t2)

	def classify(self,save=False):
		#Labels [y]
		#0 = no   anxiety
		#1 = mild anxiety
		#2 = high anxiety
		X = np.array( [self._segments_sample[0],self._segments_t1[0],self._segments_t2[0]])
		y = [0,1,2]
		clf = svm.SVC(kernel='linear', C=1.0)
		y_pred = clf.fit(X, y)

		results = []
		r = []
		for _x, _segment in enumerate(self._segments_sample):
			r.append( int(clf.predict(_segment)))
		results.append(r)
		r = []
		for _x, _segment in enumerate(self._segments_t1):",Ml.py,panzerfausten/CareMeTooServer,1
"
    print ""Training...""

    # commented out to delete .toarray() option, because len(data_X[0]) is not defined
    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.5.py,totuta/deep-supertagging,1
"print ""Time Taken to Extract Features : "", end-start 


test_data = train_data[0:3000]
test_label = train_label[0:3000] 

bnb = BernoulliNB()
gnb = GaussianNB()
mnb = MultinomialNB()
randfor = RandomForestClassifier(n_jobs=4,n_estimators=23)
supvec = SVC() 

start = time.time()
bnb.fit(train_data,train_label)
gnb.fit(train_data,train_label)
mnb.fit(train_data,train_label)
randfor.fit(train_data,train_label)

end = time.time()
ClassificationUtils.save_classifier(""bnb_cook.pickle"",bnb)",KaggleCooking.py,rupakc/Kaggle---What-s-Cooking,1
"    rng = check_random_state(37)
    rng.shuffle(p)
    X, y = X[p], y[p]
    half = int(n_samples / 2)

    # add noisy features to make the problem harder and avoid perfect results
    rng = np.random.RandomState(0)
    X = np.c_[X, rng.randn(n_samples, 200 * n_features)]

    # run classifier, get class probabilities and label predictions
    clf = svm.SVC(kernel='linear', probability=True, random_state=0)
    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])

    if binary:
        # only interested in probabilities of the positive case
        # XXX: do we really want a special API for the binary case?
        probas_pred = probas_pred[:, 1]

    y_pred = clf.predict(X[half:])
    y_true = y[half:]",m_learning/metrics/tests/test_classification.py,JosmanPS/parallel-SVM,1
"
# Perform Tf-Idf vectorization
nbr_occurences = np.sum((im_features > 0) * 1, axis=0)
idf = np.array(np.log((1.0 * len(image_paths) + 1) / (1.0 * nbr_occurences + 1)), 'float32')

# Scaling the words
stdSlr = StandardScaler().fit(im_features)
im_features = stdSlr.transform(im_features)

# Train the Linear SVM
clf = LinearSVC()
clf.fit(im_features, np.array(image_classes))

# Save the SVM
joblib.dump((clf, training_names, stdSlr, k, voc), ""bof.pkl"", compress=3)",source/bag_of_word/findFeatures.py,tz3/ml_homework,1
"    >>> from epac import Methods
    >>> _, filename = tempfile.mkstemp(suffix="".csv"")
    >>> X, y = datasets.make_classification(n_samples=12, n_features=10, \
                                            n_informative=2, random_state=1)
    >>> multi = Methods(SVM(C=1), SVM(C=10))
    >>> result_run = multi.run(X=X, y=y)
    >>> export_resultset_csv(multi.reduce(), filename)
    >>> with open(filename, 'rb') as csvfile:  # doctest: +NORMALIZE_WHITESPACE
    ...     print csvfile.read()
    key;y/true;y/pred
    LinearSVC(C=1);[1 0 0 1 0 0 1 0 1 1 0 1];[0 0 0 1 0 0 1 0 1 0 0 1]
    LinearSVC(C=10);[1 0 0 1 0 0 1 0 1 1 0 1];[1 0 0 1 0 0 1 0 1 1 0 1]
    <BLANKLINE>
    '''
    with open(filename, 'wb') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=delimiter,
                                quoting=csv.QUOTE_MINIMAL)
        result_keys = results.values()[0].keys()
        keys = []
        if ""key"" in result_keys:",epac/utils.py,neurospin/pylearn-epac,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",project/Python27_32/Lib/email/test/test_email.py,igemsoftware/SYSU-Software2013,1
"print len(n_training), len(n_testing)
# n_training['isInteracted'] = np.array([0] * len(n_training['isInteracted']))
# n_testing['isInteracted'] = np.array([0] * len(n_testing['isInteracted']))

X_train, y_train = get_params(df=p_training.append(n_training))
X_test, y_test = get_params(df=p_testing.append(n_testing))

print len(y_train), len(y_test)
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier(n_estimators=100)

# plt.figure(figsize=(10, 10))
# ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
# ax2 = plt.subplot2grid((3, 1), (2, 0))
#
# ax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly calibrated"")
for clf, name in [(lr, 'Logistic'),
                  (gnb, 'Naive Bayes'),",old/experimentos/m_svm.py,jblupus/PyLoyaltyProject,1
"    print (len(notcar_features))
    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))

    print(""start training"")
    '''Split up data into randomized training and test sets'''
    rand_state = np.random.randint(0,100)
    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y,
                                                       test_size=0.2,
                                                       random_state=rand_state)
    '''Create LearnSVC instance for training'''
    svc = LinearSVC()
    svc.fit(X_train, y_train)

    print (""Done with training"")
    print (""Saving X_scaler and svc model"")
    joblib.dump(X_scaler, X_scaler_filename)
    joblib.dump(svc, svc_filename)
    score = svc.score(X_test, y_test)
    print (""test score {:.3f}"".format(score))
    ",helper.py,manncyam/CarND-Vehicle-Detection,1
"    PCA(copy=True, iterated_power='auto', n_components=numComponents, random_state=None, svd_solver='auto', tol=0.0, whiten=False)
    
    return pca.transform(X)
       
    
def kMeans(X, k):
    return KMeans(n_clusters=k).fit(X)


def svmClassifier(X, y):
    clf = svm.SVC()
    clf.fit(X, y)
    
    svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
    
    return clf
    ",Dissertation/source/classifiers.py,raulsenaferreira/Systems-Engineering,1
"

def name_to_nclf(name):
        if name==""dt"":
        	anclf = nclf('dt',tree.DecisionTreeClassifier(),['max_depth','min_samples_split'], [[1, 60],[2,100]])
	if name==""bdt"":
		anclf = nclf('bdt',AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2)), ['learning_rate','n_estimators'], [[0.01,2.0],[100,1000]])
	if name==""xgb"":
		anclf = nclf('xgb',XGBoostClassifier(), ['n_estimators','eta'], [[10,1000],[0.01,1.0]])
	if name==""svm"":
		anclf = nclf('svm',SVC(probability=True, cache_size=7000), ['C','gamma'], [[1.0,1000.0],[1E-6,0.1]])
	if name==""nn"":
		anclf = nclf('nn',""no classifier needed for nn"", ['n_hidden_layers','dimof_middle'], [[0,1],[100,500]])
        return anclf 


#######################################################################################################################################################################################################
#######################################################################################################################################################################################################
#######################################################################################################################################################################################################
",learningml/GoF/classifier_eval.py,weissercn/learningml,1
"from sklearn import svm
from sklearn import datasets

clf = svm.SVC()
iris = datasets.load_iris()
X, y = iris.data, iris.target
models = clf.fit(X, y)

import pickle
s = pickle.dumps(clf)
clf2 = pickle.loads(s)

result = clf2.predict(X[0:1])",sklearnLearning/quickStart/saveAModel.py,zhuango/python,1
"class EvalNB(object):
    """"""docstring for EvalNB""""""
    b_preict = []
    def __init__(self, alpha):
        super(EvalNB, self).__init__()
        self.alpha_value = alpha

  

    def init_classifier(self):
        # clf = svm.SVC(kernel = 'rbf', gamma=self.gamma_value, C=self.c_value)
        # print ""SVM configuration... \n\n"", clf
        clf = MultinomialNB(alpha=self.alpha_value, class_prior=None, fit_prior=True)
        return clf



    def fit_train_data(self, clf, a_train, b_train):
        # clf = svm.SVC(kernel = 'rbf', gamma=gamma_value, C=c_value)
        # print ""SVM configuration... \n\n"", clf",implementation/evaluation/nb.py,imink/UCL_COMPIG15_Project,1
"            pred = model.predict(feature).reshape((ngrid, ngrid))
        extent = (0, 1, 0, 1)
        ax.imshow(pred, extent=extent, vmin = 0, vmax = 1,origin='lower',
            aspect='auto', cmap='cool')

if __name__ == '__main__':
    #add your models here
    models = ((""No model"", None),
              (""Logistic Regression"", LogisticRegression(C = 1e6, penalty = 'l2')),
              (""Logistic Regression (L1)"", LogisticRegression(C = 1e6, penalty = 'l1')),
              (""Linear SVM"", svm.SVC(kernel='linear', C=1e3)),
              (""Kernel SVM (polynomial)"", svm.SVC(kernel='poly', C=1e6)),
              (""Kernel SVM (RBF)"", svm.SVC(kernel='rbf', C=1e6)),
              (""Random Forest"", RandomForestClassifier()))
    i = InteractiveML(models)
    plt.show()
    while (True):
        time.sleep(1)",interactive_ml.py,ecgeil/mlworkshop,1
"    print(""False Fail Rate: "", rate)

    return rate

"""""" Train Model and Print Score """"""
def train_and_score(X, y):
    X_train, X_test, y_train, y_test = split_data(X, y)

    clf = Pipeline([
        ('reduce_dim', SelectKBest(chi2, k=2)),
        ('train', LinearSVC(C=100))
    ])

    scores = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=2)
    print(""Mean Model Accuracy:"", np.array(scores).mean())

    clf.fit(X_train, y_train)

    confuse(y_test, clf.predict(X_test))
    print()",model.py,sachanganesh/student-performance-prediction,1
"                print pca_data.shape
                ipca = IncrementalPCA(n_components=10)
                ipca.fit(pca_data)
                print ipca.components_.shape, ipca.mean_.shape
                finetune(img, win)
                modelFile = 'cnn-tracker/train_val_affine_deploy.prototxt'
                pretrainedFile = 'cnn-tracker/affine_finetune_iter_20.caffemodel'
                net = caffe.Net(modelFile, pretrainedFile)
                net.set_phase_test()
                net.set_mode_gpu()
                svmClf = svm.LinearSVC()

                videoWri = cv2.VideoWriter('cnn-tracker/result_videos/'+seqName+'.avi', cv2.cv.CV_FOURCC('F','M','P','4'), 20, (img.shape[1], img.shape[0]))
                if(not videoWri.isOpened()):
                    print 'Unable to write the video'
            else:
                winMax = prediction(net, imgName, win, templates, weights, ipca, svmClf)
                win, lost = judgment(winMax, rect, lost)
                templates, ipca = update(templates, imgName, win, ipca)
                draw(videoWri, imgName, imgNum, win, lost)",video_demo_finetune_svm.py,philo-zhang/cnn-tracker,1
"#y = y2
""""""
pca = PCA(40)
train = pca.fit_transform(train)

""""""
# params grid for SVM
grid = [{'C': [1, 3, 5, 8, 10, 12, 15, 20, 50, 100, 500, 1000, 10000], 'kernel': ['linear', 'rbf'],
        'gamma': [0.0000001, 0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1]}
        ]
svr = svm.SVC()
""""""

# params grid for SGD
grid = [{'loss' : ['log', 'hinge'], 'alpha': [0.005, 0.008, 0.01, 0.012, 0.05, 0.1, 0.15, 0.2, 0.5, 1, 10], 'penalty': ['l1', 'elasticnet'], 'l1_ratio': [0.2, 0.25, 0.3, 0.4, 0.45, 0.48, 0.5, 0.52,  0.55]}]
sgd = SGDClassifier()

#clf = grid_search.GridSearchCV(svr, grid, cv=3)
clf = grid_search.GridSearchCV(sgd, grid, cv=4)
clf.fit(train, y)",grid_search.py,NCBI-Hackathons/DASH_cell_type,1
"        data_dict=pickle.load(f)
    with open('pickles/inner_cv.pickle','rb') as f:
        inner_cv=pickle.load(f) 
    
    scores= {'train': [], 'test': []}
    for i in range(25):
        X_train= np.array([data_dict['data'][inner_cv['X_train'][i][j]] for j in range(len(inner_cv['X_train'][i]))])
        X_test= np.array([data_dict['data'][inner_cv['X_test'][i][j]] for j in range(len(inner_cv['X_test'][i]))])
        y_train= inner_cv['y_train'][i]
        y_test= inner_cv['y_test'][i]
        est = svm.SVC()
        est.fit(X_train, y_train)
        scores['train'].append(est.score(X_train, y_train))
        scores['test'].append(est.score(X_test, y_test))
    
    with open('pickles/csvc_scores.pickle','wb') as f:
        pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL) 

    return 
    ",estimators.py,jrabenoit/marvin,1
"end = time.time()
print ""Time Taken to extract all features : "", end-start

train_data,test_data,train_label,test_label = cross_validation.train_test_split(data,class_labels,test_size=0.3)

# Initializing the classifiers 

rf = RandomForestClassifier(n_estimators=101)
ada = AdaBoostClassifier(n_estimators=101)
gradboost = GradientBoostingClassifier(n_estimators=101)
svm = SVC()
gnb = GaussianNB()

classifiers = [rf,ada,gradboost,svm,gnb]
classifier_names = [""Random Forests"",""AdaBoost"",""Gradient Boost"",""SVM"",""Gaussian NB""]

print ""Starting Classification Performance Cycle ...""
start = time.time()

for classifier,classifier_name in zip(classifiers,classifier_names):",PizzaCombinedModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"	np.save('testX', testX)
	np.save('trainY', trainY)
	np.save('testY', testY)
	#'''
	#sss = StratifiedShuffleSplit(mnist.target, 1, test_size=0.1, train_size=0.1, random_state=0)
	#for train_index, test_index in sss:
	#	trainX, testX = mnist.data[train_index], mnist.data[test_index]
	#	trainY, testY = mnist.target[train_index], mnist.target[test_index]


	clf = svm.SVC(kernel=arc_cosine, cache_size=4096)
	#clf = svm.SVC(kernel = 'poly') #gaussian kernel is used
	clf.fit(trainX, trainY)

	pred = clf.predict(testX)
	print accuracy_score(testY, pred)
	print('total : %d, correct : %d, incorrect : %d\n' %(len(pred), np.sum(pred == testY), np.sum(pred != testY)))

	print('Test Time : %f Minutes\n' %((time.time()-start)/60))
",autoencoderDLKM/cifar10/arc_cosine.py,akhilpm/Masters-Project,1
"events = mne.find_events(raw, stim_channel='UPPT001')
event_id = {""faces"": 1, ""scrambled"": 2}
tmin, tmax = .1, 0.3
decim = 16  # decimate to make the example faster to run
epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True,
                    picks=picks, baseline=None, preload=True,
                    reject=dict(mag=1.5e-12), decim=decim, verbose=False)

scaler = StandardScaler()

SVC_decision = force_predict(SVC(kernel='linear'), mode='decision_function')
pipeline = Pipeline([('scaler', StandardScaler()),
                     ('svc', SVC_decision)])
# decoder
gat = GeneralizationAcrossTime(n_jobs=-1, clf=pipeline)
gat.fit(epochs)
gat.score(epochs, scorer=scorer_auc)
gat.plot()

",sandbox/decoding/compress_gat.py,kingjr/meg_perceptual_decision_symbols,1
"X = df[['sack_diff', 'sack_ydiff', 'pens_diff', 'poss_diff', 'third_diff', 'turn_diff', 'pass_diff', 'rush_diff']].copy()
# X = df[['poss_diff', 'third_diff', 'turn_diff', 'pass_diff', 'rush_diff']].copy()

# Create results vector (a home win = 1, a home loss or tie = 0)
y = np.array(np.where(df['home_score'] > df['away_score'], 1, 0))

title = ""Learning Curves (SVM)""#, RBF kernel, $\gamma=0.001$)""
# SVC is more expensive so we do a lower number of CV iterations:
cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=10,
                                   test_size=0.2, random_state=0)
estimator = svm.LinearSVC()
plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)

plt.show()


"""""" Train, test, and predict the algorithm """"""
# Scale the sample data
scaler = preprocessing.StandardScaler().fit(X)
X = scaler.transform(X)",binary_class.py,JVP3122/Python-Machine-Learning-NFL-Game-Predictor,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = ICAP.icap(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_ICAP.py,jundongl/scikit-feature,1
"    return features, classifications


def learn(args):
    """"""
    train model on input data and store model
    """"""
    raw_data = util.load_data_dir(args.data_dir)
    features, classifications = training_data(raw_data)

    model = svm.SVC(kernel=SVM_KERNEL)
    LOG.info('training model on %s data points', len(features))
    model.fit(features, classifications)
    LOG.debug('done training')

    if args.model:
        LOG.debug('writing model to %s', args.model)
        with open(args.model, 'wb') as fp:
            pickle.dump(model, fp)
",analyzer/learn.py,Magical-Chicken/378-project,1
"# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

iris = datasets.load_iris()
X = iris.data[:, :2] 
y = iris.target

svc = svm.SVC(kernel='rbf', C=1, gamma=0).fit(X, y)

x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))

plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.YlGnBu, alpha=0.8)",codes/Support Vector Machines/svm_pt.py,GaryLv/GaryLv.github.io,1
"# we have to normalize the data before supplying them to our classifier
X -= X.mean(axis=0)
X /= X.std(axis=0)

# prepare classifier
from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa

# Define a monte-carlo cross-validation generator (reduce variance):
n_splits = 10
clf = SVC(C=1, kernel='linear')
cv = ShuffleSplit(len(X), n_splits, test_size=0.2)

# setup feature selection and classification pipeline
from sklearn.feature_selection import SelectKBest, f_classif  # noqa
from sklearn.pipeline import Pipeline  # noqa

# we will use an ANOVA f-test to preselect relevant spatio-temporal units
feature_selection = SelectKBest(f_classif, k=500)  # take the best 500
# to make life easier we will create a pipeline object",examples/decoding/plot_decoding_spatio_temporal_source.py,trachelr/mne-python,1
"import numpy as np 
from clustered import ClusteredEstimator
from sklearn.svm import LinearSVC
from copy import deepcopy 

class ClusteredClassifier(ClusteredEstimator): 
    def __init__(self, k=10, base_model = LinearSVC(), verbose=False): 
        ClusteredEstimator.__init__(self, k,  base_model, verbose)
",treelearn/clustered_classifier.py,capitalk/treelearn,1
"    gr1_idx = data[data.DX_Group == gr[0]].index.values
    gr2_idx = data[data.DX_Group == gr[1]].index.values
    
    gr1_f = X[gr1_idx, :]
    gr2_f = X[gr2_idx, :]
    
    x = X[np.concatenate((gr1_idx, gr2_idx))]
    y = np.ones(len(x))
    y[len(y) - len(gr2_idx):] = 0

    estim = svm.SVC(kernel='linear')
    sss = cross_validation.StratifiedShuffleSplit(y,
                                                  n_iter=nb_iter,
                                                  test_size=0.2)
    # 1000 runs with randoms 80% / 20% : StratifiedShuffleSplit
    counter = 0
    for train, test in sss:
        Xtrain, Xtest = x[train], x[test]
        Ytrain, Ytest = y[train], y[test]
        Yscore = estim.fit(Xtrain,Ytrain)",learn_regions_baseline_fdg_pet.py,mrahim/adni_fdg_pet_analysis,1
"    [172.10, 177.78, 230.51, 0.77, 0.97],[172.35, 178.07, 230.46, 0.77, 0.97],[172.07, 177.62, 230.57, 0.77, 0.97],[172.48, 178.13, 230.45, 0.77, 0.97],
    [171.91, 177.54, 230.52, 0.77, 0.97],[172.64, 178.02, 230.40, 0.77, 0.97],[172.37, 178.00, 230.53, 0.77, 0.97],[172.18, 177.64, 230.30, 0.77, 0.97],
    [172.02, 177.58, 230.47, 0.77, 0.97],[223.25, 229.93, 229.77, 1.00, 0.97],[223.37, 229.94, 229.85, 1.00, 0.97],[222.35, 229.00, 229.80, 1.00, 0.97],
    [221.94, 228.78, 229.62, 1.00, 0.97],[221.68, 228.36, 229.59, 0.99, 0.97],[221.87, 228.55, 229.86, 0.99, 0.97],[222.08, 228.79, 229.58, 1.00, 0.97],
    [223.70, 230.29, 229.94, 1.00, 0.97],[222.94, 229.59, 229.70, 1.00, 0.97],[223.41, 229.81, 229.90, 1.00, 0.97],[222.17, 229.00, 229.88, 1.00, 0.97],
    [223.46, 229.91, 229.86, 1.00, 0.97],[221.85, 228.37, 229.75, 0.99, 0.97],[223.26, 229.89, 229.83, 1.00, 0.97],[223.35, 229.67, 229.92, 1.00, 0.97],
    [222.78, 229.50, 229.92, 1.00, 0.97],[222.19, 229.08, 229.72, 1.00, 0.97],[223.67, 230.58, 229.93, 1.00, 0.97]])
    y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
    from sklearn.svm import SVC
    from sklearn.svm import NuSVC
    clf = NuSVC(probability=True)
    clf.fit(X, y)
    #print(""Device 4 Type Data Set: "")
    #print(clf.predict([[221.41, 227.81, 229.60, 1.00, 0.95]]))
    #print(""Device 2 Type Data Set: "")
    #print(clf.predict([[112.74, 118.39, 227.55, 0.55, 0.93]]))
    #print(""Device 1 Type Data Set: "")
    #print(clf.predict([[56.28, 64.18, 227.84, 0.29, 0.96]]))
    #print(""Device 1 Type Data Set: "")
    #print(clf.predict([[59.77, 63.66, 231.34, 0.24, 0.92]]))",App_55/scripts/svmalgo.py,vibhatha/mean,1
"            #pdb.set_trace()
            #with open('outer_tree%d_%d.pkl'%(d,count), 'wb') as fptr:
            #    cPickle.dump(outer_tree, fptr, -1)
            for j,fraction in enumerate([0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]):
                new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
  
                from sklearn.svm import SVC
                from sklearn.neighbors import KNeighborsClassifier
                from sklearn.tree import DecisionTreeClassifier
    
                clf= SVC(kernel='linear', C=10)
                clf.fit(new_trn, trn_lbl)
                tst_predict= clf.predict(new_tst)
                svm_accs[count, d, j]= mean(tst_predict==tst_lbl)

                clf= KNeighborsClassifier(n_neighbors=3)
                clf.fit(new_trn, trn_lbl)
                tst_predict= clf.predict(new_tst)
                knn_accs[count, d, j]= mean(tst_predict==tst_lbl)
",problems/techTCrun_new.py,lioritan/Thesis,1
"from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import sys
sys.path.append("".."")
import data_loader
import matplotlib.pyplot as plt
import numpy as np


def process_with_svm(X_train, X_test, y_train, y_test, kernel=""linear""):
    clf = svm.SVC(kernel=kernel)
    clf.fit(X_train, X_test)

    print clf.support_vectors_
    results = clf.predict(y_train)

    accuracy = accuracy_score(results, y_test)
    print accuracy

    # draw the picture",ch06/exercise_6.2.py,jensen-lau/watermelon-book,1
"# For version 0.18
# from sklearn.model_selection import train_test_split


### set the random_state to 0 and the test_size to 0.4 so
### we can exactly check your result
features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels)

###############################################################
# DONT CHANGE ANYTHING HERE
clf = SVC(kernel=""linear"", C=1.)
clf.fit(features_train, labels_train)

print clf.score(features_test, labels_test)


##############################################################


def submitAcc():",Lectures/cross_validation.py,ZhukovGreen/UMLND,1
"import numpy as np
def SvC(X,Y):	
	#X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
	#y = np.array([1, 1, 2, 2])
	from sklearn.svm import SVC
	clf = SVC()
	clf.fit(X, y) 
	SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
	    gamma=0.0, kernel='rbf', max_iter=-1, probability=False,
	    random_state=None, shrinking=True, tol=0.001, verbose=False)
	#print(clf.predict([[-0.8, -1]]))
	",svm.py,CreationLabs/IndixHackathon,1
"        sparse (bool): whether to use l1 (sparse) regularization or l2; default True
        saveData (bool): save the results to model directory? default True
        n_jobs (int): number of jobs to pass to cv
    '''
    if sparse:
        penalty = 'l1'
    else:
        penalty = 'l2'
    if classifier == 'svc':
        from sklearn.svm import LinearSVC
        algorithm = LinearSVC(penalty = penalty)
        if penalty == 'l1':
            algorithm.set_params(dual=False)
    if cv:
        from sklearn.grid_search import GridSearchCV
        paramsToSearch = []
        paramsToSearch.append({'C': [.001,.005,.01,.1,1,10]})
        if cvmethod == 'sss':
            from sklearn.cross_validation import StratifiedShuffleSplit
            cvalgorithm = StratifiedShuffleSplit(strata, n_iter = n_iter, test_size = .3)",predictive_reliability/runCVMotionTest.py,neurohackweek/kids_rsfMRI_motion,1
"    """"""
    lessons_train = list()
    outcomes_train = list()
    for _ in tnrange(examples):
        cameras_train = eg_train.generate()
        match_id = get_match_id(cameras_train)
        goods, bads = make_good_bad(cameras_train, match_id)
        make_work(fd_train, lessons_train, outcomes_train, goods, 1)
        make_work(fd_train, lessons_train, outcomes_train, bads, 0)

    clf = svm.SVC()

    print('fitting')
    start = time.time()
    clf.fit(lessons_train, outcomes_train)
    end = time.time()
    print('fitting took {} seconds'.format(end - start))
    return clf

",pelops/models/makesvm.py,d-grossman/pelops,1
"
    def get_all_attributes(self):
        attributes = []
        p = portfolio.Portfolio()
        all_portfolio = p.get_all_portfolio_ratios()
        for count, i in enumerate(all_portfolio):
            attributes.append( [ i[0], [ i[1]['positive_returns'], i[1]['negative_returns'], self.get_avg_sharpe(i[1]['companies'])] ] )
        return attributes

    def grid_search(self, X, y):
        svc = svm.SVC()
        parameters = {'gamma': np.logspace(-3, -1, 3)}
        clf = grid_search.GridSearchCV(svc, parameters)
        clf.fit(X, y)
        return clf

    def ml(self, user_list):
        attributes = self.get_all_attributes()
        train_features = []
        train_class = []",core/risk/userClassifier.py,adarshdec23/Market,1
"def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
        
    ### your code goes here!
    from sklearn.svm import SVC
    clf = SVC(kernel=""rbf"", gamma=1.0)
    clf.fit(features_train, labels_train)
    return clf
    ",intro_to_machine_learning/lesson/lesson_2_svm/ClassifyNB.py,tuanvu216/udacity-course,1
"
#gmm_model = mixture.GMM(n_components=GMM_N_COMPONETNS)

#-------------------
# train
#-------------------
def train(x_train,y_train):
    
    #pipeline = Pipeline([
    #    ('scl', StandardScaler()),
    #    ('clf', SVC(probability=True))
    #])
        
    #param_grid = [{'clf__kernel': ['linear'], 'clf__C': [1, 1.5, 2, 5]}]
    #estimator = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
    #print 'x_train len:',len(x_train)
    #print 'y_train len:',len(y_train)
    #print 'y_train:',y_train
    #estimator = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
    estimator = SVC()",main_train.py,aixiwang/audio_feature_sender,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf, language='go').export()
print(output)

""""""
package main

import (",examples/classifier/LinearSVC/go/basics.py,nok/sklearn-porter,1
"			""blend_group"":   ""KNN"",  
			    ""getter"":   ""Lestimators = model_last.get_params()['n_neighbors']"",
			    ""updater"":   ""tries_left = 0"",
			     ""setter"":   """",
			     ""generator"":  ""n_neighbors=3 @@ "" \
						   ""n_neighbors=5 @@ "" \
						  ""n_neighbors= 8 @@ ""  \
						   ""n_neighbors=5  weights='distance' "" 
						 
			},
			{""model"": 'Pipeline([(""pre"", preprocessing.StandardScaler()),(""svc"", svm.SVC(probability=True, random_state=Lnum, C=1.0))])',
			""blend_group"":   ""SVM"",  
			    ""getter"":  ""Lc = model_last.get_params()['svc__C']"",
			    ""updater"":  ""Lc *= 0.8"",
			     ""setter"":   ""svc__C=Lc"",
			     ""generator"": ""svc__kernel= 'linear', svc__C=1 @@"" \
						 ""svc__kernel= 'linear', svc__C=0.4 @@"" \
						  ""svc__kernel= 'linear', svc__C=2 @@"" \
						  ""svc__kernel= 'linear', svc__C=4 @@"" \
						  ""svc__kernel= 'poly', svc__degree=2 @@"" \",lib/engine_models.py,djajetic/AutoML3,1
"        Callable object that returns a scalar score; greater is better.

    Examples
    --------
    >>> from sklearn.metrics import fbeta_score, make_scorer
    >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
    >>> ftwo_scorer
    make_scorer(fbeta_score, beta=2)
    >>> from sklearn.model_selection import GridSearchCV
    >>> from sklearn.svm import LinearSVC
    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
    ...                     scoring=ftwo_scorer)
    """"""
    sign = 1 if greater_is_better else -1
    if needs_proba and needs_threshold:
        raise ValueError(""Set either needs_proba or needs_threshold to True,""
                         "" but not both."")
    if needs_proba:
        cls = _ProbaScorer
    elif needs_threshold:",mlens/externals/sklearn/scorer.py,flennerhag/mlens_dev,1
"
logmessage(""Validation dataset is %s"" % len(validationSet), info)
logmessage(""Validation dataset features are %s"" % len(validationSet.domain), info)

print len(converted_train_data)
print len(converted_train_data[0])

converted_train_targets = le.transform([d[trainingSet.domain.class_var].value for d in trainingSet ])
print converted_train_targets

clf = svm.SVC(kernel=kernel,C=C)
clf.fit(converted_train_data, converted_train_targets)
logmessage(""Model learnt"", success)

# Performances


# Convert Test Dataset
converted_test_data = ([le.transform([ d[f].value for f in validationSet.domain if f != validationSet.domain.class_var]) for d in validationSet])
#converted_test_data = [dict(enumerate(d)) for d in converted_test_data]",step5.py,Sh1n/AML-ALL-classifier,1
"        fds.append(fd)
        labels.append(1)

    # Load the negative features
    for feat_path in glob.glob(os.path.join(neg_feat_path,""*.feat"")):
        fd = joblib.load(feat_path)
        fds.append(fd)
        labels.append(0)

    if clf_type is ""LIN_SVM"":
        clf = LinearSVC()
        print ""Training a Linear SVM Classifier""
        clf.fit(fds, labels)
        # If feature directories don't exist, create them
        if not os.path.isdir(os.path.split(model_path)[0]):
            os.makedirs(os.path.split(model_path)[0])
        joblib.dump(clf, model_path)
        print ""Classifier saved to {}"".format(model_path)",object-detector/train-classifier.py,bikz05/object-detector,1
"    return samples

if __name__ == '__main__':
    arguments = docopt.docopt(__doc__)
    filters = {'dancing': 0, 'walking': 1, 'sitting':2}
    if arguments['--model']:
        clf = joblib.load(arguments['--model'])
    else:
        training = dataset('../datasets/training', filters)

        svr = svm.SVC()
        exponential_range = [pow(10, i) for i in range(-4, 1)]
        parameters = {'kernel':['linear', 'rbf'], 'C':exponential_range, 'gamma':exponential_range}
        clf = grid_search.GridSearchCV(svr, parameters, n_jobs=8, verbose=True)
        clf.fit(training.data, training.target)
        joblib.dump(clf, '../models/1s_6sps.pkl')
        print clf

    print 'best_score:', clf.best_score_, 'best C:', clf.best_estimator_.C, 'best gamma:', clf.best_estimator_.gamma
    validation = dataset('../datasets/validation')",src/dataset.py,la3lma/movement-analysis,1
"    
    def predict_proba(self, X):
        return self.estimator.predict_proba(X)

class Stacker(BaseEstimator):
    '''
    !NOTE: This stack-estimator only for binary classification because of predict(self, X) function returns ! 
    
    (Инициализация идет с помощью списка слоев. Каждый слой - список классификаторов.)
    Example, how to use:
    stkr = Stacker([ [XGB, SVC(with probas=True)],[RF, KNN],[LR] ])
    # init with three layer, on last Logistic Regression.
    stkr.fit(X,y)
    stkr.predict_proba(X_test) 
    '''
    def __init__(self, estimators):
        '''
        Parameters
        ----------
        estimators : array-like, shape = [n_layers, {n_estimators (diff for each layer)}]",Python/ML/class for binary blending & stack (logloss min)/Stacking.py,Amir14111/sandbox,1
"    def optimise_sgd(self):
        self.__log.write(""Optimising SGD model"")
        self.__sgd_tuned_auroc_ = self.__cv_decorator_(self.__sgd_tuned_auroc_)
        optimal_sgd_pars, info, _ = optunity.maximize_structured(self.__sgd_tuned_auroc_, self.__sgd_space_, num_evals=150)
        print(""Optimal parameters"" + str(optimal_sgd_pars))
        print(""AUROC of tuned SVM: %1.3f"" % info.optimum)

    def __train_model_(self, x_train, y_train, kernel, C, logGamma, degree, coef0):
        """"""A generic SVM training function, with arguments based on the chosen kernel.""""""
        if kernel == 'linear':
            model = SVC(kernel=kernel, C=C, cache_size=7000)
        elif kernel == 'poly':
            model = SVC(kernel=kernel, C=C, degree=degree, coef0=coef0, cache_size=7000)
        elif kernel == 'rbf':
            model = SVC(kernel=kernel, C=C, gamma=10 ** logGamma, cache_size=7000)
        else:
            raise ArgumentError(""Unknown kernel function: %s"" % kernel)
        model.fit(x_train, y_train)
        return model
    ",model.py,navjotk/diabetic-dissert,1
"
x = x / 255.0

features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(x, y, test_size=0, random_state=42)

if os.path.isfile('./classifier_full.pickle'):
	f = open('classifier_full.pickle', 'rb')
	clf = pickle.load(f)

else:
	clf = svm.SVC(kernel=""linear"")

	t0 = time()
	clf.fit(features_train, labels_train)
	print ""Training time: "", round(time() - t0, 3), ""s""

	f = open('classifier_full.pickle', 'wb')
	pickle.dump(clf, f)
	f.close()
",Python/digit_recognition.py,samkit-jain/Handwriting-Recognition,1
"        self.records = None
        self.dictionary = None

        ratio = 'auto'
        verbose = False
        resampler = Constants.RESAMPLER
        classifier = Constants.DOCUMENT_CLASSIFIER
        random_state = Constants.DOCUMENT_CLASSIFIER_SEED
        classifiers = {
            'logistic_regression': LogisticRegression(C=100),
            'svc': SVC(),
            'kneighbors': KNeighborsClassifier(n_neighbors=10),
            'decision_tree': DecisionTreeClassifier(),
            'nu_svc': NuSVC(),
            'random_forest': RandomForestClassifier(n_estimators=100)
        }
        samplers = {
            'random_over_sampler': RandomOverSampler(
                ratio, random_state=random_state, verbose=verbose),
            'smote_regular': SMOTE(",source/python/etl/reviews_preprocessor.py,melqkiades/yelp,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = MIM.mim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_MIM.py,jundongl/PyFeaST,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_22_01_2015.py,magic2du/contact_matrix,1
"def test_base_estimator():
    """"""Test different base estimators.""""""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)
    clf.fit(X, y_regr)

    clf = AdaBoostRegressor(SVR(), random_state=0)",sklearn/ensemble/tests/test_weight_boosting.py,B3AU/waveTree,1
"
def test_performance():
    iris = datasets.load_iris()
    iris_X = iris.data
    iris_y = iris.target
    iris_y = iris_y/2

    #simple test
    X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=.5, \
                                                        random_state=0)
    svc = svm.SVC(kernel='linear')
    scores = svc.fit(X_train, y_train).decision_function(X_test)
    r = svc.predict(X_test)

    #test performence method: accuracy, confusion matrix, precision, recall
    #ROC, AUC, classification report
    
    #accuracy: number of correct prediction / number of all cases
    print ""\ntest accuracy:""
    print metrics.accuracy_score(y_test, r)",python/backup/skeleton.py,Healthcast/RSV,1
"#!/usr/bin/python3

from sklearn import svm

n_samples = [0,0]
n_features = [1,1]
training_samples = [n_samples, n_features]
labels = [0,1]

clf = svm.SVC()
clf.fit(training_samples,labels)

print(clf.predict([[2.,2.]]))
print(clf.predict([[0.,0.]]))",Predictor/predictor.py,paullj1/afp-dc,1
"# Extract the hog features
list_hog_fd = []
for feature in features:
    fd = hog(feature.reshape((28, 28)), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
    list_hog_fd.append(fd)
hog_features = np.array(list_hog_fd, 'float64')

print ""Count of digits in dataset"", Counter(labels)

# Create an linear SVM object
clf = LinearSVC()

# Perform the training
clf.fit(hog_features, labels)

# Save the classifier
joblib.dump(clf, ""digits_cls.pkl"", compress=3)",projects/sklearn/downloaded/digitRecognition/generateClassifier.py,bensinghbeno/design-engine,1
"    fh = open(path, 'w')
    fh.write(liac_arff.dumps(arff_data))
    fh.close()


def __available_classifiers():
    available_clfs = dict()
    # features of all available classifiers
    Classifier = collections.namedtuple('Classifier', ['idf', 'full_name', 'function_call',
                                                       'scaling_possible', 'predict_proba', 'numeric_labels'])
    available_clfs[""svm""] = Classifier(""svm"", ""Support Vector Machine"", svm.SVC(probability=True), True, True, False)
    available_clfs[""svm_gs1""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.1, coef0=0.01, gamma=0.01),
                                           True, True, False)
    available_clfs[""svm_gs2""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.01, coef0=0.01, gamma=0.0),
                                           True, True, False)
    available_clfs[""mnb""] = Classifier(""mnb"", ""Multinomial Naive Bayes"", naive_bayes.MultinomialNB(), False, True,
                                       False)  # MNB can't do default scaling: ValueError: Input X must be non-negative
    available_clfs[""knn""] = Classifier(""knn"", ""k Nearest Neighbour"", neighbors.KNeighborsClassifier(), True, True,",glad-main.py,rug-compling/glad,1
"
Xtrain = np.concatenate((Xtrain_r, Xtrain_c, Xtrain_p_h, Xtrain_p_b), axis=1)
Xtest = np.concatenate((Xtest_r, Xtest_c, Xtest_p_h, Xtest_p_b), axis=1)

import numpy
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.grid_search import GridSearchCV

CS = numpy.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001])
model = svm.LinearSVC()
grid_search = GridSearchCV(estimator=model, param_grid=dict(C=CS), n_jobs=3)

grid_search.fit(Xtrain, ytrain_r)

print 'best c:', grid_search.best_params_


a = dt.now()
model = svm.LinearSVC(C=grid_search.best_params_['C'])",src/scripts/grid_search_c.py,yassersouri/omgh,1
"    # split into a training and testing set
    # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)

    x_train = x
    y_train = y

    # Set the parameters by cross-validation
    C=10
    gamma=5e-7
    
    clf = svm.SVC(C=C, gamma=gamma)

    # We learn the digits on the first half of the digits
    clf.fit(x_train, y_train)

    # Pickle the model!
    outf = open('training.pkl', 'wb')
    pickle.dump(clf, outf)
    outf.close()
",DigitRecognizer/learndigits.py,n7jti/kaggle,1
"        labels = np.array(labels)

    label_names = sorted([i for i in set(labels)])

    summary = ClassificationSummary(label_names)

    print ""number of features:"", features.shape[1]

    #classifiers
    knn = neighbors.KNeighborsClassifier(n_neighbors=1)
    svmc = svm.SVC(kernel='rbf', C=300)
    nb = naive_bayes.GaussianNB()

    print ""\n@@@All Features""

    print ""\n###NB""
    scores = cross_validation.cross_val_score(nb, features, labels, cv=folds)
    print(""NB Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std()))
    predicted = cross_validation.cross_val_predict(nb, features, labels, cv=folds)
    cr = classification_report(labels, predicted)",scripts/dcase2016/train_and_classify.py,pymir3/pymir3,1
"    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatches(dataset_Monte, weights)


if __name__ == ""__main__"":
    '''X = np.array([[-1,-1],[-2,-1], [1,1], [2,1]])
    y = np.array([1,1,2,2])
    clf = SVC()
    clf.fit(X,y)
    print(clf.predict([[-0.8,-1]]))'''
    #clf = SVC()
    clf = SVC(C = 1000, kernel = 'linear')  
    monteavgavgQP = list()
    monteavgavgPLA = list()
    approxavgQP = list()
    
    for j in range(20):",Week 7/qp100.py,pramodh-bn/learn-data-edx,1
"    print(""开始训练..."")

    
#    clf = KNeighborsClassifier() 
    from sklearn.linear_model import LogisticRegression  
    clf = OneVsOneClassifier(LogisticRegression(penalty='l2'))
    clf = LogisticRegression(penalty='l2')
    from sklearn.ensemble import RandomForestClassifier  
    # clf = RandomForestClassifier(n_estimators=8)  
    # clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,gamma=0.0, kernel='rbf', probability=False, shrinking=True, tol=0.001,verbose=False)
#    clf = OneVsRestClassifier(svm.SVC(0.1))
#    clf = OneVsOneClassifier(svm.SVC(C=0.07, kernel = 'linear'))
    
    clf.fit(trainX, trainY)

    print(""开始预测..."")
#    for i in range(trainFlieNum,fileNum):
    for i in range(0,fileNum):        
        testX = X[sum(totNum[0:i]):sum(totNum[0:i+1])]
        testY = Y[sum(totNum[0:i]):sum(totNum[0:i+1])]",Uniwalk_pre/1. create_data.py,songjs1993/DeepSimRank,1
"from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA


data = np.genfromtxt('datasets/exp.csv', delimiter=',')

X = data[:, :2] 
y = data[:, [2]] 

# magic
clf = svm.SVC(kernel='rbf', C=1.0)
clf.fit(X, y)

# visualize
h = .02  # step size in the mesh

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))",ScikitlearnPlayground/ClassificationSVM.py,eboreapps/Scikit-Learn-Playground,1
"        self.proba = proba
        self.n_cores = n_cores

        if preproc_pipe == 'default':
            scaler = StandardScaler()
            transformer = SelectAboveCutoff(1, fisher_criterion_score)
            preproc_pipe = Pipeline([('transformer', transformer),
                                     ('scaler', scaler)])

        if base_clf is None:
            base_clf = SVC(C=1.0, kernel='linear', probability=True,
                           decision_function_shape='ovo')
        self.base_clf = base_clf

        base_pipe = preproc_pipe.steps
        base_pipe.extend([('base_clf', self.base_clf)])
        self.base_pipe = Pipeline(base_pipe)
        self.base_pipes = []

        if meta_clf is None:",skbold/estimators/roi_stacking_classifier.py,lukassnoek/skbold,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = FCBF.fcbf(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",PyFeaST/example/test_FCBF.py,jundongl/PyFeaST,1
"SVD1 = TruncatedSVD(n_components=300)
SVD2 = TruncatedSVD(n_components=200)
feature1 = SVD1.fit_transform(feature1)
feature2 = SVD2.fit_transform(feature2)

feature = np.hstack((feature1, feature2))
feature_unsec = feature[labels == 2]
feature = feature[labels != 2]
labels = labels[labels != 2]

clf1 = SVC(C=10000, gamma=0.75, probability=True)
#clf2 = LinearSVC(C=100, probability=True)
clf2 = SVC(kernel='linear', C=100, probability=True)
clf3 = LogisticRegression(C=100)

sfk = cv.StratifiedShuffleSplit(labels, 100)
scores = []
for train, test in sfk:
    score = []
    train_set = feature[train]",learning/final5.py,fcchou/CS229-project,1
"import numpy as np
from sklearn import svm
from sklearn.datasets import load_svmlight_file
from sklearn.metrics import accuracy_score
import time

dataset_X_sparse, dataset_Y_sparse = load_svmlight_file('rcv1_train.binary')

for i in range(0,6):
	C= 10.0**i
	model = svm.LinearSVC(C=C, loss='hinge', tol=0.001, fit_intercept= False, max_iter=1000000)
	start = time.time()
	model.fit(dataset_X_sparse, dataset_Y_sparse)
	end = time.time()

	score = 1-accuracy_score(dataset_Y_sparse, model.predict(dataset_X_sparse))",examples/Benchmark/scicit-learn/Linear_CSvm.py,egomeh/Shark,1
"	with open('test_data.csv', 'r') as data_file:
		spamreader = csv.reader(data_file)
		for row in spamreader:
			test_data.append(row[1:])
	test_data = test_data[1:]
	for instance in test_data:
		instance[3] = int(instance[3])
#========================================================================
	#start train with SVM
	if model_name == 'SVM':
		clf = svm.SVC()
	if model_name == 'decision_tree':
		clf = tree.DecisionTreeClassifier()
	if model_name == 'random_forest':
		clf = RandomForestClassifier()
	if model_name == 'logistic_regression':
		clf = LogisticRegression()
	if model_name == 'linear_regression':
		clf = LinearRegression()
		for instance in train_data:",train_and_test.py,samfu1994/cs838webpage,1
"
# For decoding, standardizing is often very important
masker = NiftiMasker(mask=data_files['mask_vt'][0], standardize=True)
masked_timecourses = masker.fit_transform(
    data_files.func[0])[resting_state == False]

### Classifiers definition

# A support vector classifier
from sklearn.svm import SVC
svm = SVC(C=1., kernel=""linear"")

from sklearn.grid_search import GridSearchCV
# GridSearchCV is slow, but note that it takes an 'n_jobs' parameter that
# can significantly speed up the fitting process on computers with
# multiple cores
svm_cv = GridSearchCV(SVC(C=1., kernel=""linear""),
                      param_grid={'C': [.1, .5, 1., 5., 10., 50., 100.]},
                      scoring='f1')
",plot_haxby_different_estimators.py,ainafp/nilearn,1
"def loadData(datafile):
    return pd.read_csv(datafile)

def decomPCA(train, test):
    pca = decomposition.PCA(n_components=256, whiten=True)
    train = pca.fit_transform(train)
    test = pca.transform(test)
    return train, test

def createSVM():
    clf = SVC()
    return clf

def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf",kaggle-digit-recognizer/src/classify.py,KellyChan/Kaggle,1
"                         'gamma': [1e-3, 1e-4], \
                         'C': [1, 10, 100, 1000]}, \
                        {'kernel': ['linear'], \
                         'C': [1, 10, 100, 1000]}]

    scores = ['precision', 'recall']
    for score in scores:

        print(""# Tuning hyper-parameters for %s\n"" % score)

        clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
        clf.fit(X_train, y_train)

        print(""Best parameters set found on development set:\n"")
        print(clf.best_estimator_)

        print(""\nGrid scores on development set:\n"")
        for params, mean_score, scores in clf.grid_scores_:
            print(""%.3f (+/-%.03f) for %r"" % (mean_score, scores.std() / 2, params))
",python/sklearn/examples/general/parameter_estimation_using_grid_search_with_a_nested_cross-validation.py,kwailamchan/programming-languages,1
"        self.X_train = self.X_train / mmax
        self.X_test = self.X_test / mmax

    def maxall_normalization(self):
        # gene norm max
        mmax = np.max(self.X_train, axis=0).max()
        self.X_train = self.X_train / mmax
        self.X_test = self.X_test / mmax

    def train(self):
        clf = OneVsRestClassifier(SVC(kernel='linear'))
        clf.fit(self.X_train, self.y_train)

        y_train_pred = clf.predict(self.X_train)
        y_test_pred = clf.predict(self.X_test)

        print 'train acc = %.4f%%' % (accuracy_score(self.y_train, y_train_pred) * 100)
        print 'test acc = %.4f%%' % (accuracy_score(self.y_test, y_test_pred) * 100)

",cmarker/svm/cancer_svm.py,bgshin/cmarker,1
"def main():
    from sklearn import svm
    from sklearn.datasets import samples_generator
    from sklearn.feature_selection import SelectKBest
    from sklearn.feature_selection import f_regression
    from sklearn.preprocessing import MinMaxScaler

    X, y = samples_generator.make_classification(n_samples=1000, n_informative=5, n_redundant=4, random_state=_random_state)
    anova_filter = SelectKBest(f_regression, k=5)
    scaler = MinMaxScaler()
    clf = svm.SVC(kernel='linear')

    steps = [scaler, anova_filter, clf]
    cached_run(steps, X, y)

if __name__ == '__main__':
    main()",benchmarks/sklearn/cache.py,yuyuz/FLASH,1
"                train_ckm = np.zeros((n_train,n_train))
                for t in range(n_km):
                    train_ckm = train_ckm + w[t]*train_km_list[t]
                test_ckm = np.zeros(test_km_list[0].shape)
                for t in range(n_km):
                    test_ckm = test_ckm + w[t]*test_km_list[t]
                
                C_range = [0.01,0.1,1,10,100]
                param_grid = dict(C=C_range)
                cv = StratifiedShuffleSplit(train_y,n_iter=5,test_size=0.2,random_state=42)
                grid = GridSearchCV(SVC(kernel='precomputed'), param_grid=param_grid, cv=cv)
                grid.fit(train_ckm, train_y)
                bestC = grid.best_params_['C']
                svm = SVC(kernel='precomputed', C=bestC)
                svm.fit(train_ckm, train_y)
                y_pred[test_ind] = svm.predict(test_ckm)

            pred_f = ""../svm_result/pred/%s_cvpred_%s.txt"" % (data, mkl)
            np.savetxt(pred_f, y_pred, fmt=""%d"")
",svm_code/run_svm.py,aalto-ics-kepaco/softALIGNF,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_09_2015_04.py,magic2du/contact_matrix,1
"#
# TODO: Create an SVC classifier. Leave C=1, but set gamma to 0.001
# and set the kernel to linear. Then train the model on the training
# data / labels:
print (""Training SVC Classifier..."")
#
# .. your code here ..

from sklearn.svm import SVC

model = SVC(C = 1, gamma = 0.001, kernel = 'rbf')
model.fit(X_train, y_train)


# TODO: Calculate the score of your SVC against the testing data
print (""Scoring SVC Classifier..."")
#
# .. your code here ..
score = model.score
print (""Score:\n"", score(X_test, y_test))",Module 6/assignment2.py,LamaHamadeh/Microsoft-DAT210x,1
"plt.legend(loc='best')
plt.tight_layout()
# plt.savefig('./figures/xor.png', dpi=300)
plt.show()

# after seeing that the data is indeed not linearly separable, try to use a kernel trick
# we changed the kernel, from linear to rbf
# rbf = radial basis function (or gaussian)
# gamma is a cut-off parameter, higher gamma, softer boundaries
# gamma controls overfit
svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor, classifier=svm)

plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('./figures/support_vector_machine_rbf_xor.png', dpi=300)
plt.show()",test/test_scikit/nonlinear_svm.py,viniciusguigo/the_magic_kingdom_of_python,1
"

# TODO: Import the three supervised learning models from sklearn
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# TODO: Initialize the three models
clf_A = GaussianNB()
clf_B = LogisticRegression(random_state=42)
clf_C = SVC(random_state=42)

# TODO: Set up the training set sizes
X_train_100 = X_train.iloc[:100, :]
y_train_100 = y_train.iloc[:100]

X_train_200 = X_train.iloc[:200, :]
y_train_200 = y_train.iloc[:200]

X_train_300 = X_train.iloc[:300, :]",student_progress_django/utils/another_tut.py,vimm0/student_progress_django,1
"digitTraining = pd.read_table('./Data/zip_train_0_2.txt',delim_whitespace=True, header=None)
digitTest = pd.read_table('./Data/zip_test_0_2.txt',delim_whitespace=True, header=None)

digitTrainingMatrix = digitTraining.as_matrix()
indexDigitTraining = np.array([i[0] for i in digitTrainingMatrix])
digitTrainingMatrix = np.array([np.delete(i,0) for i in digitTrainingMatrix])
digitTestMatrix = digitTest.as_matrix()
indexDigitTest = np.array([i[0] for i in digitTestMatrix])
digitTestMatrix = np.array([np.delete(i,0) for i in digitTestMatrix])

digitClassifier = OneVsRestClassifier(LinearSVC(random_state=0)).fit(digitTrainingMatrix,indexDigitTraining).predict(digitTestMatrix)
digitClassifier2 = OneVsOneClassifier(LinearSVC(random_state=0)).fit(digitTrainingMatrix, indexDigitTraining).predict(digitTestMatrix)
# digitClassifier2 = OneVsOneClassifier(LinearSVC(random_state=0)).fit(digitTrainingMatrix,indexDigitTraining).predict(digitTestMatrix)
# digitVariables, digitsIndex = digits.data, digits.target
# digitClassifier = OneVsRestClassifier(LinearSVC(random_state=0)).fit(digitVariables,digitsIndex).predict(digitVariables)
# digitIndexClass0 = [1 if i == 0 else 0 for i in digitsIndex]
# digitIndexClass1 = [1 if i == 1 else 0 for i in digitsIndex]
# digitIndexClass2 = [1 if i == 2 else 0 for i in digitsIndex]

MissClassifiedDigitData = []",PatternRecognition/ProgrammingProject2/testClassifier.py,marioharper182/Patterns,1
"                                             config.MODEL_FILE_NAME)))
    else:
        return _get_new_trained_model()


def _get_new_trained_model():
    logger.info('Training new model')
    training_samples, training_labels = samples.get_samples(
        os.path.join(config.INSTALL_DIR, config.POSITIVE_SAMPLE_DIR),
        os.path.join(config.INSTALL_DIR, config.NEGATIVE_SAMPLE_DIR))
    model = svm.SVC(kernel='linear')
    logger.info('Fitting new model')
    model.fit(training_samples, training_labels)

    return model


if __name__ == '__main__':
    model = get_trained_model(False)
    with open(os.path.join(config.INSTALL_DIR, config.MODEL_FILE_NAME), 'w') as f:",models.py,mattskone/garage_alarm,1
"        assert_true(sources.shape[1] == n_components)


@requires_sklearn
def test_csp_pipeline():
    """"""Test if CSP works in a pipeline
    """"""
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    csp = CSP(reg=1)
    svc = SVC()
    pipe = Pipeline([(""CSP"", csp), (""SVC"", svc)])
    pipe.set_params(CSP__reg=0.2)
    assert_true(pipe.get_params()[""CSP__reg""] == 0.2)",mne/decoding/tests/test_csp.py,kingjr/mne-python-i,1
"                     [120, 20, 40]], dtype=np.float64)
print(a)
print(preprocessing.scale(a))

X, y = make_classification(n_samples=300, n_features=2 , n_redundant=0, n_informative=2,
                           random_state=22, n_clusters_per_class=1, scale=100)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()
X = preprocessing.scale(X)    # normalization step
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
clf = SVC()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))






",DeepLearnMaterials/tutorials/sklearnTUT/sk7_normalization.py,MediffRobotics/DeepRobotics,1
"elif sys.argv[1] == 'scikit-classifier':
	split = 15000
	ct_train_data = TfidfVectorizer()
	ct_train_labels = CountVectorizer(tokenizer=lambda t: t.split(""|""))
	X = ct_train_data.fit_transform(open(pre.data_file, 'r'))
	print ""Train passages vectorized""
	Y = ct_train_labels.fit_transform(open(pre.label_file, 'r'))	
	Y = Y.todense()
	print ""Train labels vectorized""
	if not os.path.exists(randomforest_model_file):
		# svc = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, 
		# 	multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=100)
		# rfModel = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1))
		rfModel = OneVsRestClassifier(RandomForestClassifier(n_estimators=25, n_jobs=4, verbose=1, max_depth=5), n_jobs=2)
		# rfModel = OneVsRestClassifier(LogisticRegression())
		# rfModel = OneVsRestClassifier(LinearSVC())
		# rfModel = KNeighborsClassifier(n_neighbors=10, n_jobs=4)
		# rfModel = MLPClassifier(hidden_layer_sizes=(100,), batch_size=100 ,activation='logistic', max_iter=25, alpha=1e-4, solver='sgd', verbose=True, \
		# tol=0.0001, random_state=1, learning_rate_init=.1)
		rfModel.fit(X[:split], Y[:split])",Task 1/part1.py,vipmunot/Yelp-Dataset-Challenge,1
"            try:
                alphas = AlphaSelection(model())
            except YellowbrickTypeError:
                self.fail(""could not instantiate RegressorCV on alpha selection"")

    def test_only_regressors(self):
        """"""
        Assert AlphaSelection only works with regressors
        """"""
        with self.assertRaises(YellowbrickTypeError):
            model = AlphaSelection(SVC())

    def test_store_cv_values(self):
        """"""
        Assert that store_cv_values is true on RidgeCV
        """"""

        model = AlphaSelection(RidgeCV())
        self.assertTrue(model.estimator.store_cv_values)
",tests/test_regressor/test_alphas.py,balavenkatesan/yellowbrick,1
"	reader = csv.reader(f)
	foodNameTemp = list(reader)

foodName = []
for i in foodNameTemp:
	foodName += i

X = np.array(RGBValue)
y = np.array(foodName)

clf = SVC()
clf.fit(X,y)

food = raw_input(""Enter image file name here.\n"")
food = Image.open(food)
food = np.asarray(food)
average = [0,0,0]
numOfValues = 0
for i in food:
	for j in i:",python programs/Snappetite.py,SNAPPETITE/backend,1
"    LDA.append(int(var))
X = vectorizer.fit_transform(corpus)
print(X.toarray())
X_tfidf = transformer.fit_transform(X.toarray())
print(X_tfidf.toarray())#spare to intense
X_pca = pca.fit_transform(X_tfidf.toarray())
print(X_pca)

X_tfidf = np.concatenate((X_tfidf.toarray(), np.transpose(LDA).reshape(len(LDA),1)), axis=1)

#clf=svm.SVC()#rbf kernel
clf=svm.LinearSVC(C=1)#linear kernel
#clf.fit(X.toarray(),np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X.toarray(),np.transpose(y),scoring=""mean_squared_error"",cv=10)
#clf.fit(X_tfidf.toarray(),np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X_tfidf.toarray(),np.transpose(y),scoring=""mean_squared_error"",cv=10)
clf.fit(X_tfidf,np.transpose(y))
cv=cross_validation.cross_val_score(clf,X_tfidf,np.transpose(y),scoring=""accuracy"",cv=10)
#clf.fit(X_pca,np.transpose(y))
#cv=cross_validation.cross_val_score(clf,X_pca,np.transpose(y),scoring=""mean_squared_error"",cv=10)",svmcv-with-feature-extraction.py,evazyin/Capstoneproject,1
"@param.float(""C-Exp"", interval=[0, 4])
@param.float(""Gamma-Exp"", interval=[-6, 0])
def f(C_exp, gamma_exp):
    iris = datasets.load_iris()

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        iris.data, iris.target, test_size=0.4, random_state=0)

    C = 10 ** C_exp
    gamma = 10 ** gamma_exp
    clf = svm.SVC(C=C, gamma=gamma)
    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)


def main():
    from metaopt.core.optimize.optimize import optimize
    from metaopt.optimizer.cmaes import CMAESOptimizer
",examples/showcase/svm_cmaes_global_timeout.py,cigroup-ol/metaopt,1
"        label = 1
    else:
        continue
    labels.append(label)
    features.append(arr)
    names.append(name)

labels, features, names = shuffle(labels, features, names)
clf = LogisticRegression()
#clf = MultinomialNB()
#clf = sklearn.svm.SVC()
#clf = sklearn.svm.LinearSVC()

from sklearn.metrics import precision_score, accuracy_score


accu = []
prec = []
cv = cross_validation.StratifiedKFold(labels, n_folds=10)
for train, test in cv:",learning/old/test3.py,fcchou/CS229-project,1
"        negative = negative[: len(positive)]
        sentences = positive + negative
        classes = [True] * len(positive) + [False] * len(negative)
        vocabulary = self.collect_features(positive)
        if verbose:
            print 'Words considered as features:'
            print vocabulary
            print
        self.classifier = Pipeline([
            ('v', CountVectorizer(analyzer=lambda x: x, vocabulary=vocabulary, binary=True)),
            ('c', SVC(kernel='linear', probability=True)),
        ]) 
        self.classifier.fit(map(self.get_features, sentences), classes)
        self.get_most_informative_features()
        
    def get_most_informative_features(self, n=10):
        #works only with a linear kernel
        try:
            vectorizer = self.classifier.named_steps['v']
            clf = self.classifier.named_steps['c']",src/sentence_classifier.py,mzajac/DBPediaExtender,1
"
predictors_ts = tfidfts

################################
# Initialize classifiers
################################

np.random.seed(1)
print(""Ensemble: LR - linear SVC"")
clf1 = LogisticRegression(random_state=1, C=7)
clf2 = LinearSVC(random_state=1, C=0.4, penalty=""l2"", dual=False)
nb = BernoulliNB()
rfc = RandomForestClassifier(random_state=1, criterion = 'gini', n_estimators=500)
sgd = SGDClassifier(random_state=1, alpha=0.00001, penalty='l2', n_iter=50)


eclf = EnsembleClassifier(clfs=[clf1, clf2,nb, rfc, sgd], weights=[2, 2, 1, 1,2])
np.random.seed(1)
for clf, label in zip([eclf], 
    ['Ensemble']):",finalModel.py,ahadmushir/whatsCooking,1
"                                  axis=1)

        expected_eval_outputs = eval_df.Survived.values
        eval_df = eval_df.drop([""PassengerId"", ""Survived"",\
                                ""Ticket"", ""Cabin""],
                                axis=1)

        train_data = normalise_data(train_df).values
        eval_data = normalise_data(eval_df).values

        clf = svm.LinearSVC()
        clf.fit(train_data, expected_training_outputs)

        evaluation = clf.predict(eval_data)

        em = EvaluationMetrics(evaluation, expected_eval_outputs)
        f1 = em.calculate_f1()
        f_scores.append(f1)

    output = clf.predict(test_data)",src/classifiers.py,JakeCowton/titanic,1
"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print(""# Tuning hyper-parameters for %s"" % score)
    print()

    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring=score)
    clf.fit(X_train, y_train)

    print(""Best parameters set found on development set:"")
    print()
    print(clf.best_estimator_)
    print()
    print(""Grid scores on development set:"")
    print()
    for params, mean_score, scores in clf.grid_scores_:",examples/grid_search_digits.py,B3AU/waveTree,1
"from sklearn.metrics import zero_one
from sklearn.svm import SVC
from sklearn.utils import check_random_state


def test_rfe_set_params():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target
    clf = SVC(kernel=""linear"")
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    y_pred = rfe.fit(X, y).predict(X)

    clf = SVC()
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1,
              estimator_params={'kernel': 'linear'})
    y_pred2 = rfe.fit(X, y).predict(X)
    assert_array_equal(y_pred, y_pred2)
",venv/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_rfe.py,GbalsaC/bitnamiP,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def processFrame(self, dataURL, identity):
        head = ""data:image/jpeg;base64,""
        assert(dataURL.startswith(head))
        imgdata = base64.b64decode(dataURL[len(head):])
        imgF = StringIO.StringIO()
        imgF.write(imgdata)
        imgF.seek(0)
        img = Image.open(imgF)",server/openface-server/websocket-server.py,Jamesjue/FaceSwap-server,1
"evoked = epochs.average()

###############################################################################
# Decoding in sensor space using a linear SVM

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import ShuffleSplit  # noqa
from mne.decoding import CSP  # noqa

n_components = 3  # pick some components
svc = SVC(C=1, kernel='linear')
csp = CSP(n_components=n_components)

# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(labels), 10, test_size=0.2, random_state=42)
scores = []
epochs_data = epochs.get_data()

for train_idx, test_idx in cv:
    y_train, y_test = labels[train_idx], labels[test_idx]",examples/decoding/plot_decoding_csp_space.py,lorenzo-desantis/mne-python,1
"        #X = StandardScaler().fit_transform(X)

        print(""Training...."")
        t = time.time()
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

        #clf = KNeighborsClassifier(5) # EXPENSIVE at prediction, NOT suitable
        #self.clf = sklearn.linear_model.LogisticRegression() # NOT good.
        self.clf = sklearn.tree.DecisionTreeClassifier(max_depth=5)
        #self.clf = sklearn.ensemble.RandomForestClassifier(min_samples_split=len(X)/20, n_estimators=6)
        #self.clf = sklearn.svm.SVC(max_iter=1000) # can't make it work too well..
        self.clf.fit(X_train, y_train)
        print(""Training finished. T: %-3.2f"" % (time.time()-t))

        print(""Calculating scores...."")

        t = time.time()
        y_pred = self.clf.predict(X_test)
        recall = sklearn.metrics.recall_score(y_test, y_pred)
        prec = sklearn.metrics.precision_score(y_test, y_pred)",scripts/learn/predict.py,SmallLars/cryptominisat,1
"# Script passed in py2 & py3 with Ubuntu 14.04 env.
# Prerequirement: pip install numpy scipy scikit-learn
# furthermore info http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.
# furthermore info http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm
# There have a lot of descriptions of setting variables on the website, please check it if you need the further setting.

from sklearn import svm
from sklearn.feature_extraction.text import TfidfVectorizer as tfidf

vec =tfidf(smooth_idf =False)
svc = svm.SVC(kernel='poly')    # further settings on website: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html

# training set, ""List"" type.
trainset =[""good good good good good great great great"",    # corpus 1
        ""bad bad bad bad bad bad dirty dirty dirty"",        # corpus 2
]
trainTag =[""pos"", ""neg""]                                    # corpus's tags.

# test set, ""List"" type.
testset =[""good good good good good great great great"",",02.Algorithms/SVM.py,rmatam/Deep-Learning,1
"            reduce_ratio = settings['reduce_ratio'] 
            for seq_no in range(1, self.ddi_obj.total_number_of_sequences+1):
                print seq_no
                logger.info('sequence number: ' + str(seq_no))
                if settings['SVM']:
                    print ""SVM""
                    (train_X_LOO, train_y_LOO),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_LOO_training_and_reduced_traing(seq_no,fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                    standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                    scaled_train_X = standard_scaler.transform(train_X_reduced)
                    scaled_test_X = standard_scaler.transform(test_X)
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, seq_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
                # Deep learning part",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_load2-new10fold_12_30_2014_server.py,magic2du/contact_matrix,1
"        if n_folds and \
                np.any([np.sum(y == class_) < n_folds for class_ in self.classes_]):
            raise ValueError(""Requesting %d-fold cross-validation but provided""
                             "" less than %d examples for at least one class.""
                             % (n_folds, n_folds))

        self.calibrated_classifiers_ = []
        if self.base_estimator is None:
            # we want all classifiers that don't expose a random_state
            # to be deterministic (and we don't want to expose this one).
            base_estimator = LinearSVC(random_state=0)
        else:
            base_estimator = self.base_estimator

        if self.cv == ""prefit"":
            calibrated_classifier = _CalibratedClassifier(
                base_estimator, method=self.method)
            if sample_weight is not None:
                calibrated_classifier.fit(X, y, sample_weight)
            else:",projects/scikit-learn-master/sklearn/calibration.py,DailyActie/Surrogate-Model,1
"                      .format(num_train_samples,replicate))
                train_IDs = grab_n_samples_by_song(feature_file['song_IDs'],
                                                   feature_file['labels'],
                                                   num_train_samples,
                                                   song_ID_list=train_song_ID_list)
                train_IDs_arr[num_samples_ind, replicate] = train_IDs
                labels_train = labels[train_IDs]
                for model_ind, model_dict in enumerate(model_list):
                    if model_dict['model'] == 'svm':
                        print('training svm. ', end='')
                        clf = SVC(C=model_dict['hyperparameters']['C'],
                                      gamma=model_dict['hyperparameters']['gamma'],
                                      decision_function_shape='ovr')
                    elif model_dict['model'] == 'knn':
                        print('training knn. ', end='')
                        clf = neighbors.KNeighborsClassifier(model_dict['hyperparameters']['k'],
                                                             'distance')

                    feature_inds = np.in1d(feature_file['features_arr_column_IDs'],
                                           model_dict['feature_list_indices'])",hvc/modelselect.py,NickleDave/hybrid-vocal-classifier,1
"  #vectorizer=get_features_vectorizer(train_corpus,args)
  vectorizer=filter_features(vectorizer,H,feature_percent,args)
  #print('nfeatures=', len( vectorizer.vocabulary ))
  Xtrain_txt = train_corpus['text']
  Xtest_txt  = test_corpus['text']
  Ytrain = train_corpus['ton']
  Ytest  = test_corpus['ton']
  Xtest  = vectorizer.transform(Xtest_txt)
  Xtrain = vectorizer.transform(Xtrain_txt)

  clf = LinearSVC(random_state=random_state)
  clf.fit(Xtrain,Ytrain)

  pred_val = clf.predict(Xtest)
  real_val = np.array(Ytest)

  print(""feature_percent\t{}""       .format(feature_percent))
  print(""macro recall\t{}\t{}""      .format(recall_score(real_val, pred_val, average='macro')        ,feature_percent))
  print(""macro precision\t{}\t{}""   .format(precision_score(real_val, pred_val, average='macro')     ,feature_percent))
  print(""macro F1\t{}\t{}""          .format(f1_score(real_val, pred_val, average='macro')            ,feature_percent))",main.py,dzabraev/ton-analysis,1
"        if model_type == 'svr':
            print ""%s training..."" % model_type
            model = SVR(kernel=model_param['kernel'], C=model_param['C'], epsilon=model_param['epsilon'])
            model.fit( x_train, y_train )
            pred_val = model.predict( x_test )
            return pred_val

        # SVM classification
        if model_type == 'svc':
            print ""%s training..."" % model_type
            model = SVC(C=model_param['C'], epsilon=model_param['epsilon'])
            model.fit( x_train, y_train )
            pred_val = model.predict( x_test )
            return pred_val

        # random forest regression
        if model_type == 'rf':
            print ""%s training..."" % model_type
            model = RandomForestRegressor(n_estimators=model_param['n_estimators'], n_jobs=-1)
            model.fit( x_train, y_train )",model_library.py,weaponsjtu/Kaggle_xBle,1
"    if weighted:
      model = lm.LogisticRegression(class_weight='auto')
    else:
      model = lm.LogisticRegression()

    model.fit(joint_inputs, classes)
  elif model_type == 'svc':
    if weighted:
      model = svm.LinearSVC(class_weight='auto', C=100, fit_intercept=True)
    else:
      model = svm.LinearSVC(C=100, fit_intercept=True)
    model.fit(joint_inputs, classes)

    train_prediction = model.predict(train_inputs)

    # part_of_train = np.sum((1-train_prediction))/len_train
    test_prediction = model.predict(test_inputs)

    # # indexes = np.array(range(len(inputs)))
    # fig, (ax1, ax2) = plt.subplots(2,1)",data_utils_v2.py,prikhodkop/AnalysisWorkbench,1
"    # 10% most significant features
    selector = SelectPercentile(f_classif, percentile=10)
    selector.fit(X, y)

    scores = -np.log10(selector.pvalues_)
    scores /= scores.max()

    return selector, scores

def createSVM():
    clf = svm.SVC(kernel='linear')
    return clf

def predictSVM(clf, X, y):

    yHat = clf.fit(X, y)  

    svm_weights = (clf.coef_ ** 2).sum(axis=0)
    svm_weights /= svm_weights.max()
",examples/scikit-learn/examples/general/univariate_feature_selection.py,KellyChan/Python,1
"#test = test[:,:10]

print(""Dimensionality Reduction"")
""""""pca = PCA(150, 'randomized',
          whiten=True).fit(train)
train_pca = pca.transform(train)
test_pca = pca.transform(test)
print(train_pca.shape)""""""

""""""print(""Train Linear SVC"")
model = LinearSVC()
model.fit(train,train_labels.values.ravel())

print(""Begin predictions"")
print(model.predict(test))""""""

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets",src/predict.py,tobiagru/ML,1
"HOW_MANY_FEATURES = 5
print(""Selecting features based on the most {} common words."".format(HOW_MANY_FEATURES))
vectorizer = CountVectorizer(
    lowercase=True,
    stop_words=stopwords_list,
    tokenizer=tokenize
)

pipeline = Pipeline([
    ('vect', vectorizer),
    ('cls', OneVsRestClassifier(LinearSVC())),
])


def filter_tweets(data):
    return [ row['tweet'] for row in data ]

def filter_classes(data):
    y = []
    for row in data:",example6/example6.py,jramcast/ml_weather,1
" 
 	matrix_w = eig_pairs[0][1].reshape(featureSize,1)
 	for i in range(100):
 		matrix_w = numpy.hstack((matrix_w, eig_pairs[i+1][1].reshape(featureSize,1)))
 	print matrix_w.shape
 
 	transformed = matrix.dot(matrix_w)
 	print transformed.shape
 
 	#Start to train SVM
	# Z = OneVsRestClassifier(LinearSVC()).fit(transformed, labels).predict(transformed)
	Z = OneVsRestClassifier(SVC(kernel = ""rbf"")).fit(transformed, labels).predict(transformed)
 	print Z
 	correct = 0.0
 	for x in range(len(Z)):
 		if labels[x] == Z[x]:
 			correct = correct +1

 	print correct/len(Z)
 	#recData = transformed.dot(matrix_w.T) + matrix.mean(axis=1)[:, None]",Src/par_pca_old.py,motian12ps/Bigdata_proj_yanif,1
"        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,
                                            dual=False, tol=1e-3)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,
                                           penalty=penalty)))

# Train SGD with Elastic Net penalty
print('=' * 80)
print(""Elastic-Net penalty"")",analysis/bagofwords.py,rishirdua/smart-chat,1
"class TestGetSWFEngineInfo(unittest.TestCase):
    def test_engine_info(self):
        n_samples = 20
        n_features = 100
        n_proc = 2
        X, y = datasets.make_classification(n_samples=n_samples,
                                            n_features=n_features,
                                            n_informative=2,
                                            random_state=1)
        Xy = dict(X=X, y=y)
        cv_svm_local = CV(Methods(*[SVC(kernel=""linear""),
                                    SVC(kernel=""rbf"")]),
                          n_folds=3)
        swf_engine = SomaWorkflowEngine(cv_svm_local,
                                        num_processes=n_proc,
                                        resource_id=""jl237561@gabriel"",
                                        login=""jl237561"",
                                        remove_finished_wf=False,
                                        remove_local_tree=False,
                                        queue=""Global_long"")",epac/tests/test_swf_engineinfo.py,neurospin/pylearn-epac,1
"        # self.stdSlr = StandardScaler().fit(descriptors)
        # im_features = self.stdSlr.transform(descriptors)
        #
        # Train the Linear SVM
        # unique, counts = np.unique(names, return_counts=True)
        # print dict(zip(unique, counts))
        # C_range = np.logspace(-5, 10, 13)
        # gamma_range = np.logspace(-9, 5, 13)
        # param_grid = dict(gamma=gamma_range, C=C_range)
        # cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=42)
        # grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
        # grid.fit(descriptors, names)
        #
        # print(""The best parameters are %s with a score of %0.2f""
        #       % (grid.best_params_, grid.best_score_))
        self.clf = SVC(kernel='rbf', C=self.C, verbose=False, max_iter=self.iter,
                       probability=self.prob, gamma=self.gamma)
        names = names.ravel()
        self.clf.fit(descriptors, names)
        if self.prob:",src/Multimodal_Interaction/Offline_Learning/SVM.py,pazagra/catkin_ws,1
"    t_tc = time.clock()

    print ""Training...""

    # X_dim = len(data_X[0])    # commented out because when not using .toarray() option, the width is not determinized

    if   CLASSIFIER == 1:           # Linear Regression
        print ""  Linear Regression...""
        clf = linear_model.LinearRegression()
    elif CLASSIFIER == 2:           # SVM
        clf = svm.SVC(kernel='linear')
        # clf = svm.SVC(kernel='poly', degree=3, gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='poly', probability=True, degree=3)
        # clf = svm.SVC(kernel='rbf', gamma=1/X_dim*10)
        # clf = svm.SVC(kernel='rbf', probability=True)
        # clf = svm.SVC(kernel='sigmoid')
        # clf = svm.SVC(kernel='precomputed')
    elif CLASSIFIER == 3:           # Trees
        clf = tree.DecisionTreeClassifier(criterion='gini')
        # clf = tree.DecisionTreeClassifier(criterion='entropy')",DST_v2.3.1.py,totuta/deep-supertagging,1
"  knn = neighbors.KNeighborsClassifier(n_neighbors = 10)
  knn.fit(dataNP,labels)
  cm = confusion_matrix(labels, knn.predict(predNP))
  print str(cm)
  print '\tAccuracy: ' + str(knn.score(predNP, labels))

  print
  print 'Linear SVC'
  print '--------------'
  print 'Confusion Matrix'
  clf = svm.LinearSVC()
  clf.fit(dataNP, labels)  
  cm = confusion_matrix(labels, clf.predict(predNP))
  print str(cm)
  print '\tAccuracy: ' + str(clf.score(predNP, labels))

  print
  print 'Gaussian NB'
  print '--------------'
  print 'Confusion Matrix'",code/src/classify.py,dkdfirefly/speaker_project,1
"    ## C < 1 => more regularization
    ## C > 1 => more fitting to training
    C_list = [0.001, 0.01, 0.1, 1.0, 10.0]
    (l1_c, l1_f1) = find_best_c(X_train, y_train, goal_ind, penalty='l1', dual=False)
    print(""Optimizing l1 with cross-validation gives C=%f and f1=%f"" % (l1_c, l1_f1))
    (l2_c, l2_f1) = find_best_c(X_train, y_train, goal_ind)
    print(""Optimizing l2 with cross-validation gives C=%f and f1=%f"" % (l2_c, l2_f1))

    print(""Balanced bootstrapping method (add equal amounts of true/false examples)"")
    for percentage in [0.01, 0.1, 0.25]:
        svc = svm.LinearSVC()
        svc.fit(X_train, y_train)
        preds = svc.decision_function(X_test)
        added_X = []
        added_y = []
        for i in range(int(percentage * num_test_instances)):
            if i % 2 == 0:
                highest_ind = preds.argmax()
                if preds[highest_ind] <= 0:
                    break",scripts/eval_bootstrap.py,tmills/uda,1
"              'start_slideshow_at': 'selected',
              'transition':'fade',
              'scroll': False
});


# ADAPTED FROM SCIKIT-LEARN EXAMPLE
# (http://scikit-learn.org/stable/modules/svm.html)
def max_margin_classifier(fig, X, Y):
    ax = fig.add_subplot(122, adjustable=""box-forced"", aspect='equal')
    clf = sklearn.svm.SVC(kernel='linear')
    clf.fit(X, Y)

    # get the separating hyperplane
    w = clf.coef_[0]
    m = -w[0] / w[1]
    xx = np.linspace(-5, 5)
    yy = m * xx - (clf.intercept_[0]) / w[1]

    # plot the parallel lines to the separating hyperplane that pass through the",lecture08_SVM/Lec08.py,snowicecat/umich-eecs445-f16,1
"            p = np.take(Pij[i,:],dx).reshape(-1,1)
            tmp = np.dot((p*xij).T,xij)
            df2 += np.dot(tmp,A)
        df = (df1 - df2)
        df = df - self.lamda*A/(self.D*self.d);
        return -f,-df

    def evaluate(self,xtrn,ytrn,xtst,ytst):
        Axtrn = self.transform(xtrn)
        Axtst = self.transform(xtst)
        clf = svm.LinearSVC()
        clf.fit(Axtrn,ytrn)
        pred = clf.predict(Axtst)
        cm = np.zeros((2,2))
        for p,t in zip(pred,ytst):
            cm[t,p] += 1
        uwr = np.sum(np.diag(cm)/np.sum(cm,axis=1))/2
        return uwr
        
    def fit(self,x,y):",src/nca.py,shahmohit/pynca,1
"        X = np.array(df.drop(['open'], 1))
        y = np.array(df['open'])

        print ""training Data"", X
        print ""labels"", y[:-1]
        
        X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=.5)
        clf = svm.SVR()
        clf.fit(X_train, y_train)

        #clf = svm.SVC()
        #clf.fit(X, y) 

    test_set = np.array([[118.99, 117.94]]) #, 116.46]])
    #test_set = test_set.reshape(len(test_set), -1)
    prediction = clf.predict(test_set)
    print prediction

if __name__ == ""__main__"":
    main()",ifc/ml/dario_sklearn_svm.py,Darthone/bug-free-octo-parakeet,1
"#Divide train Matrix and Test Matrix (for which I don't have labels)
trainMatrixReduced = bigMatrix[0:max(indexesIm), :]
testMatrixReduced = bigMatrix[testIndexes[0]:bigMatrix.shape[0], :]

#Divide dataset for cross validation purposes
X_train, X_test, y_train, y_test = cross_validation.train_test_split(
    trainMatrixReduced, y[0:24999], test_size=0.4, random_state=0) #fix this

#random grid search of hiperparameters
#create a classifier
clf = svm.SVC(verbose = True)

# specify parameters and distributions to sample from
params2Test = {'C': [1, 3, 10, 30, 100, 300], 'gamma': [0.001], 'kernel': ['rbf']}

#run randomized search
grid_search = GridSearchCV(clf, param_grid = params2Test)

start = time()
grid_search.fit(trainMatrixReduced, y[0:24999])",wacax/catsDogs.py,mblaauw/Kaggle_CatsVsDogs,1
"    # Create dataset of classification task with many redundant and few
    # informative features
    X, y = datasets.make_classification(n_samples=100000, n_features=20,
                                        n_informative=2, n_redundant=10,
                                        random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,
                                                        random_state=42)
    # Plot calibration cuve for Gaussian Naive Bayes
    plot_calibration_curve_old(X_train, X_test, y_train, y_test, y, GaussianNB(), ""Naive Bayes"", 1)
    # Plot calibration cuve for Linear SVC
    plot_calibration_curve_old(X_train, X_test, y_train, y_test, y, LinearSVC(), ""SVC"", 2)
    plt.show()

if __name__ == '__main__':
    #test_plot_calibration_curve_old()
    #test_plot_calibration_curve()
    test_calibration_curve_nan()",kgml/plot_calibration_curve.py,orazaro/kgml,1
"
    X = np.array(X)
    Y = np.array(Y)
    #print X
    #print Y

    # X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
    # Y = np.array([1, 1, 2, 2])

    from sklearn.svm import SVC
    #clf = SVC()
    #SVC(C=3.0, cache_size=200, class_weight=None, coef0=1, degree=3, gamma=1, kernel='rbf', max_iter=-1, probability=False,random_state=None, shrinking=True, tol=0.001, verbose=False)
    clf=SVC(C=3.0, gamma=0.7, kernel='rbf')
    clf.fit(X, Y)
    #print(clf.predict([X[0]]))
    Ypred_sklearnSVC=clf.predict(X)
    print  Ypred_sklearnSVC.tolist()
    from sklearn.metrics import accuracy_score
    print accuracy_score(Y, Ypred_sklearnSVC)
    #print Y.tolist()",python/test.py,niitsuma/gtsvm,1
"import numpy as np
import pylab as pl


features_train, labels_train, features_test, labels_test = makeTerrainData()


########################## SVM #################################
### we handle the import statement and SVC creation for you here
from sklearn.svm import SVC
clf = SVC(kernel=""linear"")


#### now your job is to fit the classifier
#### using the training features/labels, and to
#### make a set of predictions on the test data

clf.fit(features_train, labels_train)

pred = clf.predict(features_test)",svm/quiz-coding-up-svm.py,askldjd/udacity-machine-learning,1
"	#write model
	joblib.dump(clf, '../model/model'+num+'.pkl', compress=9)

# choose the classifier to train
def trainClf(clf):
	if clf == '1':
		clfreport(MultinomialNB(alpha=.01),clf)
	elif clf == '2':
		clfreport(BernoulliNB(alpha=.01),clf)
	else:
		clfreport(LinearSVC(dual=False, tol=1e-3),clf)
		
# update training data
def updateTraindata(text,label):
	f = open('./data/'+label+'.txt', 'a')
	f.write('\n'+text)
	f.close()
	
if __name__ == ""__main__"":
	if (len(sys.argv)==2):",train/trainmodel.py,rishirdua/smart-chat,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",kbe/src/lib/python/Lib/test/test_email/test_email.py,Orav/kbengine,1
"			for i in range(diffNum-1):
				trainX = np.vstack((trainX,failDataArray))
				trainY = np.hstack((trainY,newGenArrayY))	
			print trainX.shape,trainY.shape  

	# 0.4 0.0001 0.01  0.51370 0.51370 0.51370  
	# clf=svm.SVC(C=0.4, cache_size=2000, decision_function_shape='ovr', gamma=0.0001, kernel='rbf', class_weight='balanced' ,tol=0.01)   
	#clf=svm.SVC(C=1, cache_size=2000, decision_function_shape='ovr', gamma=0.0001, kernel='linear', class_weight='balanced' ,tol=0.01)   
	
	# #0.95402  0.56849 l1 0.0001 False
	clf=svm.LinearSVC(penalty='l1', tol=0.0001, C=1, dual=False, fit_intercept=True,
	 intercept_scaling=1, class_weight='balanced',  max_iter=1000)
	clf=clf.fit(trainX,trainY)
	

	testingX=temLogNumPerTW[traingSetSize:]
	testingY=labelTWL[traingSetSize:]
	# testingX = trainX
	# testingY = trainY 
",SVM/SVM_BGL.py,cuhk-cse/loglizer,1
"            if count%10 == i:
                testingSet.append(feature_vector)
                testingLabels.append(currentGesture)
            else:
                trainingSet.append(feature_vector)
                trainingLabels.append(currentGesture)
            count += 1

# train the data on the new subset

        clf = svm.SVC()
        clfoutput = clf.fit(trainingSet, trainingLabels)
# classify
        result = clf.predict(testingSet)
        predictions.append(result.tolist())
        
        num_correct = 0
        for j,k in zip(result,testingLabels):
            if j == k:
                num_correct += 1",Study/Test_07_copy/CrossValidate.py,JessMcintosh/EMG-classifier,1
"    # many class dataset:
    X_blobs, y_blobs = make_blobs(n_samples=100, centers=10, random_state=0)
    X_blobs = sparse.csr_matrix(X_blobs)

    datasets = [[X_sp, Y, T], [X2_sp, Y2, T2],
                [X_blobs[:80], y_blobs[:80], X_blobs[80:]],
                [iris.data, iris.target, iris.data]]
    kernels = [""linear"", ""poly"", ""rbf"", ""sigmoid""]
    for dataset in datasets:
        for kernel in kernels:
            clf = svm.SVC(kernel=kernel, probability=True, random_state=0,
                          decision_function_shape='ovo')
            sp_clf = svm.SVC(kernel=kernel, probability=True, random_state=0,
                             decision_function_shape='ovo')
            check_svm_model_equal(clf, sp_clf, *dataset)


def test_unsorted_indices():
    # test that the result with sorted and unsorted indices in csr is the same
    # we use a subset of digits as iris, blobs or make_classification didn't",projects/scikit-learn-master/sklearn/svm/tests/test_sparse.py,DailyActie/Surrogate-Model,1
"
dataset = load_digits()

X = dataset[""data""]
y = dataset[""target""]
y_indicators = pandas.get_dummies(y).values

# Center each feature and scale the variance to be unitary
X = preprocessing.scale(X)

svc = SVC(gamma=0.001)

# Set up variables
svc_error = 0
ann_error = 0
n_folds = 10


for train_inds, test_inds in KFold(X.shape[0], n_folds=n_folds):
    X_train, X_test = X[train_inds], X[test_inds]",tyrehug/exp/deeplearning.py,charanpald/tyre-hug,1
"
iris = datasets.load_iris()
rng = np.random.RandomState(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]
n_classes = 3


def test_ovr_exceptions():
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    assert_raises(ValueError, ovr.predict, [])


def test_ovr_fit_predict():
    # A classifier which implements decision_function.
    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
    pred = ovr.fit(iris.data, iris.target).predict(iris.data)
    assert_equal(len(ovr.estimators_), n_classes)
",venv/lib/python2.7/site-packages/sklearn/tests/test_multiclass.py,chaluemwut/fbserver,1
"
logger = logging.getLogger(__name__)

def _compute_rfe(x, y, target_accuracy=1.0):
    from sklearn.cross_validation import KFold
    from sklearn.feature_selection import RFECV, RFE
    from sklearn.svm import LinearSVC, SVC
    from sklearn.metrics import zero_one

    cv = KFold(len(y), 5)
    clf = SVC(kernel='linear', C=1.) #LinearSVC(C=1.0)
    #clf = LinearSVC(C=1.0)
    rfecv = RFECV(clf, step=0.1, cv=cv, loss_func=zero_one)
    print 'About to call RFECV.fit on', x.shape, 'and', y.shape
    rfecv.fit(x, y)
    print 'RFECV done'
    # The percentage correct for each # of variables in the cross validation
    perccorrect_tot = [100 - ((100 * i) / y.shape[0]) 
                       for i in rfecv.cv_scores_]
    threshold = min(perccorrect_tot) + target_accuracy * (max(perccorrect_tot) - min(perccorrect_tot))",cpa/util/profile_svmnormalvector.py,afraser/CellProfiler-Analyst,1
"# -*- coding: utf-8 -*-

from sklearn import svm
from sklearn.datasets import load_iris
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = svm.LinearSVC(C=1., random_state=0)
clf.fit(X, y)

output = Porter(clf, language='ruby').export()
print(output)

""""""
class Brain

    def self.predict (atts)",examples/classifier/LinearSVC/ruby/basics.py,nok/sklearn-porter,1
"print len(trainDfE[u'如今'])
trainTargetE = [1 for i in range(15)]
print trainTargetE

trainDf = pd.concat([trainDfF, trainDfE])
trainTargetF.extend(trainTargetE)

print len(trainDf[u'如今'])
print trainTargetF

clf = svm.LinearSVC()
clf.fit(trainDf, trainTargetF)

predict = clf.predict(df)
print predict


",Python/MachineLearning/A Dream in Red Mansions/test.py,zhangmianhongni/MyPractice,1
"fileName = '/Users/longma/Dropbox/Athena/Feature_Reduction/test/dic_1.csv'
fileName_1 = '/Users/longma/Dropbox/Athena/Feature_Reduction/Results/Labels/BDL_target.csv'
fileName_2 = '/Users/longma/Dropbox/Athena/Feature_Reduction/test/dic_1.txt'
fileName_3 = '/Users/longma/Dropbox/Athena/Feature_Reduction/Results/FS_Outputs/BDL/Features_selected_170.txt'
#fileName_4 = 'C:/Users/lma5/Desktop/Features.csv'

data = np.genfromtxt(fileName, dtype=float, delimiter = ',').T    
label = np.genfromtxt(fileName_1, dtype=int, delimiter = ',')
feature = np.loadtxt(fileName_2, delimiter = ' ', dtype = str)

estimator = OneVsRestClassifier(LinearSVC())
#estimator = OneVsRestClassifier(SVR(kernel = 'linear'))
selector = RFE(estimator, 170, step=1)
selector = selector.fit(data,label)

support = selector.support_
#rank = selector.ranking_

feature = np.array(feature)
support = np.array(support)",Scikit/Long/feature_selection.py,AthenaSTM/ExperimentalMLScripts,1
"
    # from tsa.lib import tabular
    # printer = tabular.Printer()

    folds = cross_validation.KFold(y.size, 10, shuffle=True)
    for fold_index, (train_indices, test_indices) in iter8.sig_enumerate(folds, logger=logger):
        test_X, test_y = X[test_indices], y[test_indices]
        train_X, train_y = X[train_indices], y[train_indices]

        # model = linear_model.LogisticRegression(penalty='l2', C=1)
        # model = svm.LinearSVC(penalty='l2', )
        # model = linear_model.SGDClassifier()
        # model = neural_network.BernoulliRBM()
        model = naive_bayes.MultinomialNB()
        # model = naive_bayes.GaussianNB() # Whoa, incredibly slow
        # model = naive_bayes.BernoulliNB()

        model.fit(train_X, train_y)
        pred_y = model.predict(test_X)
        # coefs = model.coef_.ravel()",tsa/analyses/senti.py,chbrown/tsa,1
"		for i in range(len(TF_features)):
			if i in new_TF:
				boolean.append(True)
			else:
				boolean.append(False)

		features = selection_features_boolean(features, boolean)
		print str(len(features[0]))+"" features selected.\n""

	# Learning.
	clf = svm.SVC(C=10, kernel = 'rbf', gamma = 0.0625)
	clf.fit(features, labels_training)
	
	
		
	print ""----- PREDICTION""
	
	if len(sys.argv) == 3:
		print ""File ""+sys.argv[2]
		bioscope = 3",MUD.py,PAJEAN/uncertaintyDetection,1
"    # some parameters for the trained model
    param_grid = [
        {'C': [1, 10, 100, 1000],
         'kernel':['linear']},
        {'C': [1, 10, 100, 1000],
         'gamma':[0.001, 0.0001],
         'kernel':['rbf']}
    ]

    # construct the model
    clf = GridSearchCV(SVC(C=1, probability=True), param_grid, cv=5)

    # train the model
    clf.fit(features, label_num)

    # store the model
    f_name = ""../../resource/svm.pkl""
    print(""Saving classifier to '{}'"".format(f_name))
    with open(f_name, 'w') as f:
        pickle.dump((label, clf), f)",src/backend/make_classifier.py,xphongvn/smart-attendance-system-ta,1
"acc['lr']=N.zeros(len(ncomp))
pred={}
for c in range(len(ncomp)):
    pred['svm']=N.zeros(len(labels))
    pred['rbf']=N.zeros(len(labels))
    pred['lr']=N.zeros(len(labels))
    data=N.genfromtxt(melodic_dir+'datarun1_icarun2_%dcomp.txt'%ncomp[c])
    for train,test in loo:
        svm_c=linsvm[c]
        #print 'SVM: ',svm_c
        clf=LinearSVC(C=svm_c)
        clf.fit(data[train],labels[train])
        pred['svm'][test]=clf.predict(data[test])
        svm_c=rbfsvm[c,0]
        svm_gamma=rbfsvm[c,1]
        #print 'SVM-RBF: ',svm_c,svm_gamma
        clf=SVC(C=svm_c,gamma=svm_gamma)
        clf.fit(data[train],labels[train])
        pred['rbf'][test]=clf.predict(data[test])
        lrpen=logreg[c]",openfmri_paper/5.2_classify_task_ICA_randperm.py,poldrack/openfmri,1
"            train_y_reduced = y_train
            test_X = x_test
            test_y = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_01_09_2015_03.py,magic2du/contact_matrix,1
"			else:
				train_data[count_train] = data[x]
				train_label[count_train] = label[x]
				count_train += 1

		if count_eval != size_sample:
			print (""ATTENTION FOR INT"")
		return train_data, train_label, eval_data, eval_label

	def svm_train(self, data, label):
		clf = svm.SVC()
		print (""training"")
		clf.fit(data, label)
		print (""done"")
		return clf

	def svm_predict(self, clf, data):
		result = clf.predict(data)
		return result
",work/ML/tensorflow/separa/tool.py,ElvisLouis/code,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't instantiate with non-transformers on the way
    # Note that NoTrans implements fit, but not transform
    assert_raises_regex(TypeError,
                        'implement fit and transform or sample',
                        Pipeline, [('t', NoTrans()), ('svc', clf)])
",imblearn/tests/test_pipeline.py,chkoar/imbalanced-learn,1
"        y_train[i] = int(l.annotation)
    X_train = remove_nan_inf(X_train)
    standardizer = preprocessing.StandardScaler().fit(X_train)

    model = None 
    if args.model_param == 'logistic_regression':
        model = linear_model.LogisticRegression()
    elif args.model_param == 'LDA':
        model = lda.LDA()
    elif args.model_param == 'SVM':
        model = svm.LinearSVC()
    else:
        print ""Model not implemented yet""
        sys.exit(1)

    model.fit(X_train, y_train)
    print ""Score of the fit : "" , model.score(X_train, y_train)

    joblib.dump((standardizer, model), args.output_model, compress=1) #in one file",src/train_model.py,tombosc/mwer,1
"print(""y train: %s"" % str(y_train.shape))
print(""X test: %s"" % str(X_test.shape))
print(""y test: %s"" % str(y_test.shape))

# <codecell>

# Classify
gamma_parameter = 0.001

# Create model
clf = svm.SVC(C=50., kernel='rbf', gamma=gamma_parameter, probability=False)

# Clasify
clf.fit(X_train, y_train) 


# Dump info about the model
print(""\nSupported vectors length: %s"" % str(clf.support_vectors_.shape))
print(""Dual coef. length: %s"" % str(clf.dual_coef_.shape))
",SVM-training.py,rmaestre/SVM-mapreduce,1
"test = poly_feat.transform(test)

print train.shape

# transform counts to TFIDF features
tfidf = feature_extraction.text.TfidfTransformer(smooth_idf=False)
train = tfidf.fit_transform(train).toarray()
test = tfidf.transform(test).toarray()

# feature selection
feat_selector = LinearSVC(C=0.3, penalty='l1', dual=False)
train = feat_selector.fit_transform(train, labels)
test = feat_selector.transform(test)

print train.shape

# encode labels
lbl_enc = preprocessing.LabelEncoder()
labels = lbl_enc.fit_transform(labels)
",otto/model/model_01_bagging_linear/bagging_linear.py,ahara/kaggle_otto,1
"class Svc(Classifier):

    KERNEL = ""rbf""
    name = ""Support Vector Classifier""

    def __init__(self, *args, **kw):
        super(Svc, self).__init__(*args, **kw)
        self._pp = None
        self._cvwidget = None
        self._pwidget = None
        self._clf = sklearn.svm.SVC(C=1.0, kernel=self.KERNEL, gamma=0.0,
                                    probability=True)

    def parameterWidget(self, parent=None):
        """"""Returns the classifier specific parameter widget.""""""
        if self._pwidget is None:
            self._pwidget = SvcParameterWidget(parent)
        return self._pwidget

    def validationDialog(self, parent):",cat/classifiers/svc.py,rhoef/afw,1
"
    #vectorizer_binary = CountVectorizer(stop_words=stopWords, min_df=MIN_DF, binary=True, ngram_range=(1, 2))
    #vectorizer_binary = CountVectorizer(stop_words=stopWords, min_df=MIN_DF, binary=True, ngram_range=(1, 3))
    vectorizer_tfidf = TfidfVectorizer(stop_words=stopWords, min_df=MIN_DF, ngram_range=NGRAM_RANGE)
    #vectorizer = vectorizer_tfidf
    vectorizer = vectorizer_binary
    print(vectorizer)

    #clf = linear_model.LogisticRegression(penalty='l2', C=1.2)
    _ = linear_model.LogisticRegression()
    _ = svm.LinearSVC()
    _ = naive_bayes.BernoulliNB()  # useful for binary inputs (MultinomialNB is useful for counts)
    _ = naive_bayes.GaussianNB()
    _ = naive_bayes.MultinomialNB()
    _ = ensemble.AdaBoostClassifier(n_estimators=100, base_estimator=tree.DecisionTreeClassifier(max_depth=2, criterion='entropy'))
    #clf = ensemble.AdaBoostClassifier(n_estimators=100, base_estimator=tree.DecisionTreeClassifier(max_depth=2))
    _ = tree.DecisionTreeClassifier(max_depth=50, min_samples_leaf=5)
    #clf = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5, criterion='entropy')
    #clf = ensemble.RandomForestClassifier(max_depth=20, min_samples_leaf=5, n_estimators=10, oob_score=False, n_jobs=-1, criterion='entropy')
    _ = ensemble.RandomForestClassifier(max_depth=10, min_samples_leaf=5, n_estimators=50, n_jobs=-1, criterion='entropy')",learn1_experiments.py,ianozsvald/social_media_brand_disambiguator,1
"# Load data set and target values
data, target = make_classification(
    n_samples=1000,
    n_features=45,
    n_informative=12,
    n_redundant=7
)

def svccv(C, gamma):
    val = cross_val_score(
        SVC(C=C, gamma=gamma, random_state=2),
        data, target, 'f1', cv=2
    ).mean()

    return val

def rfccv(n_estimators, min_samples_split, max_features):
    val = cross_val_score(
        RFC(n_estimators=int(n_estimators),
            min_samples_split=int(min_samples_split),",examples/sklearn_example.py,fmfn/BayesianOptimization,1
"    assert_equal(pipe.get_params(deep=True),
                 dict(svc__a=None, svc__b=None, svc=clf))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)
    # Smoke test the repr:
    repr(pipe)
",python/sklearn/sklearn/tests/test_pipeline.py,seckcoder/lang-learn,1
"    #ensemble.append(xmodel)
    
    #GBC1 0.495 learning_rate 0.03 TO BE DONE
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',addFeatures=True)
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=300, learning_rate=0.03, max_depth=10,subsample=.5,verbose=1)
    #xmodel = XModel(""gbc1_r3"",classifier=model,Xtrain=Xtrain,Xtest=Xtest,ytrain=ytrain,class_names=sorted(list(set(labels))))
    #ensemble.append(xmodel)
    
    #SVM1 ~ 0.494
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',featureFilter=None,addFeatures=False,standardize=True,log_transform=True)
    #model = SVC(kernel='rbf',C=10.0, gamma=0.0, verbose = 0, probability=True)
    #xmodel = XModel(""svm1_r3"",classifier=model,Xtrain=Xtrain,Xtest=Xtest,ytrain=ytrain,class_names=sorted(list(set(labels))))
    #ensemble.append(xmodel)   
    
    #RF1 ~ 0.487
    #all_features=[u'feat_1', u'feat_2', u'feat_3', u'feat_4', u'feat_5', u'feat_6', u'feat_7', u'feat_8', u'feat_9', u'feat_10', u'feat_11', u'feat_12', u'feat_13', u'feat_14', u'feat_15', u'feat_16', u'feat_17', u'feat_18', u'feat_19', u'feat_20', u'feat_21', u'feat_22', u'feat_23', u'feat_24', u'feat_25', u'feat_26', u'feat_27', u'feat_28', u'feat_29', u'feat_30', u'feat_31', u'feat_32', u'feat_33', u'feat_34', u'feat_35', u'feat_36', u'feat_37', u'feat_38', u'feat_39', u'feat_40', u'feat_41', u'feat_42', u'feat_43', u'feat_44', u'feat_45', u'feat_46', u'feat_47', u'feat_48', u'feat_49', u'feat_50', u'feat_51', u'feat_52', u'feat_53', u'feat_54', u'feat_55', u'feat_56', u'feat_57', u'feat_58', u'feat_59', u'feat_60', u'feat_61', u'feat_62', u'feat_63', u'feat_64', u'feat_65', u'feat_66', u'feat_67', u'feat_68', u'feat_69', u'feat_70', u'feat_71', u'feat_72', u'feat_73', u'feat_74', u'feat_75', u'feat_76', u'feat_77', u'feat_78', u'feat_79', u'feat_80', u'feat_81', u'feat_82', u'feat_83', u'feat_84', u'feat_85', u'feat_86', u'feat_87', u'feat_88', u'feat_89', u'feat_90', u'feat_91', u'feat_92', u'feat_93']
    #addedFeatures_best=[u'row_median',u'arg_max',u'row_max',u'non_null',u'arg_min']
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',addFeatures=True,final_filter=all_features+addedFeatures_best)
    #basemodel = RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='entropy', max_features=20,oob_score=False)
    #model = CalibratedClassifierCV(basemodel, method='isotonic', cv=3)",competition_scripts/ensemble_otto.py,chrissly31415/amimanera,1
"    return x


if __name__ == ""__main__"":
    def list_split(l):
        center = len(l) / 2
        return l[:center], l[center:]

    MAX_LINES = -1

    #clf = LinearSVC(dual=False, C=1.1, loss='l2', penalty='l2', tol=1e-4)
    clf = LinearSVC(dual=False, C=1.1, loss='l2', penalty='l2', tol=1e-4)
    #clf = LinearSVC(dual=True, C=1.1, loss='l1', penalty='l2')
    #clf = linear_model.SGDClassifier()
    X = []
    Y = []

    reader = sys.stdin
    read_from_file = ""--read_from_file=1"" in sys.argv
    print_svm_score = ""--print_svm_score=1"" in sys.argv",2-svm/code/mapper.py,lukaselmer/ethz-data-mining,1
"print(__doc__)

import numpy as np
from sklearn import datasets, svm
from sklearn.model_selection import cross_val_score

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)

scores = list()
scores_std = list()
for C in C_s:
    svc.C = C
    this_scores = cross_val_score(svc, X, y, n_jobs=1)
    scores.append(np.mean(this_scores))
    scores_std.append(np.std(this_scores))",projects/scikit-learn-master/examples/exercises/plot_cv_digits.py,DailyActie/Surrogate-Model,1
"    print 'Using cached model...'
except:
    train(0, 20)  #NOTE
    X = np.concatenate(all_features)
    all_labels = np.concatenate(all_labels)
    all_BBox = np.concatenate(all_BBox)

    print 'Training SVM... with features.shape = ', X.shape, 'and target.shape = ', all_labels.shape

    # learn using SVM
    linSVM = svm.SVC(kernel='linear', probability=True)
    linSVM.fit(X, all_labels)
    
    joblib.dump(linSVM, 'svm_'+cls+'.pkl')

    print 'DONE!!'

print 'Testing...'

if doTesting:",VOCdevkit/train.py,rawcoder/object-detection,1
"            classfeatureindex: index of the column which defines the feature in dataset 
        '''
        ModelBase.__init__(self, dataset, 'SVM', *args, **kwargs)
        self.Test = self.Classify2
        self.Apply = self.ClassifyDataset
        self.Train = self.SVM
        self.Save = self.Dump
        self.Load = self.LoadFromFile
        self.Graph = self.ShowImage
        self.T = self.RealValue
        self.model = SVC()

    def SVM(self):
        X = np.matrix([item[:self.classfeatureindex] + [0] + item[self.classfeatureindex+1:]  if self.classfeatureindex > -1 else item[:len(item)-1] + [0] for item in self.dataset.Iter()])
        y = np.array([item[self.classfeatureindex] for item in self.dataset.Iter()])
        self.model.fit(X, y)

    def Classify2(self, test):
        '''
            predict a single new sample input with trained naive bayes",py/ml_models/svm.py,CalvinNeo/EasyMLPlatform,1
"    # The digits samples are dependent: they are apparently grouped by authors
    # although we don't have any information on the groups segment locations
    # for this data. We can highlight this fact be computing k-fold cross-
    # validation with and without shuffling: we observe that the shuffling case
    # wrongly makes the IID assumption and is therefore too optimistic: it
    # estimates a much higher accuracy (around 0.96) than than the non
    # shuffling variant (around 0.86).

    digits = load_digits()
    X, y = digits.data[:800], digits.target[:800]
    model = SVC(C=10, gamma=0.005)
    n = len(y)

    cv = cval.KFold(n, 5, shuffle=False)
    mean_score = cval.cross_val_score(model, X, y, cv=cv).mean()
    assert_greater(0.88, mean_score)
    assert_greater(mean_score, 0.85)

    # Shuffling the data artificially breaks the dependency and hides the
    # overfitting of the model with regards to the writing style of the authors",venv/lib/python2.7/site-packages/sklearn/tests/test_cross_validation.py,valexandersaulys/prudential_insurance_kaggle,1
"            np_pos = np.where(data_idx==i)[0]
            bow_trn_mat[cnt,:] = np.asarray(fsa.bow(data_mat[np_pos,:],
                                                    codebook))

        # Cross-validate (5-fold) SVM classifier and parameters
        param_selection = [{'kernel': ['rbf'],
                            'gamma': np.logspace(-6,2,10),
                            'C': [1, 10, 100, 1000]},
                           {'kernel': ['linear'],
                            'C': [1, 10, 100, 1000]}]
        clf = GridSearchCV(svm.SVC(C=1), param_selection)
        clf.fit(bow_trn_mat, np.asarray(class_info)[trn], cv=5)

        # Compute BoW histograms for testing data
        bow_tst_mat = np.zeros((len(tst), options.codewords))
        for cnt,i in enumerate(tst):
            pos =  np.where(data_idx==i)[0]
            bow_tst_mat[cnt,:] = fsa.bow(data_mat[pos,:], codebook)

        print ""yhat : "", clf.predict(bow_tst_mat)",Base/Python/pyfsa/dcl.py,cdeepakroy/TubeTK,1
"from sklearn import svm

if __name__ == '__main__':
	X = [[0, 0], [1, 1], [2,2]]
	y = [0, 1, 10]
	clf = svm.SVC()
	clf.fit(X, y)",test-sklearnsvm.py,Matchoc/stockmarketpy,1
"        # test_x = self.features_df[np.random.permutation(num_rows)[-5000:]]
        # test_y = self.y[np.random.permutation(num_rows)[-5000:]]

        train_x = self.features_df[:18000]
        train_y = self.y[:18000]

        test_x = self.features_df[18000:]
        test_y = self.y[18000:]


        # clf = SVC()
        # clf.fit(train_x, train_y) 

        names = [
            ""Nearest Neighbors"", 
            # ""Linear SVM"", 
            # ""RBF SVM"", 
            ""Decision Tree"",
            ""Random Forest"", 
            # ""AdaBoost"", ",Classifier.py,fahadsultan/CausalRelations,1
"    X_train, X_test, X_s_train, X_s_test, y_train, y_test = \
            cval.train_test_split(X, X_s, y)
    assert_array_equal(X_train, X_s_train.toarray())
    assert_array_equal(X_test, X_s_test.toarray())
    assert_array_equal(X_train[:, 0], y_train * 10)
    assert_array_equal(X_test[:, 0], y_test * 10)


def test_cross_val_score_with_score_func_classification():
    iris = load_iris()
    clf = SVC(kernel='linear')

    # Default score (should be the accuracy score)
    scores = cval.cross_val_score(clf, iris.data, iris.target, cv=5)
    assert_array_almost_equal(scores, [1., 0.97, 0.90, 0.97, 1.], 2)

    # Correct classification score (aka. zero / one score) - should be the
    # same as the default estimator score
    zo_scores = cval.cross_val_score(clf, iris.data, iris.target,
            score_func=zero_one_score, cv=5)",venv/lib/python2.7/site-packages/sklearn/tests/test_cross_validation.py,GbalsaC/bitnamiP,1
"    train_test_split(x, y, test_size = 0.3, random_state=0)

# 正規化
sc = StandardScaler()
sc.fit(x_train)
x_train_std = sc.transform(x_train)
x_test_std  = sc.transform(x_test)
x_combined_std = np.vstack((x_train_std, x_test_std))
y_combined = np.hstack((y_train, y_test))

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(x_train_std, y_train)
plot_decision_regions(x_combined_std, y_combined, classifier=svm, test_idx=range(105,150))

plt.xlabel('petal length [standardized]')
plt.ylabel('petal width  [standardized]')
plt.legend(loc='upper left')
plt.show()",3/p69_supportvectormachine.py,yukke42/machine-learning,1
"  elif not callable(score_func):
    raise ValueError(""Score function must be a string or a callable."")
  predictions = clf.predict(features)
  return score_func(labels, predictions), predictions

TestClassifier = ScoreClassifier  # deprecated

def CrossValidateClassifier(features, labels, num_folds=None, algorithm=None,
    scale=True):
  if algorithm is None:
    algorithm = LinearSVC()
  if num_folds is None:
    num_folds = 10
  if scale:
    algorithm = Scaled(algorithm)
  features = features.astype(float)
  return cross_val_score(algorithm, features, labels, cv=num_folds)",glimpse/util/learn/learn.py,mthomure/glimpse-project,1
"    grid = GridSearchCV(neighbors.KNeighborsClassifier(), param_grid=param_grid,cv=cv)
    grid.fit(x_train, y_train)

    print(accuracy_score(grid.predict(x_train_test), y_train_test))

def train_classifier_svm(dataset, feature_index, stats):
    test_set, train_set = split_sets(dataset, 0.1)
    x_train, y_train = split_train_result_set(train_set, feature_index)
    x_train_test, y_train_test = split_train_result_set(test_set, feature_index)

    clf = svm.SVC()
    clf = clf.fit(x_train, y_train)
    print(accuracy_score(clf.predict(x_train_test), y_train_test))
    print(len(test_set), len(train_set))

def train_classifier_svm2(dataset, feature_index, stats):
    test_set, train_set = split_sets(dataset, 0.1)
    x_train, y_train = split_train_result_set(train_set, feature_index)
    x_train_test, y_train_test = split_train_result_set(test_set, feature_index)
",src/mod_suggest/tree_trainer.py,Drob-AI/The-Observer,1
"    points = [(np.random.uniform(-1,1), np.random.uniform(-1,1)) for i in range(nopoint)]
    #print points
    dataset_Monte = np.array([(i[0],i[1], isLeft(pointa,pointb,i)) for i in points])
    #print dataset_Monte
    return getMisMatches(dataset_Monte, weights)


if __name__ == ""__main__"":
    '''X = np.array([[-1,-1],[-2,-1], [1,1], [2,1]])
    y = np.array([1,1,2,2])
    clf = SVC()
    clf.fit(X,y)
    print(clf.predict([[-0.8,-1]]))'''
    #clf = SVC()
    clf = SVC(C = 1000, kernel = 'linear')  
    monteavgavgQP = list()
    monteavgavgPLA = list()
    approxavgQP = list()
    vectornumberavg = list()
    predictavg = list()",Week 7/qp.py,pramodh-bn/learn-data-edx,1
"#Training:

# east parking video
""""""data1 = np.load('Minarawala_data_train_input_07.npy')
original_output = np.load('Minarawala_data_train_output_08.npy')""""""
#print(data1.shape)
# pplot video
data1 = np.load('Minarawala_data1_train_input_09.npy')
original_output = np.load('Minarawala_data1_train_output_10.npy')
t=time.time()
clf = svm.SVC(kernel='linear')
clf.fit(data1.T, original_output)
print(time.time()-t)



frame_count = Video.get(7)
frame_rate = Video.get(5)
print(frame_count,frame_rate)
skip = np.arange(0,frame_count,50)",Minarawala_code3_05.py,kush100993/Parking_space_detection_and_tracking,1
"    total = time.time()
    params = {""n_estimators"": [10, 50, 100, 200, 400, 750, 800, 1000, 2000], ""base_estimator__max_depth"": [1, 2, 3, 5], ""base_estimator__random_state"": [0], ""random_state"": [0]}
#    params = {""C"": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}

    datamanager = CaltechManager()
    categories = [c for c in os.listdir(datamanager.PATHS[""CATEGORIES_DIR""]) if c != datamanager.BACKGROUND and os.path.splitext(c)[1] != "".py""]

    #kernels, gammas = build_train_kernels(categories, datamanager)
    #print ""Finished building kernels""

    #grids = (GridSearch(SVC(kernel=""precomputed""), c) for c in categories)
    # grids = (GridSearch(RandomForestClassifier(), c) for c in categories)

    grids = [GridSearch(AdaBoostClassifier(), datamanager, c) for c in categories]

    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"")
        for g in grids:
            g.grid_search(params, weight_samples=False)
        generate_evaluation_summary(grids, ""grid_test.csv"")",runGridSearch.py,peret/visualize-bovw,1
"    chat_client = chat.connect()
    GlobalVars.client = chat_client
    cprint(""Success."", ""green"")

    GlobalVars.control_room = chat_client.get_room(config.get('control_room'))

    chat.watch_room(chat_client, config.get('control_room'), chat.process_event, config)
    cprint(""Socket watcher started for {}."".format(config.get('control_room')), ""blue"")

    values, labels = load_classification_data(config.get(""class_data_file""))
    clf = svm.SVC()
    clf.fit(values, labels)
    GlobalVars.clf = clf

    chat.send_message(config.get('control_room'), ""Started with {} classification records loaded. ""
                      ""Running on {}."".format(len(labels), config.get('location')))
    cprint(""{} classifier records loaded."".format(len(labels)), ""blue"", attrs=['bold'])

    ws = websocket.create_connection(""ws://qa.sockets.stackexchange.com/"")
    ws.send(""97-questions-newest"")",main.py,ArtOfCode-/quality-machine,1
"        X = loadBaselineData(ids)
        y = np.array(classes,dtype=np.int32)

        scaler = StandardScaler(copy=False, with_mean=True, with_std=True).fit(X) #mean_substract.MeanNormalize(copy=False).fit(X)
        X = scaler.transform(X)
        self.baseline_transforms.append(scaler)

        transform_clf = None

        if self.config.baselineClassifier.lower() == 'svm':
            self.baseline_clf = svm.LinearSVC(C=0.001)
            X = X.astype(np.float64)
        if self.config.baselineClassifier.lower() == 'svm_poly':    
            self.baseline_clf = svm.SVC(C=1.0, kernel='poly', probability=True, cache_size=2000, verbose = True)
        if self.config.baselineClassifier.lower() == 'trees':
            self.baseline_clf = getTreeClf()
        if self.config.baselineClassifier.lower() == 'trees2x':
            transform_clf = getTreeClf()
            self.baseline_clf = getTreeClf()
",sequence_cnn.py,bmilde/deepschmatzing,1
"    logger.info(
        'doing MVPA training and classification on %d subjects, each of which has %d epochs' %
        (num_subjects, num_epochs_per_subj)
    )

    processed_data, labels = io.prepare_mvpa_data(data_dir, extension, epoch_file, mask_file)

    # transpose data to facilitate training and prediction
    processed_data = processed_data.T

    clf = svm.SVC(kernel='linear', shrinking=False, C=1)
    # doing leave-one-subject-out cross validation
    for i in range(num_subjects):
        leave_start = i * num_epochs_per_subj
        leave_end = (i+1) * num_epochs_per_subj
        training_data = np.concatenate((processed_data[0:leave_start], processed_data[leave_end:]), axis=0)
        test_data = processed_data[leave_start:leave_end]
        training_labels = np.concatenate((labels[0:leave_start], labels[leave_end:]), axis=0)
        test_labels = labels[leave_start:leave_end]
        clf.fit(training_data, training_labels)",examples/fcma/mvpa_classification.py,mihaic/brainiak,1
"
                # C optimization :
                if name.lower().find('_c_') != -1:
                    grid['clf__C'] = C_range

                # Gamma optimization :
                if name.lower().find('gamma') != -1:
                    grid['clf__gamma'] = gamma_range


            clf = SVC(**kwargs)

        # ----------------------------------------------------------------
        # BUILD COMBINE
        # ----------------------------------------------------------------
        # -> FDR :
        if name.lower().find('fdr') != -1:
            combine.append((""fdr"", fdr))
            grid['features__fdr__alpha'] = fdr_alpha
",brainpipe/clf/MFpipe.py,EtienneCmb/brainpipe,1
"  cv = cross_validation.KFold(len(train_data), 10)
  for train_index, test_index in cv:
    train_X = train_data[train_index]
    train_Y = train_label[train_index]
    test_X = train_data[test_index]
    test_Y = train_label[test_index]
    gaussian_process(train_X, train_Y, test_X, test_Y)
    #pyGP(train_X, train_Y, test_X, test_Y)

def run_SVM(train_X, train_Y, test_X, test_Y, c, r):
  clf = svm.SVC(C = c, kernel = 'rbf', gamma = r, cache_size = 2048.0)
  clf.fit(train_X, train_Y)
  dist = clf.decision_function(train_X)
  return clf.score(test_X, test_Y)

def pyGP(train_X, train_Y, test_X, test_Y):
  gp = GaussianProcess(theta0 = 5e-1)
  Y = gaussian_kernel_matrix(train_X, math.pow(2, -15.0))
  print Y
  return",finalproject/finalproject.py,Technipire/Statistical-Pattern-Recognition,1
"X =  tfv.transform(traindata) 
X_test = tfv.transform(testdata)

# Initialize SVD
svd = TruncatedSVD()

# Initialize the standard scaler 
scl = StandardScaler()

# We will use SVM here..
svm_model = SVC()

# Create the pipeline 
clf = pipeline.Pipeline([('svd', svd),
         ('scl', scl),
                              ('svm', svm_model)])

# Create a parameter grid to search for best parameters for everything in the pipeline
param_grid = {'svd__n_components' : [120, 140],
              'svm__C': [1.0, 10]}",competitions/crowdflower-search-relevance/benchmark.py,gtesei/fast-furious,1
"  # TODO(cjr): make options for using other algorithms

  # train, test = sklearn.cross_validation.train_test_split(runDataList)
  positives = [rd for rd in runDataList if len(rd['labels']) > 0]
  posTemplates = magnetak_util.CreateTemplates(positives)
  negatives = [rd for rd in runDataList if len(rd['labels']) == 0]
  negTemplates = magnetak_util.CreateTemplates(negatives)

  trainX, trainY = GenerateData(runDataList, featurizer)

  # clf = sklearn.svm.LinearSVC()
  # clf = sklearn.svm.SVC(kernel='linear')
  clf = sklearn.linear_model.LogisticRegression()
  clf.fit(trainX, trainY)
  print clf.coef_

  detector = MLDetector()
  detector.clf = clf
  detector.MagnetToVectorObj = featurizer
",data/magnetak_ml.py,dodger487/MIST,1
"        print()
        clf_descr = str(clf).split('(')[0]
        return clf_descr, score, train_time, test_time

    results = []

    for penalty in [""l2"", ""l1""]:
        print('=' * 80)
        print(""%s penalty"" % penalty.upper())
        # Train Liblinear model
        results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,dual=False, tol=1e-3)))

    # Train sparse Naive Bayes classifiers
    print('=' * 80)
    print(""Naive Bayes"")
    results.append(benchmark(MultinomialNB(alpha=.01)))
    results.append(benchmark(BernoulliNB(alpha=.01)))

    print('=' * 80)
    print(""LinearSVC with L1-based feature selection"")",Project/classifyTest.py,tpsatish95/Youtube-Comedy-Comparison,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = FCBF.fcbf(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_FCBF.py,jundongl/scikit-feast,1
"            if numIdentities <= 1:
                return

            param_grid = [
                {'C': [1, 10, 100, 1000],
                 'kernel': ['linear']},
                {'C': [1, 10, 100, 1000],
                 'gamma': [0.001, 0.0001],
                 'kernel': ['rbf']}
            ]
            self.svm = GridSearchCV(SVC(C=1), param_grid, cv=5).fit(X, y)

    def processFrame(self, dataURL, identity):
        head = ""data:image/jpeg;base64,""
        assert(dataURL.startswith(head))
        imgdata = base64.b64decode(dataURL[len(head):])
        imgF = StringIO.StringIO()
        imgF.write(imgdata)
        imgF.seek(0)
        img = Image.open(imgF)",demos/web/server.py,sumsuddinshojib/openface,1
"    #     #         adaptedLabels[i]=-999
    #     #
    #     #     # if (label >= 2 ):
    #     #     #     adaptedLabels[i] = 1
    #     #     # else:
    #     #     #     adaptedLabels[i] = 0
    #
    #
    #
    #     #
    #     # svc = SVC(C=1.0,kernel='linear')
    #     # svc.fit(nb_tmp_XS,nb_tmp_YS)
    #     # svc_tmp_YT = svc.predict(XT_te)
    #     # print()
    #     # print()
    #     #
    #
    #
    #     # tmp_YT =  YT_tr + list(adaptedLabels) #[-999]*len(YT_te)
    #     # print(len(tmp_YT))",textanalysis/preprocessing/reweighting_manifold_alignment.py,arunreddy/text-analysis,1
"        int(accuracy*1000)/10.0,
        len(train_Y),
        len(test_Y),
        str(sklnr).replace('\n','')[:140])
    print 'output: {}'.format(actual_vs_predict[-10:])

# choose different learners
learner = [
        # naive_bayes.GaussianNB(),
        # linear_model.SGDClassifier(),
        # svm.SVC(),
        # tree.DecisionTreeClassifier(),
        # ensemble.RandomForestClassifier(),
        ensemble.AdaBoostRegressor(),
        ensemble.BaggingRegressor(),
        ensemble.ExtraTreesRegressor(),
        ensemble.GradientBoostingRegressor(),
        ensemble.RandomForestRegressor(),
        gaussian_process.GaussianProcessRegressor(),
        linear_model.HuberRegressor(),",ml/stock_prediction.py,james-jz-zheng/jjzz,1
"from sklearn.naive_bayes import GaussianNB

#loading datasets
digits = load_digits()

X = digits.data
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)

clfSVM = AdaBoostClassifier(base_estimator=SVC(C=1, kernel='linear'),
                            n_estimators=50,
                            learning_rate=1,
                            algorithm='SAMME')
clfSVM = clfSVM.fit(X_train, y_train)
predictSVM = clfSVM.predict(X_test)
scoreSVM = accuracy_score(y_test, predictSVM)

clfTree = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, learning_rate=1)
clfTree = clfTree.fit(X_train, y_train)",Classifiers/adaBoosterClassifier.py,pawanrai9999/SimpleMachines,1
"from sklearn import svm
from sklearn.externals import joblib
import datetime
import os

estimators = {}
estimators['bayes'] = GaussianNB()
estimators['tree'] = tree.DecisionTreeClassifier()
estimators['forest_100'] = RandomForestClassifier(n_estimators = 100)
estimators['forest_10'] = RandomForestClassifier(n_estimators = 10)
estimators['svm_c_rbf'] = svm.SVC()
estimators['svm_c_linear'] = svm.SVC(kernel='linear')
estimators['svm_linear'] = svm.LinearSVC()
estimators['svm_nusvc'] = svm.NuSVC()

import csv
scoreFile = open('csv/scorefile.txt', 'w', newline='')
#csvWriter = csv.writer(scoreFile, delimiter =',', quotechar ='""', quoting=csv.QUOTE_MINIMAL)

for k in estimators.keys():",scikit-learn/classifications.py,yuantw/MachineLearning,1
"	return trainX,trainY,testX,testY

n_examples = 100000
trainX,trainY,testX,testY = load('data/data.pkl',n_chars=12,n_examples=n_examples)
print trainX.shape,trainY.shape,testX.shape,testY.shape

t = time()
# model = lm.Ridge()
model = RandomForestClassifier(n_estimators=12,n_jobs=2,verbose=2)
# model = lm.LogisticRegression()
# model = LinearSVC()
model.fit(trainX,trainY)
pred = model.predict(testX)
# pred = np.argmax(pred,axis=1)
# print pred
# print np.argmax(testY,axis=1)
print metrics.accuracy_score(testY,pred),time()-t",model.py,Newmu/text-generation,1
"    Xrow = asRowMatrix(features)
    # Split the dataset in two equal parts
    X_train, X_test, y_train, y_test = train_test_split(Xrow, y, test_size=0.5, random_state=0)
    # Define the Classifier:
    scores = ['precision', 'recall']
    # Evaluate the Model:
    for score in scores:
        print(""# Tuning hyper-parameters for %s"" % score)
        print()
    
        clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,
                        scoring='%s_macro' % score)
        clf.fit(X_train, y_train)
    
        print(""Best parameters set found on development set:"")
        print()
        print(clf.best_params_)
        print()
        print(""Grid scores on development set:"")
        print()",py/facerec/svm.py,bytefish/facerec,1
"    """"""
    from sklearn.base import clone
    from sklearn.utils import check_random_state
    from sklearn.svm import SVC
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.cross_validation import check_cv

    if clf is None:
        scaler = StandardScaler()
        svc = SVC(C=1, kernel='linear')
        clf = Pipeline([('scaler', scaler), ('svc', svc)])

    info = epochs_list[0].info
    data_picks = pick_types(info, meg=True, eeg=True, exclude='bads')

    # Make arrays X and y such that :
    # X is 3d with X.shape[0] is the total number of epochs to classify
    # y is filled with integers coding for the class to predict
    # We must have X.shape[0] equal to y.shape[0]",mne/decoding/time_gen.py,dengemann/mne-python,1
"
        relation_file = '%s/relations-no-senses.json' % input_dataset
        parse_file = '%s/parses.json' % input_dataset
        parse = json.load(codecs.open(parse_file, encoding='utf8'))

        relation_dicts = [json.loads(x) for x in open(relation_file)]

        output_file = '%s/output.json' % output_dir
        output = codecs.open(output_file, 'wb', encoding='utf8')

        clf = SVC()
        clf = pickle.load(open(load_model_file_basename, 'rb'))


        if scale_features:
            # scaler = preprocessing.MinMaxScaler(self.scale_range)
            # scaler.transform(feats)
            scaler = pickle.load(open(load_scale_file_basename, 'rb'))
            logger.info('Scaling is enabled!')
        else:",architectures/conll16st-hd-sdp/sup_parser_v1.py,jimmycallin/master-thesis,1
"# trainingsdaten werden in eingabedaten (vektoren) umgewandelt
# features ist dann matrix bestehend aus den einzelnen vektoren
features = vectorize_text(texts)

# x sind die eingabematrizen, y sind die vektoren in denen die ergebnisse stehen
x_train, x_test, y_train, y_test = split_train_test(features, labels, ratio=0.7)

# das trainieren mit den daten
from sklearn.svm import  SVC

clf = SVC(C=1000., kernel='rbf')
clf.fit(x_train, y_train)

#variablen zum analysieren der ergebnisse
succ = 0
total = 0
dev_count = 0

#wir gehen testdaten durch und schaun wie sie eingeordnet werden
for i in xrange(len(x_test)):",Playground/svm.py,Ichaelus/Github-Classifier,1
"	print(""Number of Good Features: %d""%features_idx.shape[0])
	Xsub = Xsub[:,features_idx]

        x_mean = np.mean(Xsub, axis=0)
        x_std = np.std(Xsub, axis=0)

        Xsub = (Xsub - x_mean) / x_std

        sys.stderr.write('Applying SVM classification ... %d'%(i))

        clf = sklearn.svm.SVC(C=optimal_c, kernel='rbf', gamma=optimal_gamma)
        clf.fit(Xsub, ysub)

        Xcv = pandas.read_table(args.data_cv, sep=' ', usecols=goodfeatures, dtype='int', header=None)
        ytrue_cv = pandas.read_table(args.label_cv, sep=' ', dtype='int', header=None)[0]

        Xcv = Xcv.iloc[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0],:]

        ytrue_cv = ytrue_cv[np.where((ytrue_cv >= ymin) & (ytrue_cv <= ymax))[0]].values
        #ytrue_cv = ytrue_cv[np.where(ytrue_cv <= ymax)[0]]",codes/classify_half6.py,mirjalil/ml-visual-recognition,1
"from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from settings import *

names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(n_neighbors=25, weights='distance'),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]

def main():

	#set the timer",kpcaWithUVFS/mnistBackImage/classifiers.py,akhilpm/Masters-Project,1
"            else:
                wb = input_array
            # Normalize the input vector
            x = wb/np.linalg.norm(wb)
            h = np.tanh(self.W.dot(x))
        return np.tanh(np.dot(h, self.v))

# Class that uses Scikit-Learn's implementation of SVM to predict labels
class svm():
    def __init__(self):
        # self.clf = SVC(kernel='rbf')
        self.clf = NuSVC()

    def train(self, inputs):
        # Parameters:
        #     inputs: An array of Input objects containing input vectors along with their corresponding labels.

        # Creates lists to use for fitting model
        X = []
        Y = []",Backpropagator.py,ephraimrothschild/Neural-Network-Tools,1
"
# In[44]:

#check algorithms
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))

#evaluate each model
results = []
names = []
for name, model in models:
    kfold = model_selection.KFold(n_splits=10, random_state=seed)
    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)",Breast+Cancer+Diagnosis.py,DillonNovak/Programming-for-Chemical-Engineering-Applications,1
"    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    blah1=zeros((19,4)) #accuracy,precision,recall,F1
    blah2=zeros((19,4))
    blah3=zeros((19,4))
    for i,fraction in enumerate(fractions):
        new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
        
        clf= SVC(kernel='linear', C=100)
        clf.fit(new_trn, trn_lbl)    
        blah1[i,:]= calc_stats(clf.predict(new_tst),tst_lbl)
        
        clf= KNeighborsClassifier(n_neighbors=3)
        clf.fit(new_trn, trn_lbl)    
        blah2[i,:]= calc_stats(clf.predict(new_tst),tst_lbl)
        
        clf=DecisionTreeClassifier(criterion='entropy', min_samples_split=8, random_state=0)
        clf.fit(new_trn, trn_lbl)    ",problems/ohsumedTitleOnly_compete.py,lioritan/Thesis,1
"    trn, trn_lbl, tst, feature_names, floo= blor.get_new_table(test, tst_ents)
    
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.feature_selection import SelectKBest

#    feature_selector= SelectKBest(chi2, k=100)
#    filtered_trn= feature_selector.fit_transform(trn, trn_lbl)
#    filtered_tst= feature_selector.transform(tst)
#    blah3= SVC(kernel='linear', C=inf)
    blah3= KNeighborsClassifier(n_neighbors=5)
#    blah3= DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
    blah3.fit(trn, trn_lbl)
    
    pred3trn=blah3.predict(trn)
    print mean(pred3trn!=trn_lbl)
    pred3tst=blah3.predict(tst)
    print mean(pred3tst!=test_lbl)
    print len(blor.new_features)",problems/alg10_ohsumed_flatten.py,lioritan/Thesis,1
"import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use(""ggplot"")
from sklearn import svm
X = np.array([[1, 1], [1, 5], [5, 1], [5, 5]])
y = [-1,1,1,1]
clf = svm.SVC(kernel='linear')
clf.fit(X, y)
print clf.support_vectors_
print clf.n_support_
w = clf.coef_[0]
print 'w:'
print(w)
a = -w[0] / w[1]
xx = np.linspace(0,6)
yy = a * xx - clf.intercept_[0] / w[1]",3A/UV403/svm/classifier_svm.py,jianfeipan/TB,1
"        p.imshow(images[i], cmap=plt.cm.bone)
        
        # label the image with the target value
        p.text(0, 14, str(target[i]))
        p.text(0, 60, str(i))
        
print_faces(faces.images, faces.target, 20)

from sklearn.svm import SVC

svc_1 = SVC(kernel='linear')
print svc_1

from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
        faces.data, faces.target, test_size=0.25, random_state=0)

from sklearn.cross_validation import cross_val_score, KFold
from scipy.stats import sem",src/main/python/sentimentanalysis.py,Chaparqanatoos/kaggle-knowledge,1
"
#import matplotlib.pyplot as plt
#plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1],c='b', marker='x', label='1')
#plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1],c='r', marker='s', label='-1')
#plt.ylim(-3.0)
#plt.legend()
#plt.show()

#replace the parameter of SVC kernel = 'linear' with kernel = 'rbf'
from sklearn.svm import SVC
svm = SVC(kernel='rbf',random_state = 0,gamma=0.10,C=10.0)
svm.fit(X_xor,y_xor)

#draw decision boundary
import DecisionBoundary
DecisionBoundary.plot_decision_regions(X_xor,y_xor,classifier=svm)
plt.legend(loc='upper left')
plt.show()

#----------------------------------------------------------------",1_supervised_classification/15-SVM/svm/nonlinear_svm.py,PhenixI/machine-learning,1
"    for objective in xrange(number_of_objectives):
        cart_trees.append(DecisionTreeRegressor())
        cart_trees[objective].fit(training_independent, [td[objective] for td in training_dependent])
    return cart_trees

    # svm
    # from sklearn import svm
    # number_of_objectives = len(training_dependent[0])
    # clfs = []
    # for objective in xrange(number_of_objectives):
    #     # if objective == 1: clfs.append(svm.SVC(decision_function_shape='ovo'))
    #     # else: clfs.append(svm.SVR())
    #     clfs.append(svm.SVR())
    #     clfs[objective].fit(training_independent, [td[objective] for td in training_dependent])
    # return clfs


def get_testing_data(training_independent, filename):
    decisions = len(training_independent[0])
    import pandas as pd",script.py,ai-se/surrogate,1
"    
    #x = vectorizer.fit_transform(train)
    print x
    print ""transform test to tf matrix""
    #t = vectorizer.transform(test)

    print x.toarray
    
    
    print ""feature selection""
    #clf = LinearSVC(C=50, penalty=""l1"", dual=True)
    #svc = SVC(kernel=""linear"")
    #rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(label, 2000),scoring='accuracy')
    #rfecv.fit(x,label)
    #rfecv.transform(x)
    #rfecv.transform(t)
    #print ""x"",x.shape
    #print ""t"",t.shape

    #print ""add features""",feature.py,ezhouyang/class,1
"kfold = StratifiedKFold(y=Y[:,0], n_folds=10)
cvscores = []
error_labels_index=[]
X=X[idx];
Y=Y[idx];

#Perform 10-fold cross validation
print ""Performing 10-fold cross validation...""
for i, (train, test) in enumerate(kfold):
	 # Fit the SVM model
    clf2 = SVC(C=20, gamma=1.5e-04) 
    clf2.fit(X[train], np.ravel(Y[train]))
    accuracy=clf2.score(X[test],np.ravel(Y[test]))
    predicted_labels=clf2.predict(X[test])
    test_labels=np.reshape(Y[test],(len(Y[test])))
    error=predicted_labels-test_labels
    print ""Test Fold ""+str(i)
    print(""Accuracy= %.2f%%"" % (accuracy*100)) 
    cvscores.append(accuracy * 100)
",PYTHON/fgsd_svm_demo.py,codeanonymous/FGSD,1
"# Loading the data
df = pd.read_csv(""../../Dataset/dataset.csv"",delimiter ='\t')


# Splitting the data
X = df.ix[:, 3:42]
print X
Y = df.ix[:, 42:43]
print Y

svm = SVC(gamma=0.001)


svm.fit(X,Y)

# TO BE DONE
'''

# Predicting class labels
eval_results = svm.predict(X_test)",src/train_test/svm.py,mohitreddy1996/Gender-Detection-from-Signature,1
"            ""C"": [1, 10, 100, 1000]
        },
        {
            ""kernel"": [""rbf""],
            ""C"": [1, 10, 100, 1000],
            ""gamma"": [1e-2, 1e-3, 1e-4, 1e-5]
        }
    ]

    # request probability estimation
    svm = SVC(probability=True)

    # 10-fold cross validation, use 4 thread as each fold and each parameter set can be train in parallel
    clf = grid_search.GridSearchCV(svm, param,
            cv=10, n_jobs=20, verbose=3)

    clf.fit(X_train, y_train)

    if os.path.exists(model_output_path):
        joblib.dump(clf.best_estimator_, model_output_path)",sklearn_svm.py,mwleeds/android-malware-analysis,1
"            model.add(Activation('tanh'))
            
            # Output Layer, one neuron per class
            model.add(Dense(output_size))
            # Softmax zum Normalisieren der Werte
            model.add(Activation('softmax'))

            model.compile(metrics=['accuracy'], optimizer='sgd', loss='mse')
            self.model = model
        elif clf_type.lower() == 'svm':
            self.model = SVC()
        elif clf_type.lower() == 'nb':
            self.model = MultinomialNB()
        elif clf_type.lower() == 'lr':
            self.model = LinearRegression()
        
    def train(self, X, Y, nb_epoch=10, batch_size=1, get_accuracy=True):
        assert len(X) == len(Y)
        if type(self.model) == keras.models.Sequential:
            self.model.fit(X, one_hot_encoding(Y), nb_epoch=nb_epoch, batch_size=batch_size, verbose=False)",Playground/Classifier.py,Ichaelus/Github-Classifier,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 100    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the t-score of each feature
        score = t_score.t_score(X, y)

        # rank features in descending order according to score
        idx = t_score.feature_ranking(score)
",skfeature/example/test_t_score.py,jundongl/scikit-feature,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = CMASVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = CMAESRSVC(\
        mu = 15,
        lambd = 100,
        xmean = matrix([[10.0, 10.0]]),
        sigma = 4.5,
        beta = 0.80,
        meta_model = meta_model) 

    return method
",evopy/examples/experiments/fitness_cmaesrsvc/setup.py,jpzk/evopy,1
"                print(""Train index: "", train_index)
                print(""Test index: "", test_index)
                #logger.info('subset number: ' + str(subset_no))
                (train_X_10fold, train_y_10fold),(train_X_reduced, train_y_reduced), (test_X, test_y) = self.ddi_obj.get_ten_fold_crossvalid_one_subset(train_index, test_index, fisher_mode = fisher_mode, reduce_ratio = reduce_ratio)
                standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
                scaled_train_X = standard_scaler.transform(train_X_reduced)
                scaled_test_X = standard_scaler.transform(test_X)
                
                if settings['SVM']:
                    print ""SVM""                   
                    Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                    Linear_SVC.fit(scaled_train_X, train_y_reduced)
                    predicted_test_y = Linear_SVC.predict(scaled_test_X)
                    isTest = True; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                    predicted_train_y = Linear_SVC.predict(scaled_train_X)
                    isTest = False; #new
                    analysis_scr.append((self.ddi, subset_no, fisher_mode, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))                    
                if settings['SVM_RBF']:",Contact_maps/DeepLearning/DeepLearningTool/DL_contact_matrix_New_local_test.py,magic2du/contact_matrix,1
"    forest_clf = ExtraTreesClassifier(n_estimators=250, n_jobs=-1, max_depth=None)
    forest_clf.fit(X_train,y_train)
    y_pred = forest_clf.predict(X_test)
    print ""scoring extra-trees classifier:""
    print ""score is: "", forest_clf.score(X_test, y_test)
    print(classification_report(y_test, y_pred, target_names=ArticleDataset.Label_Names[5]))

    print ""training SVM classifier""
    Y_labels = data.get_feature('label')
    X_train, X_test, y_train, y_test = train_test_split(X, Y_labels)
    svm_clf = svm.SVC()
    svm_clf.fit(X_train, y_train)
    print ""scoring SVM classifier:""
    y_pred = svm_clf.predict(X_test)
    print ""score is: "", svm_clf.score(X_test, y_test)
    print(classification_report(y_test, y_pred, target_names=ArticleDataset.Label_Names[5]))",Machine Learning/ensemble.py,samouri/kinja-api,1
"
# Do I need to LabelEncoder y?

# logistic regression model
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)

# svm model
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train, y_train)

# decision tree model
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)
tree.fit(X_train, y_train)

# randomforest model
from sklearn.ensemble import RandomForestClassifier",Kaggle/voting.py,wei-Z/Python-Machine-Learning,1
"        '''
        Create a weak learner from sklearn part.
        The learner must has 'predict', 'fit' function.
        '''
        assert False, 'Should not enter here'


class SVMLearner(UnifiedLearner):

    def create_learner(self):
        return svm.SVC()


class DTreeLearner(UnifiedLearner):

    def create_learner(self):
        return tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=20, max_features='log2')


class KnnLeaner(UnifiedLearner):",unified_learner.py,ydawei/EnsembleTraining,1
"    pickle.dump(Un, open( ""Un_Win32Dll.p"", ""wb"" ))
    pickle.dump(IDS, open( ""IDS_Win32Dll.p"", ""wb"" ))
    pickle.dump(EP, open( ""EP_Win32Dll.p"", ""wb"" ))
    pickle.dump(target, open( ""Target_Win32Dll.p"", ""wb"" ))
 
#Given files features and targets, build an SVM model to predict files labels
def classification(f1,f2,f3,f4,target):
    X=zip(f1,f2,f3,f4)
    X,target=shuffle(X,target)
    X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3)
    clf = svm.SVC()
    clf.fit(X_train, y_train)
    labels= clf.predict(X_test)
    score = clf.score(X_test,y_test)
    print ""SVM score: "",round(score,2)
    return labels,X_test
#Show 2D classification obtained
def plot_labels(labels,f1,f2):
    for c, m, l  in [('b', 'o', 0), ('r', '^', 1)]:
        labelsP=np.array(np.where(labels==l))[0]    ",WinDll_SVM.py,greysab/MLware,1
"X = nifti_masker.fit_transform(func_filename)
# Restrict to non rest data
X = X[condition_mask]
session = session[condition_mask]

### Prediction function #######################################################

### Define the prediction function to be used.
# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

### Dimension reduction #######################################################

from sklearn.feature_selection import SelectKBest, f_classif

### Define the dimension reduction to be used.
# Here we use a classical univariate feature selection based on F-test,
# namely Anova. We set the number of features to be selected to 500
feature_selection = SelectKBest(f_classif, k=500)",examples/decoding/plot_haxby_grid_search.py,salma1601/nilearn,1
"                delete=False)

            self._create_training_examples_arc_eager(depgraphs, input_file)

            input_file.close()
            # Using the temporary file to train the libsvm classifier
            x_train, y_train = load_svmlight_file(input_file.name)
            # The parameter is set according to the paper:
            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre
            # this is very slow.
            self._model = svm.SVC(
                kernel='poly',
                degree=2,
                coef0=0,
                gamma=0.2,
                C=0.5,
                verbose=False,
                probability=True)

            print('Training support vector machine...')",coursera/intro_nlp/Assignments/Assignment1/code/providedcode/transitionparser.py,vinhqdang/my_mooc,1
"np.random.shuffle(dataset)
X_train, X_test, y_train, y_test = data[100:],data[:100],target[100:],target[:100]

print X_train
C = 1.0  # SVM regularization parameterd
X = X_train
y = y_train

h = .02  # step size in the mesh

svc = svm.SVC(kernel='linear', C=C).fit(X, y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)
poly_svc = svm.SVC(kernel='poly', degree=2, C=C).fit(X, y)

# title for the plots
titles = ['SVM con kernel lineal',
          'SVM con kernel RBF',
          'SVM con kernel poligonal (grado 2)']

for i, clf in enumerate((svc, rbf_svc, poly_svc)):",Proyecto Final/my_sarcasm_is_great.py,Sealos/Sarcasm,1
"   elif(clf_nome==CLF_NBMULTINOMIAL):
      tuned_params = [{'alpha': RANGE7}]
      clf = GridSearchCV(naive_bayes.MultinomialNB(), tuned_params, cv=CV_K_FOLD, scoring=""f1"", verbose=PARAM_VERBOSE, n_jobs=PARAM_JOBS)
      clf.fit(X, Y)
      parametros[""alpha""] = clf.best_estimator_.alpha
      parametros[""resultado""] = ""Alpha: %f"" % (clf.best_estimator_.alpha)
      clf_configurado = naive_bayes.MultinomialNB(alpha=parametros[""alpha""])

   elif(clf_nome==CLF_SVM_L):
      tuned_params = [{'kernel': ['linear'], 'C': RANGE3}]
      clf = GridSearchCV(svm.SVC(max_iter=SVC_MAX_ITER), tuned_params, cv=CV_K_FOLD, scoring=""f1"", verbose=PARAM_VERBOSE, n_jobs=PARAM_JOBS)
      clf.fit(X, Y)
      parametros[""c""] = clf.best_estimator_.C
      parametros[""kernel""] = clf.best_estimator_.kernel
      parametros[""resultado""] = ""Kernel: %s - C: %f"" % (clf.best_estimator_.kernel,clf.best_estimator_.C)
      clf_configurado = svm.SVC(max_iter=SVC_MAX_ITER,kernel=parametros[""kernel""],C=parametros[""c""])

   elif(clf_nome==CLF_SVM_R):
      tuned_params = [{'kernel': ['rbf'], 'C': RANGE3, 'gamma': RANGE3}]
      clf = GridSearchCV(svm.SVC(max_iter=SVC_MAX_ITER), tuned_params, cv=CV_K_FOLD, scoring=""f1"", verbose=PARAM_VERBOSE, n_jobs=SVM_PARAM_JOBS)",lib/classificar.py,fgbulsoni/pcs5031-projeto-retinopatia-diabetica,1
"						passwords.append(password)

			return [features,  labels, passwords]


		# Prepare the data
		trainingData = parseData( 'training.txt' )
		testingData = parseData( 'test.txt' )

		#A SVM Classifier
		clf = svm.SVC(kernel='linear', C = 1.0)

		#Training the classifier with the passwords and their labels.
		clf = clf.fit(trainingData[0], trainingData[1])

		#Predicting a password Strength
		prediction = clf.predict(testingData[0])

		target = len(testingData[1])
		current = 0",__init__.py,OmkarPathak/Password-Strength-Evaluator-using-Machine-Learning,1
"csvFile = pandas.read_csv('Datasets/train_metadata_clean_category_balanced.csv')
data = numpy.array(csvFile)

#We won't be using the first column (movie title)
X_train = data[:,2:-1]
y_train = data[:,-1]
X_train_scaled = preprocessing.scale(X_train)

#SVM with rbf kernel
rbf_params = {'C':numpy.logspace(-3,4,8), 'kernel':['rbf'],'gamma':numpy.logspace(-3,4,8)}
rbf_grid = GridSearchCV(svm.SVC(), param_grid=rbf_params, cv=20, n_jobs=4, verbose=5)
print 'Starting rbf fit...'
rbf_grid.fit(X_train_scaled,y_train)
print ""Best Params="",rbf_grid.best_params_, ""Accuracy="", rbf_grid.best_score_
val_rbf = rbf_grid.best_params_

#SVM with linear kernel
#This takes a long time
linear_params = {'C':numpy.logspace(-3,3,7), 'kernel':['linear']} #C=[0.001,0.01,0.1,1,10,100,1000]
linear_grid = GridSearchCV(svm.SVC(), param_grid=linear_params, cv=20, n_jobs=4, verbose=5)",SVM.py,murq/movie-datamining,1
"  mnb_clf = MultinomialNB (fit_prior=False)
  mnb_clf.fit(train_data, train_labels)
  mnb_labels = mnb_clf.predict(test_data) 
  print ""Number of mislabeled objects (MNB) : %d"" % (test_labels != mnb_labels).sum()
  save_results(test_labels, mnb_labels, ""bayes"", classification_dir)

  gamma_range = [10. ** -1, 1, 10. ** 1]
  C_range = [10. ** -2, 1, 10. ** 2]
  for C in C_range:
      for gamma in gamma_range:
        svm_clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)
        svm_clf.fit(train_data_normalized, train_labels)
        svm_labels = svm_clf.predict(test_data_normalized)
        print ""Number of mislabeled objects (SVM) : %d"" % (test_labels != svm_labels).sum()
        save_results(test_labels, svm_labels, ""svm_gamma_%.1f_C%.2f"" % (gamma, C), classification_dir)

  mark2 = time.time();
  fos = open(classification_dir + ""/classification_time.txt"", 'w')  
  fos.write(str(mark2-mark1));     
  fos.close() ",bmvc12/bof/classify.py,mirestrepo/voxels-at-lems,1
"import warnings
from epac.map_reduce.inputs import ReduceInput


class ResultSet(Set):
    """"""ResultSet is a set of Result

    Example
    -------
    >>> from epac import Result, ResultSet
    >>> r1 = Result('SVC(C=1)', **dict(a=1, b=2))
    >>> r2 = Result('SVC(C=2)', **dict(a=1, b=2))
    >>> r3 = Result('SVC(C=3)', **dict(a=1, b=2))
    >>> r4 = Result('SVC(C=4)', **dict(a=1, b=2))
    >>> set1 = ResultSet(r1, r2)
    >>> print set1
    ResultSet(
    [{'key': SVC(C=1), 'a': 1, 'b': 2},
     {'key': SVC(C=2), 'a': 1, 'b': 2}])
    >>> set2 = ResultSet(r3, r4)",epac/map_reduce/results.py,neurospin/pylearn-epac,1
"		X.append(value)
		y.append(1)
	rand_smpl = [nonAllStar[pos][i] for i in sorted(random.sample(xrange(len(nonAllStar[pos])),len(allStar[pos])))]

	for value in rand_smpl:
		X.append(value)
		y.append(0)
	# print(X)
	# X=np.array(X)
	# y=np.array(y)
	clf=svm.SVC(probability=True)
	clf.fit(X,y)
	allStarModels[pos]=clf

for pos in mvpModels:
	X=[]
	y=[]
	for value in mvp[pos]:
		X.append(value)
		y.append(1)",rahulCode_redo/project1Part3/nfl/final/playerProbabilityGenerator.py,bam2332g/proj1part3,1
"    def __init__(self, config=dict()):
        GenericRanking.__init__(self, config)
        self.logger = logging.getLogger(__name__)
        self.name = 'svm'
        for data_tier in self.data_tiers:
            try:
                self.clf_trend[data_tier] = joblib.load(self.data_path + '/' + self.name + '_trend_' + data_tier + '.pkl')
                self.clf_avg[data_tier] = joblib.load(self.data_path + '/' + self.name + '_avg_' + data_tier + '.pkl')
            except:
                self.logger.info('%s classifier and regressor for data tier %s need to be trained', self.name, data_tier)
                self.clf_trend[data_tier] = SVC(kernel='poly', probability=True, C=0.5)
                self.clf_avg[data_tier] = SVR()
        self.train()
        self.test()",CUADRnT/src/python/cuadrnt/data_analysis/rankings/svm.py,vlimant/IntelROCCS,1
"plt.xscale('log')
plt.legend(loc='lower right')
plt.xlabel('Parameter C')
plt.ylabel('Accuracy')
plt.ylim([0.8, 1.0])
plt.show()

#tuning hperparameters via grid search
from sklearn.grid_search import GridSearchCV
from sklearn.svm import SVC
pipe_svc = Pipeline([('scl',StandardScaler()),('clf',SVC(random_state = 1))])
param_range = [0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0]
param_grid = [{'clf__C':param_range,'clf__kernel':['linear']},{'clf__C':param_range,'clf__gamma':param_range,'clf__kernel':['rbf']}]
gs = GridSearchCV(estimator = pipe_svc, param_grid = param_grid,scoring='accuracy',cv=10,n_jobs = -1)
gs = gs.fit(X_train,y_train)
print (gs.best_score_)
print (gs.best_params_)

clf = gs.best_estimator_
clf.fit(X_train,y_train)",1_supervised_classification/5-Model Evalution and Hyperparameter Tuning/model evaluation/model_evaluation/debugging_algorithms.py,PhenixI/machine-learning,1
"
        self.test_indices = self.partition.test
        self.test_features = numpy.asarray([self.features[i] for i in self.test_indices])
        self.testing_labels = [self.full_labels[i] for i in self.test_indices] 

        self.kernel = None
        self.svm = None


    def train(self) :
        self.svm = svm.SVC(kernel='rbf', C = self.C)
        self.svm.fit(self.train_features, self.training_labels)
        return self.training_labels
    
    def test(self) :
        results = self.svm.predict(self.test_features)
        return LearningResult(self.partition.state, self.training_labels, self.testing_labels, results.tolist())

    @staticmethod
    def get_learning_filename(config, gz=True) :",python/persistence/FeatureLearning.py,gpersistence/tstop,1
"# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""
]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)",from_sklearn_website.py,Abhijith1995/tumor_classifier,1
"    testdata  = open(""testnew.txt"")
    traindata.readline() # 跳过第一行
    testdata.readline()
    train = np.loadtxt(traindata)
    test = np.loadtxt(testdata)
    X = train[0:4628,0:27]
    y = train[0:4628,27]
    test_x = test[0:1437,0:27]
    test_y = test[0:1437,27]

    model1 = LinearSVC()
    model2 = LogisticRegression()
    model3 = GaussianNB()
    model4 = RandomForestClassifier()
    model5 = KNeighborsClassifier()
    model1.fit(X,y)
    model2.fit(X,y)
    model3.fit(X,y)
    model4.fit(X,y)
    model5.fit(X,y)",分类和回归/svm.py,vimilimiv/weibo-popularity_judge-and-content_optimization,1
"    ind = np.arange(150)  # indices into the dataset
    ind = rd.permutation(ind)  # random permutation
    L = ind[0:90]  # learning set indices
    T = ind[90:]  # test set indices

    # Learning set.
    X_learn = X_scaled[L, :]
    y_learn = y[L]

    # Create SVM.
    clf = svm.SVC()

    # Fit data.
    clf.fit(X_learn, y_learn)

    # Test set.
    X_test = X_scaled[T, :]
    y_test = y[T]

    # Test all data.",6/iris_svm.py,JelteF/statistics,1
"    plt.title('Support Vector Regression')
#    plt.legend()
    plt.show()


#    dataMat = data['Data']
#    labelMat = data['Labels']
#    train_dat, test_dat, train_lab, test_lab = cross_validation.train_test_split(dataMat, labelMat, test_size=testpart, random_state=0)
#
#    print 'Training SVM'
#    clf = svm.SVC(kernel='linear', C=1).fit(train_dat, train_lab)
#    print 'Training Complete'
#    print '-----------------'
#    print 'Testing SVM' 
#    score = clf.score(test_dat, test_lab)",Modules/Regression/Support Vector Machine/Module.py,blakebjorn/DeepLearnR,1
"        param = {'n_clusters':np.arange(1,7)}
        kmcCV = GridSearchCV(KMeans(),param)
        kMeansPredClass = pd.DataFrame(columns=['kmeans'],index=range(len_df))
        A = kmcCV.fit(X,cv=LeaveOneOut(len(X)))
        #print kmcCV.best_estimator_
        kMeansPredClass['kmeans'] = A.best_estimator_.predict(X)
        kMeansPredClass['kmeans']=kMeansPredClass['kmeans']/float(kMeansPredClass['kmeans'].max())
        X=X.join(kMeansPredClass,how='inner')
    #run over parameter space
    param_rbf={'C':np.logspace(-2,5,8),'gamma':np.logspace(-5,-1,5)}
    gsCV = GridSearchCV(SVC(kernel='linear'),param_rbf,score_func=f1_score,n_jobs=-1)

    #Fit the svm
    #print ""X:""+str(X) 
    #print ""Y:""+str(Y) 
    B = gsCV.fit(X,Y,cv=LeaveOneOut(len_df))
   # cPickle.dump(B.best_estimator_,open('/usr/local/mitll/dev_backend/kinship_2.0/'+training_model+'_best_estimator_'+str(rel_degree)+"".pkl"",'wb'))
   # cPickle.dump(featClass,open('/usr/local/mitll/dev_backend/kinship_2.0/'+training_model+'_featClass_'+str(rel_degree)+"".pkl"",'wb'))
    #print SVM performance
    #print B",python_analysis_modules/kinship/ml_helpers.py,annashcherbina/sherlockstoolkit,1
"    self.training_set_port = training_set_port
    self.labels_port = labels_port
    self.model_port = Port([], self.run)
    self.output_ports = {'model': self.model_port }

  @run_once
  def run(self):
    training_set = self.training_set_port.get()
    labels = self.labels_port.get()
    classif = nltk.classify.scikitlearn.SklearnClassifier(
        sklearn.svm.LinearSVC())
    self.model_port.update(classify.train(zip(training_set, labels)))

  def get_output_ports(self):
    return self.output_ports.keys()

  def get_port(self, port):
    return self.output_ports[port]

",src/graph.py,sshihata/mash,1
"        data = np.concatenate((pos_data, neg_data))
        labels = np.array([1]*num_data + [0]*num_data)
        
        data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 0.3, random_state=0)
        
        pca = decomposition.PCA(n_components=5)
        pca.fit(data_train)
        data_train = pca.transform(data_train)
        data_test = pca.transform(data_test)
        
        clf = svm.SVC(kernel='linear')
        clf.fit(data_train, labels_train)
        score = clf.score(data_test, labels_test)
        print score",scripts/svm.py,manderelee/csc2521_final,1
"for x in X:

    xx.append(np.histogram(x, density=True, range=r)[0])

    print(xx[-1])
X = np.asarray(xx)
y = np.asarray(y)

# clf = LinearRegression()
clf = RandomForestClassifier(n_estimators=20)
# clf = SVC(gamma=0.001, kernel='rbf', C=100)

skf = StratifiedKFold(y, n_folds=2)
for train_index, test_index in skf:
    print(""Detailed classification report:"")
    print()
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)
    y_true, y_pred = y_test, clf.predict(X_test)",tests/lr.py,schae234/gingivere,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CMIM.cmim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeature/example/test_CMIM.py,jundongl/scikit-feature,1
"           and platform.architecture()[0] == '64bit' \
           and self.oBuild.sKind == 'development' \
           and os.getenv('VERSIONER_PYTHON_PREFER_32_BIT') != 'yes':
            print ""WARNING: 64-bit python on darwin, 32-bit VBox development build => crash""
            print ""WARNING:   bash-3.2$ /usr/bin/python2.5 ./testdriver""
            print ""WARNING: or""
            print ""WARNING:   bash-3.2$ VERSIONER_PYTHON_PREFER_32_BIT=yes ./testdriver""
            return False;

        # Start VBoxSVC and load the vboxapi bits.
        if self._startVBoxSVC() is True:
            assert(self.oVBoxSvcProcess is not None);

            sSavedSysPath = sys.path;
            self._setupVBoxApi();
            sys.path = sSavedSysPath;

            # Adjust the default machine folder.
            if self.fImportedVBoxApi and not self.fUseDefaultSvc and self.fpApiVer >= 4.0:
                sNewFolder = os.path.join(self.sScratchPath, 'VBoxUserHome', 'Machines');",src/VBox/ValidationKit/testdriver/vbox.py,progress-linux/cairon-backports_virtualbox,1
"    `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_

    This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.
    Only supports L2 regularization currently.

    >>> from pyspark.sql import Row
    >>> from pyspark.ml.linalg import Vectors
    >>> df = sc.parallelize([
    ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),
    ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()
    >>> svm = LinearSVC(maxIter=5, regParam=0.01)
    >>> model = svm.fit(df)
    >>> model.coefficients
    DenseVector([0.0, -0.2792, -0.1833])
    >>> model.intercept
    1.0206118982229047
    >>> model.numClasses
    2
    >>> model.numFeatures
    3",python/pyspark/ml/classification.py,bOOm-X/spark,1
"        train_y_reduced = y_train_minmax
        test_X = x_test_minmax
        test_y = y_test_minmax
        ###original data###
        ################ end of data ####################
        if settings['SVM']:
            print ""SVM""                   
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            Linear_SVC = LinearSVC(C=1, penalty=""l2"")
            Linear_SVC.fit(scaled_train_X, train_y_reduced)
            predicted_test_y = Linear_SVC.predict(scaled_test_X)
            isTest = True; #new
            analysis_scr.append((subset_no,  'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

            predicted_train_y = Linear_SVC.predict(scaled_train_X)
            isTest = False; #new
            analysis_scr.append(( subset_no, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_11_18_2014.py,magic2du/contact_matrix,1
"# Load the Spam Email dataset
# You will have X, y in your environment
data = scipy.io.loadmat('spamTrain.mat')
X = data['X']
y = data['y'].flatten()

print 'Training Linear SVM (Spam Classification)'
print '(this may take 1 to 2 minutes) ...'

C = 0.1
clf = svm.SVC(C=C, kernel='linear', tol=1e-3, max_iter=200)
model = clf.fit(X, y)

p = model.predict(X)

print 'Training Accuracy: %f', np.mean(np.double(p == y)) * 100

## =================== Part 4: Test Spam Classification ================
#  After training the classifier, we can evaluate it on a test set. We have
#  included a test set in spamTest.mat",ex6/ex6_spam.py,JediKoder/coursera-ML,1
"
# Split up data into randomized training and test sets
rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(
    scaled_X, y, test_size=0.1, random_state=rand_state) #test_size=0.2 on lesson

print('Using:',orient,'orientations',pix_per_cell,
    'pixels per cell and', cell_per_block,'cells per block')
print('Feature vector length:', len(X_train[0]))
# Use a linear SVC
svc = LinearSVC()
# Wrap linear SVC on calibrated classifier so we can get probabilities
clf = CalibratedClassifierCV(svc)
# Check the training time for the SVC
t=time.time()
print('Training svc...')
clf.fit(X_train, y_train)
print(round(time.time()-t, 2), 'seconds to train SVC...')
# Check the score of the SVC
print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))",Term 1- Computer Vision and Deep Learning/Project 5 - Vehicle Detection and Tracking/p5.py,jlema/Udacity-Self-Driving-Car-Engineer-Nanodegree,1
"	>>> import numpy as np
	>>> from sklearn.datasets import load_iris
	>>> from sklearn.svm import SVC
	>>> from nonconformist.base import ClassifierAdapter
	>>> from nonconformist.cp import TcpClassifier
	>>> from nonconformist.nc import ClassifierNc, MarginErrFunc
	>>> iris = load_iris()
	>>> idx = np.random.permutation(iris.target.size)
	>>> train = idx[:int(idx.size / 2)]
	>>> test = idx[int(idx.size / 2):]
	>>> model = ClassifierAdapter(SVC(probability=True))
	>>> nc = ClassifierNc(model, MarginErrFunc())
	>>> tcp = TcpClassifier(nc)
	>>> tcp.fit(iris.data[train, :], iris.target[train])
	>>> tcp.predict(iris.data[test, :], significance=0.10)
	...             # doctest: +SKIP
	array([[ True, False, False],
		[False,  True, False],
		...,
		[False,  True, False],",nonconformist/cp.py,donlnz/nonconformist,1
"    k(X, Y) = X  (    ) Y.T
                 (0  1)
    """"""
    M = np.array([[2, 0], [0, 1.0]])
    return np.dot(np.dot(X, M), Y.T)


h = .02  # step size in the mesh

# we create an instance of SVM and fit out data.
clf = svm.SVC(kernel=my_kernel)
clf.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
",projects/scikit-learn-master/examples/svm/plot_custom_kernel.py,DailyActie/Surrogate-Model,1
"           by Robert C. Moore, John DeNero.
           <http://www.ttic.edu/sigml/symposium2011/papers/
           Moore+DeNero_Regularization.pdf>`_

    Examples
    --------
    >>> from sklearn import svm
    >>> from sklearn.metrics import hinge_loss
    >>> X = [[0], [1]]
    >>> y = [-1, 1]
    >>> est = svm.LinearSVC(random_state=0)
    >>> est.fit(X, y)
    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
         verbose=0)
    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
    >>> pred_decision  # doctest: +ELLIPSIS
    array([-2.18...,  2.36...,  0.09...])
    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS",summary/sumy/sklearn/metrics/classification.py,WangWenjun559/Weiss,1
"Face-3:\x20
 iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = ('Received: from siimage.com '
             '([172.25.1.3]) by zima.siliconimage.com with '
             'Microsoft SMTPSVC(5.0.2195.4905); '
             'Wed, 16 Oct 2002 07:41:11 -0700')
        msg = email.message_from_string(m)
        eq(msg.as_string(maxheaderlen=78), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",www/src/Lib/test/test_email/test_email.py,amrdraz/brython,1
"                   learning_rate=0.01, verbose=2)
clf_gpsvi.fit()
pd = clf_gpsvi.predict(xTe)
print('SVI error = {}'.format(np.sum(len(np.where(pd != yTe)[0])) / float(xTe.shape[0])))
ax = fig.add_subplot(222, projection='3d')
#ax = plt.subplot(222)
ax.set_title('GP with Stochastic Variational Inference')
ax.scatter(xTe[:,0], xTe[:,1], xTe[:,2], c=[colors[y] for y in pd], s=40)

#%%
clf_svc = SVC()
clf_svc.fit(xTr, yTr)
pd = clf_svc.predict(xTe)
print('SVM error = {}'.format(np.sum(len(np.where(pd != yTe)[0])) / float(xTe.shape[0])))
#plt.figure(fig_name)
#ax = plt.subplot(223)
ax = fig.add_subplot(223, projection='3d')
ax.set_title('SVM with RBF Kernel')
ax.scatter(xTe[:,0], xTe[:,1], xTe[:,2], c=[colors[y] for y in pd], s=40)
",GPSVI/test/playtoyblobs.py,AlchemicalChest/Gaussian-Process-with-Stochastic-Variational-Inference,1
"    test = pca.transform(test)
    return train, test

def normalize(train, test):
    norm = preprocessing.Normalizer()
    train = norm.fit_transform(train)
    test = norm.transform(test)
    return train, test

def createSVM():
    clf = SVC()
    return clf

def createKNN():
    clf = KNeighborsRegressor(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeRegressor(max_depth=None, min_samples_split=1, random_state=0)
    return clf",libs/predictors.py,KellyChan/Kaggle,1
"
        with open(""input/Dataset/YAll.processed"") as f:
            yAll = pickle.load(f)
    
    xTrain, xTest, yTrain, yTest = cross_validation.train_test_split(XAll, yAll, test_size=0.2)
##### SVC 
    classifierTime = time()
    print ""Training Classifier""
    svc80Exists = os.path.exists(""input/Dataset/svc80.classifier"")
    if not svc80Exists:
        svcClassifier = OneVsRestClassifier(LinearSVC()).fit(xTrain, yTrain)
        print ""Training classifier took : ""+str(time()- classifierTime)
        with open(""input/Dataset/svc80.classifier"", 'w') as f:
            pickle.dump(svcClassifier, f)
        print ""Classifier dumped on disk""
    else:
        print ""Loading SVC 80 Classifier from pickle""
        with open(""input/Dataset/svc80.classifier"") as f:
            svcClassifier = pickle.load(f)
",Task1/CreateClassifiers.py,njetty/Yelp-Review-Analysis,1
"    
    # 3) general_SVC_exclude_test_set
    # Trains general models, where power at a specific (s,t) is estimated by training 
    # a model on all the data *excluding* data from that (s,t). There is there NO DANGER
    # of overfitting, as the testing data is completely held out of training. Also, the 
    # maximal amount of data for training is used. WARNING: VERY SLOW.

    # 4) pairwise_SVC
    # learns (s,t)-specific models, but computes their power across all (s,t) pairs.
    # The diagonal of the resulting power matrix (each model applied to its respective 
    # scenario) is what results from specific_SVC().

    # Note, In Ronen et al. 2013 (GENETICS) the results we report were obtained using (3). 
    # However, in reality (2) and (3) produce *very* similar results and thus we recommend 
    # using (2) for most practical cases. Also note that in all methods, the FULL DATASET is 
    # used to train the final models, which may be input to the program 'SFselect.py'.

    specific_SVC()
    #general_SVC_partition_data(general_svm_file, specific_svm_file) 
    #general_SVC_exclude_test_set(specific_svm_file)",SFselect_train.py,rronen/SFselect,1
"from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

classifers_dict = {'AdaBoost': AdaBoostClassifier(n_estimators=50,
                                                  learning_rate=1),
                    'Random Forest': RandomForestClassifier(n_estimators=10,
                                                            # max_depth=None,
                                                            min_samples_split=50,
                                                            random_state=0),
                    'Naive Bayes': GaussianNB(),
                    'SVM': SVC(C=1.5, kernel='poly', degree=2, gamma=0.),
                    'Tree': DecisionTreeClassifier(),
                    'KNN': KNeighborsClassifier(n_neighbors=22)
                    }
                    
for name, clf in classifers_dict.iteritems():
    print name
    t0 = time()
    clf.fit(features_train, labels_train)
    print ""\tTime to train: "", time() - t0, ""s""",Intro to Machine Learning/choose_your_own/your_algorithm.py,myselfHimanshu/Udacity-DataML,1
"
    def set_params(self, **params):
        return self


def test_rfe_set_params():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target
    clf = SVC(kernel=""linear"")
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    y_pred = rfe.fit(X, y).predict(X)

    clf = SVC()
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1,
              estimator_params={'kernel': 'linear'})
    y_pred2 = rfe.fit(X, y).predict(X)
    assert_array_equal(y_pred, y_pred2)
",sklearn/feature_selection/tests/test_rfe.py,smartscheduling/scikit-learn-categorical-tree,1
"    pca_measure = pca.transform(measures)
    nfeats_white = (nfeats_orig - nfeats_orig.mean()) / nfeats_orig.std()
    pca_white = (pca_measure - pca_measure.mean()) / pca_measure.std()
    sortx = nfeats_white.argsort()
    pt.plt.plot(pca_white[sortx], 'x')
    pt.plt.plot(nfeats_white[sortx], '.')


    pyhesaff.detect_feats_in_image

    svc = sklearn.svm.LinearSVC()
    svc.fit(measures, nfeats_orig > 500)
    svc.predict(measures) == (nfeats_orig > 500)

    svr = sklearn.svm.LinearSVR()
    svr.fit(measures, nfeats_orig)
    svr.predict(measures)

    depc['feat'].get_config_history(z1)
    depc['feat'].get_config_history(z2)",_broken/workspace.py,SU-ECE-17-7/ibeis,1
"
# standardize features: removing the mean and scaling to unit variance
sc = StandardScaler()
sc.fit(X_train) # compute the mean and std to be used for later scaling
X_train_std = sc.transform(X_train) # fit to data, then transform it
X_test_std = sc.transform(X_test)

# define svm
# parameter C controls the width of margin between classifications (regularization)
# high c, high penalties to misclassifications, narrower the margin
svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

# after training, predict
y_pred = svm.predict(X_test_std)
print('Misclassified samples: %d' % (y_test != y_pred).sum())

# evaluate
# misclassification error = misclassified/total_samples
# classification accuracy = 1 - misclassification error",test/test_scikit/iris_svm.py,viniciusguigo/the_magic_kingdom_of_python,1
"outdir=os.path.join(basedir,'classifier/subject_classifier')

labels1=N.loadtxt(os.path.join(basedir,'data_prep/data_key_run1.txt'))[:,1]
labels2=N.loadtxt(os.path.join(basedir,'data_prep/data_key_run2.txt'))[:,1]

print 'loading data...'
data1=N.load(os.path.join(basedir,'data_prep/zstat_run1_allgood.npy')).T


print 'training...'
clf=OneVsRestClassifier(LinearSVC()).fit(data1,labels1)

del data1

print 'loading test data...'
data2=N.load(os.path.join(basedir,'data_prep/zstat_run2_allgood.npy')).T

print 'predicting...'
pred=clf.predict(data2)
print 'Mean accuracy:',N.mean(pred==labels2)",openfmri_paper/9_classify_subjects.py,poldrack/openfmri,1
"    for depth in range(1, 20):
        idx= depth-1
        import pdb
        pdb.set_trace()
        trn, trn_lbl, tst, feature_num= blor.get_new_table(test, tst_ents, depth)
        blue[idx]=feature_num

        for i,fraction in enumerate(fractions):
            new_trn, new_tst= feature_select_ig(trn, trn_lbl, tst, fraction)
        
            clf= SVC(kernel='linear', C=10)
            clf.fit(new_trn, trn_lbl)    
            blah1[i,idx, :]= calc_stats(clf.predict(new_tst),tst_lbl)
        
            clf= KNeighborsClassifier(n_neighbors=3)
            clf.fit(new_trn, trn_lbl)    
            blah2[i,idx, :]= calc_stats(clf.predict(new_tst),tst_lbl)
               
    return blah1, blah2, blue
",problems/ohsumedTitleOnly_depth.py,lioritan/Thesis,1
"

def cls_compare_shuff():
    data = Data()
    data.load_data(shfl=True)
    cls = Classifiers(data)
    dtc = DecisionTreeClassifier()
    gnb = GaussianNB()
    lda = LinearDiscriminantAnalysis()
    qda = QuadraticDiscriminantAnalysis()
    linear_svc = svm.SVC(kernel='linear')
    poly_svc = svm.SVC(kernel='poly')
    rbf_svc = svm.SVC(kernel='rbf')
    # LinearSVC minimizes the squared hinge loss while SVC minimizes the
    # regular hinge loss.
    # LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass
    # reduction while
    # SVC uses the One-vs-One multiclass reduction.

    rfc = RandomForestClassifier()",src/hac.py,bhanupratapjain/human-activity-recognition,1
"def estimate_pvalue(score_unpermuted, scores_null):
    iterations = len(scores_null)
    p_value = max(1.0/iterations, (scores_null > score_unpermuted).sum() /
                  float(iterations))
    return p_value


def compute_svm_score(K, y, n_folds, scoring='accuracy', random_state=0):
    cv = StratifiedKFold(y, n_folds=n_folds, shuffle=True,
                         random_state=random_state)
    clf = SVC(C=1.0, kernel='precomputed')
    scores = cross_val_score(clf, K, y, scoring=scoring, cv=cv, n_jobs=1)
    score = scores.mean()
    return score


def compute_svm_score_nestedCV(K, y, n_folds, scoring='accuracy',
                               random_state=None,
                               param_grid=[{'C': np.logspace(-5, 5, 20)}]):
    cv = StratifiedKFold(y, n_folds=n_folds, shuffle=True,",simulation.py,emanuele/jstsp2015,1
"print 'Decision Tree Classifier predicts:' + str(prediction1)

#Kneighbors Classifier Test
clf2 = neighbors.KNeighborsClassifier()
clf2 = clf2.fit(train,label)
prediction2 = clf2.predict(test)

print 'K-nearest neighbor classifier predicts:' + str(prediction2)

#Support Vector Classifier
clf3 = svm.SVC()
clf3 = clf3.fit(train, label)
prediction3 = clf3.predict(test)

print 'SVC Classifier predicts' + str(prediction3)


plt.figure(1)
plt.plot(train[:,0], le.transform(label), 'ro')
plt.plot(train[:,0], le.transform(clf1.predict(train)), 'bx')",learning/dtree.py,samuxiii/prototypes,1
"                      **pipe.get_params(deep=False)))

    # Check that params are set
    pipe.set_params(svc__a=0.1)
    assert_equal(clf.a, 0.1)
    assert_equal(clf.b, None)
    # Smoke test the repr:
    repr(pipe)

    # Test with two objects
    clf = SVC()
    filter1 = SelectKBest(f_classif)
    pipe = Pipeline([('anova', filter1), ('svc', clf)])

    # Check that we can't use the same stage name twice
    assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])

    # Check that params are set
    pipe.set_params(svc__C=0.1)
    assert_equal(clf.C, 0.1)",projects/scikit-learn-master/sklearn/tests/test_pipeline.py,DailyActie/Surrogate-Model,1
"                 ""Log Reg"",
                 # ""Gaussian Process"",
                 # ""Decision Tree"",
                 ""Random Forest"",
                 ""Neural Net"",
                 ""AdaBoost"",
                 ""Naive Bayes"",
                 ""QDA""]

        classifiers = [KNeighborsClassifier(3),
                       SVC(kernel=""linear"", C=0.025),
                       SVC(gamma=2, C=1),
                       linear_model.LogisticRegression(C=1e5),
                       # GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
                       # DecisionTreeClassifier(max_depth=5),
                       RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                       MLPClassifier(alpha=1),
                       AdaBoostClassifier(),
                       GaussianNB(),
                       QuadraticDiscriminantAnalysis()]",models/src/classification.py,Semen52/nlp4u,1
"	# Linear SVM with HOG features								 #
	##############################################################

	list_hog_fd =[]
	for feature in X_train:
		fd = hog(feature.reshape(28, 28), orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1), visualise=False)
		list_hog_fd.append(fd)

	X_train = np.array(list_hog_fd, 'float64')

	clf = LinearSVC()
	start_time = time.time()
	print ""Training ""
	clf.fit(X_train, y_train)
	print "" Total time : "", time.time() - start_time

	#Pickling :-> joblib.dump(clf, 'digits_clf.pkl', compress=3)

	list_hog_fd =[]
	count = 0",digitrecognition.py,SiddharthaAnand/HandwrittenDigitRecognition,1
"    degree = 6
    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])
    lw = 2

    for cl in ['linear']:
        args.classifier = cl
        print(""RUNNING "", cl)

        if args.classifier == 'poly':
            classifier_count = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                              (""extra trees"", svm.SVC(kernel=""poly"", degree=degree, C=C, gamma=gamma, probability=True))])

            classifier_tfidf = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                              (""extra trees"", svm.SVC(kernel=""poly"", degree=3,C=C))])
        elif args.classifier == 'linear':
            classifier_count = Pipeline([(""word2vec vectorizer"", MeanEmbeddingVectorizer(w2v)),
                                         (""extra trees"", svm.SVC(kernel=""linear"", C=C, probability=True))])

            classifier_tfidf = Pipeline([(""word2vec vectorizer"", TfidfEmbeddingVectorizer(w2v)),
                                         (""extra trees"", svm.SVC(kernel=""linear"", C=C, probability=True))])",train_model_graph.py,eamosse/word_embedding,1
"'''
def listModels():
    models = list()
    models.append(KNeighborsClassifier(n_neighbors=20))
    models.append(LogisticRegression(penalty='l1'))
    models.append(AdaBoostClassifier())
    models.append(GradientBoostingClassifier())
    models.append(RandomForestClassifier())
    models.append(LDA())
    models.append(DecisionTreeClassifier())
    models.append(SVC(kernel='linear'))
    return models

'''
-----------------------
-- Useful
-----------------------
'''
def savePredictions(pred_res,filename='y_pred.txt'):
    np.savetxt(filename, pred_res, fmt='%s')",src/models.py,franblas/eegChallenge,1
"    testing = documents[-ntesting:]
    documents = documents[:-ntesting]

    # SAVE FOR NOW
    cPickle.dump(testing, open(""testing-data.p"", 'w'))

    X, y = documents2feature_vectors(documents)
    Xtest, ytest = documents2feature_vectors(testing)

    print ""Fitting""
    clf = svm.SVC(C=0.02, kernel='linear', probability=True)
    clf.fit(X, y)

    # Test
    y_pred = clf.predict(Xtest)
    print(classification_report(ytest, y_pred))

    return clf

",scripts/train_model.py,tokestermw/politeness,1
"        (""tfidf_vectorizer"", TfidfVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000, use_idf = True, smooth_idf = True,
                                             sublinear_tf = True)), 
        (""linear svc"", MultinomialNB())
    ])


    ct_svc = Pipeline([
        (""tfidf_vectorizer"", CountVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000)), 
        (""linear svc"", SVC(kernel=""linear"", probability=True))
    ])

    tfidf_svc = Pipeline([
        (""tfidf_vectorizer"", TfidfVectorizer(ngram_range = (1, 4), stop_words = None, lowercase = False,
                                             max_features = 40000, use_idf = True, smooth_idf = True,
                                             sublinear_tf = True)), 
        (""linear svc"", SVC(kernel=""linear"", probability=True))
    ])
    ",research/MLModelCreatorWord.py,Ninad998/deepstylometry-python,1
"    yval = Decimal(yval).quantize(Decimal('.001'))
    labeled_points.append([xval,yval,functype])
    
# create anonymized data
labels = [ p[2] for p in labeled_points ] 

unlabeled_points = [ p[:2] for p in labeled_points ]
unlabeled_points = array(unlabeled_points)
labels = array(labels)

clf = svm.SVC()
clf.fit(unlabeled_points, labels)
import pdb; pdb.set_trace()
# Type clf.predict([x, y]) to get info on the type of function!",speech/svmdemo.py,darylsew/audiolearn,1
"# for machine learning with scikit-learn
fmri_masked = nifti_masker.fit_transform(func_filename)

# Restrict the classification to the face vs cat discrimination
fmri_masked = fmri_masked[condition_mask]

### Prediction ################################################################

# Here we use a Support Vector Classification, with a linear kernel
from sklearn.svm import SVC
svc = SVC(kernel='linear')

# And we run it
svc.fit(fmri_masked, target)
prediction = svc.predict(fmri_masked)

### Cross-validation ##########################################################

from sklearn.cross_validation import KFold
",examples/plot_haxby_simple.py,salma1601/nilearn,1
"    word_scores = create_word_scores(posWords, negWords)
    dimension = ['100','500','800','1500','5000']
    #best_words = find_best_words(word_scores, 100)
    for d in dimension:
        best_words = find_best_words(word_scores, int(d))
        (trainFeatures, testFeatures) = achieve_features(best_word_features)
        
        evaluate(BernoulliNB())
        evaluate(MultinomialNB())
        evaluate(LogisticRegression())
        evaluate(SVC(gamma=0.001, C=100.))
        evaluate(NuSVC())
        evaluate(KNeighborsClassifier())
        print '------------------------------------------'",process.py,delili/NLP_Comments_Sentiment_Analysis,1
"acc['lr']=N.zeros(len(ncomp))
pred={}
for c in range(len(ncomp)):
    pred['svm']=N.zeros(len(labels))
    pred['rbf']=N.zeros(len(labels))
    pred['lr']=N.zeros(len(labels))
    data=N.genfromtxt(os.path.join(melodic_dir,'datarun1_icarun2_%dcomp.txt'%ncomp[c]))
    for train,test in loo:
        svm_c=linsvm[c]
        print 'SVM: ',svm_c
        clf=LinearSVC(C=svm_c)
        clf.fit(data[train],labels[train])
        pred['svm'][test]=clf.predict(data[test])
        svm_c=rbfsvm[c,0]
        svm_gamma=rbfsvm[c,1]
        print 'SVM-RBF: ',svm_c,svm_gamma
        clf=SVC(C=svm_c,gamma=svm_gamma)
        clf.fit(data[train],labels[train])
        pred['rbf'][test]=clf.predict(data[test])
        lrpen=logreg[c]",openfmri_paper/5_classify_task_ICA.py,poldrack/openfmri,1
"            y_train_minmax = y_train
            y_validation_minmax = y_validation
            y_test_minmax = y_test
            ###original data###
            ################ end of data ####################
            standard_scaler = preprocessing.StandardScaler().fit(train_X_reduced)
            scaled_train_X = standard_scaler.transform(train_X_reduced)
            scaled_test_X = standard_scaler.transform(test_X)
            if settings['SVM']:
                print ""SVM""                   
                Linear_SVC = LinearSVC(C=1, penalty=""l2"")
                Linear_SVC.fit(scaled_train_X, y_train)
                predicted_test_y = Linear_SVC.predict(scaled_test_X)
                isTest = True; #new
                analysis_scr.append((subset_no, number_of_training, 'SVM', isTest) + tuple(performance_score(test_y, predicted_test_y).values())) #new

                predicted_train_y = Linear_SVC.predict(scaled_train_X)
                isTest = False; #new
                analysis_scr.append(( subset_no,number_of_training, 'SVM', isTest) + tuple(performance_score(train_y_reduced, predicted_train_y).values()))
",Contact_maps/mnist_psuedo_ipython_dl_ppi/code/DL_Stacked_Model_Mnist_Psuedo_05_19_2015.py,magic2du/contact_matrix,1
"

sampleFile = ""Sample.txt""
whichColumn = 2

mapfile = ""map.txt""
clustfile = ""clust.txt""
trainedData = ""trained.pkl""
kernel = 'rbf'  #Use either 'linear' or 'rbf' (for large number of features)

#clf = svm.SVC(kernel = 'linear', C = 1.0)
#clf.fit(A,Cl)

try:
    with open(trainedData):
        print("" Opening training data..."")
        clf = joblib.load(trainedData)
except:
        print("" Opening learning files and parameters..."")
        f = open(mapfile, 'r')",Other/obsolete/SVM_learning_spectra_Labspec-full.py,feranick/SpectralMachine,1
"
df = pd.read_csv('../datasets/breast-cancer-wisconsin.data.txt')
df.replace('?', -99999, inplace=True)
df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)

clf = svm.SVC()
clf.fit(X_train, y_train)

accuracy = clf.score(X_test, y_test)
print(accuracy)

example_measures = np.array([[4, 2, 1, 1, 1, 2, 3, 2, 1], [4, 2, 1, 1, 1, 2, 3, 2, 1]])
example_measures = example_measures.reshape(len(example_measures), -1)
prediction = clf.predict(example_measures)
print(prediction)",svm/svm-eg.py,mtdx/ml-algorithms,1
"plt.xscale('log')
# plt.savefig('./figures/regression_path.png', dpi=300)
plt.show()

#############################################################################
print(50 * '=')
print('Section: Dealing with the nonlinearly'
      'separable case using slack variables')
print(50 * '-')

svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(105, 150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
# plt.tight_layout()
# plt.savefig('./figures/support_vector_machine_linear.png', dpi=300)",code/optional-py-scripts/ch03.py,wei-Z/Python-Machine-Learning,1
"class _NotFittedError(ValueError, AttributeError):
  """"""Exception class to raise if estimator is used before fitting.

  This class inherits from both ValueError and AttributeError to help with
  exception handling and backward compatibility.

  Examples:
  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.exceptions import NotFittedError
  >>> try:
  ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
  ... except NotFittedError as e:
  ...     print(repr(e))
  ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
  NotFittedError('This LinearSVC instance is not fitted yet',)

  Copied from
  https://github.com/scikit-learn/scikit-learn/master/sklearn/exceptions.py
  """"""
",tensorflow/contrib/learn/python/learn/estimators/_sklearn.py,petewarden/tensorflow_makefile,1
"
# Perform Tf-Idf vectorization
nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)
idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')

# Scaling the words
stdSlr = StandardScaler().fit(im_features)
im_features = stdSlr.transform(im_features)

# Train the Linear SVM
clf = LinearSVC()
clf.fit(im_features, np.array(image_classes))

# Save the SVM",bow/findFeatures.py,wy4515/gesture-crawl,1
"        C = float(self.complexity.get())
        gamma = float(self.gamma.get())
        coef0 = float(self.coef0.get())
        degree = int(self.degree.get())
        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}
        if len(np.unique(y)) == 1:
            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
                                  gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X)
        else:
            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
                          gamma=gamma, coef0=coef0, degree=degree)
            clf.fit(X, y)
        if hasattr(clf, 'score'):
            print(""Accuracy:"", clf.score(X, y) * 100)
        X1, X2, Z = self.decision_surface(clf)
        self.model.clf = clf
        self.model.set_surface((X1, X2, Z))
        self.model.surface_type = self.surface_type.get()
        self.fitted = True",examples/applications/svm_gui.py,B3AU/waveTree,1
"    training_data = np.array(rubik_generated_data_list + office_data_list)
    training_labels = np.array([1] * len(rubik_generated_data_list) + [0] * len(office_data_list))
    
    testing_data = np.array(rubik_real_data_list)
    testing_labels = np.ones(len(testing_data))

    print(""%i training samples, %i testing samples"" % 
          (len(training_labels), len(testing_labels)))
    print(""="" * 50)
    
    my_svm = SVC(kernel=intersection_kernel, C=1e0)
    my_svm.fit(training_data, training_labels)

    train_predictions = my_svm.predict(training_data)
    test_predictions = my_svm.predict(testing_data)

    print(""%.2f accuracy during training (%i predictions)"" % 
          (np.mean(train_predictions == training_labels), len(training_labels)))
    print(""%.2f accuracy during testing (%i predictions)"" % 
          (np.mean(test_predictions == testing_labels), len(testing_labels)))",src/py/deprecated/2/svm_classifier.py,leonardbj/rubik,1
"from preprocess import *
from visual import *
from sklearn import cross_validation, svm

X, y = load_preprocessed_dataset('data/rescaled28x28.dat')
X_test, _ = preprocess('data/test1.dat')

clf = svm.SVC(
    C=2., kernel='poly', degree=9, gamma=1./512., coef0=1).fit(X, y)

yhat = clf.predict(X_test)

# Output the predicted classes, one per line
for i in yhat:
  print i",svm_test.py,ielashi/chinese-zodiac-classifier,1
"# y is filled with integers coding for the class to predict
# We must have X.shape[0] equal to y.shape[0]
X = [e.get_data()[:, data_picks, :] for e in epochs_list]
y = [k * np.ones(len(this_X)) for k, this_X in enumerate(X)]
X = np.concatenate(X)
y = np.concatenate(y)

from sklearn.svm import SVC  # noqa
from sklearn.cross_validation import cross_val_score, ShuffleSplit  # noqa

clf = SVC(C=1, kernel='linear')
# Define a monte-carlo cross-validation generator (reduce variance):
cv = ShuffleSplit(len(X), 10, test_size=0.2)

scores = np.empty(n_times)
std_scores = np.empty(n_times)

for t in range(n_times):
    Xt = X[:, :, t]
    # Standardize features",examples/decoding/plot_decoding_sensors.py,Odingod/mne-python,1
"from sklearn.svm import LinearSVC
from splearn.svm import SparkLinearSVC
from splearn.utils.testing import SplearnTestCase, assert_array_almost_equal


class TestLinearSVC(SplearnTestCase):

    def test_same_coefs(self):
        X, y, Z = self.make_classification(2, 100000)

        local = LinearSVC()
        dist = SparkLinearSVC()

        local.fit(X, y)
        dist.fit(Z, classes=np.unique(y))

        assert_array_almost_equal(local.coef_, dist.coef_, decimal=3)",splearn/svm/tests/test_classes.py,drlinux/sparkit-learn,1
"norm_tst_data = normdata.loc[sel['tst'], sel['feats']]
tst_data = rawdata.loc[sel['tst'], sel['feats']]

t1 = time()
#################### CLASSIFICATION ################
########################################
########################################
########################################
sklda = LDA()
skknn = KNN(3, warn_on_equidistant=False)
sksvm = SVC()
sklda.fit(norm_trn_data, sel['trnl'])
skknn.fit(norm_trn_data, sel['trnl'])
sksvm.fit(norm_trn_data, sel['trnl'])
errors['lda'] = (1-sklda.score(norm_tst_data, sel['tstl']))
errors['knn'] = (1-skknn.score(norm_tst_data, sel['tstl']))
errors['svm'] = (1-sksvm.score(norm_tst_data, sel['tstl']))
print(""skLDA error: %f"" % errors['lda'])
print(""skKNN error: %f"" % errors['knn'])
print(""skSVM error: %f"" % errors['svm'])",exps/mpm_play.py,binarybana/samcnet,1
"    self.X_train = self.vectorizer.fit_transform(self.X)

  def transform_Y(self):
    self.le = preprocessing.LabelEncoder()
    self.sorted_y = sorted(set(self.y))
    self.le.fit(self.sorted_y)
    self.y_train = self.le.transform(self.y)

  def create_svc_model_framework(self):
    self.pipeline = Pipeline([
      ('clf', SVC(kernel='rbf', gamma=0.01, C=100, probability=True))
    ])

    self.parameters = {
      'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
      'clf__C': (0.1, 0.3, 1, 2, 10, 30)
    }
    self.grid_search = GridSearchCV(self.pipeline, self.parameters, n_jobs=2, verbose=1,
                              scoring='accuracy')
",create_subtopic_mapper.py,rupendrab/py_unstr_parse,1
"    
    
    
    def run(self):
        
        
        digits = datasets.load_digits()
        trainCount = 1500
        predictCount = len(digits.data) - trainCount
    
        clf = svm.SVC(gamma = 0.001, C= 100.)
        logistc = linear_model.LogisticRegression()
        neighbor = neighbors.KNeighborsClassifier()
    
        
    
        trainX = digits.data[:trainCount]
        trainY = digits.target[:trainCount]
        
        import time",Scikit/Scikit/exercise_2_1.py,TechnicHail/COMP188,1
"
    `Linear SVM Classifier <https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM>`_

    This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.

    >>> from pyspark.sql import Row
    >>> from pyspark.ml.linalg import Vectors
    >>> df = sc.parallelize([
    ...     Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),
    ...     Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()
    >>> svm = LinearSVC(maxIter=5, regParam=0.01)
    >>> model = svm.fit(df)
    >>> model.coefficients
    DenseVector([0.0, -0.2792, -0.1833])
    >>> model.intercept
    1.0206118982229047
    >>> model.numClasses
    2
    >>> model.numFeatures
    3",python/pyspark/ml/classification.py,patrick-nicholson/spark,1
"    for file in glob.glob(sys.argv[1]+'.mat'):
        data = scipy.io.loadmat(file)

        #print(""\nTreinando SVM multi classe..."")
        #print(""\n""+file.split(""gram_"")[1].split(""_"")[0])
        ytrain = data['Ytrain'].T.reshape(data['Ytrain'].shape[1])

        x_train, x_val, y_train, y_val = cross_validation.train_test_split(data['Xtrain'], ytrain, test_size=0.2, random_state=0)
        tuned_parameters = []

	cl = fusion.FusionClassifier([sklearn.linear_model.LogisticRegression(), sklearn.svm.SVC(C=10, gamma=1e-3, probability=True), sklearn.naive_bayes.MultinomialNB()])

        #print "" -- TRAINNING: grid search with 5 fold cross-validation""

	#cl.fit(x_train, y_train)

	#print cl.predict(x_val)	

	scores = cross_val_score(cl, x_train, y_train, cv=5)
",classifier_fusion.py,alan-mnix/MLFinalProject,1
"
from preprocessing import normalize, micro_tokenize
import conf

def evaluate(cat, fold, txt_train, txt_test, y_train, y_test):
    fe = CountVectorizer(
        preprocessor=normalize,
        tokenizer=micro_tokenize,
        binary=True,
    )
    predictor = SVC(
        kernel=conf.SVM_KERNEL,
        class_weight=conf.SVM_CLWEIGHT,
        C=conf.SVM_C,
        random_state=conf.SEED,
    )
    fe.fit(txt_train)
    X = fe.transform(txt_train)
    predictor.fit(X, y_train)
    X_test = fe.transform(txt_test)",experiments/src/evaluate_bow.py,OFAI/million-post-corpus,1
"    sklearn_cv = SVCCVSkGridLinear(\
        C_range = [2 ** i for i in range(-3, 14, 2)],
        cv_method = KFold(20, 5))

    meta_model = DSESSVCLinearMetaModel(\
        window_size = 10,
        scaling = ScalingStandardscore(),
        crossvalidation = sklearn_cv,
        repair_mode = 'none')

    method = ORIDSESSVC(\
        mu = 15,
        lambd = 100,
        theta = 0.3,
        pi = 15,
        initial_sigma = matrix([[5.0, 5.0]]),
        delta = 5.0,
        tau0 = 0.5, 
        tau1 = 0.6,
        initial_pos = matrix([[10.0, 10.0]]),",evopy/examples/experiments/fitness_dses_dsessvc/setup.py,jpzk/evopy,1
"    X = X.astype(float)
    y = mat['Y']    # label
    y = y[:, 0]
    n_samples, n_features = X.shape    # number of samples and number of features

    # split data into 10 folds
    ss = cross_validation.KFold(n_samples, n_folds=10, shuffle=True)

    # perform evaluation on classification task
    num_fea = 10    # number of selected features
    clf = svm.LinearSVC()    # linear SVM

    correct = 0
    for train, test in ss:
        # obtain the index of each feature on the training set
        idx = CMIM.cmim(X[train], y[train], n_selected_features=num_fea)

        # obtain the dataset on the selected features
        features = X[:, idx[0:num_fea]]
",skfeast/example/test_CMIM.py,jundongl/scikit-feast,1
"Face-1: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp
Face-2: iVBORw0KGgoAAAANSUhEUgAAADAAAAAwBAMAAAClLOS0AAAAGFBMVEUAAAAkHiJeRUIcGBi9
 locQDQ4zJykFBAXJfWDjAAACYUlEQVR4nF2TQY/jIAyFc6lydlG5x8Nyp1Y69wj1PN2I5gzp

"""""")

    def test_another_long_multiline_header(self):
        eq = self.ndiffAssertEqual
        m = '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with Microsoft SMTPSVC(5.0.2195.4905);
 Wed, 16 Oct 2002 07:41:11 -0700'''
        msg = email.message_from_string(m)
        eq(msg.as_string(), '''\
Received: from siimage.com ([172.25.1.3]) by zima.siliconimage.com with
 Microsoft SMTPSVC(5.0.2195.4905); Wed, 16 Oct 2002 07:41:11 -0700

''')

    def test_long_lines_with_different_header(self):",bin/Lib/email/test/test_email_renamed.py,beiko-lab/gengis,1
"	features = vstack(features)
	dump_svmlight_file(features, np.ones((features.shape[0],)), './1_feature/test.tfidf.xgboost.txt')


def test(X, y):
	from sklearn.linear_model import LogisticRegression
	model = LogisticRegression(penalty='l2', dual=True, tol=0.0005,
                           C=10.0, fit_intercept=True, intercept_scaling=2.0,
                           class_weight='auto', random_state=1981)
	# from sklearn.svm import SVC
	# model = SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False,
	# 	tol=0.001, cache_size=200, class_weight=None, verbose=True, max_iter=-1, random_state=None)
	from sklearn import cross_validation
	print ""5 Fold CV Test start""
	score = np.mean(cross_validation.cross_val_score(model, X, y, cv=5, verbose=1, scoring='mean_squared_error'))
	print ""5 Fold CV Score: "", score



",feature/feature_tfidf.py,jasonwbw/JustAPlaceholder,1
"# 
# where $d$ is specified by the keyword `degree`, and $r$ by `coef0`.

# ### 2.1.1 One vs Rest Classification
# In the following subroutine, the data is split into ""one-vs-rest"", where $y=1$ corresponds to a match to the digit, and $y=0$ corresponds to all the other digits.  The training step is implemented in the call to `clf.fit()`.

# In[4]:

def get_misclassification_ovr(X_train,y_train,X_test,y_test,digit,
                              Q=2,r=1.0,C=0.01,kernel='poly',verbose=False):
    clf = SVC(C=C, kernel=kernel, degree=Q, coef0 = r, gamma = 1.0,
          decision_function_shape='ovr', verbose=False)
    y_in  = (y_train==digit).astype(int)
    y_out = (y_test==digit).astype(int)
    model = clf.fit(X_train,y_in)  # print(model)
    E_in  = np.mean(y_in  != clf.predict(X_train))
    E_out = np.mean(y_out != clf.predict(X_test))
    n_support_vectors = len(clf.support_vectors_)
    if verbose is True:
        print()",perceptron/digit-recognition.py,nathanielng/machine-learning,1
"from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE
from sklearn.svm import SVC

# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

# Create the RFE object and rank each pixel
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit(X, y)
ranking = rfe.ranking_.reshape(digits.images[0].shape)

# Plot pixel ranking
plt.matshow(ranking, cmap=plt.cm.Blues)
plt.colorbar()
plt.title(""Ranking of pixels with RFE"")
plt.show()",projects/scikit-learn-master/examples/feature_selection/plot_rfe_digits.py,DailyActie/Surrogate-Model,1
