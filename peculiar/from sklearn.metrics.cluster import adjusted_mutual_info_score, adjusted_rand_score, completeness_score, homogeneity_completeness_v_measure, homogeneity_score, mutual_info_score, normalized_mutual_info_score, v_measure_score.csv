id,size,content,binary,copies,sample_repo_name,sample_path
f8cc092efb46e4c48f51c52034db886981fe1df8,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_gbm_only.py
549ad782f4d46eeb2493b1d14060b3b6e6a51f8c,7288,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/meta_only.py
8765960cd6adc0ed9c655599128f5745fff88ec1,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_dt_only.py
c6580be9bfbc9e7f03a1549e1542d6c8739c9d4c,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_ridge_only.py
734a2af75852d8a932e134d3eb7418aa0ed96d18,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_nb_only.py
099473f3c96afcb7ee66d9fafe8047b6448ee19a,7287,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/default_no_meta.py
750c79f626802c38fe982833b83bd5c8fd77228b,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_rf_only.py
a10bf87fa1655c80a17393e44583a40d646f17ce,7294,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/max_aggregate_only.py
328115a7bdc1dd64fc23cee0ddf167ce0de0d76d,7294,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/median_aggregate_only.py
a4e42ddeec63b392c2d38c797010363a43e25b4f,7291,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    # lambda x, y: y,
    # lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/a_to_b_only.py
266248b54d79d4107d19f6c98a29de709fc31eae,7287,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/default_numerical_only.py
b22c2cf3b88ec4ca2f2b90d63fe5cdcf9c43b29b,7426,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_none.py
7f8a0b1b33d3feb0a7863163cf53cf401c01247c,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_none.py
b3b48bcc6df385ad549558969b3f872c381f4bb4,7441,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    # lambda x, y: x,
    # lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/trial1.py
fd1a97033d1cb69dfc2079639119ad04605ef6ae,7407,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/trial3.py
357a2f768fb727378c2fa1975ab7f30ea529e87e,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_lr_only.py
a6aca42700f1ebcb4c533d43cbca35d003a5795a,7291,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    # lambda x, y: x,
    lambda x, y: y,
    # lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/b_to_a_only.py
5687bc761d7d5f7cbf8cf1faeb69f04188b64660,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_rf_only.py
6e436c3bca1ee33837373183de4309ab5a25105f,7446,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False

UNARY_NUMERICAL_FEATURES = []
UNARY_CATEGORICAL_FEATURES = []
BINARY_NN_FEATURES = []
BINARY_NC_FEATURES = []
BINARY_CN_FEATURES = []
BINARY_CC_FEATURES = []
",false,1,diogo149/CauseEffectPairsPaper,configs/only_fit.py
c6d2ac740aa14bf92dd6165dd36c45e5b8870db4,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_dt_only.py
f4bd67580a1300f578f66bda709368375b5c4b5a,7296,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/no_aggregate.py
ad0b545016a5c6a85e94e66814ce5bedc655c3d3,7294,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/min_aggregate_only.py
a2a275c58308489bf8c76e7cdeb573ae1248173f,7287,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/default_categorical_only.py
0c2b1930531b1ce3480237825e0f34ab7c55e1d4,7411,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans3""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans3.py
c26a9584f40e9341023a4a759eb332c8c926e672,7294,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    to_aggregator(""mode""),
    # to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/mode_aggregate_only.py
897420735cca5dd8c2b5469b7599517ae688152e,7294,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/mean_aggregate_only.py
9427fd2c1e4fd7f15f4bbfcc2bf645e4183078d5,7600,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    # lambda x, y: x,
    # lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False

UNARY_NUMERICAL_FEATURES = []
UNARY_CATEGORICAL_FEATURES = []
BINARY_NN_FEATURES = []
BINARY_NC_FEATURES = []
BINARY_CN_FEATURES = []
BINARY_CC_FEATURES = []
",false,1,diogo149/CauseEffectPairsPaper,configs/trial2.py
7037999ee17b6fa8b52193211eb718537dcbd5a8,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_gbm_only.py
93e6b24f534a16d8a645015c416d1483b9eb9968,7346,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False

REGRESSION_ESTIMATORS = []
CLASSIFICATION_ESTIMATORS = []
",false,1,diogo149/CauseEffectPairsPaper,configs/no_fit.py
51c1020fdcc2a1958fe8d36f7dd1ac0847e81fe5,7414,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""noop""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_noop.py
6a1cd90e2d4b67cb1e72b20947b828b91232cf6f,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_knn_only.py
c92ffbbb1d9bf6e8367d6174ed600fe50048ef59,5361,"""""""
warnings: some of these functions only take in floats, convert first with x.astype(np.float)
""""""

import numpy as np

from scipy.stats import pearsonr, chisquare, f_oneway, kruskal, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, dice, euclidean, hamming, jaccard, kulsinski, matching, rogerstanimoto, russellrao, sokalmichener, sokalsneath, sqeuclidean, yule
from sklearn.decomposition import FastICA
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score
from collections import defaultdict

from classification_metrics import categorical_gini_coefficient
from regression_metrics import gini_coefficient


def correlation_magnitude(x, y):
    return abs(pearsonr(x, y)[0])


def chi_square(x, y):
    return chisquare(x - min(x) + 1, y - min(y) + 1)


def categorical_categorical_homogeneity(x, y):
    grouped = defaultdict(list)
    [grouped[x_val].append(y_val) for x_val, y_val in zip(x, y)]
    homogeneity = [categorical_gini_coefficient(val) for val in grouped.values()]
    return (max(homogeneity), np.mean(homogeneity), min(homogeneity), np.std(homogeneity))


def categorical_numerical_homogeneity(x, y):
    grouped = defaultdict(list)
    [grouped[x_val].append(y_val) for x_val, y_val in zip(x, y)]
    homogeneity = [gini_coefficient(val) for val in grouped.values()]
    return (max(homogeneity), np.mean(homogeneity), min(homogeneity), np.std(homogeneity))


def anova(x, y):
    grouped = defaultdict(list)
    [grouped[x_val].append(y_val) for x_val, y_val in zip(x, y)]
    grouped_values = grouped.values()
    if len(grouped_values) < 2:
        return (0, 0, 0, 0)
    f_oneway_res = list(f_oneway(*grouped_values))
    try:
        kruskal_res = list(kruskal(*grouped_values))
    except ValueError:  # when all numbers are identical
        kruskal_res = [0, 0]
    return f_oneway_res + kruskal_res


def bucket_variance(x, y):
    grouped = defaultdict(list)
    [grouped[x_val].append(y_val) for x_val, y_val in zip(x, y)]
    grouped_values = grouped.values()
    weighted_avg_var = 0.0
    max_var = 0.0
    for bucket in grouped_values:
        var = np.std(bucket) ** 2
        max_var = max(var, max_var)
        weighted_avg_var += len(bucket) * var
    weighted_avg_var /= len(x)
    return (max_var, weighted_avg_var)


def independent_component(x, y):
    clf = FastICA(random_state=1)
    clf.fit(x.reshape(-1, 1), y)
    comp = clf.components_[0][0]
    mm = clf.get_mixing_matrix()[0][0]
    sources = clf.sources_.flatten()
    src_max = max(sources)
    src_min = min(sources)
    return [comp, mm, src_max, src_min]


def dice_(x, y):
    try:
        return dice(x, y)
    except (ZeroDivisionError, TypeError):
        return 0


def sokalsneath_(x, y):
    try:
        return sokalsneath(x, y)
    except ValueError:
        return 0


def yule_(x, y):
    try:
        return yule(x, y)
    except ZeroDivisionError:
        return 0


ALL_BINARY_FEATURES = (
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
)

NN_BINARY_FEATURES = (
    independent_component,
)

CN_BINARY_FEATURES = (
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
)

CC_BINARY_FEATURES = (
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto,
    russellrao,
    sokalmichener,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
)

NC_BINARY_FEATURES = (
)


def binary_feature_wrapper(f):
    def inner_func(x, y):
        result = f(x, y)
        try:
            list_result = list(result)
        except TypeError:
            list_result = [result]
        feature_names = [""{}_{}"".format(f.func_name, i) for i in range(len(list_result))]
        features = zip(feature_names, list_result)
        return features
    return inner_func

BINARY_FEATURES = []

for f in ALL_BINARY_FEATURES:
    for desired_type in [""NN"", ""NC"", ""CN"", ""CC""]:
        name = ""{}_{}"".format(desired_type, f.func_name)
        BINARY_FEATURES.append((name, binary_feature_wrapper, f))

for f in NN_BINARY_FEATURES:
    desired_type = ""NN""
    name = ""{}_{}"".format(desired_type, f.func_name)
    BINARY_FEATURES.append((name, binary_feature_wrapper, f))

for f in CN_BINARY_FEATURES:
    desired_type = ""CN""
    name = ""{}_{}"".format(desired_type, f.func_name)
    BINARY_FEATURES.append((name, binary_feature_wrapper, f))

for f in CC_BINARY_FEATURES:
    desired_type = ""CC""
    name = ""{}_{}"".format(desired_type, f.func_name)
    BINARY_FEATURES.append((name, binary_feature_wrapper, f))

for f in NC_BINARY_FEATURES:
    desired_type = ""NC""
    name = ""{}_{}"".format(desired_type, f.func_name)
    BINARY_FEATURES.append((name, binary_feature_wrapper, f))
",false,2,diogo149/CauseEffectPairsChallenge,code/binary_features.py
478651b62c53046ee4b2acb2649417939a01ff0d,7295,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS  = [
    # to_aggregator(""max""),
    # to_aggregator(""min""),
    # to_aggregator(""median""),
    # to_aggregator(""mode""),
    # to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/sum_aggregate_only.py
f6250758c0154118520109f2ab1c034921d46d6c,7291,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    # lambda x, y: x,
    # lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = False

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/relative_only.py
33f9573b4953ab0576f82810c38a12a002b7405e,7424,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1_knn_only.py
a0cdba6639c12320825f57e1f2466ca0846a6ad4,7284,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/autocause_default_with_sum_aggregation.py
dba4fb50e295cd5c02543498806e7ed0c55b2210,7412,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10.py
1ffe78686e2e20ac3fcd9b96027f6b44bca572b2,7286,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=lambda x, *args: x,  # identity function
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: LabelBinarizer().fit_transform(x),
)
CATEGORICAL_CONVERTERS = dict(
    N=lambda x, *args: Discretizer().fit_transform(x).flatten(),
    B=lambda x, *args: x,  # identity function
    C=lambda x, *args: x,  # identity function
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/autocause_default.py
177751f026dc2453d3a91d5a61e97faf65122e01,7414,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans_gap""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans_gap.py
3debab7ac20c0e60de2075a6717f066b3f95081d,7427,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""mean_ordinal_pred""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_mean_ordinal.py
d35511c22b9d70fcc9d64fe54fb2accf5e50847e,7422,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = False
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""kmeans10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/categorical_kmeans10_lr_only.py
09c7f7b56e172a7b462986b01e292823715fa100,7414,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    # to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = False

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""pca1""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = False
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/CauseEffectPairsPaper,configs/numerical_pca1.py
864f453ef9950d7b17edf80bffb29563b16047aa,7414,"import numpy as np
from scipy.stats import skew, kurtosis, shapiro, pearsonr, ansari, mood, levene, fligner, bartlett, mannwhitneyu
from scipy.spatial.distance import braycurtis, canberra, chebyshev, cityblock, correlation, cosine, euclidean, hamming, jaccard, kulsinski, matching, russellrao, sqeuclidean
from sklearn.preprocessing import LabelBinarizer
from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, roc_auc_score, average_precision_score, f1_score, hinge_loss, matthews_corrcoef, precision_score, recall_score, zero_one_loss
from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, completeness_score, homogeneity_completeness_v_measure, homogeneity_score, mutual_info_score, normalized_mutual_info_score, v_measure_score

from boomlet.utils.aggregators import to_aggregator
from boomlet.metrics import max_error, error_variance, relative_error_variance, gini_loss, categorical_gini_loss
from boomlet.transform.type_conversion import Discretizer

from autocause.feature_functions import *
from autocause.converters import NUMERICAL_TO_NUMERICAL, NUMERICAL_TO_CATEGORICAL, BINARY_TO_NUMERICAL, BINARY_TO_CATEGORICAL, CATEGORICAL_TO_NUMERICAL, CATEGORICAL_TO_CATEGORICAL


""""""
Functions used to combine a list of features into one coherent one.

Sample use:
1. to convert categorical to numerical, we perform a one hot encoding
2. treat each binary column as a separate numerical feature
3. compute numerical features as usual
4. use each of the following functions to create a new feature
   (with the input as the nth feature for each of the columns)

WARNING: these will be used in various locations throughout the code base
and will result in feature size growing at faster than a linear rate
""""""
AGGREGATORS = [
    to_aggregator(""max""),
    to_aggregator(""min""),
    to_aggregator(""median""),
    to_aggregator(""mode""),
    to_aggregator(""mean""),
    to_aggregator(""sum""),
]

""""""
Boolean flags specifying whether or not to perform conversions
""""""
CONVERT_TO_NUMERICAL = True
CONVERT_TO_CATEGORICAL = True

""""""
Functions that compute a metric on a single 1-D array
""""""
UNARY_NUMERICAL_FEATURES = [
    normalized_entropy,
    skew,
    kurtosis,
    np.std,
    shapiro,
]
UNARY_CATEGORICAL_FEATURES = [
    lambda x: len(set(x)),  # number of unique
]

""""""
Functions that compute a metric on two 1-D arrays
""""""
BINARY_NN_FEATURES = [
    independent_component,
    chi_square,
    pearsonr,
    correlation_magnitude,
    braycurtis,
    canberra,
    chebyshev,
    cityblock,
    correlation,
    cosine,
    euclidean,
    hamming,
    sqeuclidean,
    ansari,
    mood,
    levene,
    fligner,
    bartlett,
    mannwhitneyu,
]
BINARY_NC_FEATURES = [
]
BINARY_CN_FEATURES = [
    categorical_numerical_homogeneity,
    bucket_variance,
    anova,
]
BINARY_CC_FEATURES = [
    categorical_categorical_homogeneity,
    anova,
    dice_,
    jaccard,
    kulsinski,
    matching,
    rogerstanimoto_,
    russellrao,
    sokalmichener_,
    sokalsneath_,
    yule_,
    adjusted_mutual_info_score,
    adjusted_rand_score,
    completeness_score,
    homogeneity_completeness_v_measure,
    homogeneity_score,
    mutual_info_score,
    normalized_mutual_info_score,
    v_measure_score,
]

""""""
Dictionaries of input type (e.g. B corresponds to pairs where binary
data is the input) to pairs of converter functions and a boolean flag
of whether or not to aggregate over the output of the converter function

converter functions should have the type signature:
    converter(X_raw, X_current_type, Y_raw, Y_type)
where X_raw is the data to convert
""""""
NUMERICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_NUMERICAL[""identity""],
    B=BINARY_TO_NUMERICAL[""identity""],
    C=CATEGORICAL_TO_NUMERICAL[""binarize""],
)
CATEGORICAL_CONVERTERS = dict(
    N=NUMERICAL_TO_CATEGORICAL[""discretizer10""],
    B=BINARY_TO_CATEGORICAL[""identity""],
    C=CATEGORICAL_TO_CATEGORICAL[""identity""],
)

""""""
Whether or not the converters can result in a 2D output. This must be set to True
if any of the respective converts can return a 2D output.
""""""
NUMERICAL_CAN_BE_2D = True
CATEGORICAL_CAN_BE_2D = False

""""""
Estimators used to provide a fit for a variable
""""""
REGRESSION_ESTIMATORS = [
    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [
    explained_variance_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    max_error,
    error_variance,
    relative_error_variance,
    gini_loss,
] + BINARY_NN_FEATURES
REGRESSION_RESIDUAL_METRICS = [
] + UNARY_NUMERICAL_FEATURES
BINARY_PROBABILITY_CLASSIFICATION_METRICS = [
    roc_auc_score,
    hinge_loss,
] + REGRESSION_METRICS
RESIDUAL_PROBABILITY_CLASSIFICATION_METRICS = [
] + REGRESSION_RESIDUAL_METRICS
BINARY_CLASSIFICATION_METRICS = [
    accuracy_score,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
    zero_one_loss,
    categorical_gini_loss,
]
ND_CLASSIFICATION_METRICS = [ # metrics for N-dimensional classification
] + BINARY_CC_FEATURES

""""""
Functions to assess the model (e.g. complexity) of the fit on a numerical variable

of type signature:
    metric(clf, X, y)
""""""
REGRESSION_MODEL_METRICS = [
    # TODO model complexity metrics
]
CLASSIFICATION_MODEL_METRICS = [
    # TODO use regression model metrics on predict_proba
]

""""""
The operations to perform on the A->B features and B->A features.
""""""
RELATIVE_FEATURES = [
    # Identity functions, comment out the next 2 lines for only relative features
    lambda x, y: x,
    lambda x, y: y,
    lambda x, y: x - y,
]

""""""
Whether or not to treat each observation (A,B) as two observations: (A,B) and (B,A)

If this is done and training labels are given, those labels will have to be
reflected as well. The reflection is performed through appending at the end.
(e.g. if we have N training examples, observation N+1 in the output will be
the first example reflected)
""""""
REFLECT_DATA = False

""""""
Whether or not metafeatures based on the types of A and B are generated.

e.g. 1/0 feature on whether or not A is Numerical, etc.
""""""
ADD_METAFEATURES = True

""""""
Whether or not to generate combination features between the computed
features and metafeatures.

e.g. for each feature and metafeature, generate a new feature which is the
product of the two

WARNING: will generate a LOT of features (approximately 21 times as many)
""""""
COMPUTE_METAFEATURE_COMBINATIONS = False
",false,1,diogo149/autocause,autocause/autocause_settings.py
